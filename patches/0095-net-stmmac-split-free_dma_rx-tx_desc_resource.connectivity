From 6e029bb74f2222d2b077fae001f3b93ae47a582a Mon Sep 17 00:00:00 2001
From: "Wong, Vincent Por Yin" <vincent.por.yin.wong@intel.com>
Date: Thu, 19 Sep 2019 22:48:45 +0800
Subject: [PATCH 095/108] net: stmmac: split free_dma_rx|tx_desc_resources to
 ring specific function

This is in preparation for XDP setup phase in future so that we can
perform queue-specific setting.

Signed-off-by: Wong, Vincent Por Yin <vincent.por.yin.wong@intel.com>
Signed-off-by: Ong Boon Leong <boon.leong.ong@intel.com>
Signed-off-by: Voon Weifeng <weifeng.voon@intel.com>
---
 .../net/ethernet/stmicro/stmmac/stmmac_main.c | 94 ++++++++++---------
 1 file changed, 51 insertions(+), 43 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 5c79e2deca07..f68179ea840d 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -1531,6 +1531,30 @@ static void dma_free_tx_skbufs(struct stmmac_priv *priv, u32 queue)
 		stmmac_free_tx_buffer(priv, queue, i);
 }
 
+static void free_dma_rx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+
+	/* Release the DMA RX socket buffers */
+	dma_free_rx_skbufs(priv, queue);
+
+	/* Free DMA regions of consistent memory previously allocated */
+	if (!priv->extend_desc)
+		dma_free_coherent(priv->device, priv->dma_rx_size *
+					sizeof(struct dma_desc),
+					rx_q->dma_rx, rx_q->dma_rx_phy);
+	else
+		dma_free_coherent(priv->device, priv->dma_rx_size *
+					sizeof(struct dma_extended_desc),
+					rx_q->dma_erx, rx_q->dma_rx_phy);
+
+	kfree(rx_q->buf_pool);
+	if (rx_q->page_pool) {
+		page_pool_request_shutdown(rx_q->page_pool);
+		page_pool_destroy(rx_q->page_pool);
+	}
+}
+
 /**
  * free_dma_rx_desc_resources - free RX dma desc resources
  * @priv: private structure
@@ -1541,28 +1565,33 @@ static void free_dma_rx_desc_resources(struct stmmac_priv *priv)
 	u32 queue;
 
 	/* Free RX queue resources */
-	for (queue = 0; queue < rx_count; queue++) {
-		struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	for (queue = 0; queue < rx_count; queue++)
+		free_dma_rx_desc_resources_q(priv, queue);
+}
 
-		/* Release the DMA RX socket buffers */
-		dma_free_rx_skbufs(priv, queue);
+static void free_dma_tx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
+{
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
 
-		/* Free DMA regions of consistent memory previously allocated */
-		if (!priv->extend_desc)
-			dma_free_coherent(priv->device, priv->dma_rx_size *
-					  sizeof(struct dma_desc),
-					  rx_q->dma_rx, rx_q->dma_rx_phy);
-		else
-			dma_free_coherent(priv->device, priv->dma_rx_size *
-					  sizeof(struct dma_extended_desc),
-					  rx_q->dma_erx, rx_q->dma_rx_phy);
-
-		kfree(rx_q->buf_pool);
-		if (rx_q->page_pool) {
-			page_pool_request_shutdown(rx_q->page_pool);
-			page_pool_destroy(rx_q->page_pool);
-		}
-	}
+	/* Release the DMA TX socket buffers */
+	dma_free_tx_skbufs(priv, queue);
+
+	/* Free DMA regions of consistent memory previously allocated */
+	if (priv->extend_desc)
+		dma_free_coherent(priv->device, priv->dma_tx_size *
+					sizeof(struct dma_extended_desc),
+					tx_q->dma_etx, tx_q->dma_tx_phy);
+	else if (priv->enhanced_tx_desc)
+		dma_free_coherent(priv->device, priv->dma_tx_size *
+					sizeof(struct dma_enhanced_tx_desc),
+					tx_q->dma_enhtx, tx_q->dma_tx_phy);
+	else
+		dma_free_coherent(priv->device, priv->dma_tx_size *
+					sizeof(struct dma_desc),
+					tx_q->dma_tx, tx_q->dma_tx_phy);
+
+	kfree(tx_q->tx_skbuff_dma);
+	kfree(tx_q->tx_skbuff);
 }
 
 /**
@@ -1575,29 +1604,8 @@ static void free_dma_tx_desc_resources(struct stmmac_priv *priv)
 	u32 queue;
 
 	/* Free TX queue resources */
-	for (queue = 0; queue < tx_count; queue++) {
-		struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
-
-		/* Release the DMA TX socket buffers */
-		dma_free_tx_skbufs(priv, queue);
-
-		/* Free DMA regions of consistent memory previously allocated */
-		if (priv->extend_desc)
-			dma_free_coherent(priv->device, priv->dma_tx_size *
-					  sizeof(struct dma_extended_desc),
-					  tx_q->dma_etx, tx_q->dma_tx_phy);
-		else if (priv->enhanced_tx_desc)
-			dma_free_coherent(priv->device, priv->dma_tx_size *
-					  sizeof(struct dma_enhanced_tx_desc),
-					  tx_q->dma_enhtx, tx_q->dma_tx_phy);
-		else
-			dma_free_coherent(priv->device, priv->dma_tx_size *
-					  sizeof(struct dma_desc),
-					  tx_q->dma_tx, tx_q->dma_tx_phy);
-
-		kfree(tx_q->tx_skbuff_dma);
-		kfree(tx_q->tx_skbuff);
-	}
+	for (queue = 0; queue < tx_count; queue++)
+		free_dma_tx_desc_resources_q(priv, queue);
 }
 
 /**
-- 
2.17.1

