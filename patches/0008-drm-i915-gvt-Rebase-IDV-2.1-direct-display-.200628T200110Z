From 2521b097a48688b899e3db9cb855aa1ec59b02e8 Mon Sep 17 00:00:00 2001
From: Colin Xu <colin.xu@intel.com>
Date: Tue, 14 Apr 2020 12:10:05 -0600
Subject: [PATCH 08/24] drm/i915/gvt: Rebase IDV 2.1 direct display to 5.4.x.

This patch squash IDV 2.1 direct display from 4.19.x to 5.4.x kernel.
Squashed patches are listed as below:
=====================================

drm/i915/gvt: Fix missing mmio handler for CFL.

Fixes: 43226e6fe798 (drm/i915/gvt: replaced register address with name)

Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Refactored SKL and BXT plane registers

Refactored the SKL and BXT plane registers and implemented their plane
register handlers.

(cherry picked from commit 0ee7dda9f922e639781b35d10c78c162f10a628d)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 0c7aaffd3b170aa75cc744a4901f7ac8d9747bdf)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: local display support

Added local display dispaly support for GVT-g.

(cherry picked from commit efa83a6471c85f8acf70bcce2407f0611c77816e)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: only remove the idr of vgpu during destroy

Fix one issue of windows guest shutdown, the idr of vgpu will be missed
after windows gusest shutdown. It may cause TDR from windows guest.

(cherry picked from commit 0b35a02dbaba9559173f37a5f0cf81d34ce93970)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit d4948891ddb4de3fed9cca29f0bcdadfa885a9dd)

drm/i915/gvt: Setup virtual edid as physical edid

enable on skylake plus platform.

(cherry picked from commit fb7026552d75f950b00304f588236a6463fe2a5f)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit a1564b93182912f55cf912c7ab8f15574d043ec8)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Refine to use port num to correspond with edid

(cherry picked from commit 2797bff225d531b0bca151907623c1302db93581)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 958b7a85a079567ccfc7c4c87fef009fd195352f)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Only emulate one monitor per vGPU

There is no case to use multi-display in guest for IDV project, here only
emulate one monitor per vGPU, and copy the edid of the physical monitor
which is going to assign to vGPU for the virtual monitor.

(cherry picked from commit e924bb61f679e2d4ba1f896c4bb1ad61af645877)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 3b3bf151c40a89bd2577f633ae2c32fd4941e813)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Add pipe assign/retrieve api

vGPU doesn't have the full control of physical pipes, but let it can adjust
the pipe registers which is assigned to it.

(cherry picked from commit 75ab92b14da869982e408afb572fe7a871f68540)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit d3b9258264826e159d6f158a81d0883b8de5397b)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable pipe assignment to vGPU thru mdev

Add disp_pipe_mask to mdev attributes to assign pipe for vGPU.
Each hex digit represents the assigned pipe for vgpu id.
e.g.
0x231. vgpu 1: PIPE A; vgpu 2: PIPE C; vgpu 3: PIPE B

(cherry picked from commit 6ebce2716bf99428f9c91ef9702f7db3e74e0e17)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit c63367f880c55db80b6dc2e12bddbc4a7fe31977)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Add pipe timing and panel scaler reg pass-through

Let vgpu which is pipe owner can adjust pipe timing and panel scaler
registers.

(cherry picked from commit 9ccfbfdcf6db3569d4b7b51e03372d33c3b71689)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 76f7f75016734044678d7a1687782dcc44991a79)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Change direct display assignment from pipe to port

According to real scenario, each physical display is assigned to
a user, so end user is not aware of pipe-port mapping change but
the assigned port should always output his vgpu content.
When host hotplug happens, the pipe-port assignment may also change.

Make below changes:
- Notify gvt from i915 when host port connectivity changes so that
  gvt will update saved pipe-port mapping.
- When create vgpu, check assigned port and find mapped pipe, then
  assign the found pipe to vgpu.
- Move port selection and status from gvt debugfs to i915 sysfs.
  "/sys/class/drm/card0/gvt_disp_ports_mask" and
  "/sys/class/drm/card0/gvt_disp_ports_status".
- Now internally port and pipe matches define PORT_A, PORT_B, ...
  and PIPE_A PIPE_B, .... Externally, user override the port by
  number 1(PORT_A), 2(PORT_B), 3(PORT_C), 4(PORT_D), etc.

(cherry picked from commit e84002a3e573da08933362e0552dc5bc50c7b0c9)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 498ae3c679864d06a3a1563daf38871911c39746)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable direct display switch between host and VGPU

Add gvt_disp_ports_owner to i915 sysfs to switch.
Each hex digit represent if corresponding vgpu owns the display.
  0: display owned by host
  1: display owned by vgpu
  e.g. 0x010
       vgpu 1: doesn't own display, display owned by host
       vgpu 2: currently own the display
       vgpu 3: doesn't own display, display owned by host
To change the ownership on the fly, echo 0 or 1 to:
  "/sys/class/drm/card0/gvt_disp_ports_owner"

All display ports status can be checked by:
  "/sys/class/drm/card0/gvt_disp_ports_status"

(cherry picked from commit 264ed5b983d8adb58e788afb413816f13ded514d)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit b0b4ce0948ac4596a18e359c981aa55a969a51e5)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: enable autoswitch when guest display is ready

(cherry picked from commit 5ef91b1074aeb5ef5c6d7aa9ec0bf7a0a485f18c)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 91f05818fcc59175fa6d4e7a949e65b991accfdf)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: add display auto switch control

Add gvt_disp_auto_switch control in sysfs, 'enabled' is default.

disp_auto_switch: enabled
1. Auto switch to guest display when guest's display is ready.
2. Auto switch to host display when guest trigger device level reset,
and auto switch back to guest again when guest's display is ready later.

disp_auto_switch: disabled
No auto switch, display owner is controlled by gvt_disp_owner.

(cherry picked from commit 69c770494cc390f7792c6c64fbbc71813aec2da9)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit ba1362db1816988e64808842832e4d119b13ef06)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: passthru PIPE_DSL regiser to guest

(cherry picked from commit 22ec16624b4d56555fdbe1ddaf580768a5874f36)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit c2d28743240dd24b054d60765a1282ad37f8fc87)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: setup virtual monitor code refine

(cherry picked from commit 65c0a81de9baa6dd7a81d027b18ed52570dde2bc)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit a8baf41ed5b85bb7b45211081242ea5d51575c53)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: refine vblank event emulation

emulate virtual blank when one VM has no assigned pipe to avoid vblank
time out.

(cherry picked from commit e19f5bcdd2e13264063331465eada844bc63770d)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit c1a52577474d6882afbae2c09d977d8e31b18592)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: translate gma to hma of plane surface address

For supporting Live Migration, the GTT base address in guest view may
not be 0x0, the plane surface address which is wrote by guest need to be
translated to physic GTT address.

(cherry picked from commit fcca4ebf43ec7298ae78572969d034216091f409)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 068e8d5819981977023beb71cbc180eb9a9b4754)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Implement scaler update policy for vgpu direct display

Previous implementation passthrough primary plane settings and scaler
settings, but still keep host crtc timing unchange. In cases that vgpu
enabling scaler on non-preferred crtc timing, host timing, plane/pipe
size, and scaler setting will mismatch, which leads to the corruption.

The solution decode the desired scaling effect from the vgpu crtc timing,
plane size and scaler settings, then re-calculate the appropriate scaler
settings on host side, so that the desired scaling effect can be applied.

(cherry picked from commit 31cfb0ea80dbe44e1fad809c7f167d2a0195e02a)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit fffe842ff9f560633cc673f341ab6a3fd74f961a)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Implement wm update policy for vgpu direct display

Previously we passthrough vgpu display watermark settings to hw. With
the newly implemented scaler update policy, the calculation based on
values some comes from vreg (i.e. plane size) while some reuse host value
(i.e. timing related) and scaler settings are re-calcuated based on them,
thus we can no longer directly passthrough vgpu display wm settings to HW.

The new implementation re-calculate the correct wm per plane based on
host crtc timing and the new scaler config.

(cherry picked from commit 5f072291c0185e2952d673f2e6b2a181afedd2f3)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 7e247ee3a19cab6163977ecd78c114594657484d)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Compansate vsync event to vgpu on HW vsync miss

IDV doesn't support HPD for now. However in some cases HPD still happens
due to reasons like unstable cable connectivity or dongle. From guest OS
perspective the missing vsync interrupt without HPD event will cause
vsync TDR which leads to BSOD.

The solution leverages existing vblank timer to compansate the missing
emulated vsync to vgpu, when gvt detects vsync hasn't been emulated
after certain HW vsync interval.

(cherry picked from commit e351074b6c01e08174c0631c711277fc2cbf368f)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit f33cf0a3884de0546cfba1ffebe325225f19f0e3)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: add handler of the 3rd plane surf

Some guests driver may access the 3rd plane registers of one pipe which is
not tracked, add them to avoid unnecessary debug message.

(cherry picked from commit 67abc4d41830acabc8b69931f8c04c5bffad7a27)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 5600f228ec55f243c7fcb1aed27a63e074d384a5)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Disable scaler during transition state of resolution change.

We expect valid PS_WIN_SZ when PLANE_CTL_ENABLE and PS_CTRL are bot ON
so that GVT can decode VGPU plane & scaler settings and re-compute the
new values. In a transition state that vreg receives PS_WIN_SZ is 0 while
PLANE_CTL_ENABLE and PS_CTRL are both ON, the unexpected PS_WIN_SZ will
lead to new scaler and watermark calculation error.

To fix the issue, disable scaler during the transition so that new scaler
and watermark calculate won't hit divide by zero error. The final state
is expected to be correct so the re-calculation logic can still keep.

(cherry picked from commit 2c2c00f3a5c173db6ac5dc433db86148b3088d46)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 12593734f1908e621df6e0b23d5979fb71183a1e)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix incorrect y_tiled check when compute wm.

Fix below:
- Decode y_tiled from vreg PLANE_CTL so need check PLANE_CTL_TILED_Y
  or PLANE_CTL_TILED_YF instead of using I915_FORMAT_MOD directly.
- fixed16_to_u32_round_up only applies to Y_TILED_CCS and Yf_TILED_CCS.

(cherry picked from commit 6fe824089e58ac90bf96c77699b24775a8df58b2)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit cfb6738650a9e059c52b77316526fea54e9688b7)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix issues after port idv-4.14 to idv-4.17.

(cherry picked from commit 7949815ca4637d19e6d226e8759073ed14d98338)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 6cf4d09901a25812ca2330f0399328c7368b5d1b)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Align vgpu wm calculate logic to 4.17 drm/i915.

Since parameters that wm calucation based on for host display vs. vgpu
direct display are different, we re-use the logic from i915 with changed
parameter source. From 4.14 to 4.17, drm/i915 has update the sequence
and algorithm, so update it for vgpu as well.

(cherry picked from commit 09af15f3aa78d17596ccb5dde3849728e0268d7c)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit c40a2fe5365b1ec01a5e6272da7a72f75390ffe3)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable HW Cursor for direct display

Previous implementation only enabled primary plane in direct display.
The patch enable cursor plane, and re-use exising wm calculation logic
for cursor plane wm update.
Now guest OS can support HW cursor and no longer need fallback to SW.

(cherry picked from commit 696dbed36a3437f5166e5db0e765e5dcd79f72a1)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit aeefe9c909274e40f250661ae178549605448da3)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Reduce unnecessary scaler and wm write to HW.

Besides switching display between vgpu and host display, scaler and wm
will be updated on PLANE_CTL_ENABLE. However the value may not change
since last update. Optimize by skipping these unnecessary HW access.

(cherry picked from commit 06380f1ea998033a2282ca44f53e96475904d198)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit ef4ff6ac720643355daadaf74b4293e9f06edf64)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix unexpected display flashing in IDV

There exist 3 major reasons that cause unexpected flashing or black out:
1. Miss handle some plane mmio like PLANE_CTL when vgpu own the pipe.
2. Miss flush back host cursor plane wm settings.
3. Race-condition that host and vgpu both trying to update plane mmio.

Solution:
1. Always check if vgpu owns the pipe when host i915 check hw state by
reading from mmio, like in assert_plane, to fix call trace like:
"plane 1A assertion failure (expected off, current on)"
2. Flush cursor plane wm settings when pipe owner switch back to host.
3. Protect vgpu flushed mmio with drm_modeset_lock on the target crtc.
The lock dependency is complicated, that drm could update plane mmio in
atomic or non-atomic way. So can't simply protect i915 plane mmio update
with gvt->lock or vgpu->disp_lock. The race-condition may still exist.
Need further investigation on how to fully avoid race-condition.
On GVT side, such updates only happens on switch_display_work and some
specific mmio write trap, which are already protected by gvt->lock and
vgpu->disp_lock.

(cherry picked from commit 2ae68f3bc0a64e4894f932bf78bab0ef9a9ea176)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit ea21f8c76465a9f963d9e711503c7ebb5e383f4f)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable EDID filter in direct display

[1/2]: drm/i915/gvt: Add edid filter function

Add edid filter function based on edid spec.
(cherry picked from commit af7a832bda6371ea3e47df19f9178d56ddef1fc5)

[2/2]: drm/i915/gvt: Add display edid filter control

Add gvt_disp_edid filter control in sysfs.

(cherry picked from commit fb71020ce3ba850473d1b10bec6a6944b438f5ad)

Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix gvt->pipe_info[] and vreg indexing overflow

Unsafe access to pipe_info or vreg existing in previous implementation:
- pipe_own_of_vgpu() is expected to return INVALID_PIPE when vgpu not
  owning the pipe. However pipe_own_of_vgpu() is defined as unsigned int
  which will always return PIPE_A in such case. Any addressing using the
  unexpected return of pipe_own_of_vgpu() goes wrong. Issue happens when
  guest OS access vreg before vgpu owns the physical display, or physical
  dislay is switched back to host.
- define of INTEL_GVT_MAX_PIPE unaligned to I915_MAX_PIPES. Some display
  info array is defined with I915_MAX_PIPES at most but indexing using,
  INTEL_GVT_MAX_PIPE, or vice versa. Array indexing overflow or access
  uninitialized data is quite dangerous.

Change direct display related pipe/plane/port to corresponding enum type,
and indexing within enum range as well.

(cherry picked from commit d007730c776ddae376e83790c62b25503c120e57)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit c4353f4de978b59d1b97bef7173028acead76da7)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Add structure to support async flip for direct display

Add per-plane flipdone_work from HW plane flip_done interrupt. Since
there are several per-plane information, combine them into one new
structure intel_gvt_plane_info.
Only defines change. Function change in separate patch.

(cherry picked from commit fa73f2e232cc4724deb9ecfc3e319d194a3f5297)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 29aae63bd6c59b8ba1371c5e5007a1640943608d)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable asynchronous plane flip for direct display

Current GVT implements synchronous flip only, on handling MI_DISPLAY_FLIP
or mmio update of plane surface address. Without async flip, some 3D
application could not reach better performance and the maximum
performance is no higher than vsync frequency, like 3DMark on Windows.
There also exist some mistakes on emulating PIPE_FRMCNT and PIPE_FLIPCNT:
- PIPE_FRMCNT will only get incremented on every start of vsync, not on
  plane flip.
- PIPE_FLIPCNT indicates the start of flip when the plane surface address
  is updated, not when the flip completes.
- On GVT currently supported platforms including BDW and SKL+,
  PIPE_FLIPCNT only count flips of primary plane.

The patch set enables asynchronous plane flip for both MI_DISPLAY_FLIP
update and plane surface address mmio update. With the patch, the render
performance of some 3D application especially benchmark tools can be
increased significantly.

(cherry picked from commit 407272162fdaecec20fb4bd32efe2fb5f7356d89)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 61da5eaf13590ca470babb9b116240fe847ef43c)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Prevent double vsync inject to vgpu for direct display

Unnecessary vsync injected to vgpu could caused unexpected behavior.

- When vgpu owns the physical display, vsync will triggered from HW pipe.
- When vgpu doesn't owns the display, vblank_timer will send the request
so that gvt service thread could inject.

Check the status so that no double vsync injected.

(cherry picked from commit be9f02bfa0a3d384918f8d6ea2d87142d8b27a36)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit ca057b393854be7ee89a3b7a3d1a3ecd4b3b46de)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Adjust cursor plane address offset when switch to vgpu

Already done for all plane in mmio handler, but miss the cursor plane
when switch back to vgpu, which may cause corrupted screen.

(cherry picked from commit a1d6f8d6fb1fffc987203ca1b594960e95b3ec7c)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 7e6d0c7830f6afcb18fb2de779243095fe1c9220)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Always simulate monitor starting from guest PORT B

Host have different active ports depends on the connected monitors.
GVT direct display will simulate one host port to guest as connected.
Previous logic align guest port with host port, which will hit issue
when host has eDP connection which not starts from PORT B. Since GVT
currently only support one display per guest, the simulated connection
to guest can always starts from PORT B.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit b2d3fd519d75493ae1972022f36f06d842b12d94)
Signed-off-by: Colin Xu <colin.xu@intel.com>

Fix issues after port idv-4.17 to idv-4.19.

Do not move plane address due to aperture offset is non-zero.
Display untracked mmio by default, align with idv-4.17.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit b5873fe4fc4a7387766cadb389ed3e55fd0805f6)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: remove planes cursor assert in idv

The planes or cursor state may be different between guest and host in
idv environment, so the state assert during pipe enable or disable
should be removed.

(cherry picked from commit 95bdf52d2b50e7397799a833420fbd1e9ef0b7ba)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 5a47cb3ddd32ba4d5adad59d3af2e10b973130d3)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Do not program ddb since it's statically allocated

In direct display, ddb for each pipe is statically allocated and doesn't
support dynamic re-allocate. So we program the HW at allocation time and
skip all HW update when i915 update wm.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 72f9e16e3229a68fe0df8ec5e072598015bc1b24)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable host port sharing between multiple vGPUs.

Multiple vGPUs can be assigned to same host pipe/port so that
the display switching could be host<--> vgpu and vgpu<-->vgpu.
Display switching sysfs is also refined accordingly.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 3222df8bf812592acdf24d9523c487a4cb863293)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Decouple virtual display port with type

Currently GVT always creates DP virtual display on port B for
guests using GVT_DP_B as the parameter. However in the view of a
real display encoder, port number and type binding are not fixed.
Decouple them when setup virtual monitor.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit facdc61201f4b55bc443358de5c6505b7155b664)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Change per-vGPU port assignment from id to bit mask

Previous we use enum type PORT_X + 1 to indicate assigned port for each
vGPU, we limit only 1 port can be assigned to a vGPU. Change it to bit
mask so that several ports can be assigned to same vGPU.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 6b16ff2b3592a18dcba775bd8a186322801b6759)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Store per pipe/port info for direct display into structure

Currenly gvt-g vGPU only support 1 virtual display or direct display.
All related variables are stored in struct intel_vgpu directly.
In order to scale to more pipes/ports support in future, move them into
struct vgpu->disp_topo.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 0fa41b5b02cab9727849ce8466f4f75dfd0afd73)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Merge per pipe/port info into intel_vgpu_display.

Merge disp_topo with existing structure intel_vgpu_display since
all intended for saving one virtual display path related info.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit d7d1946f69638c1c10e2f62a3ec0758cd4648e53)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Use same code path for both virtual and direct display

Previously direct display and virtual display use different path to
setup virtual DP monitor. Since logically they all represents a display
path only differs in whether or not physical pipe/port is assigned.
Merge different paths to unified one for future scalability.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 9080477da6cae479129dbad4e13ff872dccbc49d)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Enable multiple virtual display for vGPU

This patch made the following change:
- Change per pipe/port intel_vgpu_display from structure to list.
  This introduce a concept display path, which represents a legacy
  virtual display, including pipe/port information like physical
  assignment and EDID. By using list, vGPU virutal display or direct
  display is not limited on single one but can be expend to more.
- Move vgpu display related defines from gvt.h to display.h.
- Change the pipe/port assignment and handle logic for direct display.
  So that extend display is supported and vGPU can manipulate more than
  1 physical pipe/port.
- Re-write the messy emulate_monitor_status_change() logic for both
  direct display and virtual display.
- Implement auto switch owner finding logic in the function
  intel_vgpu_display_find_owner().

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 497bfa096114943d5ab7dd7e632f405928edd866)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Disable plane other than primary and cursor to support 4K

Due to direct display need split DDB in average among 3 pipes, and each
pipe could be owned by different vGPU, dynamically reallocate ddb for
different vGPUs can't be supported (reset DDB requires all other pipes
disabled). Within a pipe, if still leave DDB for sprite plane, there
could be no enough DDB for primary surface in some configuration, i.e.
4K resolution + tiling combination, and could result in issues like
host GDM can't show desktop (drm_mode_atomic_ioctl fails due to
drm_atomic_check_only fails on building plane WM), pipe FIFO underrun.

As has requested guest driver to not using sprite plane for video
playback, we disable sprite plane direct display in this patch.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 107e8f65cdbb372444e5e2b2aa9e45354c3b71f0)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix incorrect tile mode decoding from PLANE_CTL

Bit 8-10 of PLANE_CTL is not bit mask but value to indicate tile mode.
Incorrect tile mode will lead to wrong WM calculation.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 6ab1e792bc044249c1462d00aa71ceeb04805b79)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: DO not create sysfs for gvt direct display

When GVT is disabled, gvt sysfs should not be created.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 38335a9e1459134911bd8db15b07a0208da1c28d)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Check all display assignment mask

Make sure all ports in assigned mask for a vGPU is validated.
Invalid mask will be dropped.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 2c3ff90c5b3dc82e57761f9198dd99108c5504cd)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Add display pipe CG mmio to tracked list

Display pipe clock gating is handled by host so add below to list:
_CLKGATE_DIS_PSL_A
_CLKGATE_DIS_PSL_B
_CLKGATE_DIS_PSL_C

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit f8b0e26bdf199853930731e5cd81992948d6f4ab)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Add HDCP mmio to tracked list

GVT doesn't support HDCP yet so add untracked mmio to list.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit f1fdc0931f9f0b8b93900f6d423d0af4c69181c3)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Do not switch display to host on dmlr

vGPU owns the display between receiving pv display ready and deactive.
There is no need to switch display back to host on dmlr since during
reset, vGPU still owns the display regardless in reset state or not.
By doing this, during S3 resume, display won't switch back to host.

Signed-off-by: Colin Xu <colin.xu@intel.com>
Reviewed-by: Hang Yuan <hang.yuan@intel.com>
(cherry picked from commit 0befd463f2d4e3491f37d797bc46247126ef62b4)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Do not update direct display availability and mask on HPD

Although direct display doesn't support manual HPD, HPD during PM is
inevitable. vGPU can't be created during PM suspend and resume, so
we block the update to display availability and mask. Otherwise, any
reference to the mask after resume will go wrong.

Signed-off-by: Colin Xu <colin.xu@intel.com>
Reviewed-by: Hang Yuan <hang.yuan@intel.com>
(cherry picked from commit 8743a344c42dd28afee1b4b4427d97e1c2191ec7)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix direct display issue when vGPU turn on/off PIPECONF

Sometimes host will print "ERROR Fault errors on pipe A: 0x00000080".
When guest on monitor timeout or during suspend/resume, OS will call
setpowerstate on monitor to disable or enable it, then guest driver
will turn pipe on/off by diable PIPECONF.
Old implementation will enable flip_done IRQ on pipe to support async
flip, change IIR during pipe active is dangerous which leads pipe go
to unexpected status. Furthermore, some direct display mmios are still
allowed programmed to HW when vGPU PIPECONF is not active. And when
vGPU PIPECONF is off, direct display should not inject HW vSync.

The patch made below changes:
- When vGPU disable PIPECONF, turn all PLANE off to simluate vGPU
  PIPECONF off, instead of really turn the HW PIPECONF off. Otherwise,
  both host and vGPU will lost the entire pipe.
- Only enable and notify flip done interrupt for primary to support
  async flip, since direct display only support primary and cursor.
- Enable plane flip done interrupt in i915 together with enable vblank
  instead of on vGPU PIPECONF enablement.
- When vGPU PIPECONF isn't active, do not inject HW vSync or simulate.
- Do not allow vGPU program plane mmio when PIPECONF isn't active.

Signed-off-by: Colin Xu <colin.xu@intel.com>
Acked-by: Hang Yuan <hang.yuan@intel.com>
(cherry picked from commit fe138c042559f6640099de82c00f5e88279d82c6)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Fix mismatch host DDB state after resume

Sometimes host will print below message during suspend/resume:
*ERROR* mismatch in DDB state pipe B plane 1 (expected (297,586), found (0,0)

The reason is an error handling in skl_ddb_get_hw_plane_state(), that
i915 checks PLANE_CTL_ENABLE to decide whether or not updating the SW
ddb to verify the wm state. However during that time, the pipe/plane
is still owned by some vGPU, so the reading of PLANE_CTL actually
return the direct display state of vGPU plane, not host plane.
In such case, i915 should read from PLANE_CTL cache.
Furthremore, since ddb is always statically allocated in direct display
and all dynamic update to ddb from i915 is blocked. Get ddb entry should
also from gvt cache instead of from HW.

Signed-off-by: Colin Xu <colin.xu@intel.com>
Reviewed-by: Hang Yuan <hang.yuan@intel.com>
(cherry picked from commit 11f33cddd96fc226df3a4f2be0027535041b82c0)
Signed-off-by: Colin Xu <colin.xu@intel.com>

drm/i915/gvt: Use correct routine to access plane register in IDV

Some direct display programming doesn't follow i915 routine
skl_update_plane(), and doesn't get pm before mmio access. After long
run, host will print trace like:

Unclaimed write to register 0x7119c
WARNING: CPU: 6 PID: 12537 at drivers/gpu/drm/i915/intel_uncore.c:1083
	__unclaimed_reg_debug+0x4e/0x60 [i915]
...
Call Trace:
fwtable_write32+0x203/0x240 [i915]
skl_plane_surf_write+0x1b6/0x230 [i915]
intel_vgpu_mmio_reg_rw+0x2af/0x440 [i915]
...

Use same mmio access routine as i915, and make sure pm and lock got.

Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit 97683210bacd55c46cd037c1b538559dc2895fa5)
Signed-off-by: Colin Xu <colin.xu@intel.com>
(cherry picked from commit b15949a52bfec9103d2e849d2f8ede5a1939ddce)
Signed-off-by: Colin Xu <colin.xu@intel.com>
---
 drivers/gpu/drm/i915/display/intel_display.c |  189 +-
 drivers/gpu/drm/i915/display/intel_dp.c      |    2 +-
 drivers/gpu/drm/i915/display/intel_dp.h      |    1 +
 drivers/gpu/drm/i915/display/intel_hotplug.c |    4 +
 drivers/gpu/drm/i915/display/intel_sprite.c  |   77 +
 drivers/gpu/drm/i915/gvt/display.c           | 2332 ++++++++++++++++--
 drivers/gpu/drm/i915/gvt/display.h           |  102 +-
 drivers/gpu/drm/i915/gvt/edid.c              |  256 +-
 drivers/gpu/drm/i915/gvt/edid.h              |    7 +-
 drivers/gpu/drm/i915/gvt/gvt.c               |    2 +
 drivers/gpu/drm/i915/gvt/gvt.h               |  140 +-
 drivers/gpu/drm/i915/gvt/handlers.c          | 1031 +++++---
 drivers/gpu/drm/i915/gvt/interrupt.c         |    7 +
 drivers/gpu/drm/i915/gvt/interrupt.h         |    3 +
 drivers/gpu/drm/i915/gvt/kvmgt.c             |   12 +-
 drivers/gpu/drm/i915/gvt/mmio.c              |   45 +-
 drivers/gpu/drm/i915/gvt/reg.h               |   10 +-
 drivers/gpu/drm/i915/gvt/vgpu.c              |   18 +
 drivers/gpu/drm/i915/i915_debugfs.c          |   11 +-
 drivers/gpu/drm/i915/i915_irq.c              |   47 +-
 drivers/gpu/drm/i915/i915_sysfs.c            |  329 +++
 drivers/gpu/drm/i915/intel_pm.c              |  166 +-
 22 files changed, 4115 insertions(+), 676 deletions(-)

diff --git a/drivers/gpu/drm/i915/display/intel_display.c b/drivers/gpu/drm/i915/display/intel_display.c
index a13a9045d5b2..b47434a01a23 100644
--- a/drivers/gpu/drm/i915/display/intel_display.c
+++ b/drivers/gpu/drm/i915/display/intel_display.c
@@ -82,6 +82,10 @@
 #include "intel_tc.h"
 #include "intel_vga.h"
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+#include "gvt.h"
+#endif
+
 /* Primary plane formats for gen <= 3 */
 static const u32 i8xx_primary_formats[] = {
 	DRM_FORMAT_C8,
@@ -1328,6 +1332,13 @@ static void assert_planes_disabled(struct intel_crtc *crtc)
 	struct drm_i915_private *dev_priv = to_i915(crtc->base.dev);
 	struct intel_plane *plane;
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt && gvt->pipe_info[crtc->pipe].owner)
+		return;
+#endif
+
 	for_each_intel_plane_on_crtc(&dev_priv->drm, crtc, plane)
 		assert_plane_disabled(plane);
 }
@@ -4433,6 +4444,22 @@ static void skl_detach_scaler(struct intel_crtc *intel_crtc, int id)
 	struct drm_device *dev = intel_crtc->base.dev;
 	struct drm_i915_private *dev_priv = to_i915(dev);
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt) {
+		struct intel_dom0_pipe_regs *dom0_pipe_regs =
+			&gvt->pipe_info[intel_crtc->pipe].dom0_pipe_regs;
+
+		dom0_pipe_regs->scaler_ctl[id] = 0;
+		dom0_pipe_regs->scaler_win_pos[id] = 0;
+		dom0_pipe_regs->scaler_win_size[id] = 0;
+
+		if (gvt->pipe_info[intel_crtc->pipe].owner)
+			return;
+	}
+#endif
+
 	I915_WRITE(SKL_PS_CTRL(intel_crtc->pipe, id), 0);
 	I915_WRITE(SKL_PS_WIN_POS(intel_crtc->pipe, id), 0);
 	I915_WRITE(SKL_PS_WIN_SZ(intel_crtc->pipe, id), 0);
@@ -6164,6 +6191,9 @@ static void skylake_pfit_enable(const struct intel_crtc_state *crtc_state)
 	enum pipe pipe = crtc->pipe;
 	const struct intel_crtc_scaler_state *scaler_state =
 		&crtc_state->scaler_state;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	if (crtc_state->pch_pfit.enabled) {
 		u16 uv_rgb_hphase, uv_rgb_vphase;
@@ -6183,6 +6213,25 @@ static void skylake_pfit_enable(const struct intel_crtc_state *crtc_state)
 		uv_rgb_vphase = skl_scaler_calc_phase(1, vscale, false);
 
 		id = scaler_state->scaler_id;
+
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		if (gvt) {
+			struct intel_dom0_pipe_regs *dom0_pipe_regs =
+				&gvt->pipe_info[pipe].dom0_pipe_regs;
+
+			dom0_pipe_regs->scaler_ctl[id] = PS_SCALER_EN
+				| PS_FILTER_MEDIUM
+				| scaler_state->scalers[id].mode;
+			dom0_pipe_regs->scaler_win_pos[id] =
+				crtc->config->pch_pfit.pos;
+			dom0_pipe_regs->scaler_win_size[id] =
+				crtc->config->pch_pfit.size;
+
+			if (gvt->pipe_info[pipe].owner)
+				return;
+		}
+#endif
+
 		I915_WRITE(SKL_PS_CTRL(pipe, id), PS_SCALER_EN |
 			PS_FILTER_MEDIUM | scaler_state->scalers[id].mode);
 		I915_WRITE_FW(SKL_PS_VPHASE(pipe, id),
@@ -6743,6 +6792,9 @@ static void intel_encoders_enable(struct intel_crtc *crtc,
 	struct drm_connector_state *conn_state;
 	struct drm_connector *conn;
 	int i;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct drm_i915_private *dev_priv = to_i915(crtc->base.dev);
+#endif
 
 	for_each_new_connector_in_state(&state->base, conn, conn_state, i) {
 		struct intel_encoder *encoder =
@@ -6755,6 +6807,11 @@ static void intel_encoders_enable(struct intel_crtc *crtc,
 			encoder->enable(encoder, crtc_state, conn_state);
 		intel_opregion_notify_encoder(encoder, true);
 	}
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (dev_priv->gvt)
+		queue_work(system_unbound_wq,
+			   &dev_priv->gvt->connector_change_work);
+#endif
 }
 
 static void intel_encoders_disable(struct intel_crtc *crtc,
@@ -6764,6 +6821,9 @@ static void intel_encoders_disable(struct intel_crtc *crtc,
 	struct drm_connector_state *old_conn_state;
 	struct drm_connector *conn;
 	int i;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct drm_i915_private *dev_priv = to_i915(crtc->base.dev);
+#endif
 
 	for_each_old_connector_in_state(&state->base, conn, old_conn_state, i) {
 		struct intel_encoder *encoder =
@@ -6776,6 +6836,11 @@ static void intel_encoders_disable(struct intel_crtc *crtc,
 		if (encoder->disable)
 			encoder->disable(encoder, old_crtc_state, old_conn_state);
 	}
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (dev_priv->gvt)
+		queue_work(system_unbound_wq,
+			   &dev_priv->gvt->connector_change_work);
+#endif
 }
 
 static void intel_encoders_post_disable(struct intel_crtc *crtc,
@@ -8764,10 +8829,25 @@ static void intel_set_pipe_src_size(const struct intel_crtc_state *crtc_state)
 	struct intel_crtc *crtc = to_intel_crtc(crtc_state->uapi.crtc);
 	struct drm_i915_private *dev_priv = to_i915(crtc->base.dev);
 	enum pipe pipe = crtc->pipe;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	/* pipesrc controls the size that is scaled from, which should
 	 * always be the user's requested size.
 	 */
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt) {
+		struct intel_dom0_pipe_regs *dom0_pipe_regs =
+			&gvt->pipe_info[pipe].dom0_pipe_regs;
+
+		dom0_pipe_regs->pipesrc =
+			((crtc->config->pipe_src_w - 1) << 16) |
+			(crtc->config->pipe_src_h - 1);
+		if (gvt->pipe_info[pipe].owner)
+			return;
+	}
+#endif
 	I915_WRITE(PIPESRC(pipe),
 		   ((crtc_state->pipe_src_w - 1) << 16) |
 		   (crtc_state->pipe_src_h - 1));
@@ -8838,9 +8918,17 @@ static void intel_get_pipe_src_size(struct intel_crtc *crtc,
 {
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = to_i915(dev);
-	u32 tmp;
+	u32 tmp = I915_READ(PIPESRC(crtc->pipe));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt && gvt->pipe_info[crtc->pipe].owner) {
+		struct intel_dom0_pipe_regs *dom0_pipe_regs =
+			&gvt->pipe_info[crtc->pipe].dom0_pipe_regs;
 
-	tmp = I915_READ(PIPESRC(crtc->pipe));
+		tmp = dom0_pipe_regs->pipesrc;
+	}
+#endif
 	pipe_config->pipe_src_h = (tmp & 0xffff) + 1;
 	pipe_config->pipe_src_w = ((tmp >> 16) & 0xffff) + 1;
 
@@ -10350,18 +10438,33 @@ static void skylake_get_pfit_config(struct intel_crtc *crtc,
 	struct drm_device *dev = crtc->base.dev;
 	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_crtc_scaler_state *scaler_state = &pipe_config->scaler_state;
-	u32 ps_ctrl = 0;
+	u32 ps_ctrl, ps_win_pos, ps_win_sz = 0;
 	int id = -1;
 	int i;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	/* find scaler attached to this pipe */
 	for (i = 0; i < crtc->num_scalers; i++) {
 		ps_ctrl = I915_READ(SKL_PS_CTRL(crtc->pipe, i));
+		ps_win_pos = I915_READ(SKL_PS_WIN_POS(crtc->pipe, i));
+		ps_win_sz = I915_READ(SKL_PS_WIN_SZ(crtc->pipe, i));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		if (gvt && gvt->pipe_info[crtc->pipe].owner) {
+			struct intel_dom0_pipe_regs *dom0_pipe_regs =
+				&gvt->pipe_info[crtc->pipe].dom0_pipe_regs;
+
+			ps_ctrl = dom0_pipe_regs->scaler_ctl[i];
+			ps_win_pos = dom0_pipe_regs->scaler_win_pos[i];
+			ps_win_sz = dom0_pipe_regs->scaler_win_size[i];
+		}
+#endif
 		if (ps_ctrl & PS_SCALER_EN && !(ps_ctrl & PS_PLANE_SEL_MASK)) {
 			id = i;
 			pipe_config->pch_pfit.enabled = true;
-			pipe_config->pch_pfit.pos = I915_READ(SKL_PS_WIN_POS(crtc->pipe, i));
-			pipe_config->pch_pfit.size = I915_READ(SKL_PS_WIN_SZ(crtc->pipe, i));
+			pipe_config->pch_pfit.pos = ps_win_pos;
+			pipe_config->pch_pfit.size = ps_win_sz;
 			scaler_state->scalers[i].in_use = true;
 			break;
 		}
@@ -10383,12 +10486,17 @@ skylake_get_initial_plane_config(struct intel_crtc *crtc,
 	struct drm_i915_private *dev_priv = to_i915(dev);
 	struct intel_plane *plane = to_intel_plane(crtc->base.primary);
 	enum plane_id plane_id = plane->id;
-	enum pipe pipe;
+	enum pipe pipe = crtc->pipe;
 	u32 val, base, offset, stride_mult, tiling, alpha;
 	int fourcc, pixel_format;
 	unsigned int aligned_height;
 	struct drm_framebuffer *fb;
 	struct intel_framebuffer *intel_fb;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_dom0_plane_regs *dom0_regs =
+		&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+#endif
 
 	if (!plane->get_hw_state(plane, &pipe))
 		return;
@@ -10406,6 +10514,10 @@ skylake_get_initial_plane_config(struct intel_crtc *crtc,
 	fb->dev = dev;
 
 	val = I915_READ(PLANE_CTL(pipe, plane_id));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		val = dom0_regs->plane_ctl;
+#endif
 
 	if (INTEL_GEN(dev_priv) >= 11)
 		pixel_format = val & ICL_PLANE_CTL_FORMAT_MASK;
@@ -10478,16 +10590,33 @@ skylake_get_initial_plane_config(struct intel_crtc *crtc,
 		plane_config->rotation |= DRM_MODE_REFLECT_X;
 
 	base = I915_READ(PLANE_SURF(pipe, plane_id)) & 0xfffff000;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		base = dom0_regs->plane_surf;
+#endif
 	plane_config->base = base;
 
 	offset = I915_READ(PLANE_OFFSET(pipe, plane_id));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		offset = dom0_regs->plane_offset;
+#endif
 
 	val = I915_READ(PLANE_SIZE(pipe, plane_id));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		val = dom0_regs->plane_size;
+#endif
 	fb->height = ((val >> 16) & 0xffff) + 1;
 	fb->width = ((val >> 0) & 0xffff) + 1;
 
 	val = I915_READ(PLANE_STRIDE(pipe, plane_id));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		val = dom0_regs->plane_stride;
+#endif
 	stride_mult = skl_plane_stride_mult(fb, 0, DRM_MODE_ROTATE_0);
+
 	fb->pitches[0] = (val & 0x3ff) * stride_mult;
 
 	aligned_height = intel_fb_align_height(fb, 0, fb->height);
@@ -11635,6 +11764,9 @@ static void i9xx_update_cursor(struct intel_plane *plane,
 	enum pipe pipe = plane->pipe;
 	u32 cntl = 0, base = 0, pos = 0, fbc_ctl = 0;
 	unsigned long irqflags;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	if (plane_state && plane_state->uapi.visible) {
 		unsigned width = drm_rect_width(&plane_state->uapi.dst);
@@ -11650,6 +11782,28 @@ static void i9xx_update_cursor(struct intel_plane *plane,
 		pos = intel_cursor_position(plane_state);
 	}
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt) {
+		struct intel_dom0_plane_regs *dom0_regs =
+			&gvt->pipe_info[pipe].plane_info[PLANE_CURSOR].dom0_regs;
+
+		dom0_regs->plane_ctl = cntl;
+		dom0_regs->plane_pos = pos;
+		dom0_regs->plane_surf = base;
+		if (HAS_CUR_FBC(dev_priv))
+			dom0_regs->cur_fbc_ctl = fbc_ctl;
+		else
+			dom0_regs->cur_fbc_ctl = 0;
+
+		if (gvt->pipe_info[pipe].owner) {
+			plane->cursor.base = base;
+			plane->cursor.size = fbc_ctl;
+			plane->cursor.cntl = cntl;
+			return;
+		}
+	}
+#endif
+
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	/*
@@ -11709,6 +11863,22 @@ static bool i9xx_cursor_get_hw_state(struct intel_plane *plane,
 	intel_wakeref_t wakeref;
 	bool ret;
 	u32 val;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_dom0_plane_regs *dom0_regs =
+		&gvt->pipe_info[plane->pipe].plane_info[PLANE_CURSOR].dom0_regs;
+
+	if (gvt && gvt->pipe_info[plane->pipe].owner) {
+		val = dom0_regs->plane_ctl;
+		ret = val & MCURSOR_MODE;
+		if (INTEL_GEN(dev_priv) >= 5 || IS_G4X(dev_priv))
+			*pipe = plane->pipe;
+		else
+			*pipe = (val & MCURSOR_PIPE_SELECT_MASK) >>
+				MCURSOR_PIPE_SELECT_SHIFT;
+		return ret;
+	}
+#endif
 
 	/*
 	 * Not 100% correct for planes that can move between pipes,
@@ -13779,6 +13949,13 @@ static void verify_wm_state(struct intel_crtc *crtc,
 	const enum pipe pipe = crtc->pipe;
 	int plane, level, max_level = ilk_wm_max_level(dev_priv);
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt && gvt->pipe_info[pipe].owner)
+		return;
+#endif
+
 	if (INTEL_GEN(dev_priv) < 9 || !new_crtc_state->hw.active)
 		return;
 
diff --git a/drivers/gpu/drm/i915/display/intel_dp.c b/drivers/gpu/drm/i915/display/intel_dp.c
index 483ec108efa1..05478e47b649 100644
--- a/drivers/gpu/drm/i915/display/intel_dp.c
+++ b/drivers/gpu/drm/i915/display/intel_dp.c
@@ -699,7 +699,7 @@ u32 intel_dp_pack_aux(const u8 *src, int src_bytes)
 	return v;
 }
 
-static void intel_dp_unpack_aux(u32 src, u8 *dst, int dst_bytes)
+void intel_dp_unpack_aux(u32 src, u8 *dst, int dst_bytes)
 {
 	int i;
 	if (dst_bytes > 4)
diff --git a/drivers/gpu/drm/i915/display/intel_dp.h b/drivers/gpu/drm/i915/display/intel_dp.h
index 3da166054788..33f5c657bd77 100644
--- a/drivers/gpu/drm/i915/display/intel_dp.h
+++ b/drivers/gpu/drm/i915/display/intel_dp.h
@@ -77,6 +77,7 @@ int intel_dp_max_lane_count(struct intel_dp *intel_dp);
 int intel_dp_rate_select(struct intel_dp *intel_dp, int rate);
 void intel_power_sequencer_reset(struct drm_i915_private *dev_priv);
 u32 intel_dp_pack_aux(const u8 *src, int src_bytes);
+void intel_dp_unpack_aux(u32 src, u8 *dst, int dst_bytes);
 
 void intel_edp_drrs_enable(struct intel_dp *intel_dp,
 			   const struct intel_crtc_state *crtc_state);
diff --git a/drivers/gpu/drm/i915/display/intel_hotplug.c b/drivers/gpu/drm/i915/display/intel_hotplug.c
index 60c192fb7488..182308de1454 100644
--- a/drivers/gpu/drm/i915/display/intel_hotplug.c
+++ b/drivers/gpu/drm/i915/display/intel_hotplug.c
@@ -359,6 +359,10 @@ static void i915_hotplug_work_func(struct work_struct *work)
 	mutex_lock(&dev->mode_config.mutex);
 	DRM_DEBUG_KMS("running encoder hotplug functions\n");
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	DRM_WARN("hotplug can't be supported in idv\n");
+#endif
+
 	spin_lock_irq(&dev_priv->irq_lock);
 
 	hpd_event_bits = dev_priv->hotplug.event_bits;
diff --git a/drivers/gpu/drm/i915/display/intel_sprite.c b/drivers/gpu/drm/i915/display/intel_sprite.c
index 4ba1ce711861..331ddde06f46 100644
--- a/drivers/gpu/drm/i915/display/intel_sprite.c
+++ b/drivers/gpu/drm/i915/display/intel_sprite.c
@@ -48,6 +48,10 @@
 #include "intel_psr.h"
 #include "intel_sprite.h"
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+#include "gvt.h"
+#endif
+
 int intel_usecs_to_scanlines(const struct drm_display_mode *adjusted_mode,
 			     int usecs)
 {
@@ -598,6 +602,13 @@ skl_program_plane(struct intel_plane *plane,
 	unsigned long irqflags;
 	u32 keymsk, keymax;
 	u32 plane_ctl = plane_state->ctl;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_dom0_plane_regs *dom0_regs =
+		&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+	struct intel_dom0_pipe_regs *dom0_pipe_regs =
+		&gvt->pipe_info[pipe].dom0_pipe_regs;
+#endif
 
 	plane_ctl |= skl_plane_ctl_crtc(crtc_state);
 
@@ -621,6 +632,49 @@ skl_program_plane(struct intel_plane *plane,
 		crtc_y = 0;
 	}
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt) {
+		u32 crtc_w = drm_rect_width(&plane_state->uapi.dst);
+		u32 crtc_h = drm_rect_height(&plane_state->uapi.dst);
+
+		dom0_regs->plane_keyval =  key->min_value;
+		dom0_regs->plane_keymax =  key->max_value;
+		dom0_regs->plane_keymsk =  key->channel_mask;
+		dom0_regs->plane_offset =  (y << 16) | x;
+		dom0_regs->plane_stride = stride;
+		dom0_regs->plane_size = (src_h << 16) | src_w;
+		dom0_regs->plane_aux_dist =
+			(plane_state->color_plane[1].offset - surf_addr) |
+			aux_stride;
+		dom0_regs->plane_aux_offset =
+			(plane_state->color_plane[1].y << 16) |
+			plane_state->color_plane[1].x;
+		dom0_regs->plane_pos = (crtc_y << 16) | crtc_x;
+		dom0_regs->plane_ctl = plane_ctl;
+		dom0_regs->plane_surf =
+			intel_plane_ggtt_offset(plane_state) +
+			surf_addr;
+
+		if (plane_state->scaler_id >= 0) {
+			int scaler_id = plane_state->scaler_id;
+			const struct intel_scaler *scaler;
+
+			scaler = &crtc_state->scaler_state.scalers[scaler_id];
+			dom0_pipe_regs->scaler_ctl[scaler_id] = PS_SCALER_EN
+				| PS_PLANE_SEL(plane_id) | scaler->mode;
+			dom0_pipe_regs->scaler_pwr_gate[scaler_id] = 0;
+			dom0_pipe_regs->scaler_win_pos[scaler_id] =
+				(crtc_x << 16) | crtc_y;
+			dom0_pipe_regs->scaler_win_size[scaler_id] =
+				(crtc_w << 16) | crtc_h;
+			dom0_regs->plane_pos = 0;
+		}
+
+		if (gvt->pipe_info[pipe].plane_info[plane_id].owner)
+			return;
+	}
+#endif
+
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	I915_WRITE_FW(PLANE_STRIDE(pipe, plane_id), stride);
@@ -689,6 +743,18 @@ skl_disable_plane(struct intel_plane *plane,
 	enum plane_id plane_id = plane->id;
 	enum pipe pipe = plane->pipe;
 	unsigned long irqflags;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt) {
+		struct intel_dom0_plane_regs *dom0_regs =
+			&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+		dom0_regs->plane_ctl = 0;
+		dom0_regs->plane_surf = 0;
+		if (gvt->pipe_info[pipe].plane_info[plane_id].owner)
+			return;
+	}
+#endif
 
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
@@ -712,6 +778,17 @@ skl_plane_get_hw_state(struct intel_plane *plane,
 	enum plane_id plane_id = plane->id;
 	intel_wakeref_t wakeref;
 	bool ret;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_dom0_plane_regs *dom0_regs =
+		&gvt->pipe_info[plane->pipe].plane_info[plane_id].dom0_regs;
+
+	if (gvt && gvt->pipe_info[plane->pipe].plane_info[plane_id].owner) {
+		ret = dom0_regs->plane_ctl & PLANE_CTL_ENABLE;
+		*pipe = plane->pipe;
+		return ret;
+	}
+#endif
 
 	power_domain = POWER_DOMAIN_PIPE(plane->pipe);
 	wakeref = intel_display_power_get_if_enabled(dev_priv, power_domain);
diff --git a/drivers/gpu/drm/i915/gvt/display.c b/drivers/gpu/drm/i915/gvt/display.c
index a62bdf9be682..ebeb334f1741 100644
--- a/drivers/gpu/drm/i915/gvt/display.c
+++ b/drivers/gpu/drm/i915/gvt/display.c
@@ -34,11 +34,12 @@
 
 #include "i915_drv.h"
 #include "gvt.h"
+#include <drm/drm_fourcc.h>
 
-static int get_edp_pipe(struct intel_vgpu *vgpu)
+static enum pipe get_edp_pipe(struct intel_vgpu *vgpu)
 {
 	u32 data = vgpu_vreg(vgpu, _TRANS_DDI_FUNC_CTL_EDP);
-	int pipe = -1;
+	enum pipe pipe = INVALID_PIPE;
 
 	switch (data & TRANS_DDI_EDP_INPUT_MASK) {
 	case TRANS_DDI_EDP_INPUT_A_ON:
@@ -67,11 +68,12 @@ static int edp_pipe_is_enabled(struct intel_vgpu *vgpu)
 	return 1;
 }
 
-int pipe_is_enabled(struct intel_vgpu *vgpu, int pipe)
+int pipe_is_enabled(struct intel_vgpu *vgpu, enum pipe pipe)
 {
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 
-	if (WARN_ON(pipe < PIPE_A || pipe >= I915_MAX_PIPES))
+	if (WARN_ON(pipe == INVALID_PIPE ||
+		    pipe >= INTEL_NUM_PIPES(dev_priv)))
 		return -EINVAL;
 
 	if (vgpu_vreg_t(vgpu, PIPECONF(pipe)) & PIPECONF_ENABLE)
@@ -169,182 +171,336 @@ static u8 dpcd_fix_data[DPCD_HEADER_SIZE] = {
 static void emulate_monitor_status_change(struct intel_vgpu *vgpu)
 {
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	int pipe;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum port port;
+	enum pipe pipe;
+	enum transcoder trans;
 
-	if (IS_BROXTON(dev_priv)) {
+	if (IS_BROADWELL(dev_priv)) {
+		vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) &= ~GEN8_PORT_DP_A_HOTPLUG;
+		vgpu_vreg_t(vgpu, SDEISR) &= ~(SDE_PORTB_HOTPLUG_CPT |
+			SDE_PORTC_HOTPLUG_CPT |
+			SDE_PORTD_HOTPLUG_CPT);
+		vgpu_vreg_t(vgpu, PCH_ADPA) &= ~ADPA_CRT_HOTPLUG_MONITOR_MASK;
+		vgpu_vreg_t(vgpu, SFUSE_STRAP) &= ~(SFUSE_STRAP_DDIB_DETECTED |
+			SFUSE_STRAP_DDIC_DETECTED |
+			SFUSE_STRAP_DDID_DETECTED);
+		for (port = PORT_A; port <= PORT_E; port++) {
+			vgpu_vreg_t(vgpu, PORT_CLK_SEL(port)) |=
+				PORT_CLK_SEL_NONE;
+		}
+	} else if (IS_GEN9_LP(dev_priv)) {
 		vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) &= ~(BXT_DE_PORT_HP_DDIA |
 			BXT_DE_PORT_HP_DDIB |
 			BXT_DE_PORT_HP_DDIC);
+	} else {
+		vgpu_vreg_t(vgpu, SDEISR) &= ~(SDE_PORTA_HOTPLUG_SPT |
+			SDE_PORTB_HOTPLUG_CPT |
+			SDE_PORTC_HOTPLUG_CPT |
+			SDE_PORTD_HOTPLUG_CPT |
+			SDE_PORTE_HOTPLUG_SPT);
+		vgpu_vreg_t(vgpu, SFUSE_STRAP) &= ~(SFUSE_STRAP_DDIB_DETECTED |
+			SFUSE_STRAP_DDIC_DETECTED |
+			SFUSE_STRAP_DDID_DETECTED |
+			SFUSE_STRAP_DDIF_DETECTED);
+		// Keep same copy as host until HPD support
+		// for (port = PORT_A; port <= PORT_E; port++) {
+		// 	vgpu_vreg_t(vgpu, DPLL_CTRL2) &=
+		// 			~DPLL_CTRL2_DDI_CLK_SEL_MASK(port);
+		// 	vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
+		// 		(DPLL_CTRL2_DDI_CLK_OFF(port) |
+		// 		DPLL_CTRL2_DDI_SEL_OVERRIDE(port));
+		// }
+	}
 
-		if (intel_vgpu_has_monitor_on_port(vgpu, PORT_A)) {
-			vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
-				BXT_DE_PORT_HP_DDIA;
-		}
-
-		if (intel_vgpu_has_monitor_on_port(vgpu, PORT_B)) {
-			vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
-				BXT_DE_PORT_HP_DDIB;
-		}
-
-		if (intel_vgpu_has_monitor_on_port(vgpu, PORT_C)) {
-			vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
-				BXT_DE_PORT_HP_DDIC;
-		}
+	for_each_pipe(dev_priv, pipe) {
+		vgpu_vreg_t(vgpu, PIPECONF(pipe)) &=
+			~(PIPECONF_ENABLE | I965_PIPECONF_ACTIVE);
+		vgpu_vreg_t(vgpu, DSPCNTR(pipe)) &= ~DISPLAY_PLANE_ENABLE;
+		vgpu_vreg_t(vgpu, SPRCTL(pipe)) &= ~SPRITE_ENABLE;
+		vgpu_vreg_t(vgpu, CURCNTR(pipe)) &= ~MCURSOR_MODE;
+		vgpu_vreg_t(vgpu, CURCNTR(pipe)) |= MCURSOR_MODE_DISABLE;
+	}
 
-		return;
+	for (trans = TRANSCODER_A; trans <= TRANSCODER_EDP; trans++) {
+		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(trans)) &=
+			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
+			TRANS_DDI_PORT_MASK | TRANS_DDI_FUNC_ENABLE);
 	}
 
-	vgpu_vreg_t(vgpu, SDEISR) &= ~(SDE_PORTB_HOTPLUG_CPT |
-			SDE_PORTC_HOTPLUG_CPT |
-			SDE_PORTD_HOTPLUG_CPT);
+	for (port = PORT_A; port <= PORT_E; port++) {
+		if (IS_GEN9_LP(dev_priv) && port > PORT_C)
+			continue;
+		vgpu_vreg_t(vgpu, DDI_BUF_CTL(port)) &=
+			~(DDI_INIT_DISPLAY_DETECTED | DDI_BUF_CTL_ENABLE);
+		vgpu_vreg_t(vgpu, DDI_BUF_CTL(port)) |=
+			DDI_BUF_IS_IDLE;
 
-	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
-	    IS_COFFEELAKE(dev_priv)) {
-		vgpu_vreg_t(vgpu, SDEISR) &= ~(SDE_PORTA_HOTPLUG_SPT |
-				SDE_PORTE_HOTPLUG_SPT);
-		vgpu_vreg_t(vgpu, SKL_FUSE_STATUS) |=
-				SKL_FUSE_DOWNLOAD_STATUS |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG0) |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG1) |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG2);
-		vgpu_vreg_t(vgpu, LCPLL1_CTL) |=
-				LCPLL_PLL_ENABLE |
-				LCPLL_PLL_LOCK;
-		vgpu_vreg_t(vgpu, LCPLL2_CTL) |= LCPLL_PLL_ENABLE;
-
-	}
-
-	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_B)) {
-		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDIB_DETECTED;
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
-			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
-			TRANS_DDI_PORT_MASK);
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) |=
-			(TRANS_DDI_BPC_8 | TRANS_DDI_MODE_SELECT_DVI |
-			(PORT_B << TRANS_DDI_PORT_SHIFT) |
-			TRANS_DDI_FUNC_ENABLE);
-		if (IS_BROADWELL(dev_priv)) {
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_B)) &=
-				~PORT_CLK_SEL_MASK;
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_B)) |=
-				PORT_CLK_SEL_LCPLL_810;
-		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_B)) |= DDI_BUF_CTL_ENABLE;
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_B)) &= ~DDI_BUF_IS_IDLE;
-		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTB_HOTPLUG_CPT;
 	}
 
-	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_C)) {
-		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTC_HOTPLUG_CPT;
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
-			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
-			TRANS_DDI_PORT_MASK);
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) |=
-			(TRANS_DDI_BPC_8 | TRANS_DDI_MODE_SELECT_DVI |
-			(PORT_C << TRANS_DDI_PORT_SHIFT) |
-			TRANS_DDI_FUNC_ENABLE);
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		pipe = disp_path->pipe;
+		port = disp_path->port;
+		trans = disp_path->trans;
+		if (port == PORT_NONE)
+			continue;
 		if (IS_BROADWELL(dev_priv)) {
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_C)) &=
-				~PORT_CLK_SEL_MASK;
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_C)) |=
-				PORT_CLK_SEL_LCPLL_810;
+			if (port > PORT_D)
+				continue;
+		} else if (IS_GEN9_LP(dev_priv)) {
+			if (port > PORT_C)
+				continue;
+		} else {
+			if (port > PORT_E)
+				continue;
 		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_C)) |= DDI_BUF_CTL_ENABLE;
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_C)) &= ~DDI_BUF_IS_IDLE;
-		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDIC_DETECTED;
-	}
+		if (intel_vgpu_display_has_monitor(disp_path)) {
+			u32 ddi_fuse[] = {
+				[PORT_A] = 0,
+				[PORT_B] = SFUSE_STRAP_DDIB_DETECTED,
+				[PORT_C] = SFUSE_STRAP_DDIC_DETECTED,
+				[PORT_D] = SFUSE_STRAP_DDID_DETECTED,
+			};
 
-	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_D)) {
-		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTD_HOTPLUG_CPT;
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
-			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
-			TRANS_DDI_PORT_MASK);
-		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) |=
-			(TRANS_DDI_BPC_8 | TRANS_DDI_MODE_SELECT_DVI |
-			(PORT_D << TRANS_DDI_PORT_SHIFT) |
-			TRANS_DDI_FUNC_ENABLE);
-		if (IS_BROADWELL(dev_priv)) {
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_D)) &=
-				~PORT_CLK_SEL_MASK;
-			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_D)) |=
-				PORT_CLK_SEL_LCPLL_810;
+			if (IS_BROADWELL(dev_priv)) {
+				vgpu_vreg_t(vgpu, PORT_CLK_SEL(port)) &=
+					~PORT_CLK_SEL_MASK;
+				vgpu_vreg_t(vgpu, PORT_CLK_SEL(port)) |=
+					PORT_CLK_SEL_LCPLL_2700;
+				if (port >= PORT_B && port <= PORT_D)
+					vgpu_vreg_t(vgpu, SFUSE_STRAP) |=
+						ddi_fuse[port];
+			} else if (IS_GEN9_LP(dev_priv)) {
+			} else {
+				// vgpu_vreg_t(vgpu, DPLL_CTRL2) &=
+				// 	~DPLL_CTRL2_DDI_CLK_OFF(port);
+				// vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
+				// 	(DPLL_CTRL2_DDI_CLK_SEL(DPLL_ID_SKL_DPLL0 + port, port) |
+				// 	DPLL_CTRL2_DDI_SEL_OVERRIDE(port));
+				if (port >= PORT_B && port <= PORT_D)
+					vgpu_vreg_t(vgpu, SFUSE_STRAP) |=
+						ddi_fuse[port];
+			}
+
+			vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(trans)) |=
+				TRANS_DDI_BPC_8 | TRANS_DDI_MODE_SELECT_DVI |
+				(port << TRANS_DDI_PORT_SHIFT) |
+				TRANS_DDI_FUNC_ENABLE;
+			if (trans == TRANSCODER_A)
+				vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(trans)) &=
+					~TRANS_DDI_EDP_INPUT_MASK;
+				vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(trans)) |=
+					TRANS_DDI_EDP_INPUT_A_ON;
+
+			if (port == PORT_A)
+				vgpu_vreg_t(vgpu, DDI_BUF_CTL(port)) |=
+					DDI_INIT_DISPLAY_DETECTED;
+			vgpu_vreg_t(vgpu, DDI_BUF_CTL(port)) |=
+				DDI_BUF_CTL_ENABLE;
+			vgpu_vreg_t(vgpu, DDI_BUF_CTL(port)) &=
+				~DDI_BUF_IS_IDLE;
+
+			vgpu_vreg_t(vgpu, PIPECONF(pipe)) |=
+				(PIPECONF_ENABLE | I965_PIPECONF_ACTIVE);
+
+			if (IS_BROADWELL(dev_priv)) {
+				u32 bdw_hpd_pin[] = {
+					[PORT_A] = GEN8_PORT_DP_A_HOTPLUG,
+					[PORT_B] = SDE_PORTB_HOTPLUG_CPT,
+					[PORT_C] = SDE_PORTC_HOTPLUG_CPT,
+					[PORT_D] = SDE_PORTD_HOTPLUG_CPT,
+				};
+
+				if (port == PORT_A)
+					vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
+						bdw_hpd_pin[port];
+				else
+					vgpu_vreg_t(vgpu, SDEISR) |=
+						bdw_hpd_pin[port];
+			} else if (IS_GEN9_LP(dev_priv)) {
+				u32 bxt_hpd_pin[] = {
+					[PORT_A] = BXT_DE_PORT_HP_DDIA,
+					[PORT_B] = BXT_DE_PORT_HP_DDIB,
+					[PORT_C] = BXT_DE_PORT_HP_DDIC
+				};
+
+				vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
+					bxt_hpd_pin[port];
+			} else {
+				u32 skl_hpd_pin[] = {
+					[PORT_A] = SDE_PORTA_HOTPLUG_SPT,
+					[PORT_B] = SDE_PORTB_HOTPLUG_CPT,
+					[PORT_C] = SDE_PORTC_HOTPLUG_CPT,
+					[PORT_D] = SDE_PORTD_HOTPLUG_CPT,
+					[PORT_E] = SDE_PORTE_HOTPLUG_SPT,
+				};
+
+				vgpu_vreg_t(vgpu, SDEISR) |=
+					skl_hpd_pin[port];
+			}
+
+			gvt_dbg_dpy("vgpu:%d emulate monitor on PIPE_%c TRANSCODE_%s PORT_%c\n",
+				    vgpu->id, pipe_name(pipe),
+				    transcoder_name(trans), port_name(port));
 		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_D)) |= DDI_BUF_CTL_ENABLE;
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_D)) &= ~DDI_BUF_IS_IDLE;
-		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDID_DETECTED;
 	}
+}
 
-	if ((IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
-	     IS_COFFEELAKE(dev_priv)) &&
-			intel_vgpu_has_monitor_on_port(vgpu, PORT_E)) {
-		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTE_HOTPLUG_SPT;
+static void clean_virtual_dp_monitor(struct intel_vgpu *vgpu,
+				     struct intel_vgpu_display_path *disp_path)
+{
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return;
 	}
 
-	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_A)) {
-		if (IS_BROADWELL(dev_priv))
-			vgpu_vreg_t(vgpu, GEN8_DE_PORT_ISR) |=
-				GEN8_PORT_DP_A_HOTPLUG;
-		else
-			vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTA_HOTPLUG_SPT;
+	kfree(disp_path->edid);
+	disp_path->edid = NULL;
 
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_A)) |= DDI_INIT_DISPLAY_DETECTED;
-	}
+	kfree(disp_path->dpcd);
+	disp_path->dpcd = NULL;
 
-	/* Clear host CRT status, so guest couldn't detect this host CRT. */
-	if (IS_BROADWELL(dev_priv))
-		vgpu_vreg_t(vgpu, PCH_ADPA) &= ~ADPA_CRT_HOTPLUG_MONITOR_MASK;
+	disp_path->port_type = INTEL_OUTPUT_UNUSED;
 
-	/* Disable Primary/Sprite/Cursor plane */
-	for_each_pipe(dev_priv, pipe) {
-		vgpu_vreg_t(vgpu, DSPCNTR(pipe)) &= ~DISPLAY_PLANE_ENABLE;
-		vgpu_vreg_t(vgpu, SPRCTL(pipe)) &= ~SPRITE_ENABLE;
-		vgpu_vreg_t(vgpu, CURCNTR(pipe)) &= ~MCURSOR_MODE;
-		vgpu_vreg_t(vgpu, CURCNTR(pipe)) |= MCURSOR_MODE_DISABLE;
+	gvt_dbg_dpy("vgpu-%d virtual DP monitor on path-%d cleaned\n",
+		    vgpu->id, disp_path->id);
+}
+
+#define GVT_EDID_EST_TIMINGS 16
+#define GVT_EDID_STD_TIMINGS 8
+#define GVT_EDID_DETAILED_TIMINGS 4
+
+static int filter_detailed_modes(struct edid *edid)
+{
+	int i;
+
+	for (i = 1; i < GVT_EDID_DETAILED_TIMINGS; i++)
+		edid->detailed_timings[i].pixel_clock = 0x00;
+
+	return 0;
+}
+
+
+static int filter_standard_modes(struct edid *edid)
+{
+	int i;
+
+	for (i = 0; i < GVT_EDID_STD_TIMINGS; i++) {
+		edid->standard_timings[i].hsize = 0x01;
+		edid->standard_timings[i].vfreq_aspect = 0x01;
 	}
 
-	vgpu_vreg_t(vgpu, PIPECONF(PIPE_A)) |= PIPECONF_ENABLE;
+	return 0;
+}
+
+static int filter_established_modes(struct edid *edid)
+{
+	edid->established_timings.t1 = 0;
+	edid->established_timings.t2 = 0x00;
+	edid->established_timings.mfg_rsvd &= ~0x80;
+
+	return 0;
+}
+
+static int filter_ext_blocks(u8 *raw_edid)
+{
+	raw_edid[EDID_LENGTH-2] = 0;
+
+	return 0;
 }
 
-static void clean_virtual_dp_monitor(struct intel_vgpu *vgpu, int port_num)
+static int  gvt_edid_set_checksum(u8 *raw_edid)
 {
-	struct intel_vgpu_port *port = intel_vgpu_port(vgpu, port_num);
+	int i;
+	u8 csum = 0;
 
-	kfree(port->edid);
-	port->edid = NULL;
+	for (i = 0; i < EDID_LENGTH-1; i++)
+		csum += raw_edid[i];
+	raw_edid[EDID_LENGTH-1] = 256 - csum%256;
 
-	kfree(port->dpcd);
-	port->dpcd = NULL;
+	return csum;
 }
 
-static int setup_virtual_dp_monitor(struct intel_vgpu *vgpu, int port_num,
-				    int type, unsigned int resolution)
+
+static int setup_virtual_dp_monitor_edid(struct intel_vgpu *vgpu,
+					 struct intel_vgpu_display_path *disp_path,
+					 void *edid)
 {
-	struct intel_vgpu_port *port = intel_vgpu_port(vgpu, port_num);
+	int valid_extensions = 1;
+	struct edid *raw_edid;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return -EINVAL;
+	}
+
+	if (WARN_ON(disp_path->port_type != INTEL_OUTPUT_DP &&
+		    disp_path->port_type != INTEL_OUTPUT_EDP &&
+		    disp_path->port_type != INTEL_OUTPUT_DP_MST)) {
+		gvt_err("vgpu-%d: Unsupported virtual DP monitor type:%d on PORT_%c\n",
+			vgpu->id, disp_path->port_type, disp_path->port);
+		return -EINVAL;
+	}
 
-	if (WARN_ON(resolution >= GVT_EDID_NUM))
+	if (WARN_ON(!edid && disp_path->resolution >= GVT_EDID_NUM)) {
+		gvt_err("vgpu-%d: Unsupported virtual DP monitor resolution:%llx\n",
+			vgpu->id, disp_path->resolution);
 		return -EINVAL;
+	}
 
-	port->edid = kzalloc(sizeof(*(port->edid)), GFP_KERNEL);
-	if (!port->edid)
+	if (edid)
+		valid_extensions += ((struct edid *)edid)->extensions;
+
+	disp_path->edid = kzalloc(sizeof(*disp_path->edid)
+			+ valid_extensions * EDID_SIZE, GFP_KERNEL);
+	if (!disp_path->edid) {
+		gvt_err("vgpu-%d: Fail to allocate EDID for virtual DP monitor\n",
+			vgpu->id);
 		return -ENOMEM;
+	}
 
-	port->dpcd = kzalloc(sizeof(*(port->dpcd)), GFP_KERNEL);
-	if (!port->dpcd) {
-		kfree(port->edid);
+	disp_path->dpcd = kzalloc(sizeof(*disp_path->dpcd), GFP_KERNEL);
+	if (!disp_path->dpcd) {
+		kfree(disp_path->edid);
+		gvt_dbg_dpy("vgpu-%d: Fail to allocate DPCD for virtual DP monitor\n",
+			    vgpu->id);
 		return -ENOMEM;
 	}
 
-	memcpy(port->edid->edid_block, virtual_dp_monitor_edid[resolution],
-			EDID_SIZE);
-	port->edid->data_valid = true;
+	if (edid)
+		memcpy(disp_path->edid->edid_block, edid,
+		       EDID_SIZE * valid_extensions);
+	else
+		memcpy(disp_path->edid->edid_block,
+		       virtual_dp_monitor_edid[disp_path->resolution],
+		       EDID_SIZE);
+
+	raw_edid = (struct edid *)disp_path->edid->edid_block;
+	if (READ_ONCE(vgpu->gvt->disp_edid_filter) &&
+		edid && ((raw_edid->version > 1) ||
+		(raw_edid->version == 1 && raw_edid->revision > 2))) {
+		filter_detailed_modes(raw_edid);
+		filter_standard_modes(raw_edid);
+		filter_established_modes(raw_edid);
+		filter_ext_blocks((u8 *)raw_edid);
+		gvt_edid_set_checksum((u8 *)raw_edid);
+		gvt_dbg_dpy(" vgpu:%d  edid filter is done\n",
+			vgpu->id);
+	} else {
+		if ((raw_edid->version == 1 && raw_edid->revision <= 2))
+			gvt_dbg_dpy("vgpu:%d  edid  is lower than 1.3\n",
+				vgpu->id);
+	}
 
-	memcpy(port->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
-	port->dpcd->data_valid = true;
-	port->dpcd->data[DPCD_SINK_COUNT] = 0x1;
-	port->type = type;
-	port->id = resolution;
+	disp_path->edid->data_valid = true;
 
-	emulate_monitor_status_change(vgpu);
+	memcpy(disp_path->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
+	disp_path->dpcd->data_valid = true;
+	disp_path->dpcd->data[DPCD_SINK_COUNT] = 0x1;
+
+	gvt_dbg_dpy("vgpu:%d setup edid:%p on virtual DP monitor on port:%d, type:%d\n",
+		    vgpu->id, edid, disp_path->port, disp_path->port_type);
 
 	return 0;
 }
@@ -358,32 +514,20 @@ static int setup_virtual_dp_monitor(struct intel_vgpu *vgpu, int port_num,
  * enabled/disabled virtual pipes.
  *
  */
-void intel_gvt_check_vblank_emulation(struct intel_gvt *gvt)
+void intel_gvt_check_vblank_emulation(struct intel_vgpu *vgpu, enum pipe pipe)
 {
+	struct intel_gvt *gvt = vgpu->gvt;
 	struct intel_gvt_irq *irq = &gvt->irq;
-	struct intel_vgpu *vgpu;
-	int pipe, id;
-	int found = false;
 
 	mutex_lock(&gvt->lock);
-	for_each_active_vgpu(gvt, vgpu, id) {
-		for (pipe = 0; pipe < I915_MAX_PIPES; pipe++) {
-			if (pipe_is_enabled(vgpu, pipe)) {
-				found = true;
-				break;
-			}
-		}
-		if (found)
-			break;
-	}
 
-	/* all the pipes are disabled */
-	if (!found)
-		hrtimer_cancel(&irq->vblank_timer.timer);
-	else
+	if (pipe_is_enabled(vgpu, pipe))
 		hrtimer_start(&irq->vblank_timer.timer,
 			ktime_add_ns(ktime_get(), irq->vblank_timer.period),
 			HRTIMER_MODE_ABS);
+	else
+		hrtimer_cancel(&irq->vblank_timer.timer);
+
 	mutex_unlock(&gvt->lock);
 }
 
@@ -398,7 +542,7 @@ static void emulate_vblank_on_pipe(struct intel_vgpu *vgpu, int pipe)
 	};
 	int event;
 
-	if (pipe < PIPE_A || pipe > PIPE_C)
+	if (pipe == INVALID_PIPE || pipe >= INTEL_NUM_PIPES(dev_priv))
 		return;
 
 	for_each_set_bit(event, irq->flip_done_event[pipe],
@@ -410,20 +554,8 @@ static void emulate_vblank_on_pipe(struct intel_vgpu *vgpu, int pipe)
 		intel_vgpu_trigger_virtual_event(vgpu, event);
 	}
 
-	if (pipe_is_enabled(vgpu, pipe)) {
-		vgpu_vreg_t(vgpu, PIPE_FRMCOUNT_G4X(pipe))++;
-		intel_vgpu_trigger_virtual_event(vgpu, vblank_event[pipe]);
-	}
-}
-
-static void emulate_vblank(struct intel_vgpu *vgpu)
-{
-	int pipe;
-
-	mutex_lock(&vgpu->vgpu_lock);
-	for_each_pipe(vgpu->gvt->dev_priv, pipe)
-		emulate_vblank_on_pipe(vgpu, pipe);
-	mutex_unlock(&vgpu->vgpu_lock);
+	vgpu_vreg_t(vgpu, PIPE_FRMCOUNT_G4X(pipe))++;
+	intel_vgpu_trigger_virtual_event(vgpu, vblank_event[pipe]);
 }
 
 /**
@@ -436,14 +568,375 @@ static void emulate_vblank(struct intel_vgpu *vgpu)
 void intel_gvt_emulate_vblank(struct intel_gvt *gvt)
 {
 	struct intel_vgpu *vgpu;
+	struct intel_vgpu_display *disp_cfg = NULL;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	int id;
+
+	mutex_lock(&gvt->lock);
+	for_each_active_vgpu(gvt, vgpu, id) {
+		mutex_lock(&vgpu->vgpu_lock);
+		disp_cfg = &vgpu->disp_cfg;
+		list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+			if (disp_path->p_pipe == INVALID_PIPE ||
+			    disp_path->p_pipe != gvt->pipe_info[disp_path->p_pipe].pipe_num ||
+			    vgpu->id != gvt->pipe_info[disp_path->p_pipe].owner) {
+				emulate_vblank_on_pipe(vgpu, disp_path->pipe);
+			} else {
+				u64 ns = jiffies_to_nsecs(get_jiffies_64());
+
+				if ((ns - disp_path->last_hwvsync_ns > 2 * disp_path->vsync_interval_ns) &&
+				    !test_and_clear_bit(INTEL_GVT_DIRECT_DISPLAY_HW_VSYNC, (void *)&disp_path->stat)) {
+					emulate_vblank_on_pipe(vgpu, disp_path->pipe);
+					disp_path->sw_vsync_injected++;
+					gvt_dbg_dpy("vgpu:%d inject vsync %lld to PIPE_%c <- HW vsync miss on PIPE_%c\n",
+						    vgpu->id,
+						    disp_path->sw_vsync_injected,
+						    pipe_name(disp_path->pipe),
+						    pipe_name(disp_path->p_pipe));
+				}
+			}
+		}
+		mutex_unlock(&vgpu->vgpu_lock);
+	}
+	mutex_unlock(&gvt->lock);
+}
+
+static void intel_gvt_vblank_work(struct work_struct *w)
+{
+	struct intel_gvt_pipe_info *pipe_info = container_of(w,
+			struct intel_gvt_pipe_info, vblank_work);
+	struct intel_gvt *gvt = pipe_info->gvt;
+	struct intel_vgpu *vgpu;
+	struct intel_vgpu_display *disp_cfg = NULL;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	int id;
+
+	mutex_lock(&gvt->lock);
+	for_each_active_vgpu(gvt, vgpu, id) {
+		mutex_lock(&vgpu->vgpu_lock);
+		disp_cfg = &vgpu->disp_cfg;
+		mutex_lock(&disp_cfg->sw_lock);
+		list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+			if (disp_path->p_pipe != INVALID_PIPE &&
+			    disp_path->p_pipe == pipe_info->pipe_num &&
+			    vgpu->id == pipe_info->owner &&
+			    pipe_is_enabled(vgpu, disp_path->pipe)) {
+				emulate_vblank_on_pipe(vgpu, disp_path->pipe);
+				disp_path->last_hwvsync_ns = jiffies_to_nsecs(get_jiffies_64());
+				set_bit(INTEL_GVT_DIRECT_DISPLAY_HW_VSYNC, (void *)&disp_path->stat);
+			}
+		}
+		mutex_unlock(&disp_cfg->sw_lock);
+		mutex_unlock(&vgpu->vgpu_lock);
+	}
+	mutex_unlock(&gvt->lock);
+}
+
+static void intel_gvt_flipdone_work(struct work_struct *w)
+{
+	struct intel_gvt_plane_info *plane_info = container_of(w,
+			struct intel_gvt_plane_info, flipdone_work);
+	struct intel_gvt *gvt = plane_info->gvt;
+	struct intel_vgpu *vgpu;
+	struct intel_vgpu_display *disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
 	int id;
+	int event = SKL_FLIP_EVENT(plane_info->pipe, plane_info->plane);
+
+	mutex_lock(&gvt->lock);
+	for_each_active_vgpu(gvt, vgpu, id) {
+		mutex_lock(&vgpu->vgpu_lock);
+		disp_cfg = &vgpu->disp_cfg;
+		mutex_lock(&disp_cfg->sw_lock);
+		list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+			if (plane_info->pipe == disp_path->p_pipe)
+				intel_vgpu_trigger_virtual_event(vgpu, event);
+		}
+		mutex_unlock(&disp_cfg->sw_lock);
+		mutex_unlock(&vgpu->vgpu_lock);
+	}
+	mutex_unlock(&gvt->lock);
+}
+
+static void intel_gvt_init_pipe_info(struct intel_gvt *gvt)
+{
+	enum pipe pipe = INVALID_PIPE;
+	enum plane_id plane;
+
+	for (pipe = PIPE_A; pipe < I915_MAX_PIPES; pipe++) {
+		memset(&gvt->pipe_info[pipe], 0, sizeof(gvt->pipe_info[pipe]));
+		gvt->pipe_info[pipe].pipe_num = pipe;
+		gvt->pipe_info[pipe].gvt = gvt;
+		INIT_WORK(&gvt->pipe_info[pipe].vblank_work,
+			  intel_gvt_vblank_work);
+		for (plane = PLANE_PRIMARY; plane < I915_MAX_PLANES; plane++) {
+			memset(&gvt->pipe_info[pipe].plane_info[plane], 0,
+			       sizeof(gvt->pipe_info[pipe].plane_info[plane]));
+			gvt->pipe_info[pipe].plane_info[plane].gvt = gvt;
+			gvt->pipe_info[pipe].plane_info[plane].pipe = pipe;
+			gvt->pipe_info[pipe].plane_info[plane].plane = plane;
+			INIT_WORK(&gvt->pipe_info[pipe].plane_info[plane].flipdone_work,
+				  intel_gvt_flipdone_work);
+		}
+	}
+	// By default, each vGPU will be assigned a different available port
+	// when host setup each port on detection.
+	// Display auto switch is enabled by default.
+	WRITE_ONCE(gvt->disp_auto_switch, true);
+	gvt->disp_owner = 0;
+}
+
+static void intel_gvt_connector_change_work(struct work_struct *w)
+{
+	struct intel_gvt *gvt = container_of(w,
+			struct intel_gvt, connector_change_work);
+	enum pipe pipe = INVALID_PIPE;
+	u8 port_ext = 0;
+	u8 id = 0;
 
+	/*
+	 * Doesn't support hotplug after vgpu created.
+	 */
+	gvt_dbg_dpy("Host connector status changed\n");
 	mutex_lock(&gvt->lock);
-	for_each_active_vgpu(gvt, vgpu, id)
-		emulate_vblank(vgpu);
+
+	if (!idr_is_empty(&gvt->vgpu_idr)) {
+		mutex_unlock(&gvt->lock);
+		gvt_dbg_dpy("Available port mask %08x and selected mask 0x%016llx "
+			    "unchanged due to hotplug after vGPU creation\n",
+			    gvt->avail_disp_port_mask, gvt->sel_disp_port_mask);
+		return;
+	}
+
+	gvt->avail_disp_port_mask = 0;
+	gvt->sel_disp_port_mask = 0;
+	for_each_pipe(gvt->dev_priv, pipe) {
+		enum port port;
+
+		port = intel_gvt_port_from_pipe(gvt->dev_priv, pipe);
+		if (port == PORT_NONE)
+			continue;
+
+		port_ext = intel_gvt_external_disp_id_from_port(port);
+		// Available port is set in corresponding port position.
+		gvt->avail_disp_port_mask |= (port_ext << port * 4);
+		// Available port is assigned to vGPU in sequence.
+		gvt->sel_disp_port_mask |= ((1 << port) << id * 8);
+		++id;
+		gvt_dbg_dpy("PIPE_%c PORT_%c is default assigned to vGPU-%d\n",
+			    pipe_name(pipe), port_name(port), id);
+	}
 	mutex_unlock(&gvt->lock);
 }
 
+static void intel_gvt_ddb_entry_write(struct drm_i915_private *dev_priv,
+				      i915_reg_t reg,
+				      const struct skl_ddb_entry *entry)
+{
+	if (entry->end)
+		I915_WRITE_FW(reg, (entry->end - 1) << 16 | entry->start);
+	else
+		I915_WRITE_FW(reg, 0);
+}
+/*
+ * When enabling multi-plane in DomU, an issue is that the PLANE_BUF_CFG
+ * register cannot be updated dynamically, since Dom0 has no idea of the
+ * plane information of DomU's planes, so here we statically allocate the
+ * ddb entries for all the possible enabled planes.
+ */
+static void intel_gvt_init_ddb(struct intel_gvt *gvt)
+{
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_device_info *info = mkwrite_device_info(dev_priv);
+	struct intel_gvt_pipe_info *pipe_info;
+	unsigned int pipe_size, ddb_size, plane_size, plane_cnt;
+	u16 start, end;
+	enum pipe pipe = INVALID_PIPE;
+	enum plane_id plane;
+
+	ddb_size = info->ddb_size;
+	ddb_size -= 4; /* 4 blocks for bypass path allocation */
+	pipe_size = ddb_size / INTEL_NUM_PIPES(dev_priv);
+
+	for_each_pipe(dev_priv, pipe) {
+		pipe_info = &gvt->pipe_info[pipe];
+		memset(&pipe_info->ddb_y, 0, sizeof(pipe_info->ddb_y));
+		memset(&pipe_info->ddb_uv, 0, sizeof(pipe_info->ddb_uv));
+		start = pipe * ddb_size / INTEL_NUM_PIPES(dev_priv);
+		end = start + pipe_size;
+
+		pipe_info->ddb_y[PLANE_CURSOR].start = end - 8;
+		pipe_info->ddb_y[PLANE_CURSOR].end = end;
+		intel_gvt_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe),
+					  &pipe_info->ddb_y[PLANE_CURSOR]);
+
+		// Disable other plane to support 4K on primary
+		plane_cnt = RUNTIME_INFO(dev_priv)->num_sprites[(pipe)] + 1;
+		plane_cnt = 1;
+		plane_size = (pipe_size - 8) / plane_cnt;
+		for_each_universal_plane(dev_priv, pipe, plane) {
+			if (plane == PLANE_PRIMARY) {
+				pipe_info->ddb_y[plane].start = start +
+					plane * plane_size;
+				pipe_info->ddb_y[plane].end =
+					pipe_info->ddb_y[plane].start + plane_size;
+			}
+			intel_gvt_ddb_entry_write(dev_priv, PLANE_BUF_CFG(pipe, plane),
+						  &pipe_info->ddb_y[plane]);
+			intel_gvt_ddb_entry_write(dev_priv, PLANE_NV12_BUF_CFG(pipe, plane),
+						  &pipe_info->ddb_uv[plane]);
+		}
+	}
+}
+
+static int intel_gvt_switch_pipe_owner(struct intel_gvt *gvt, enum pipe pipe,
+				       int owner)
+{
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_gvt_pipe_info *pipe_info = gvt->pipe_info;
+	struct intel_vgpu *vgpu;
+	enum plane_id plane;
+	bool valid_owner = false;
+	int id;
+
+	if (pipe == INVALID_PIPE || pipe >= INTEL_NUM_PIPES(dev_priv)) {
+		gvt_err("Invalid PIPE_%c for new owner %d\n",
+			pipe_name(pipe), owner);
+		return -EINVAL;
+	}
+
+	for_each_active_vgpu(gvt, vgpu, id) {
+		if (owner == id) {
+			valid_owner = true;
+			break;
+		}
+	}
+
+	if (owner && !valid_owner) {
+		gvt_err("Invalid owner %d for PIPE_%c\n", owner,
+			pipe_name(pipe));
+		return -EINVAL;
+	}
+
+	pipe_info[pipe].owner = owner;
+	for (plane = PLANE_PRIMARY; plane < I915_MAX_PLANES - 1; plane++)
+		pipe_info[pipe].plane_info[plane].owner = owner;
+
+	if (owner)
+		gvt_dbg_dpy("PIPE_%c owner switched to vGPU-%d\n",
+			    pipe_name(pipe), owner);
+	else
+		gvt_dbg_dpy("PIPE_%c owner switched to host\n", pipe_name(pipe));
+
+	return 0;
+}
+
+int setup_vgpu_virtual_display_path(struct intel_vgpu *vgpu,
+				    struct intel_vgpu_display_path *disp_path)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_connector *connector = NULL;
+	struct drm_connector_list_iter conn_iter;
+	struct intel_digital_port *dig_port;
+	struct intel_crtc *intel_crtc;
+	int ret = -EINVAL;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return ret;
+	}
+
+	intel_vgpu_init_i2c_edid(vgpu, &disp_path->i2c_edid);
+
+	disp_path->p_pipe = intel_gvt_pipe_from_port(vgpu->gvt->dev_priv, disp_path->p_port);
+	disp_path->pipe = disp_path->id - 1 + PIPE_A;
+	disp_path->trans = disp_path->id - 1 + TRANSCODER_A;
+	if (IS_GEN9_LP(dev_priv)) {
+		disp_path->port = disp_path->id - 1 + PORT_A;
+		if (disp_path->port == PORT_A) {
+			disp_path->port_type = INTEL_OUTPUT_EDP;
+			disp_path->trans = TRANSCODER_EDP;
+		}
+
+	} else {
+		disp_path->port = disp_path->id - 1 + PORT_B;
+		disp_path->port_type = INTEL_OUTPUT_DP;
+	}
+
+	clear_bit(INTEL_GVT_DIRECT_DISPLAY_HW_VSYNC, (void *)&disp_path->stat);
+	disp_path->sw_vsync_injected = 0;
+	disp_path->vsync_interval_ns = vgpu->gvt->irq.vblank_timer.period;
+	disp_path->last_hwvsync_ns = 0;
+
+	if (disp_path->p_port == PORT_NONE) {
+		ret = setup_virtual_dp_monitor_edid(vgpu, disp_path, NULL);
+		gvt_dbg_dpy("Virtual DP monitor is assigned to vgpu-%d PORT_%c\n",
+			    vgpu->id, port_name(disp_path->port));
+	} else {
+		gvt_dbg_dpy("Host PIPE_%c PORT_%c is assigned to vgpu-%d PORT_%c\n",
+			    pipe_name(disp_path->p_pipe), port_name(disp_path->p_port),
+			    vgpu->id, port_name(disp_path->port));
+	}
+
+	drm_connector_list_iter_begin(&vgpu->gvt->dev_priv->drm, &conn_iter);
+	for_each_intel_connector_iter(connector, &conn_iter) {
+		dig_port = enc_to_dig_port(&connector->encoder->base);
+		if (connector->detect_edid && dig_port) {
+			if (dig_port->base.port == disp_path->p_port) {
+				ret = setup_virtual_dp_monitor_edid(vgpu, disp_path, connector->detect_edid);
+			}
+		}
+	}
+	drm_connector_list_iter_end(&conn_iter);
+
+	for_each_intel_crtc(&vgpu->gvt->dev_priv->drm, intel_crtc) {
+		drm_modeset_lock(&intel_crtc->base.mutex, NULL);
+		if (disp_path->p_pipe == intel_crtc->pipe) {
+			struct drm_display_mode *mode = &(&intel_crtc->base)->mode;
+
+			if (mode->vrefresh)
+				disp_path->vsync_interval_ns = NSEC_PER_SEC / mode->vrefresh;
+			drm_modeset_unlock(&intel_crtc->base.mutex);
+			break;
+		}
+		drm_modeset_unlock(&intel_crtc->base.mutex);
+	}
+
+	if (vgpu->gvt->irq.vblank_timer.period > disp_path->vsync_interval_ns) {
+		gvt_dbg_dpy("vgpu:%d vblank timer freq (%lld) is slower than HW vsync freq (%lld) on PIPE_%c\n",
+			    vgpu->id, vgpu->gvt->irq.vblank_timer.period,
+			    disp_path->vsync_interval_ns,
+			    pipe_name(disp_path->p_pipe));
+	}
+
+	return ret;
+}
+
+void clean_vgpu_virtual_display_path(struct intel_vgpu *vgpu,
+				     struct intel_vgpu_display_path *disp_path)
+{
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return;
+	}
+
+	clean_virtual_dp_monitor(vgpu, disp_path);
+
+	disp_path->id = 0;
+	disp_path->resolution = GVT_EDID_NUM;
+	disp_path->edid_id = 0;
+	disp_path->p_port = PORT_NONE;
+	disp_path->p_pipe = INVALID_PIPE;
+	disp_path->port = PORT_NONE;
+	disp_path->pipe = INVALID_PIPE;
+	clear_bit(INTEL_GVT_DIRECT_DISPLAY_HW_VSYNC, (void *)&disp_path->stat);
+	disp_path->sw_vsync_injected = 0;
+	disp_path->last_hwvsync_ns = 0;
+
+	gvt_dbg_dpy("vgpu-%d virtual display path-%d cleaned\n",
+		    vgpu->id, disp_path->id);
+}
+
 /**
  * intel_vgpu_emulate_hotplug - trigger hotplug event for vGPU
  * @vgpu: a vGPU
@@ -484,13 +977,14 @@ void intel_vgpu_emulate_hotplug(struct intel_vgpu *vgpu, bool connected)
  */
 void intel_vgpu_clean_display(struct intel_vgpu *vgpu)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
 
-	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
-	    IS_COFFEELAKE(dev_priv))
-		clean_virtual_dp_monitor(vgpu, PORT_D);
-	else
-		clean_virtual_dp_monitor(vgpu, PORT_B);
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		clean_vgpu_virtual_display_path(vgpu, disp_path);
+		list_del_init(&disp_path->list);
+		kfree(disp_path);
+	}
 }
 
 /**
@@ -506,17 +1000,74 @@ void intel_vgpu_clean_display(struct intel_vgpu *vgpu)
  */
 int intel_vgpu_init_display(struct intel_vgpu *vgpu, u64 resolution)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum port port = PORT_NONE;
+	u64 *port_mask = &vgpu->gvt->sel_disp_port_mask;
+	u32 path = 0;
+	int ret = -EINVAL;
 
-	intel_vgpu_init_i2c_edid(vgpu);
+	memset(disp_cfg, 0, sizeof(*disp_cfg));
 
-	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
-	    IS_COFFEELAKE(dev_priv))
-		return setup_virtual_dp_monitor(vgpu, PORT_D, GVT_DP_D,
-						resolution);
-	else
-		return setup_virtual_dp_monitor(vgpu, PORT_B, GVT_DP_B,
-						resolution);
+	INIT_LIST_HEAD(&disp_cfg->path_list);
+	mutex_init(&disp_cfg->sw_lock);
+
+	// Check the number of host display pipe/port assigned and create paths.
+	// If no host pipe/port assigned, create 1 path for virtual display.
+	for (port = PORT_A; port < I915_MAX_PORTS; port++) {
+		if ((*port_mask >> (vgpu->id - 1) * 8) & 0xFF & (1 << port)) {
+			path++;
+			disp_path = kmalloc(sizeof(*disp_path), GFP_KERNEL);
+			if (unlikely(!disp_path)) {
+				gvt_vgpu_err("vgpu-%d alloc display path failed\n", vgpu->id);
+				goto out_free_list;
+			}
+			memset(disp_path, 0, sizeof(*disp_path));
+			disp_path->id = path;
+			disp_path->resolution = GVT_EDID_NUM;
+			disp_path->edid_id = GVT_EDID_NUM;
+			disp_path->foreground = false;
+			disp_path->foreground_state = false;
+			disp_path->p_port = port;
+			INIT_LIST_HEAD(&disp_path->list);
+			list_add_tail(&disp_path->list, &disp_cfg->path_list);
+		}
+	}
+
+	if (!disp_path) {
+		disp_path = kmalloc(sizeof(*disp_path), GFP_KERNEL);
+		if (unlikely(!disp_path)) {
+			gvt_vgpu_err("vgpu-%d alloc display path failed\n", vgpu->id);
+			goto out_free_list;
+		}
+		memset(disp_path, 0, sizeof(*disp_path));
+		disp_path->id = 1;
+		disp_path->resolution = resolution;
+		disp_path->edid_id = resolution;
+		disp_path->foreground = false;
+		disp_path->foreground_state = false;
+		disp_path->p_port = PORT_NONE;
+		INIT_LIST_HEAD(&disp_path->list);
+		list_add_tail(&disp_path->list, &disp_cfg->path_list);
+	}
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		ret = setup_vgpu_virtual_display_path(vgpu, disp_path);
+		if (ret != 0) {
+			gvt_dbg_dpy("vgpu-%d setup virtual display path-%d failed:%d\n",
+				    vgpu->id, disp_path->id, ret);
+			goto out_free_list;
+		}
+	}
+
+	if (ret == 0)
+		emulate_monitor_status_change(vgpu);
+
+	return ret;
+
+out_free_list:
+	intel_vgpu_clean_display(vgpu);
+	return -ENOMEM;
 }
 
 /**
@@ -530,3 +1081,1408 @@ void intel_vgpu_reset_display(struct intel_vgpu *vgpu)
 {
 	emulate_monitor_status_change(vgpu);
 }
+
+u8 intel_gvt_external_disp_id_from_port(enum port port)
+{
+	if (port > PORT_NONE && port < I915_MAX_PORTS)
+		return (u8)(port + 1);
+	else
+		return 0;
+}
+
+enum port intel_gvt_port_from_external_disp_id(u8 port_id)
+{
+	enum port port = PORT_NONE;
+
+	if (port_id)
+		port = (enum port)(port_id - 1);
+	return port;
+}
+
+enum pipe intel_gvt_pipe_from_port(struct drm_i915_private *dev_priv,
+				   enum port port)
+{
+	enum pipe pipe = INVALID_PIPE;
+	struct drm_device *dev = &dev_priv->drm;
+	struct intel_crtc *crtc_intel;
+	struct drm_crtc *crtc_drm;
+	struct intel_encoder *intel_encoder;
+
+	if (port == PORT_NONE)
+		return INVALID_PIPE;
+
+	for_each_intel_crtc(dev, crtc_intel) {
+		struct intel_crtc_state *pipe_config;
+
+		drm_modeset_lock(&crtc_intel->base.mutex, NULL);
+		pipe_config = to_intel_crtc_state(crtc_intel->base.state);
+		if (pipe_config->hw.active && pipe == INVALID_PIPE) {
+			crtc_drm = &crtc_intel->base;
+			for_each_encoder_on_crtc(dev, crtc_drm, intel_encoder) {
+				if (intel_encoder->port == port &&
+				    pipe == INVALID_PIPE) {
+					pipe = crtc_intel->pipe;
+				}
+			}
+		}
+		drm_modeset_unlock(&crtc_intel->base.mutex);
+	}
+
+	if (pipe != INVALID_PIPE) {
+		gvt_dbg_dpy("PORT_%c is connected to PIPE_%d\n",
+			    port_name(port), pipe);
+	} else {
+		gvt_dbg_dpy("PORT_%c isn't assigned to any pipe\n",
+			    pipe_name(port));
+	}
+
+	return pipe;
+}
+
+enum port intel_gvt_port_from_pipe(struct drm_i915_private *dev_priv,
+				   enum pipe pipe)
+{
+	enum port port = PORT_NONE;
+	struct drm_device *dev = &dev_priv->drm;
+	struct intel_crtc *crtc_intel;
+	struct drm_crtc *crtc_drm;
+	struct intel_encoder *intel_encoder;
+
+	if (pipe == INVALID_PIPE)
+		gvt_err("Unable to find port from INVALID_PIPE\n");
+
+	for_each_intel_crtc(dev, crtc_intel) {
+		struct intel_crtc_state *pipe_config;
+
+		drm_modeset_lock(&crtc_intel->base.mutex, NULL);
+		pipe_config = to_intel_crtc_state(crtc_intel->base.state);
+		if (pipe_config->hw.active && pipe == crtc_intel->pipe) {
+			crtc_drm = &crtc_intel->base;
+			for_each_encoder_on_crtc(dev, crtc_drm, intel_encoder) {
+				if (port == PORT_NONE)
+					port = intel_encoder->port;
+			}
+		}
+		drm_modeset_unlock(&crtc_intel->base.mutex);
+	}
+
+	if (port != PORT_NONE)
+		gvt_dbg_dpy("PIPE_%c is attached PORT_%c\n",
+			    pipe_name(pipe), port_name(port));
+	else
+		gvt_dbg_dpy("No port attached to PIPE_%c\n", pipe_name(pipe));
+
+	return port;
+}
+
+int skl_format_to_fourcc(int format, bool rgb_order, bool alpha);
+uint_fixed_16_16_t
+skl_wm_method1(const struct drm_i915_private *dev_priv, uint32_t pixel_rate,
+	       uint8_t cpp, uint32_t latency, uint32_t dbuf_block_size);
+uint_fixed_16_16_t
+skl_wm_method2(uint32_t pixel_rate,
+	       uint32_t pipe_htotal,
+	       uint32_t latency,
+	       uint_fixed_16_16_t plane_blocks_per_line);
+uint_fixed_16_16_t intel_get_linetime_us(struct intel_crtc_state *cstate);
+
+int
+vgpu_compute_plane_wm_params(struct intel_vgpu *vgpu,
+			    struct intel_crtc_state *intel_cstate,
+			    enum plane_id plane,
+			    struct skl_wm_params *wp)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_crtc *crtc = to_intel_crtc(intel_cstate->uapi.crtc);
+	struct intel_plane *prim_plane = to_intel_plane(crtc->base.primary);
+	struct intel_plane_state *prim_pstate = to_intel_plane_state(prim_plane->base.state);
+	uint32_t interm_pbpl;
+	const struct drm_format_info *info;
+	u64 original_pixel_rate;
+	uint_fixed_16_16_t downscale_amount;
+	u32 pipe_src_w, pipe_src_h, src_w, src_h, dst_w, dst_h;
+	uint_fixed_16_16_t fp_w_ratio, fp_h_ratio;
+	uint_fixed_16_16_t downscale_h, downscale_w;
+	bool apply_memory_bw_wa = IS_GEN9_BC(dev_priv) || IS_BROXTON(dev_priv);
+	bool rot_90_or_270;
+	int scaler, plane_scaler;
+	u32 reg_val;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe host_pipe = INVALID_PIPE;
+	enum pipe vgpu_pipe = INVALID_PIPE;
+
+	if (!intel_cstate->hw.active || !prim_pstate->uapi.visible)
+		return 0;
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+		if (disp_path->p_pipe == crtc->pipe)
+			break;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return -EINVAL;
+	}
+
+	host_pipe = disp_path->p_pipe;
+	vgpu_pipe = disp_path->pipe;
+
+	reg_val = vgpu_vreg_t(vgpu, PIPESRC(disp_path->trans));
+	pipe_src_w = ((reg_val >> 16) & 0xfff) + 1;
+	pipe_src_h = (reg_val & 0xfff) + 1;
+	original_pixel_rate = intel_cstate->pixel_rate;
+
+	rot_90_or_270 = false;
+	if (plane == PLANE_CURSOR) {
+		reg_val = vgpu_vreg_t(vgpu, CURCNTR(vgpu_pipe));
+		switch (reg_val & SKL_CURSOR_MODE_MASK) {
+		case 0x04:
+		case 0x05:
+		case 0x06:
+		case 0x07:
+		case 0x24:
+		case 0x27:
+			wp->width = 64;
+			break;
+		case 0x02:
+		case 0x22:
+		case 0x25:
+			wp->width = 128;
+			break;
+		case 0x03:
+		case 0x23:
+		case 0x26:
+			wp->width = 256;
+			break;
+		case 0:
+			wp->width = 0;
+			gvt_dbg_dpy("vgpu-%d: pipe(%d->%d) HW cursor is disabled\n",
+				    vgpu->id, vgpu_pipe, host_pipe);
+			return 0;
+		default:
+			wp->width = 0;
+			gvt_dbg_dpy("vgpu-%d: pipe(%d->%d) unsupported HW cursor mode %x\n",
+				    vgpu->id, vgpu_pipe, host_pipe, reg_val & SKL_CURSOR_MODE_MASK);
+			return 0;
+		}
+
+		// Cursor is always linear
+		wp->x_tiled = 0;
+		wp->y_tiled = 0;
+		wp->rc_surface = 0;
+
+		switch (reg_val & SKL_CURSOR_MODE_MASK) {
+		// 32bpp AND/INVERT
+		case 0x02:
+		case 0x03:
+		case 0x07:
+		// 32bpp ARGB
+		case 0x22:
+		case 0x23:
+		case 0x27:
+		// 32bpp AND/XOR
+		case 0x24:
+		case 0x25:
+		case 0x26:
+			wp->cpp = 4;
+			break;
+		// 2bpp 3/2/4 color
+		case 0x04:
+		case 0x05:
+		case 0x06:
+		default:
+			wp->width = 0;
+			wp->cpp = 0;
+			gvt_dbg_dpy("vgpu-%d: pipe(%d->%d) unsupported HW cursor format %x\n",
+				    vgpu->id, vgpu_pipe, host_pipe, reg_val & SKL_CURSOR_MODE_MASK);
+			return 0;
+		}
+	} else {
+		reg_val = vgpu_vreg_t(vgpu, PLANE_CTL(vgpu_pipe, plane));
+
+		wp->x_tiled = (reg_val & PLANE_CTL_TILED_MASK) == PLANE_CTL_TILED_X;
+		/*
+		* Check if Y tiled is one of the below:
+		* I915_FORMAT_MOD_Y_TILED
+		* I915_FORMAT_MOD_Yf_TILED
+		* I915_FORMAT_MOD_Y_TILED_CCS
+		* I915_FORMAT_MOD_Yf_TILED_CCS
+		*/
+		wp->y_tiled = ((reg_val & PLANE_CTL_TILED_MASK) == PLANE_CTL_TILED_Y) ||
+			((reg_val & PLANE_CTL_TILED_MASK) == PLANE_CTL_TILED_YF);
+		if ((reg_val & PLANE_CTL_RENDER_DECOMPRESSION_ENABLE) && wp->y_tiled)
+			wp->rc_surface = 1;
+		else
+			wp->rc_surface = 0;
+
+
+		info = drm_format_info(
+			skl_format_to_fourcc(reg_val & PLANE_CTL_FORMAT_MASK,
+					     reg_val & PLANE_CTL_ORDER_RGBX,
+					     reg_val & PLANE_CTL_ALPHA_MASK));
+		wp->cpp = info->cpp[0];
+
+		reg_val &= PLANE_CTL_ROTATE_MASK;
+		if (reg_val == PLANE_CTL_ROTATE_90 || reg_val == PLANE_CTL_ROTATE_270)
+			rot_90_or_270 = true;
+	}
+
+	if (plane == PLANE_CURSOR) {
+		src_w = wp->width;
+		src_h = wp->width;
+	} else {
+		reg_val = vgpu_vreg_t(vgpu, PLANE_SIZE(vgpu_pipe, plane));
+		src_w = (reg_val & 0xfff) + 1;
+		src_h = ((reg_val >> 16) & 0xfff) + 1;
+		if (rot_90_or_270)
+			wp->width = src_h;
+		else
+			wp->width = src_w;
+	}
+
+	// Assume host scaler has been rebuilt for vgpu
+	plane_scaler = -1;
+	for (scaler = 0; scaler < crtc->num_scalers; scaler++) {
+		reg_val = disp_path->scaler_cfg.ctrl[scaler];
+
+		if (reg_val & PS_SCALER_EN &&
+		    (reg_val & PS_PLANE_SEL(plane) ||
+		    !(reg_val & PS_PLANE_SEL_MASK))) {
+			plane_scaler = scaler;
+			break;
+		}
+	}
+
+	if (plane_scaler >= 0) {
+		reg_val = disp_path->scaler_cfg.win_size[scaler];
+		dst_w = reg_val >> 16 & 0xfff;
+		dst_h = reg_val & 0xfff;
+	} else {
+		dst_w = drm_rect_width(&prim_pstate->uapi.dst);
+		dst_h = drm_rect_height(&prim_pstate->uapi.dst);
+	}
+
+	fp_w_ratio = div_fixed16(src_w, dst_w);
+	fp_h_ratio = div_fixed16(src_h, dst_h);
+	downscale_w = max_fixed16(fp_w_ratio, u32_to_fixed16(1));
+	downscale_h = max_fixed16(fp_h_ratio, u32_to_fixed16(1));
+	downscale_amount = mul_fixed16(downscale_w, downscale_h);
+
+	wp->plane_pixel_rate = mul_round_up_u32_fixed16(original_pixel_rate,
+						    downscale_amount);
+
+	gvt_dbg_dpy("vgpu-%d: pipe(%d->%d), plane(%d), plane_ctl(%08x), scaler-%d, pipe src(%dx%d) src(%dx%d)->dst(%dx%d), pixel rate(%lld->%d)\n",
+		    vgpu->id, vgpu_pipe, host_pipe, plane,
+		    (plane == PLANE_CURSOR) ? vgpu_vreg_t(vgpu, CURCNTR(vgpu_pipe)) : vgpu_vreg_t(vgpu, PLANE_CTL(vgpu_pipe, plane)),
+		    plane_scaler, pipe_src_w, pipe_src_h,
+		    src_w, src_h, dst_w, dst_h, original_pixel_rate, wp->plane_pixel_rate);
+
+	reg_val = vgpu_vreg_t(vgpu, PLANE_CTL(vgpu_pipe, plane));
+	if (INTEL_GEN(dev_priv) >= 11 &&
+	    plane != PLANE_CURSOR &&
+	    !(reg_val & PLANE_CTL_RENDER_DECOMPRESSION_ENABLE) &&
+	    (reg_val & PLANE_CTL_TILED_YF) &&
+	    wp->cpp == 8)
+		wp->dbuf_block_size = 256;
+	else
+		wp->dbuf_block_size = 512;
+
+	if (rot_90_or_270) {
+		switch (wp->cpp) {
+		case 1:
+			wp->y_min_scanlines = 16;
+			break;
+		case 2:
+			wp->y_min_scanlines = 8;
+			break;
+		case 4:
+			wp->y_min_scanlines = 4;
+			break;
+		default:
+			MISSING_CASE(wp->cpp);
+			return -EINVAL;
+		}
+	} else {
+		wp->y_min_scanlines = 4;
+	}
+
+	if (apply_memory_bw_wa)
+		wp->y_min_scanlines *= 2;
+
+	wp->plane_bytes_per_line = wp->width * wp->cpp;
+	if (wp->y_tiled) {
+		interm_pbpl = DIV_ROUND_UP(wp->plane_bytes_per_line *
+					   wp->y_min_scanlines,
+					   wp->dbuf_block_size);
+
+		if (INTEL_GEN(dev_priv) >= 10)
+			interm_pbpl++;
+
+		wp->plane_blocks_per_line = div_fixed16(interm_pbpl,
+							wp->y_min_scanlines);
+	} else if (wp->x_tiled && IS_GEN(dev_priv, 9)) {
+		interm_pbpl = DIV_ROUND_UP(wp->plane_bytes_per_line,
+					   wp->dbuf_block_size);
+		wp->plane_blocks_per_line = u32_to_fixed16(interm_pbpl);
+	} else {
+		interm_pbpl = DIV_ROUND_UP(wp->plane_bytes_per_line,
+					   wp->dbuf_block_size) + 1;
+		wp->plane_blocks_per_line = u32_to_fixed16(interm_pbpl);
+	}
+
+	wp->y_tile_minimum = mul_u32_fixed16(wp->y_min_scanlines,
+					     wp->plane_blocks_per_line);
+	wp->linetime_us = fixed16_to_u32_round_up(
+					intel_get_linetime_us(intel_cstate));
+
+	gvt_dbg_dpy("vgpu-%d: pipe(%d->%d), plane(%d), x_tiled(%d), y_tiled(%d), rc_surface(%d), width(%x), cpp(%x), "
+		    "plane_pixel_rate(%d), y_min_scanlines(%x), plane_bytes_per_line(%x), plane_blocks_per_line(%x), "
+		    "y_tile_minimum(%x), linetime_us(%x), dbuf_block_size(%x)\n",
+		    vgpu->id, vgpu_pipe, host_pipe, plane, wp->x_tiled, wp->y_tiled, wp->rc_surface, wp->width, wp->cpp,
+		    wp->plane_pixel_rate, wp->y_min_scanlines, wp->plane_bytes_per_line, wp->plane_blocks_per_line.val,
+		    wp->y_tile_minimum.val, wp->linetime_us, wp->dbuf_block_size);
+
+	return 0;
+}
+
+int vgpu_compute_plane_wm(struct intel_vgpu *vgpu,
+			  struct intel_crtc_state *intel_cstate,
+			  enum plane_id plane,
+			  u16 ddb_allocation,
+			  int level,
+			  const struct skl_wm_params *wp,
+			  u16 *out_blocks, u8 *out_lines, bool *enabled)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	u32 latency = dev_priv->wm.skl_latency[level];
+	uint_fixed_16_16_t method1, method2;
+	uint_fixed_16_16_t selected_result;
+	u32 res_blocks, res_lines;
+	bool apply_memory_bw_wa = IS_GEN9_BC(dev_priv) || IS_BROXTON(dev_priv);
+	uint32_t min_disp_buf_needed;
+
+	if (latency == 0 ||
+	    !intel_cstate->hw.active ||
+	    wp->width == 0)
+		return 0;
+
+	/* Display WA #1141: kbl,cfl */
+	if ((IS_KABYLAKE(dev_priv) || IS_COFFEELAKE(dev_priv) ||
+	    IS_CNL_REVID(dev_priv, CNL_REVID_A0, CNL_REVID_B0)) &&
+	    dev_priv->ipc_enabled)
+		latency += 4;
+
+	if (apply_memory_bw_wa && wp->x_tiled)
+		latency += 15;
+
+	method1 = skl_wm_method1(dev_priv, wp->plane_pixel_rate,
+				 wp->cpp, latency, wp->dbuf_block_size);
+	method2 = skl_wm_method2(wp->plane_pixel_rate,
+				 intel_cstate->hw.adjusted_mode.crtc_htotal,
+				 latency,
+				 wp->plane_blocks_per_line);
+
+	if (wp->y_tiled) {
+		selected_result = max_fixed16(method2, wp->y_tile_minimum);
+	} else {
+		if ((wp->cpp * intel_cstate->hw.adjusted_mode.crtc_htotal /
+		     wp->dbuf_block_size < 1) &&
+		     (wp->plane_bytes_per_line / wp->dbuf_block_size < 1))
+			selected_result = method2;
+		else if (ddb_allocation >=
+			 fixed16_to_u32_round_up(wp->plane_blocks_per_line))
+			selected_result = min_fixed16(method1, method2);
+		else if (latency >= wp->linetime_us)
+			selected_result = min_fixed16(method1, method2);
+		else
+			selected_result = method1;
+	}
+
+	res_blocks = fixed16_to_u32_round_up(selected_result) + 1;
+	res_lines = div_round_up_fixed16(selected_result,
+					 wp->plane_blocks_per_line);
+
+	/* Display WA #1125: skl,bxt,kbl,glk */
+	if (level == 0 && wp->rc_surface)
+		res_blocks += fixed16_to_u32_round_up(wp->y_tile_minimum);
+
+	/* Display WA #1126: skl,bxt,kbl,glk */
+	if (level >= 1 && level <= 7) {
+		if (wp->y_tiled) {
+			res_blocks += fixed16_to_u32_round_up(
+							wp->y_tile_minimum);
+			res_lines += wp->y_min_scanlines;
+		} else {
+			res_blocks++;
+		}
+	}
+
+	if (INTEL_GEN(dev_priv) >= 11) {
+		if (wp->y_tiled) {
+			uint32_t extra_lines;
+			uint_fixed_16_16_t fp_min_disp_buf_needed;
+
+			if (res_lines % wp->y_min_scanlines == 0)
+				extra_lines = wp->y_min_scanlines;
+			else
+				extra_lines = wp->y_min_scanlines * 2 -
+					      res_lines % wp->y_min_scanlines;
+
+			fp_min_disp_buf_needed = mul_u32_fixed16(res_lines +
+						extra_lines,
+						wp->plane_blocks_per_line);
+			min_disp_buf_needed = fixed16_to_u32_round_up(
+						fp_min_disp_buf_needed);
+		} else {
+			min_disp_buf_needed = DIV_ROUND_UP(res_blocks * 11, 10);
+		}
+	} else {
+		min_disp_buf_needed = res_blocks;
+	}
+
+	if ((level > 0 && res_lines > 31) ||
+	    res_blocks >= ddb_allocation ||
+	    min_disp_buf_needed >= ddb_allocation) {
+		gvt_dbg_dpy("Requested display configuration exceeds system watermark limitations\n");
+		gvt_dbg_dpy("[PLANE:%d:%c] blocks required = %u/%u, lines required = %u/31\n",
+			    plane, plane_name(plane),
+			    res_blocks, ddb_allocation, res_lines);
+	}
+
+	/* The number of lines are ignored for the level 0 watermark. */
+	*out_lines = level ? res_lines : 0;
+	*out_blocks = res_blocks;
+	*enabled = true;
+
+	return 0;
+}
+
+void skl_compute_transition_wm(struct intel_crtc_state *cstate,
+				      struct skl_wm_params *wp,
+				      struct skl_wm_level *wm_l0,
+				      uint16_t ddb_allocation,
+				      struct skl_wm_level *trans_wm /* out */);
+
+inline u32 vgpu_calc_wm_level(const struct skl_wm_level *level)
+{
+	u32 val = 0;
+
+	if (level->plane_en) {
+		val |= PLANE_WM_EN;
+		val |= level->plane_res_b;
+		val |= level->plane_res_l << PLANE_WM_LINES_SHIFT;
+	}
+	return val;
+}
+
+void intel_vgpu_update_plane_scaler(struct intel_vgpu *vgpu,
+				    struct intel_crtc *intel_crtc, enum plane_id plane)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_runtime_info *runtime = RUNTIME_INFO(dev_priv);
+	struct intel_dom0_plane_regs *dom0_regs;
+	struct drm_display_mode *mode = NULL;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe host_pipe = INVALID_PIPE;
+	enum pipe vgpu_pipe = INVALID_PIPE;
+	u32 host_hactive = 0, host_vactive = 0;
+	u32 host_plane_w, host_plane_h;
+	u32 vreg;
+	u32 vgpu_hactive, vgpu_vactive;
+	u32 vgpu_plane_w, vgpu_plane_h;
+	u32 ps_win_pos[2], ps_win_size[2], ps_ctrl[2];
+	int max_scaler;
+	int scaler, vps_id, hps_id = 0;
+	bool scaler_passthru;
+
+	if (plane == PLANE_PRIMARY)
+		hps_id = 0;
+	else
+		return;
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+		if (disp_path->p_pipe == intel_crtc->pipe)
+			break;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return;
+	}
+
+	host_pipe = disp_path->p_pipe;
+	max_scaler = runtime->num_scalers[host_pipe];
+	vgpu_pipe = disp_path->pipe;
+
+	// Get host h/v active from active crtc status
+	mode = &(&intel_crtc->base)->mode;
+	if (mode) {
+		host_hactive = mode->hdisplay;
+		host_vactive = mode->vdisplay;
+	} else {
+		host_hactive = 0;
+		host_vactive = 0;
+		gvt_dbg_dpy("vgpu-%d: host PIPE %c h/v active is invalid (%dx%d)\n",
+			    vgpu->id, pipe_name(host_pipe), host_hactive, host_vactive);
+		return;
+	}
+
+	// Get host plane size from cache
+	dom0_regs = &gvt->pipe_info[host_pipe].plane_info[plane].dom0_regs;
+	host_plane_w = (dom0_regs->plane_size & 0xfff) + 1;
+	host_plane_h = ((dom0_regs->plane_size >> 16) & 0xfff) + 1;
+
+	// Decode vgpu h/v active from vgpu timing
+	vreg = vgpu_vreg_t(vgpu, HTOTAL(disp_path->trans));
+	vgpu_hactive = (vreg & 0xfff) + 1;
+	vreg = vgpu_vreg_t(vgpu, VTOTAL(disp_path->trans));
+	vgpu_vactive = (vreg & 0xfff) + 1;
+
+	// Decode vgpu plane size
+	vreg = vgpu_vreg_t(vgpu, PLANE_SIZE(vgpu_pipe, plane));
+	vgpu_plane_w = (vreg & 0xfff) + 1;
+	vgpu_plane_h = ((vreg >> 16) & 0xfff) + 1;
+
+	gvt_dbg_dpy("vgpu-%d: PIPE %c->%c h/v active (%dx%d)->(%dx%d)\n",
+		    vgpu->id, pipe_name(vgpu_pipe), pipe_name(host_pipe),
+		    vgpu_hactive, vgpu_vactive, host_hactive, host_vactive);
+	gvt_dbg_dpy("vgpu-%d: PIPE %c->%c plane-%d size (%dx%d)->(%dx%d)\n",
+		    vgpu->id, pipe_name(vgpu_pipe), pipe_name(host_pipe), plane,
+		    vgpu_plane_w, vgpu_plane_h, host_plane_w, host_plane_h);
+
+	vps_id = -1;
+	for (scaler = 0; scaler < max_scaler; scaler++) {
+		ps_win_pos[scaler] =
+			vgpu_vreg_t(vgpu, SKL_PS_WIN_POS(vgpu_pipe, scaler));
+		ps_win_size[scaler] =
+			vgpu_vreg_t(vgpu, SKL_PS_WIN_SZ(vgpu_pipe, scaler));
+		ps_ctrl[scaler] =
+			vgpu_vreg_t(vgpu, SKL_PS_CTRL(vgpu_pipe, scaler));
+
+		// Find which vgpu scaler is assigned to current plane or pipe
+		if (ps_ctrl[scaler] & PS_SCALER_EN &&
+		    (ps_ctrl[scaler] & PS_PLANE_SEL(plane) ||
+		    !(ps_ctrl[scaler] & PS_PLANE_SEL_MASK)))
+			vps_id = scaler;
+
+		gvt_dbg_dpy("vgpu-%d: PIPE %c->%c scaler-%d PS_CTRL(%08x) PS_WIN_POS(%08x) PS_WIN_SZ(%08x)\n",
+			    vgpu->id, pipe_name(vgpu_pipe),
+			    pipe_name(host_pipe), scaler, ps_ctrl[scaler],
+			    ps_win_pos[scaler], ps_win_size[scaler]);
+	}
+
+	/*
+	 * Decode vgpu scaler settings:
+	 * Assume host is always set to resolution matching its preferred timing.
+	 * If vgpu matches host timing:
+	 * - If scaler is enabled, surf can be stretched or centered (with offset).
+	 * - If scaler is not enabled, surf can be identitied or stretched by SW.
+	 * - In both cases we can pass vgpu scaler settings to host.
+	 * If vgpu doesn't match host timing:
+	 * - If scaler is enabled, then surf is using custom scaling ratio.
+	 *   We should decode the vgpu scaling ratio and apply to host scaler.
+	 * - If scaler is not enabled, surf is identitied to vgpu timing which is
+	 *   expected to be fullscreen. We should enable host scaler to stretch.
+	 * - In both cases we should ignore vgpu scaler settings and recalculate
+	 *   new settings for host.
+	 */
+	if (host_hactive == vgpu_hactive && host_vactive == vgpu_vactive) {
+		// If no scaler assigned to selected plane, disable the host scaler
+		disp_path->scaler_cfg.win_pos[hps_id] = 0;
+		disp_path->scaler_cfg.win_size[hps_id] = 0;
+		disp_path->scaler_cfg.ctrl[hps_id] = 0;
+		if (vps_id >= 0) {
+			disp_path->scaler_cfg.win_pos[hps_id] = ps_win_pos[vps_id];
+			disp_path->scaler_cfg.win_size[hps_id] = ps_win_size[vps_id];
+			disp_path->scaler_cfg.ctrl[hps_id] = ps_ctrl[vps_id];
+		}
+		scaler_passthru = true;
+	} else {
+		disp_path->scaler_cfg.win_pos[hps_id] = 0;
+		disp_path->scaler_cfg.win_size[hps_id] = (host_plane_w << 16) | host_plane_h;
+		disp_path->scaler_cfg.ctrl[hps_id] = PS_SCALER_EN;
+
+		if (vps_id >= 0) {
+			// Decode vgpu custom scaling ratio and calcuate host ratio
+			// Only handle downscaling case, i.e. plane size < h/v active
+			u32 vgpu_dst_w = ps_win_size[vps_id] >> 16 & 0xfff;
+			u32 vgpu_dst_h = ps_win_size[vps_id] & 0xfff;
+
+			if (vgpu_dst_w <= vgpu_hactive && vgpu_dst_h <= vgpu_vactive) {
+				u32 x_pct, y_pct, dst_w, dst_h, dst_x, dst_y;
+				u32 vgpu_x_scale_limit = vgpu_hactive * 12 / 100;
+				u32 vgpu_y_scale_limit = vgpu_vactive * 12 / 100;
+				u32 host_x_scale_limit = host_hactive * 12 / 100;
+				u32 host_y_scale_limit = host_vactive * 12 / 100;
+
+				x_pct = (vgpu_hactive - vgpu_dst_w) * 100 / vgpu_x_scale_limit;
+				y_pct = (vgpu_vactive - vgpu_dst_h) * 100 / vgpu_y_scale_limit;
+
+				dst_w = host_hactive - host_x_scale_limit * x_pct / 100;
+				dst_h = host_vactive - host_y_scale_limit * y_pct / 100;
+				dst_x = (host_hactive - dst_w) / 2;
+				dst_y = (host_vactive - dst_h) / 2;
+
+				disp_path->scaler_cfg.win_pos[hps_id] = (dst_x << 16) | dst_y;
+				disp_path->scaler_cfg.win_size[hps_id] = (dst_w << 16) | dst_h;
+
+				gvt_dbg_dpy("vgpu-%d: PIPE %c->%c scaler-%d "
+					    "customize aspect ratio from %dx%d -> %dx%d @ %dx%d "
+					    "to %dx%d -> %dx%d @ %dx%d, "
+					    "x: %d%%, y: %d%%\n",
+					    vgpu->id, pipe_name(vgpu_pipe), pipe_name(host_pipe),
+					    vps_id, vgpu_hactive, vgpu_vactive,
+					    vgpu_dst_w, vgpu_dst_h,
+					    ps_win_pos[vps_id] >> 16 & 0xfff,
+					    ps_win_pos[vps_id] & 0xfff,
+					    host_hactive, host_vactive, dst_w, dst_h,
+					    dst_x, dst_y,
+					    x_pct, y_pct);
+			}
+		}
+		scaler_passthru = false;
+	}
+
+	/*
+	 * In a transition state that PLANE_CTL_ENABLE and PS_SCALER_EN are
+	 * both on, SKL_PS_WIN_SZ shouldn't be 0, otherwise WM calculation
+	 * will hit divide by zero error. Disable scaler during transition
+	 * to prevent unexpected scaler and watermark settings.
+	 */
+	if (disp_path->scaler_cfg.ctrl[hps_id] & PS_SCALER_EN &&
+	    ((disp_path->scaler_cfg.win_size[hps_id] >> 16 & 0xfff) == 0 ||
+	    (disp_path->scaler_cfg.win_size[hps_id] & 0xfff) == 0)) {
+		disp_path->scaler_cfg.win_pos[hps_id] = 0;
+		disp_path->scaler_cfg.win_size[hps_id] = 0;
+		disp_path->scaler_cfg.ctrl[hps_id] = 0;
+		scaler_passthru = false;
+		gvt_dbg_dpy("vgpu-%d: disable scaler PIPE %c->%c scaler %d->%d "
+			    "for plane-%d during plane state transition",
+			    vgpu->id, pipe_name(vgpu_pipe),
+			    pipe_name(host_pipe), vps_id, hps_id, plane);
+	}
+
+	gvt_dbg_dpy("vgpu-%d: %s PIPE %c->%c scaler %d->%d for plane-%d "
+		    "PS_WIN_POS(%08x) PS_WIN_SZ(%08x) PS_CTRL(%08x)\n",
+		    vgpu->id, scaler_passthru ? "passthrough" : "override",
+		    pipe_name(vgpu_pipe), pipe_name(host_pipe),
+		    vps_id, hps_id, plane, disp_path->scaler_cfg.win_pos[hps_id],
+		    disp_path->scaler_cfg.win_size[hps_id],
+		    disp_path->scaler_cfg.ctrl[hps_id]);
+}
+
+void intel_vgpu_update_plane_wm(struct intel_vgpu *vgpu,
+				struct intel_crtc *intel_crtc, enum plane_id plane)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_crtc_state *intel_cstate = NULL;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe host_pipe = INVALID_PIPE;
+	enum pipe vgpu_pipe = INVALID_PIPE;
+	int level, max_level = ilk_wm_max_level(dev_priv);
+	struct skl_ddb_entry *ddb_y;
+	u16 ddb_blocks;
+	struct skl_plane_wm *wm;
+	struct skl_wm_params wm_params;
+	int ret;
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+		if (disp_path->p_pipe == intel_crtc->pipe)
+			break;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return;
+	}
+
+	host_pipe = disp_path->p_pipe;
+	vgpu_pipe = disp_path->pipe;
+
+	intel_cstate = to_intel_crtc_state(intel_crtc->base.state);
+
+	wm = &disp_path->wm_cfg.planes[plane];
+	ddb_y = gvt->pipe_info[host_pipe].ddb_y;
+	ddb_blocks = skl_ddb_entry_size(&ddb_y[plane]);
+	memset(&wm_params, 0, sizeof(struct skl_wm_params));
+	ret = vgpu_compute_plane_wm_params(vgpu, intel_cstate,
+						  plane, &wm_params);
+
+	for (level = 0; level <= max_level; level++) {
+		ret = vgpu_compute_plane_wm(vgpu,
+					    intel_cstate,
+					    plane,
+					    ddb_blocks,
+					    level,
+					    &wm_params,
+					    &wm->wm[level].plane_res_b,
+					    &wm->wm[level].plane_res_l,
+					    &wm->wm[level].plane_en);
+		gvt_dbg_dpy("vgpu-%d: pipe(%d->%d), plane(%d), level(%d), wm(%x)\n",
+			    vgpu->id, vgpu_pipe, host_pipe, plane, level,
+			    vgpu_calc_wm_level(&wm->wm[level]));
+		if (ret)
+			break;
+	}
+
+	skl_compute_transition_wm(intel_cstate, &wm_params,
+		&wm->wm[0], ddb_blocks, &wm->trans_wm);
+	gvt_dbg_dpy("vgpu-%d: pipe(%d->%d), plane(%d), wm_trans(%x)\n",
+		    vgpu->id, vgpu_pipe, host_pipe, plane,
+		    vgpu_calc_wm_level(&wm->trans_wm));
+}
+
+static int prepare_for_switch_display(struct intel_gvt *gvt, enum pipe pipe)
+{
+	const struct intel_crtc *crtc;
+
+	if (!(pipe >= PIPE_A && pipe <= PIPE_C)) {
+		gvt_err("Invalid PIPE_%c for v-blank wait\n", pipe_name(pipe));
+		return -EINVAL;
+	}
+
+	crtc = intel_get_crtc_for_pipe(gvt->dev_priv, pipe);
+	if (!crtc->active) {
+		gvt_err("CRTC attached to PIPE_%c is not active\n",
+			pipe_name(pipe));
+		return -EINVAL;
+	}
+
+	intel_wait_for_vblank(gvt->dev_priv, pipe);
+	return 0;
+}
+
+static void intel_gvt_switch_display_pipe(struct intel_gvt *gvt, enum pipe pipe,
+					  struct intel_vgpu *old_v,
+					  struct intel_vgpu *new_v)
+{
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_runtime_info *runtime = RUNTIME_INFO(dev_priv);
+	struct drm_device *dev = &dev_priv->drm;
+	struct intel_crtc *crtc = NULL;
+	struct intel_dom0_pipe_regs *d0_pipe_regs = NULL;
+	struct intel_vgpu_display_path *disp_path = NULL, *disp_path_old, *n;
+	enum pipe v_pipe = INVALID_PIPE;
+	enum plane_id plane = PLANE_PRIMARY;
+	int scaler, level, max_scaler = 0;
+	int max_level = ilk_wm_max_level(dev_priv);
+	unsigned long irqflags;
+
+	if (pipe == INVALID_PIPE || pipe > PIPE_C) {
+		gvt_err("Invalid PIPE_%c to switch\n", pipe_name(pipe));
+		return;
+	}
+
+	if (!old_v && !new_v) {
+		gvt_err("Can't switch display from host to host\n");
+		return;
+	}
+
+	for_each_intel_crtc(dev, crtc) {
+		drm_modeset_lock(&crtc->base.mutex, NULL);
+		if (pipe == crtc->pipe)
+			break;
+		drm_modeset_unlock(&crtc->base.mutex);
+	}
+
+	if (!crtc)
+		return;
+
+	d0_pipe_regs = &gvt->pipe_info[pipe].dom0_pipe_regs;
+	max_scaler = runtime->num_scalers[pipe];
+
+	if (new_v) {
+		list_for_each_entry_safe(disp_path, n, &new_v->disp_cfg.path_list, list) {
+			if (disp_path->p_pipe == pipe)
+				break;
+		}
+
+		if (!disp_path)
+			return;
+
+		v_pipe = disp_path->pipe;
+		mutex_lock(&new_v->disp_cfg.sw_lock);
+	}
+
+	if (intel_gvt_switch_pipe_owner(gvt, pipe, new_v ? new_v->id : 0)) {
+		gvt_err("Can't flush PIPE_%c due to owner switch fail\n",
+			pipe_name(pipe));
+		if (new_v)
+			mutex_unlock(&new_v->disp_cfg.sw_lock);
+		drm_modeset_unlock(&crtc->base.mutex);
+		return;
+	}
+
+	mmio_hw_access_pre(dev_priv);
+
+	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+	for_each_plane_id_on_crtc(crtc, plane) {
+		if (plane == PLANE_CURSOR) {
+			I915_WRITE_FW(CURCNTR(pipe), 0);
+			I915_WRITE_FW(CURBASE(pipe), 0);
+			I915_WRITE_FW(CUR_FBC_CTL(pipe), 0);
+			intel_uncore_posting_read_fw(&dev_priv->uncore, CURBASE(pipe));
+		} else {
+			I915_WRITE_FW(PLANE_CTL(pipe, plane), 0);
+			I915_WRITE_FW(PLANE_SURF(pipe, plane), 0);
+			intel_uncore_posting_read_fw(&dev_priv->uncore, PLANE_SURF(pipe, plane));
+		}
+	}
+
+	I915_WRITE_FW(PIPESRC(pipe), new_v ?
+		      vgpu_vreg_t(new_v, PIPESRC(v_pipe)) :
+		      d0_pipe_regs->pipesrc);
+
+	for (scaler = 0; scaler < max_scaler; scaler++) {
+		I915_WRITE_FW(SKL_PS_CTRL(pipe, scaler), new_v ?
+			      disp_path->scaler_cfg.ctrl[scaler] :
+			      d0_pipe_regs->scaler_ctl[scaler]);
+		I915_WRITE_FW(SKL_PS_PWR_GATE(pipe, scaler), new_v ? 0 :
+			      d0_pipe_regs->scaler_pwr_gate[scaler]);
+		I915_WRITE_FW(SKL_PS_WIN_POS(pipe, scaler), new_v ?
+			      disp_path->scaler_cfg.win_pos[scaler] :
+			      d0_pipe_regs->scaler_win_pos[scaler]);
+		I915_WRITE_FW(SKL_PS_WIN_SZ(pipe, scaler), new_v ?
+			      disp_path->scaler_cfg.win_size[scaler] :
+			      d0_pipe_regs->scaler_win_size[scaler]);
+	}
+
+	for_each_plane_id_on_crtc(crtc, plane) {
+		u32 ctl = 0, size = 0, pos = 0, offset = 0, stride = 0;
+		u32 keyval = 0, keymax = 0, keymsk = 0;
+		u32 fbc_ctl = 0, aux_dist = 0, aux_offset = 0;
+		u32 wm_level[8] = {0}, wm_trans = 0;
+		u32 hma = 0;
+		struct intel_dom0_plane_regs *d0_plane_regs =
+			&gvt->pipe_info[pipe].plane_info[plane].dom0_regs;
+
+		if (new_v && plane != PLANE_PRIMARY && plane != PLANE_CURSOR)
+			continue;
+
+		if (new_v) {
+			intel_vgpu_update_plane_wm(new_v, crtc, plane);
+			for (level = 0; level <= max_level; level++)
+				wm_level[level] = vgpu_calc_wm_level(&disp_path->wm_cfg.planes[plane].wm[level]);
+			wm_trans = vgpu_calc_wm_level(&disp_path->wm_cfg.planes[plane].trans_wm);
+		} else {
+			for (level = 0; level <= max_level; level++)
+				wm_level[level] = d0_plane_regs->plane_wm[level];
+			wm_trans = d0_plane_regs->plane_wm_trans;
+		}
+
+		if (plane == PLANE_CURSOR) {
+			if (new_v) {
+				pos = vgpu_vreg_t(new_v, CURPOS(v_pipe));
+				fbc_ctl = vgpu_vreg_t(new_v, CUR_FBC_CTL(v_pipe));
+				ctl = vgpu_vreg_t(new_v, CURCNTR(v_pipe));
+				hma = vgpu_vreg_t(new_v, CURBASE(v_pipe));
+			} else {
+				pos = d0_plane_regs->plane_pos;
+				fbc_ctl = d0_plane_regs->cur_fbc_ctl;
+				ctl = d0_plane_regs->plane_ctl;
+				hma = d0_plane_regs->plane_surf;
+			}
+			for (level = 0; level <= max_level; level++) {
+				I915_WRITE_FW(CUR_WM(pipe, level), wm_level[level]);
+			}
+			I915_WRITE_FW(CUR_WM_TRANS(pipe), wm_trans);
+			I915_WRITE_FW(CURPOS(pipe), pos);
+			I915_WRITE_FW(CUR_FBC_CTL(pipe), fbc_ctl);
+			I915_WRITE_FW(CURCNTR(pipe), ctl);
+			I915_WRITE_FW(CURBASE(pipe), hma);
+			intel_uncore_posting_read_fw(&dev_priv->uncore, CURBASE(pipe));
+		} else {
+			if (new_v) {
+				keyval = vgpu_vreg_t(new_v, PLANE_KEYVAL(v_pipe, plane));
+				keymax = vgpu_vreg_t(new_v, PLANE_KEYMAX(v_pipe, plane));
+				keymsk = vgpu_vreg_t(new_v, PLANE_KEYMSK(v_pipe, plane));
+				offset = vgpu_vreg_t(new_v, PLANE_OFFSET(v_pipe, plane));
+				stride = vgpu_vreg_t(new_v, PLANE_STRIDE(v_pipe, plane));
+				pos = vgpu_vreg_t(new_v, PLANE_POS(v_pipe, plane));
+				size = vgpu_vreg_t(new_v, PLANE_SIZE(v_pipe, plane));
+				aux_dist = vgpu_vreg_t(new_v, PLANE_AUX_DIST(v_pipe, plane));
+				aux_offset = vgpu_vreg_t(new_v, PLANE_AUX_OFFSET(v_pipe, plane));
+				ctl = vgpu_vreg_t(new_v, PLANE_CTL(v_pipe, plane));
+				hma = vgpu_vreg_t(new_v, PLANE_SURF(v_pipe, plane));
+			} else {
+				keyval = d0_plane_regs->plane_keyval;
+				keymax = d0_plane_regs->plane_keymax;
+				keymsk = d0_plane_regs->plane_keymsk;
+				offset = d0_plane_regs->plane_offset;
+				stride = d0_plane_regs->plane_stride;
+				pos = d0_plane_regs->plane_pos;
+				size = d0_plane_regs->plane_size;
+				aux_dist = d0_plane_regs->plane_aux_dist;
+				aux_offset = d0_plane_regs->plane_aux_offset;
+				ctl = d0_plane_regs->plane_ctl;
+				hma = d0_plane_regs->plane_surf;
+			}
+
+			if (!new_v || plane ==  PLANE_PRIMARY) {
+				for (level = 0; level <= max_level; level++) {
+					I915_WRITE_FW(PLANE_WM(pipe, plane, level), wm_level[level]);
+				}
+				I915_WRITE_FW(PLANE_WM_TRANS(pipe, plane), wm_trans);
+				I915_WRITE_FW(PLANE_KEYVAL(pipe, plane), keyval);
+				I915_WRITE_FW(PLANE_KEYMAX(pipe, plane), keymax);
+				I915_WRITE_FW(PLANE_KEYMSK(pipe, plane), keymsk);
+				I915_WRITE_FW(PLANE_OFFSET(pipe, plane), offset);
+				I915_WRITE_FW(PLANE_STRIDE(pipe, plane), stride);
+				I915_WRITE_FW(PLANE_POS(pipe, plane), pos);
+				I915_WRITE_FW(PLANE_SIZE(pipe, plane), size);
+				I915_WRITE_FW(PLANE_AUX_DIST(pipe, plane), aux_dist);
+				I915_WRITE_FW(PLANE_AUX_OFFSET(pipe, plane), aux_offset);
+				I915_WRITE_FW(PLANE_CTL(pipe, plane), ctl);
+				I915_WRITE_FW(PLANE_SURF(pipe, plane), hma);
+				intel_uncore_posting_read_fw(&dev_priv->uncore, PLANE_SURF(pipe, plane));
+			}
+
+			if (new_v) {
+				if (ctl & PLANE_CTL_ASYNC_FLIP)
+					intel_vgpu_trigger_virtual_event(new_v, SKL_FLIP_EVENT(v_pipe, plane));
+				else
+					set_bit(SKL_FLIP_EVENT(v_pipe, plane), new_v->irq.flip_done_event[v_pipe]);
+			}
+		}
+	}
+	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+
+	mmio_hw_access_post(dev_priv);
+
+	if (old_v) {
+		disp_path_old = NULL;
+		list_for_each_entry_safe(disp_path_old, n, &old_v->disp_cfg.path_list, list) {
+			if (disp_path_old->p_pipe == pipe)
+				disp_path_old->foreground_state = false;
+		}
+	}
+
+	if (new_v) {
+		disp_path->foreground_state = true;
+		mutex_unlock(&new_v->disp_cfg.sw_lock);
+	}
+
+	drm_modeset_unlock(&crtc->base.mutex);
+}
+
+static void intel_gvt_switch_display_work(struct work_struct *w)
+{
+	struct intel_gvt *gvt = container_of(w,
+		struct intel_gvt, switch_display_work);
+	struct intel_vgpu *vgpu, *old_v, *new_v;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	int id, old, new;
+	u32 new_owner = 0;
+	enum pipe pipe;
+	enum port port;
+	bool do_switch = false;
+
+	mutex_lock(&gvt->lock);
+	mutex_lock(&gvt->sw_in_progress);
+
+	/* Switch base on port */
+	for_each_pipe(gvt->dev_priv, pipe) {
+		port = intel_gvt_port_from_pipe(gvt->dev_priv, pipe);
+
+		if (port == PORT_NONE)
+			continue;
+
+		gvt_dbg_dpy("Check display switch on PIPE_%c, PORT_%c\n",
+			    pipe_name(pipe), port_name(port));
+
+		old = gvt->pipe_info[pipe].owner;
+		new = (gvt->disp_owner >> port * 4) & 0xF;
+
+		old_v = NULL;
+		new_v = NULL;
+		do_switch = false;
+		// Get old and new owners, either host or vGPU-x
+		idr_for_each_entry((&(gvt)->vgpu_idr), vgpu, id) {
+			if (old && id == old)
+				old_v = vgpu;
+			if (new && id == new)
+				new_v = vgpu;
+		}
+
+		if (new != old) {
+			if (new_v) {
+				disp_path = NULL;
+				list_for_each_entry_safe(disp_path, n, &new_v->disp_cfg.path_list, list) {
+					if (disp_path->p_pipe != INVALID_PIPE &&
+					    disp_path->p_port == port)
+						break;
+				}
+				if (atomic_read(&new_v->active) && disp_path &&
+				    disp_path->foreground &&
+				    !disp_path->foreground_state) {
+					do_switch = true;
+					gvt_dbg_dpy("Switch display to vGPU-%d, path-%d, PIPE_%c, PORT_%c\n",
+						    new_v->id, disp_path->id,
+						    pipe_name(disp_path->p_pipe),
+						    port_name(disp_path->p_port));
+				} else {
+					// If can't switch to new vGPU,
+					// maintain old owner.
+					new_owner = gvt->disp_owner;
+					new_owner &= ~(0xF << port * 4);
+					new_owner |= (old << port * 4);
+					gvt->disp_owner = new_owner;
+					do_switch = false;
+					gvt_dbg_dpy("Can't switch display to vGPU-%d active-%d, path-%d, PIPE_%c, PORT_%c, foreground-%d, foreground_state-%d\n",
+						    new_v->id, atomic_read(&new_v->active),
+						    disp_path->id,
+						    pipe_name(disp_path->p_pipe),
+						    port_name(disp_path->p_port),
+						    disp_path->foreground,
+						    disp_path->foreground_state);
+				}
+			} else {
+				// Switch to host is always expected possible
+				do_switch = true;
+				gvt_dbg_dpy("Switch display to host\n");
+			}
+		} else {
+			do_switch = false;
+			gvt_dbg_dpy("Skip switch display due to unchanged owner\n");
+		}
+
+		if (do_switch && !prepare_for_switch_display(gvt, pipe)) {
+			intel_gvt_switch_display_pipe(gvt, pipe, old_v, new_v);
+			if (old_v && new_v)
+				gvt_dbg_dpy("Change PIPE_%c PORT_%c owner from "
+					    "vGPU-%d to vGPU-%d\n",
+					    pipe_name(pipe), port_name(port),
+					    old_v->id, new_v->id);
+			else if (old_v && !new_v)
+				gvt_dbg_dpy("Change PIPE_%c PORT_%c owner from "
+					    "vGPU-%d to host\n",
+					    pipe_name(pipe), port_name(port),
+					    old_v->id);
+			else if (!old_v && new_v)
+				gvt_dbg_dpy("Change PIPE_%c PORT_%c owner from "
+					    "host to vGPU-%d\n",
+					    pipe_name(pipe), port_name(port),
+					    new_v->id);
+			else
+				gvt_dbg_dpy("Unexpected change PIPE_%c PORT_%c "
+					    "owner from host to host\n",
+					    pipe_name(pipe), port_name(port));
+		}
+	}
+
+	mutex_unlock(&gvt->sw_in_progress);
+	mutex_unlock(&gvt->lock);
+}
+
+void intel_gvt_store_vgpu_display_owner(struct drm_i915_private *dev_priv,
+					u32 disp_owner)
+{
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_vgpu *vgpu = NULL;
+	int id;
+	enum pipe pipe = INVALID_PIPE;
+	enum port port = PORT_NONE;
+	u8 owner;
+	u32 owner_mask = 0;
+	bool valid_owner = true, vgpu_found;
+
+	mutex_lock(&gvt->lock);
+	mutex_lock(&gvt->sw_in_progress);
+
+	if (disp_owner == gvt->disp_owner) {
+		mutex_unlock(&gvt->sw_in_progress);
+		mutex_unlock(&gvt->lock);
+		return;
+	}
+
+	// Strip out unavailable ports
+	for_each_pipe(dev_priv, pipe) {
+		port = intel_gvt_port_from_pipe(dev_priv, pipe);
+		if (port == PORT_NONE)
+			continue;
+
+		owner_mask |= 0xF << port * 4;
+	}
+	disp_owner &= owner_mask;
+
+	// Verify that vGPU id is valid and the select port
+	// is already assigned to the new owner.
+	for_each_pipe(dev_priv, pipe) {
+		port = intel_gvt_port_from_pipe(dev_priv, pipe);
+		if (port == PORT_NONE)
+			continue;
+
+		owner = (disp_owner >> port * 4) & 0xF;
+
+		vgpu_found = false;
+		for_each_active_vgpu(gvt, vgpu, id) {
+			if (id == owner) {
+				vgpu_found = true;
+				break;
+			}
+		}
+
+		if (owner != 0) {
+			if (vgpu_found) {
+				// port_assign = intel_gvt_external_disp_id_from_port(port);
+				if (!(((gvt->sel_disp_port_mask >> (owner - 1) * 8) & 0xFF) & (1 << port))) {
+					gvt_err("PORT_%c(%d) isn't assigned to vGPU-%d\n",
+						port_name(port),
+						intel_gvt_external_disp_id_from_port(port),
+						owner);
+					valid_owner = false;
+					break;
+				}
+			} else {
+				gvt_err("Selected owner vGPU-%d doesn't exist\n",
+					owner);
+				valid_owner = false;
+				break;
+			}
+		}
+	}
+
+	if (valid_owner) {
+		for_each_pipe(dev_priv, pipe) {
+			port = intel_gvt_port_from_pipe(dev_priv, pipe);
+			if (port == PORT_NONE)
+				continue;
+
+			owner = (disp_owner >> port * 4) & 0xF;
+			gvt->disp_owner &= ~(0xF << port * 4);
+			gvt->disp_owner |= (owner << port * 4);
+			if (owner)
+				gvt_dbg_dpy("PORT_%c owner changed to vGPU-%d\n",
+					    port_name(port), owner);
+			else
+				gvt_dbg_dpy("PORT_%c owner changed to host\n",
+					    port_name(port));
+		}
+		queue_work(system_unbound_wq, &gvt->switch_display_work);
+	}
+
+	mutex_unlock(&gvt->sw_in_progress);
+	mutex_unlock(&gvt->lock);
+}
+
+void intel_gvt_store_vgpu_display_mask(struct drm_i915_private *dev_priv,
+				       u64 mask)
+{
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_vgpu *vgpu;
+	int id;
+	int num_vgpu = 0;
+	u8 port_sel, mask_vgpu;
+	enum port port = PORT_NONE;
+	bool valid_mask = true;
+
+	mutex_lock(&gvt->lock);
+
+	if (mask == gvt->sel_disp_port_mask) {
+		mutex_unlock(&gvt->lock);
+		return;
+	}
+
+	idr_for_each_entry((&(gvt)->vgpu_idr), vgpu, id)
+		num_vgpu++;
+
+	if (num_vgpu == 0) {
+		// Check whether the assigned port available or not
+		for (id = 0; valid_mask && id < GVT_MAX_VGPU; id++) {
+			mask_vgpu = (mask >> id * 8) & 0xFF;
+			if (mask_vgpu == 0)
+				continue;
+			for (port = PORT_A; port < I915_MAX_PORTS; port++) {
+				if (mask_vgpu & (1 << port)) {
+					port_sel = intel_gvt_external_disp_id_from_port(port);
+					if (port_sel == 0 ||
+					    !(gvt->avail_disp_port_mask & (port_sel << port * 4))) {
+						gvt_err("Selected PORT_%c for vGPU-%d isn't available\n",
+							port_name(port), id + 1);
+						valid_mask = false;
+						break;
+					}
+				}
+			}
+		}
+
+		if (valid_mask) {
+			gvt->sel_disp_port_mask = mask;
+			gvt_dbg_dpy("Display port mask changed to 0x%016llx\n",
+				    mask);
+		}
+	} else {
+		gvt_err("Can't modify display port mask after vGPU created\n");
+	}
+
+	mutex_unlock(&gvt->lock);
+}
+
+void intel_gvt_store_vgpu_display_switch(struct drm_i915_private *dev_priv,
+					 bool auto_switch)
+{
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_vgpu *vgpu;
+	int id;
+	int num_vgpu = 0;
+
+	mutex_lock(&gvt->lock);
+
+	if (auto_switch == READ_ONCE(gvt->disp_auto_switch)) {
+		mutex_unlock(&gvt->lock);
+		return;
+	}
+
+	// Disable auto switch if any port is already shared by several vGPUs.
+	idr_for_each_entry((&(gvt)->vgpu_idr), vgpu, id)
+		num_vgpu++;
+
+	// Disable auto switch if any port assigned to several vGPUs.
+	if (num_vgpu == 0) {
+		WRITE_ONCE(gvt->disp_auto_switch, auto_switch);
+		gvt_dbg_dpy("Display auto switch set to %s\n",
+			    auto_switch ? "Y" : "N");
+	} else {
+		gvt_err("Can't modify display auto switch after vGPU created\n");
+	}
+
+	mutex_unlock(&gvt->lock);
+}
+
+/**
+ * intel_vgpu_display_find_owner - find auto switch owner
+ * @vgpu: vGPU which receives the event
+ * @reset: to reset owner of the input vGPU
+ * @next: allow search for next vGPU
+ * Auto switch only operates in unit of all ports of the same vGPU.
+ * 1) reset == 0 && next == 0:
+ *    Do not take back ownership of assigned ports from current vGPU.
+ *    Return current vGPU as the new owner if the assigned ports are available.
+ *    Do not search for next available vGPU.
+ *    The common usage is on receiving DISPLAY_READY.
+ * 2) reset == 0 && next == 1:
+ *    Do not take back ownership of assigned ports from current vGPU.
+ *    Return current vGPU as the new owner if the assigned ports are available.
+ *    Can search for next available vGPU.
+ *    The common usage is to switch to next vGPU display.
+ * 3) reset == 1 && next == 0:
+ *    Take back ownership of assigned ports from current vGPU.
+ *    The common usage is to switch display from current vGPU to host.
+ * 4) reset == 1 && next == 1:
+ *    Take back ownership of assigned ports from current vGPU.
+ *    Search for next vGPU with same or less ports assigned.
+ *    The common usage is to switch display to next vGPU on receiving deactivate
+ *    or DMLR.
+ */
+u32 intel_vgpu_display_find_owner(struct intel_vgpu *vgpu, bool reset, bool next)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct intel_vgpu *other_v;
+	struct intel_vgpu_display *disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe pipe = INVALID_PIPE;
+	enum port port = PORT_NONE;
+	u32 id, owner_id, available, new, candidate;
+	bool found = false;
+
+	new = gvt->disp_owner;
+
+	candidate = 0;
+	disp_cfg = &vgpu->disp_cfg;
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->p_pipe != INVALID_PIPE) {
+			if (reset) {
+				/* Take back port ownership from current vGPU */
+				if (((new >> disp_path->p_port * 4) & 0xF) == vgpu->id)
+					new &= ~(0xF << disp_path->p_port * 4);
+			} else {
+				/* Get assignment from current vGPU */
+				candidate |= 1 << disp_path->p_port;
+			}
+		}
+	}
+
+	/* Mark unused or already owned host ports as available */
+	available = 0;
+	for_each_pipe(gvt->dev_priv, pipe) {
+		port = intel_gvt_port_from_pipe(gvt->dev_priv, pipe);
+		if (port == PORT_NONE)
+			continue;
+
+		owner_id = ((new >> port * 4) & 0xF);
+		if (owner_id == 0 || owner_id == vgpu->id)
+			available |= 1 << port;
+	}
+
+	/* Check if non-reset candidate can be applied to available */
+	if (candidate && (~available & candidate) == 0) {
+		list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+			if (disp_path->p_pipe != INVALID_PIPE) {
+				new |= (vgpu->id << disp_path->p_port * 4);
+				found = true;
+			}
+		}
+	}
+
+	/* Return if not allow search for next or already found current */
+	if (!next || found)
+		return new;
+
+	/* Check all other vGPU, find one with same or less port assigned */
+	for_each_active_vgpu(gvt, other_v, id) {
+		if (other_v->id != vgpu->id) {
+			/* Update available according */
+			available = 0;
+			for_each_pipe(gvt->dev_priv, pipe) {
+				port = intel_gvt_port_from_pipe(gvt->dev_priv, pipe);
+				if (port == PORT_NONE)
+					continue;
+
+				owner_id = ((new >> port * 4) & 0xF);
+				if (owner_id == 0 || owner_id == other_v->id)
+					available |= 1 << port;
+			}
+
+			/* Pick a candidate vGPU and mark assigned port */
+			candidate = 0;
+			disp_cfg = &other_v->disp_cfg;
+			list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+				if (disp_path->p_pipe != INVALID_PIPE)
+					candidate |= 1 << disp_path->p_port;
+
+			/* The candidate vGPU can only use available port */
+			if (candidate && (~available & candidate) == 0) {
+				list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+					if (disp_path->p_pipe != INVALID_PIPE)
+						new |= (other_v->id << disp_path->p_port * 4);
+				break;
+			}
+		}
+	}
+
+	return new;
+}
+
+void intel_vgpu_display_set_foreground(struct intel_vgpu *vgpu, bool set)
+{
+	struct intel_vgpu_display *disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+
+	disp_cfg = &vgpu->disp_cfg;
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		disp_path->foreground = set;
+	}
+}
+
+void intel_gvt_init_display(struct intel_gvt *gvt)
+{
+	mutex_init(&gvt->sw_in_progress);
+	INIT_WORK(&gvt->connector_change_work, intel_gvt_connector_change_work);
+	INIT_WORK(&gvt->switch_display_work, intel_gvt_switch_display_work);
+	intel_gvt_init_pipe_info(gvt);
+	intel_gvt_init_ddb(gvt);
+}
diff --git a/drivers/gpu/drm/i915/gvt/display.h b/drivers/gpu/drm/i915/gvt/display.h
index a87f33e6a23c..7b6f10a7a10a 100644
--- a/drivers/gpu/drm/i915/gvt/display.h
+++ b/drivers/gpu/drm/i915/gvt/display.h
@@ -35,21 +35,19 @@
 #ifndef _GVT_DISPLAY_H_
 #define _GVT_DISPLAY_H_
 
+#include "display/intel_display_types.h"
+#include "edid.h"
+
 #define SBI_REG_MAX	20
 #define DPCD_SIZE	0x700
 
-#define intel_vgpu_port(vgpu, port) \
-	(&(vgpu->display.ports[port]))
-
-#define intel_vgpu_has_monitor_on_port(vgpu, port) \
-	(intel_vgpu_port(vgpu, port)->edid && \
-		intel_vgpu_port(vgpu, port)->edid->data_valid)
+#define intel_vgpu_display_has_monitor(disp_path) \
+	((disp_path)->edid && (disp_path)->edid->data_valid)
 
-#define intel_vgpu_port_is_dp(vgpu, port) \
-	((intel_vgpu_port(vgpu, port)->type == GVT_DP_A) || \
-	(intel_vgpu_port(vgpu, port)->type == GVT_DP_B) || \
-	(intel_vgpu_port(vgpu, port)->type == GVT_DP_C) || \
-	(intel_vgpu_port(vgpu, port)->type == GVT_DP_D))
+#define intel_vgpu_display_is_dp(disp_path) \
+	(((disp_path)->port_type == INTEL_OUTPUT_DP) || \
+	((disp_path)->port_type == INTEL_OUTPUT_EDP) || \
+	((disp_path)->port_type == INTEL_OUTPUT_DP_MST))
 
 #define INTEL_GVT_MAX_UEVENT_VARS	3
 
@@ -134,31 +132,75 @@ struct intel_vgpu_dpcd_data {
 	u8 data[DPCD_SIZE];
 };
 
-enum intel_vgpu_port_type {
-	GVT_CRT = 0,
-	GVT_DP_A,
-	GVT_DP_B,
-	GVT_DP_C,
-	GVT_DP_D,
-	GVT_HDMI_B,
-	GVT_HDMI_C,
-	GVT_HDMI_D,
-	GVT_PORT_MAX
-};
-
 enum intel_vgpu_edid {
 	GVT_EDID_1024_768,
 	GVT_EDID_1920_1200,
 	GVT_EDID_NUM,
 };
 
-struct intel_vgpu_port {
-	/* per display EDID information */
+struct vgpu_scaler_config {
+	u32 win_pos[2];
+	u32 win_size[2];
+	u32 ctrl[2];
+};
+
+enum {
+	INTEL_GVT_DIRECT_DISPLAY_HW_VSYNC = 0
+};
+
+/* Per-path display configuration */
+struct intel_vgpu_display_path {
+	/* virtual display path id */
+	u32 id;
+	/* resolution from fake EDID */
+	u64 resolution;
+	/* assigned host display port */
+	enum port p_port;
+	/* assigned host pipe corresponding to port */
+	enum pipe p_pipe;
+	/* current vgpu port, starts from PORT_B */
+	enum port port;
+	/* current vgpu port type */
+	enum intel_output_type port_type;
+	/* I2C EDID information */
+	struct intel_vgpu_i2c_edid i2c_edid;
+	/* EDID information */
 	struct intel_vgpu_edid_data *edid;
-	/* per display DPCD information */
+	/* Resolution ID from QEMU */
+	enum intel_vgpu_edid edid_id;
+	/* DPCD information */
 	struct intel_vgpu_dpcd_data *dpcd;
-	int type;
-	enum intel_vgpu_edid id;
+	/* current vgpu pipe, starts from PIPE_A */
+	enum pipe pipe;
+	/* current vgpu transcoder, starts from TRANSCODER_A */
+	enum transcoder trans;
+	/* overridden scaler settings for vgpu pipe */
+	struct vgpu_scaler_config scaler_cfg;
+	/* watermark for vgpu */
+	struct skl_pipe_wm wm_cfg;
+	/* current foreground state of display */
+	u32 foreground_state;
+	/* request to switch this vgpu as foreground */
+	u32 foreground;
+	/* display status bits */
+	u32 stat;
+	/* vsync freq */
+	u64 vsync_interval_ns;
+	/* last hw vsync injected */
+	u64 last_hwvsync_ns;
+	/* number of emulated vsync injected */
+	u64 sw_vsync_injected;
+	/* pointer to prev or next path */
+	struct list_head list;
+};
+
+struct intel_vgpu_display {
+	/* SBI information */
+	struct intel_vgpu_sbi sbi;
+	/* Per-path virtual display configuration */
+	struct list_head path_list;
+	/* display switch lock */
+	struct mutex sw_lock;
 };
 
 static inline char *vgpu_edid_str(enum intel_vgpu_edid id)
@@ -198,12 +240,12 @@ static inline unsigned int vgpu_edid_yres(enum intel_vgpu_edid id)
 }
 
 void intel_gvt_emulate_vblank(struct intel_gvt *gvt);
-void intel_gvt_check_vblank_emulation(struct intel_gvt *gvt);
+void intel_gvt_check_vblank_emulation(struct intel_vgpu *vgpu, enum pipe pipe);
 
 int intel_vgpu_init_display(struct intel_vgpu *vgpu, u64 resolution);
 void intel_vgpu_reset_display(struct intel_vgpu *vgpu);
 void intel_vgpu_clean_display(struct intel_vgpu *vgpu);
 
-int pipe_is_enabled(struct intel_vgpu *vgpu, int pipe);
+int pipe_is_enabled(struct intel_vgpu *vgpu, enum pipe pipe);
 
 #endif
diff --git a/drivers/gpu/drm/i915/gvt/edid.c b/drivers/gpu/drm/i915/gvt/edid.c
index 1fe6124918f1..1eda312e70f6 100644
--- a/drivers/gpu/drm/i915/gvt/edid.c
+++ b/drivers/gpu/drm/i915/gvt/edid.c
@@ -33,6 +33,7 @@
  */
 
 #include "i915_drv.h"
+#include "display/intel_dp.h"
 #include "gvt.h"
 
 #define GMBUS1_TOTAL_BYTES_SHIFT 16
@@ -46,17 +47,21 @@
 /* GMBUS0 bits definitions */
 #define _GMBUS_PIN_SEL_MASK     (0x7)
 
-static unsigned char edid_get_byte(struct intel_vgpu *vgpu)
+static unsigned char edid_get_byte(struct intel_vgpu *vgpu,
+				   struct intel_vgpu_display_path *disp_path)
 {
-	struct intel_vgpu_i2c_edid *edid = &vgpu->display.i2c_edid;
+	struct intel_vgpu_i2c_edid *edid = NULL;
 	unsigned char chr = 0;
 
-	if (edid->state == I2C_NOT_SPECIFIED || !edid->slave_selected) {
-		gvt_vgpu_err("Driver tries to read EDID without proper sequence!\n");
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
 		return 0;
 	}
-	if (edid->current_edid_read >= EDID_SIZE) {
-		gvt_vgpu_err("edid_get_byte() exceeds the size of EDID!\n");
+
+	edid = &disp_path->i2c_edid;
+
+	if (edid->state == I2C_NOT_SPECIFIED || !edid->slave_selected) {
+		gvt_vgpu_err("Driver tries to read EDID without proper sequence!\n");
 		return 0;
 	}
 
@@ -65,9 +70,8 @@ static unsigned char edid_get_byte(struct intel_vgpu *vgpu)
 		return 0;
 	}
 
-	if (intel_vgpu_has_monitor_on_port(vgpu, edid->port)) {
-		struct intel_vgpu_edid_data *edid_data =
-			intel_vgpu_port(vgpu, edid->port)->edid;
+	if (intel_vgpu_display_has_monitor(disp_path)) {
+		struct intel_vgpu_edid_data *edid_data = disp_path->edid;
 
 		chr = edid_data->edid_block[edid->current_edid_read];
 		edid->current_edid_read++;
@@ -77,108 +81,111 @@ static unsigned char edid_get_byte(struct intel_vgpu *vgpu)
 	return chr;
 }
 
-static inline int cnp_get_port_from_gmbus0(u32 gmbus0)
+static inline int get_port_from_gmbus0(struct intel_vgpu *vgpu)
 {
-	int port_select = gmbus0 & _GMBUS_PIN_SEL_MASK;
-	int port = -EINVAL;
-
-	if (port_select == GMBUS_PIN_1_BXT)
-		port = PORT_B;
-	else if (port_select == GMBUS_PIN_2_BXT)
-		port = PORT_C;
-	else if (port_select == GMBUS_PIN_3_BXT)
-		port = PORT_D;
-	else if (port_select == GMBUS_PIN_4_CNP)
-		port = PORT_E;
-	return port;
-}
-
-static inline int bxt_get_port_from_gmbus0(u32 gmbus0)
-{
-	int port_select = gmbus0 & _GMBUS_PIN_SEL_MASK;
-	int port = -EINVAL;
-
-	if (port_select == GMBUS_PIN_1_BXT)
-		port = PORT_B;
-	else if (port_select == GMBUS_PIN_2_BXT)
-		port = PORT_C;
-	else if (port_select == GMBUS_PIN_3_BXT)
-		port = PORT_D;
-	return port;
-}
-
-static inline int get_port_from_gmbus0(u32 gmbus0)
-{
-	int port_select = gmbus0 & _GMBUS_PIN_SEL_MASK;
-	int port = -EINVAL;
-
-	if (port_select == GMBUS_PIN_VGADDC)
-		port = PORT_E;
-	else if (port_select == GMBUS_PIN_DPC)
-		port = PORT_C;
-	else if (port_select == GMBUS_PIN_DPB)
-		port = PORT_B;
-	else if (port_select == GMBUS_PIN_DPD)
-		port = PORT_D;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	int port_select = vgpu_vreg_t(vgpu, PCH_GMBUS0) & _GMBUS_PIN_SEL_MASK;
+	enum port port = PORT_NONE;
+
+	if (IS_BROXTON(dev_priv)) {
+		if (port_select == GMBUS_PIN_1_BXT)
+			port = PORT_B;
+		else if (port_select == GMBUS_PIN_2_BXT)
+			port = PORT_C;
+		else if (port_select == GMBUS_PIN_3_BXT)
+			port = PORT_D;
+	} else if (IS_COFFEELAKE(dev_priv)) {
+		if (port_select == GMBUS_PIN_1_BXT)
+			port = PORT_B;
+		else if (port_select == GMBUS_PIN_2_BXT)
+			port = PORT_C;
+		else if (port_select == GMBUS_PIN_3_BXT)
+			port = PORT_D;
+		else if (port_select == GMBUS_PIN_4_CNP)
+			port = PORT_E;
+	} else {
+		if (port_select == GMBUS_PIN_VGADDC)
+			port = PORT_E;
+		else if (port_select == GMBUS_PIN_DPC)
+			port = PORT_C;
+		else if (port_select == GMBUS_PIN_DPB)
+			port = PORT_B;
+		else if (port_select == GMBUS_PIN_DPD)
+			port = PORT_D;
+	}
 	return port;
 }
 
 static void reset_gmbus_controller(struct intel_vgpu *vgpu)
 {
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+
 	vgpu_vreg_t(vgpu, PCH_GMBUS2) = GMBUS_HW_RDY;
-	if (!vgpu->display.i2c_edid.edid_available)
-		vgpu_vreg_t(vgpu, PCH_GMBUS2) |= GMBUS_SATOER;
-	vgpu->display.i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (!disp_path->i2c_edid.edid_available)
+			vgpu_vreg_t(vgpu, PCH_GMBUS2) |= GMBUS_SATOER;
+		disp_path->i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+	}
 }
 
 /* GMBUS0 */
 static int gmbus0_mmio_write(struct intel_vgpu *vgpu,
 			unsigned int offset, void *p_data, unsigned int bytes)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	int port, pin_select;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum port port;
 
 	memcpy(&vgpu_vreg(vgpu, offset), p_data, bytes);
 
-	pin_select = vgpu_vreg(vgpu, offset) & _GMBUS_PIN_SEL_MASK;
-
-	intel_vgpu_init_i2c_edid(vgpu);
-
-	if (pin_select == 0)
+	if ((vgpu_vreg_t(vgpu, PCH_GMBUS0) & _GMBUS_PIN_SEL_MASK) == 0)
 		return 0;
 
-	if (IS_BROXTON(dev_priv))
-		port = bxt_get_port_from_gmbus0(pin_select);
-	else if (IS_COFFEELAKE(dev_priv))
-		port = cnp_get_port_from_gmbus0(pin_select);
-	else
-		port = get_port_from_gmbus0(pin_select);
-	if (WARN_ON(port < 0))
+	port = get_port_from_gmbus0(vgpu);
+	if (WARN_ON(port == PORT_NONE))
 		return 0;
 
-	vgpu->display.i2c_edid.state = I2C_GMBUS;
-	vgpu->display.i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
-
 	vgpu_vreg_t(vgpu, PCH_GMBUS2) &= ~GMBUS_ACTIVE;
-	vgpu_vreg_t(vgpu, PCH_GMBUS2) |= GMBUS_HW_RDY | GMBUS_HW_WAIT_PHASE;
-
-	if (intel_vgpu_has_monitor_on_port(vgpu, port) &&
-			!intel_vgpu_port_is_dp(vgpu, port)) {
-		vgpu->display.i2c_edid.port = port;
-		vgpu->display.i2c_edid.edid_available = true;
-		vgpu_vreg_t(vgpu, PCH_GMBUS2) &= ~GMBUS_SATOER;
-	} else
-		vgpu_vreg_t(vgpu, PCH_GMBUS2) |= GMBUS_SATOER;
+	vgpu_vreg_t(vgpu, PCH_GMBUS2) |= GMBUS_HW_RDY | GMBUS_HW_WAIT_PHASE | GMBUS_SATOER;
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		intel_vgpu_init_i2c_edid(vgpu, &disp_path->i2c_edid);
+		disp_path->i2c_edid.state = I2C_GMBUS;
+		disp_path->i2c_edid.gmbus.phase = GMBUS_IDLE_PHASE;
+		if (disp_path->port == port &&
+		    intel_vgpu_display_has_monitor(disp_path) &&
+		    !intel_vgpu_display_is_dp(disp_path)) {
+			disp_path->i2c_edid.port = port;
+			disp_path->i2c_edid.edid_available = true;
+			vgpu_vreg_t(vgpu, PCH_GMBUS2) &= ~GMBUS_SATOER;
+		}
+	}
+
 	return 0;
 }
 
 static int gmbus1_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
-	struct intel_vgpu_i2c_edid *i2c_edid = &vgpu->display.i2c_edid;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	struct intel_vgpu_i2c_edid *i2c_edid = NULL;
+	enum port port;
 	u32 slave_addr;
 	u32 wvalue = *(u32 *)p_data;
 
+	port = get_port_from_gmbus0(vgpu);
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->port == port)
+			i2c_edid = &disp_path->i2c_edid;
+	}
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return -EINVAL;
+	}
+
 	if (vgpu_vreg(vgpu, offset) & GMBUS_SW_CLR_INT) {
 		if (!(wvalue & GMBUS_SW_CLR_INT)) {
 			vgpu_vreg(vgpu, offset) &= ~GMBUS_SW_CLR_INT;
@@ -236,7 +243,7 @@ static int gmbus1_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 			 */
 			if (gmbus1_bus_cycle(vgpu_vreg(vgpu, offset))
 				!= GMBUS_NOCYCLE) {
-				intel_vgpu_init_i2c_edid(vgpu);
+				intel_vgpu_init_i2c_edid(vgpu, i2c_edid);
 				/* After the 'stop' cycle, hw state would become
 				 * 'stop phase' and then 'idle phase' after a
 				 * few milliseconds. In emulation, we just set
@@ -283,13 +290,28 @@ static int gmbus3_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 static int gmbus3_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
-	int i;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	struct intel_vgpu_i2c_edid *i2c_edid = NULL;
 	unsigned char byte_data;
-	struct intel_vgpu_i2c_edid *i2c_edid = &vgpu->display.i2c_edid;
-	int byte_left = i2c_edid->gmbus.total_byte_count -
-				i2c_edid->current_edid_read;
-	int byte_count = byte_left;
+	int i = 0, byte_left = 0, byte_count = 0;
 	u32 reg_data = 0;
+	enum port port = get_port_from_gmbus0(vgpu);
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->port == port) {
+			i2c_edid = &disp_path->i2c_edid;
+			byte_left = i2c_edid->gmbus.total_byte_count -
+				i2c_edid->current_edid_read;
+			byte_count = byte_left;
+			break;
+		}
+	}
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return -EINVAL;
+	}
 
 	/* Data can only be recevied if previous settings correct */
 	if (vgpu_vreg_t(vgpu, PCH_GMBUS1) & GMBUS_SLAVE_READ) {
@@ -301,7 +323,7 @@ static int gmbus3_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		if (byte_count > 4)
 			byte_count = 4;
 		for (i = 0; i < byte_count; i++) {
-			byte_data = edid_get_byte(vgpu);
+			byte_data = edid_get_byte(vgpu, disp_path);
 			reg_data |= (byte_data << (i << 3));
 		}
 
@@ -320,7 +342,7 @@ static int gmbus3_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 				i2c_edid->gmbus.phase = GMBUS_WAIT_PHASE;
 				break;
 			}
-			intel_vgpu_init_i2c_edid(vgpu);
+			intel_vgpu_init_i2c_edid(vgpu, i2c_edid);
 		}
 		/*
 		 * Read GMBUS3 during send operation,
@@ -469,23 +491,42 @@ static inline int get_aux_ch_reg(unsigned int offset)
  *
  */
 void intel_gvt_i2c_handle_aux_ch_write(struct intel_vgpu *vgpu,
-				int port_idx,
-				unsigned int offset,
-				void *p_data)
+				       enum port port,
+				       unsigned int offset,
+				       void *p_data)
 {
-	struct intel_vgpu_i2c_edid *i2c_edid = &vgpu->display.i2c_edid;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	struct intel_vgpu_i2c_edid *i2c_edid = NULL;
 	int msg_length, ret_msg_size;
 	int msg, addr, ctrl, op;
 	u32 value = *(u32 *)p_data;
 	int aux_data_for_write = 0;
 	int reg = get_aux_ch_reg(offset);
+	uint8_t rxbuf[20];
+	size_t rxsize;
 
 	if (reg != AUX_CH_CTL) {
 		vgpu_vreg(vgpu, offset) = value;
 		return;
 	}
 
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->port == port) {
+			i2c_edid = &disp_path->i2c_edid;
+			break;
+		}
+	}
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return;
+	}
+
 	msg_length = AUX_CTL_MSG_LENGTH(value);
+	for (rxsize = 0; rxsize < msg_length; rxsize += 4)
+		intel_dp_unpack_aux(vgpu_vreg(vgpu, offset + 4 + rxsize),
+				rxbuf + rxsize, msg_length - rxsize);
 	// check the msg in DATA register.
 	msg = vgpu_vreg(vgpu, offset + 4);
 	addr = (msg >> 8) & 0xffff;
@@ -506,38 +547,38 @@ void intel_gvt_i2c_handle_aux_ch_write(struct intel_vgpu *vgpu,
 	if (msg_length == 3) {
 		if (!(op & GVT_AUX_I2C_MOT)) {
 			/* stop */
-			intel_vgpu_init_i2c_edid(vgpu);
+			intel_vgpu_init_i2c_edid(vgpu, i2c_edid);
 		} else {
 			/* start or restart */
 			i2c_edid->aux_ch.i2c_over_aux_ch = true;
 			i2c_edid->aux_ch.aux_ch_mot = true;
 			if (addr == 0) {
 				/* reset the address */
-				intel_vgpu_init_i2c_edid(vgpu);
+				intel_vgpu_init_i2c_edid(vgpu, i2c_edid);
 			} else if (addr == EDID_ADDR) {
 				i2c_edid->state = I2C_AUX_CH;
-				i2c_edid->port = port_idx;
+				i2c_edid->port = port;
 				i2c_edid->slave_selected = true;
-				if (intel_vgpu_has_monitor_on_port(vgpu,
-					port_idx) &&
-					intel_vgpu_port_is_dp(vgpu, port_idx))
+				if (intel_vgpu_display_has_monitor(disp_path) &&
+				    intel_vgpu_display_is_dp(disp_path))
 					i2c_edid->edid_available = true;
 			}
 		}
 	} else if ((op & 0x1) == GVT_AUX_I2C_WRITE) {
-		/* TODO
-		 * We only support EDID reading from I2C_over_AUX. And
-		 * we do not expect the index mode to be used. Right now
-		 * the WRITE operation is ignored. It is good enough to
-		 * support the gfx driver to do EDID access.
+		/* We only support EDID reading from I2C_over_AUX.
+		 * But if EDID has extension blocks, we use this write
+		 * operation to set block starting address
 		 */
+		if (addr == EDID_ADDR) {
+			i2c_edid->current_edid_read = rxbuf[4];
+		}
 	} else {
 		if (WARN_ON((op & 0x1) != GVT_AUX_I2C_READ))
 			return;
 		if (WARN_ON(msg_length != 4))
 			return;
 		if (i2c_edid->edid_available && i2c_edid->slave_selected) {
-			unsigned char val = edid_get_byte(vgpu);
+			unsigned char val = edid_get_byte(vgpu, disp_path);
 
 			aux_data_for_write = (val << 16);
 		} else
@@ -558,10 +599,9 @@ void intel_gvt_i2c_handle_aux_ch_write(struct intel_vgpu *vgpu,
  * This function is used to initialize vGPU i2c edid emulation stuffs
  *
  */
-void intel_vgpu_init_i2c_edid(struct intel_vgpu *vgpu)
+void intel_vgpu_init_i2c_edid(struct intel_vgpu *vgpu,
+			      struct intel_vgpu_i2c_edid *edid)
 {
-	struct intel_vgpu_i2c_edid *edid = &vgpu->display.i2c_edid;
-
 	edid->state = I2C_NOT_SPECIFIED;
 
 	edid->port = -1;
diff --git a/drivers/gpu/drm/i915/gvt/edid.h b/drivers/gpu/drm/i915/gvt/edid.h
index f6dfc8b795ec..b3ba6f8ca03f 100644
--- a/drivers/gpu/drm/i915/gvt/edid.h
+++ b/drivers/gpu/drm/i915/gvt/edid.h
@@ -48,7 +48,7 @@
 
 struct intel_vgpu_edid_data {
 	bool data_valid;
-	unsigned char edid_block[EDID_SIZE];
+	unsigned char edid_block[0];
 };
 
 enum gmbus_cycle_type {
@@ -134,7 +134,8 @@ struct intel_vgpu_i2c_edid {
 	struct intel_vgpu_i2c_aux_ch aux_ch;
 };
 
-void intel_vgpu_init_i2c_edid(struct intel_vgpu *vgpu);
+void intel_vgpu_init_i2c_edid(struct intel_vgpu *vgpu,
+			      struct intel_vgpu_i2c_edid *edid);
 
 int intel_gvt_i2c_handle_gmbus_read(struct intel_vgpu *vgpu,
 		unsigned int offset, void *p_data, unsigned int bytes);
@@ -143,7 +144,7 @@ int intel_gvt_i2c_handle_gmbus_write(struct intel_vgpu *vgpu,
 		unsigned int offset, void *p_data, unsigned int bytes);
 
 void intel_gvt_i2c_handle_aux_ch_write(struct intel_vgpu *vgpu,
-		int port_idx,
+		enum port port,
 		unsigned int offset,
 		void *p_data);
 
diff --git a/drivers/gpu/drm/i915/gvt/gvt.c b/drivers/gpu/drm/i915/gvt/gvt.c
index 8f37eefa0a02..d86ed78c35cf 100644
--- a/drivers/gpu/drm/i915/gvt/gvt.c
+++ b/drivers/gpu/drm/i915/gvt/gvt.c
@@ -367,6 +367,8 @@ int intel_gvt_init_device(struct drm_i915_private *dev_priv)
 		goto out_clean_types;
 	}
 
+	intel_gvt_init_display(gvt);
+
 	vgpu = intel_gvt_create_idle_vgpu(gvt);
 	if (IS_ERR(vgpu)) {
 		ret = PTR_ERR(vgpu);
diff --git a/drivers/gpu/drm/i915/gvt/gvt.h b/drivers/gpu/drm/i915/gvt/gvt.h
index 89dd15b31649..d22918403518 100644
--- a/drivers/gpu/drm/i915/gvt/gvt.h
+++ b/drivers/gpu/drm/i915/gvt/gvt.h
@@ -40,7 +40,6 @@
 #include "interrupt.h"
 #include "gtt.h"
 #include "display.h"
-#include "edid.h"
 #include "execlist.h"
 #include "scheduler.h"
 #include "sched_policy.h"
@@ -49,6 +48,7 @@
 #include "fb_decoder.h"
 #include "dmabuf.h"
 #include "page_track.h"
+#include "intel_pm.h"
 
 #define GVT_MAX_VGPU 8
 
@@ -124,12 +124,6 @@ struct intel_vgpu_opregion {
 
 #define vgpu_opregion(vgpu) (&(vgpu->opregion))
 
-struct intel_vgpu_display {
-	struct intel_vgpu_i2c_edid i2c_edid;
-	struct intel_vgpu_port ports[I915_MAX_PORTS];
-	struct intel_vgpu_sbi sbi;
-};
-
 struct vgpu_sched_ctl {
 	int weight;
 };
@@ -189,7 +183,7 @@ struct intel_vgpu {
 	struct intel_vgpu_irq irq;
 	struct intel_vgpu_gtt gtt;
 	struct intel_vgpu_opregion opregion;
-	struct intel_vgpu_display display;
+	struct intel_vgpu_display disp_cfg;
 	struct intel_vgpu_submission submission;
 	struct radix_tree_root page_track_tree;
 	u32 hws_pga[I915_NUM_ENGINES];
@@ -231,6 +225,7 @@ struct intel_vgpu {
 	u32 scan_nonprivbb;
 };
 
+
 /* validating GM healthy status*/
 #define vgpu_is_vm_unhealthy(ret_val) \
 	(((ret_val) == -EBADRQC) || ((ret_val) == -EFAULT))
@@ -298,6 +293,51 @@ struct intel_vgpu_type {
 	enum intel_vgpu_edid resolution;
 };
 
+struct intel_dom0_pipe_regs {
+	u32 pipesrc;
+	u32 scaler_ctl[I915_MAX_PIPES];
+	u32 scaler_win_pos[I915_MAX_PIPES];
+	u32 scaler_win_size[I915_MAX_PIPES];
+	u32 scaler_pwr_gate[I915_MAX_PIPES];
+};
+
+struct intel_dom0_plane_regs {
+	u32 plane_ctl;
+	u32 plane_stride;
+	u32 plane_pos;
+	u32 plane_size;
+	u32 plane_keyval;
+	u32 plane_keymsk;
+	u32 plane_keymax;
+	u32 plane_offset;
+	u32 plane_aux_dist;
+	u32 plane_aux_offset;
+	u32 plane_surf;
+	u32 plane_wm[8];
+	u32 plane_wm_trans;
+	u32 cur_fbc_ctl;
+};
+
+struct intel_gvt_plane_info {
+	struct intel_gvt *gvt;
+	enum pipe pipe;
+	enum plane_id plane;
+	int owner;
+	struct work_struct flipdone_work;
+	struct intel_dom0_plane_regs dom0_regs;
+};
+
+struct intel_gvt_pipe_info {
+	enum pipe pipe_num;
+	int owner;
+	struct intel_gvt *gvt;
+	struct work_struct vblank_work;
+	struct intel_dom0_pipe_regs dom0_pipe_regs;
+	struct intel_gvt_plane_info plane_info[I915_MAX_PLANES];
+	struct skl_ddb_entry ddb_y[I915_MAX_PLANES];
+	struct skl_ddb_entry ddb_uv[I915_MAX_PLANES];
+};
+
 struct intel_gvt {
 	/* GVT scope lock, protect GVT itself, and all resource currently
 	 * not yet protected by special locks(vgpu and scheduler lock).
@@ -330,6 +370,7 @@ struct intel_gvt {
 	 * use it with atomic bit ops so that no need to use gvt big lock.
 	 */
 	unsigned long service_request;
+	struct intel_gvt_pipe_info pipe_info[I915_MAX_PIPES];
 
 	struct {
 		struct engine_mmio *mmio;
@@ -341,6 +382,67 @@ struct intel_gvt {
 	} engine_mmio_list;
 
 	struct dentry *debugfs_root;
+
+	/*
+	 * Notify to gvt when host encoder connectivity changed
+	 */
+	struct work_struct connector_change_work;
+	/* display switch work lock */
+	struct mutex sw_in_progress;
+	struct work_struct switch_display_work;
+
+	/*
+	 * Available display port mask for PORT_A to PORT_A+7 (low to high).
+	 * Each hex digit represents the availability of corresponding port.
+	 * 0: Port isn't available.
+	 * x: Port x is available.
+	 * Be noticed the hex digit is enum type PORT_x + 1.
+	 * e.g. 0x4320
+	 *      PORT_A: N/A.
+	 *      PORT_B: Available.
+	 *      PORT_C: Available.
+	 *      PORT_D: Available.
+	 */
+	u32 avail_disp_port_mask;
+
+	/*
+	 * Bit mask of selected ports for vGPU-1 to vGPU-8 (low to high).
+	 * Each byte represents the bit mask of assigned ports for vGPU id.
+	 * From LSB to MSB (PORT_A to PORT_A+7):
+	 *   0: This port isn't assigned to that vGPU.
+	 *   1: This port is assigned to that vGPU.
+	 * Be noticed that bit position base is same as enum type PORT_x, and
+	 *   equal to avail_disp_port_mask minus 1 for same port.
+	 * e.g. 0x0A0602.
+	 *      vGPU 1: Bit mask is 2, port 2 assigned, PORT_B.
+	 *      vGPU 2: Bit mask is 6, port 2 & 3 assigned, PORT_B and PORT_C.
+	 *      vGPU 3: Bit mask is A, port 2 & 4 assigned, PORT_B and PORT_D.
+	 */
+	u64 sel_disp_port_mask;
+
+	/*
+	 * Display owner for PORT_A to PORT_A+7 (low to high).
+	 * Each hex digit represents the owner vGPU id of corresponding port.
+	 * 0: display N/A or owned by host.
+	 * x: display owned by vGPU x.
+	 * e.g. 0x2010
+	 *      PORT_A: N/A or owned by host.
+	 *      PORT_B: Owned by vGPU-1.
+	 *      PORT_C: N/A or owned by host.
+	 *      PORT_D: Owned by vGPU-2.
+	 */
+	u32 disp_owner;
+
+	/*
+	 * Auto switch disp, it contains:
+	 * a. Switch to guest display when guest display is ready.
+	 * b. Switch to next guest display when same port/pipe assigned.
+	 * c. Switch to host display if no active guest.
+	 * true: enable auto switch as default.
+	 */
+	bool disp_auto_switch;
+	bool disp_edid_filter;
+
 	struct work_struct active_hp_work;
 };
 
@@ -693,6 +795,28 @@ void intel_gvt_debugfs_init(struct intel_gvt *gvt);
 void intel_gvt_debugfs_clean(struct intel_gvt *gvt);
 
 
+u8 intel_gvt_external_disp_id_from_port(enum port port);
+enum port intel_gvt_port_from_external_disp_id(u8 port_id);
+enum pipe intel_gvt_pipe_from_port(
+	struct drm_i915_private *dev_priv, enum port port);
+enum port intel_gvt_port_from_pipe(
+	struct drm_i915_private *dev_priv, enum pipe pipe);
+void intel_gvt_store_vgpu_display_owner(
+	struct drm_i915_private *dev_priv, u32 disp_owner);
+void intel_gvt_store_vgpu_display_mask(struct drm_i915_private *dev_priv,
+				       u64 mask);
+void intel_gvt_store_vgpu_display_switch(struct drm_i915_private *dev_priv,
+					 bool auto_switch);
+u32 intel_vgpu_display_find_owner(struct intel_vgpu *vgpu, bool reset, bool next);
+void intel_vgpu_display_set_foreground(struct intel_vgpu *vgpu, bool reset);
+
+void intel_gvt_init_display(struct intel_gvt *gvt);
+void intel_vgpu_update_plane_scaler(struct intel_vgpu *vgpu,
+	struct intel_crtc *intel_crtc, enum plane_id plane);
+void intel_vgpu_update_plane_wm(struct intel_vgpu *vgpu,
+	struct intel_crtc *intel_crtc, enum plane_id plane);
+u32 vgpu_calc_wm_level(const struct skl_wm_level *level);
+
 #include "trace.h"
 #include "mpt.h"
 
diff --git a/drivers/gpu/drm/i915/gvt/handlers.c b/drivers/gpu/drm/i915/gvt/handlers.c
index 418ad493765b..38cebb340336 100644
--- a/drivers/gpu/drm/i915/gvt/handlers.c
+++ b/drivers/gpu/drm/i915/gvt/handlers.c
@@ -440,18 +440,90 @@ static int dpy_reg_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 static int pipeconf_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
-	u32 data;
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe pipe = SKL_PLANE_REG_TO_PIPE(offset);
+	enum pipe phy_pipe = INVALID_PIPE;
 
 	write_vreg(vgpu, offset, p_data, bytes);
-	data = vgpu_vreg(vgpu, offset);
 
-	if (data & PIPECONF_ENABLE)
+	// Simulate PIPECONF ACTIVE on PIPECONF ON/OFF
+	if (vgpu_vreg(vgpu, offset) & PIPECONF_ENABLE) {
 		vgpu_vreg(vgpu, offset) |= I965_PIPECONF_ACTIVE;
-	else
+		gvt_dbg_dpy("vgpu:%d request to enable PIPECONF on pipe %d\n",
+			    vgpu->id, pipe);
+	} else {
 		vgpu_vreg(vgpu, offset) &= ~I965_PIPECONF_ACTIVE;
+		gvt_dbg_dpy("vgpu:%d request to disable PIPECONF on pipe %d\n",
+			    vgpu->id, pipe);
+	}
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (pipe == disp_path->pipe) {
+			phy_pipe = disp_path->p_pipe;
+			break;
+		}
+	}
+
+	// Can't really disable PIPECONF, otherwise HW vsync will lost and host
+	//   and guest will go wrong. However, we can turn on/off all planes
+	//   to simulate vGPU PIPECONF on/off.
+	if (disp_path && phy_pipe != INVALID_PIPE &&
+	    vgpu->gvt->pipe_info[phy_pipe].owner == vgpu->id) {
+		struct intel_crtc *intel_crtc = NULL;
+		unsigned long irqflags;
+
+		for_each_intel_crtc(&dev_priv->drm, intel_crtc) {
+			drm_modeset_lock(&intel_crtc->base.mutex, NULL);
+			if (phy_pipe == intel_crtc->pipe)
+				break;
+			drm_modeset_unlock(&intel_crtc->base.mutex);
+		}
+
+		mutex_lock(&disp_cfg->sw_lock);
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+		mmio_hw_access_pre(dev_priv);
+		if (vgpu_vreg(vgpu, offset) & I965_PIPECONF_ACTIVE) {
+			I915_WRITE_FW(PLANE_CTL(phy_pipe, PLANE_PRIMARY),
+				      vgpu_vreg_t(vgpu, PLANE_CTL(pipe, PLANE_PRIMARY)));
+			I915_WRITE_FW(PLANE_SURF(phy_pipe, PLANE_PRIMARY),
+				      vgpu_vreg_t(vgpu, PLANE_SURF(pipe, PLANE_PRIMARY)));
+			intel_uncore_posting_read_fw(&dev_priv->uncore,
+						     PLANE_SURF(phy_pipe, PLANE_PRIMARY));
+			I915_WRITE_FW(CURCNTR(phy_pipe),
+				      vgpu_vreg_t(vgpu, CURCNTR(pipe)));
+			I915_WRITE_FW(CURBASE(phy_pipe),
+				      vgpu_vreg_t(vgpu, CURBASE(pipe)));
+			intel_uncore_posting_read_fw(&dev_priv->uncore,
+						     CURBASE(phy_pipe));
+
+			gvt_dbg_dpy("vgpu:%d enable all planes on PIPECONF enable on pipe %d->%d\n",
+				    vgpu->id, pipe, phy_pipe);
+		} else {
+			I915_WRITE_FW(PLANE_CTL(phy_pipe, PLANE_PRIMARY), 0);
+			I915_WRITE_FW(PLANE_SURF(phy_pipe, PLANE_PRIMARY), 0);
+			intel_uncore_posting_read_fw(&dev_priv->uncore,
+						     PLANE_SURF(phy_pipe, PLANE_PRIMARY));
+			I915_WRITE_FW(CURCNTR(phy_pipe),
+				      vgpu_vreg_t(vgpu, CURCNTR(pipe)) & ~0x3F);
+			I915_WRITE_FW(CURBASE(phy_pipe), 0);
+			intel_uncore_posting_read_fw(&dev_priv->uncore,
+						     CURBASE(phy_pipe));
+
+			gvt_dbg_dpy("vgpu:%d disable all planes on PIPECONF enable on pipe %d->%d\n",
+				    vgpu->id, pipe, phy_pipe);
+		}
+		mmio_hw_access_post(dev_priv);
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+		mutex_unlock(&disp_cfg->sw_lock);
+		drm_modeset_unlock(&intel_crtc->base.mutex);
+	}
+
 	/* vgpu_lock already hold by emulate mmio r/w */
 	mutex_unlock(&vgpu->vgpu_lock);
-	intel_gvt_check_vblank_emulation(vgpu->gvt);
+	intel_gvt_check_vblank_emulation(vgpu, pipe);
 	mutex_lock(&vgpu->vgpu_lock);
 	return 0;
 }
@@ -533,6 +605,15 @@ static int force_nonpriv_write(struct intel_vgpu *vgpu,
 	return 0;
 }
 
+static int pipe_dsl_mmio_read(struct intel_vgpu *vgpu,
+		unsigned int offset, void *p_data, unsigned int bytes)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+
+	vgpu_vreg(vgpu, offset) = I915_READ(_MMIO(offset));
+	return intel_vgpu_default_mmio_read(vgpu, offset, p_data, bytes);
+}
+
 static int ddi_buf_ctl_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
@@ -788,21 +869,118 @@ static int spr_surf_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 	return 0;
 }
 
+static int skl_plane_surf_write(struct intel_vgpu *vgpu, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe pipe = SKL_PLANE_REG_TO_PIPE(offset);
+	enum plane_id plane = SKL_PLANE_REG_TO_PLANE(offset);
+	enum pipe phy_pipe = INVALID_PIPE;
+	u32 phy_offset;
+	int event = SKL_FLIP_EVENT(pipe, plane);
+
+	write_vreg(vgpu, offset, p_data, bytes);
+	vgpu_vreg_t(vgpu, SKL_PLANE_SURFLIVE(pipe, plane)) =
+		vgpu_vreg(vgpu, offset);
+
+	if (plane == PLANE_PRIMARY)
+		vgpu_vreg_t(vgpu, PIPE_FLIPCOUNT_G4X(pipe))++;
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->pipe == pipe) {
+			phy_pipe = disp_path->p_pipe;
+			break;
+		}
+	}
+
+	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
+	    disp_path &&
+	    phy_pipe != INVALID_PIPE &&
+	    plane == PLANE_PRIMARY &&
+	    gvt->pipe_info[phy_pipe].owner == vgpu->id) {
+		unsigned long irqflags;
+
+		phy_offset = offset + (phy_pipe - pipe) * 0x1000;
+
+		gvt_dbg_dpy("Plane surf update for vgpu:%d, PIPE_%c, PLANE_%c, offset:0x%x->0x%x, val:0x%x->0x%x\n",
+			    vgpu->id, pipe_name(pipe), plane_name(plane),
+			    offset, phy_offset,
+			    vgpu_vreg(vgpu, offset), vgpu_vreg(vgpu, offset));
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+		mmio_hw_access_pre(dev_priv);
+		I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+		mmio_hw_access_post(dev_priv);
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+	}
+
+	if (vgpu_vreg_t(vgpu, PLANE_CTL(pipe, plane)) & PLANE_CTL_ASYNC_FLIP)
+		intel_vgpu_trigger_virtual_event(vgpu, event);
+	else
+		set_bit(event, vgpu->irq.flip_done_event[pipe]);
+	return 0;
+}
+
 static int reg50080_mmio_write(struct intel_vgpu *vgpu,
 			       unsigned int offset, void *p_data,
 			       unsigned int bytes)
 {
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_runtime_info *runtime = RUNTIME_INFO(dev_priv);
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
 	enum pipe pipe = REG_50080_TO_PIPE(offset);
 	enum plane_id plane = REG_50080_TO_PLANE(offset);
+	enum pipe phy_pipe = INVALID_PIPE;
+	u32 phy_offset;
 	int event = SKL_FLIP_EVENT(pipe, plane);
 
 	write_vreg(vgpu, offset, p_data, bytes);
-	if (plane == PLANE_PRIMARY) {
-		vgpu_vreg_t(vgpu, DSPSURFLIVE(pipe)) = vgpu_vreg(vgpu, offset);
+	vgpu_vreg_t(vgpu, SKL_PLANE_SURFLIVE(pipe, plane)) =
+		vgpu_vreg(vgpu, offset);
+
+	if (plane == PLANE_PRIMARY)
 		vgpu_vreg_t(vgpu, PIPE_FLIPCOUNT_G4X(pipe))++;
-	} else {
-		vgpu_vreg_t(vgpu, SPRSURFLIVE(pipe)) = vgpu_vreg(vgpu, offset);
+
+	if (pipe == INVALID_PIPE || pipe >= INTEL_NUM_PIPES(dev_priv)) {
+		gvt_dbg_dpy("vgpu:%d: Invalid pipe and for reg_50080 offset:0x%x\n",
+			    vgpu->id, offset);
+		return 0;
+	}
+
+	if (plane >= 1 + runtime->num_sprites[pipe]) {
+		gvt_dbg_dpy("vgpu:%d: Invalid plane and for reg_50080 offset:0x%x\n",
+			    vgpu->id, offset);
+		return 0;
+	}
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->pipe == pipe) {
+			phy_pipe = disp_path->p_pipe;
+			break;
+		}
+	}
+
+	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
+	    disp_path &&
+	    phy_pipe != INVALID_PIPE &&
+	    gvt->pipe_info[phy_pipe].owner == vgpu->id) {
+		unsigned long irqflags;
+
+		/* Make sure pipe and plane are valid before indexing */
+		phy_offset = REG_50080(phy_pipe, plane).reg;
+
+		gvt_dbg_dpy("reg_50080 update for vgpu:%d, offset:0x%x->0x%x, val:0x%x->0x%x\n",
+			    vgpu->id, offset, phy_offset,
+			    vgpu_vreg(vgpu, offset), vgpu_vreg(vgpu, offset));
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+		mmio_hw_access_pre(dev_priv);
+		I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+		mmio_hw_access_post(dev_priv);
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
 	}
 
 	if ((vgpu_vreg(vgpu, offset) & REG50080_FLIP_TYPE_MASK) == REG50080_FLIP_TYPE_ASYNC)
@@ -813,6 +991,203 @@ static int reg50080_mmio_write(struct intel_vgpu *vgpu,
 	return 0;
 }
 
+static int skl_plane_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
+		void *p_data, unsigned int bytes)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_runtime_info *runtime = RUNTIME_INFO(dev_priv);
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe pipe = SKL_PLANE_REG_TO_PIPE(offset);
+	enum plane_id plane = SKL_PLANE_REG_TO_PLANE(offset);
+	enum pipe phy_pipe = INVALID_PIPE;
+	unsigned int phy_offset;
+
+	write_vreg(vgpu, offset, p_data, bytes);
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->pipe == pipe) {
+			phy_pipe = disp_path->p_pipe;
+			break;
+		}
+	}
+
+	if (disp_path &&
+	    phy_pipe != INVALID_PIPE &&
+	    plane == PLANE_PRIMARY &&
+	    vgpu->gvt->pipe_info[phy_pipe].owner == vgpu->id &&
+	    (vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE)) {
+		struct drm_device *drm_dev = &dev_priv->drm;
+		struct intel_crtc *intel_crtc = NULL;
+		unsigned long irqflags;
+
+		phy_offset = offset + (phy_pipe - pipe) * 0x1000;
+
+		for_each_intel_crtc(drm_dev, intel_crtc) {
+			drm_modeset_lock(&intel_crtc->base.mutex, NULL);
+			if (disp_path->p_pipe == intel_crtc->pipe)
+				break;
+			drm_modeset_unlock(&intel_crtc->base.mutex);
+		}
+
+		if (!intel_crtc) {
+			gvt_dbg_dpy("No active host crtc for plane mmio update for vgpu:%d, pipe:%d->%d, offset:0x%x->0x%x\n",
+				    vgpu->id, pipe, phy_pipe, offset, phy_offset);
+			return 0;
+		}
+
+		gvt_dbg_dpy("Plane mmio update for vgpu:%d, PIPE_%c, PLANE_%c, offset:0x%x->0x%x, val:0x%x\n",
+			    vgpu->id, pipe_name(pipe), plane_name(plane),
+			    offset, phy_offset, vgpu_vreg(vgpu, offset));
+
+		mutex_lock(&disp_cfg->sw_lock);
+		spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+		mmio_hw_access_pre(dev_priv);
+		if ((phy_offset == PLANE_CTL(phy_pipe, plane).reg) &&
+		    (vgpu_vreg(vgpu, offset) & PLANE_CTL_ENABLE)) {
+			int max_scaler = runtime->num_scalers[phy_pipe];
+			int scaler = 0;
+			int level, max_level = ilk_wm_max_level(dev_priv);
+			struct vgpu_scaler_config scl_cfg_old;
+			struct skl_pipe_wm wm_cfg_old;
+
+			gvt_dbg_dpy("vgpu-%d: update scaler on plane-%d PLANE_CTL_ENABLE\n",
+				    vgpu->id, plane);
+
+			memcpy(&scl_cfg_old, &disp_path->scaler_cfg, sizeof(scl_cfg_old));
+			memcpy(&wm_cfg_old, &disp_path->wm_cfg, sizeof(wm_cfg_old));
+
+			intel_vgpu_update_plane_scaler(vgpu, intel_crtc, plane);
+			intel_vgpu_update_plane_wm(vgpu, intel_crtc, plane);
+
+			I915_WRITE_FW(PLANE_CTL(phy_pipe, plane), 0);
+
+			I915_WRITE_FW(PIPESRC(phy_pipe), vgpu_vreg_t(vgpu, PIPESRC(pipe)));
+
+			if (memcmp(&scl_cfg_old, &disp_path->scaler_cfg, sizeof(scl_cfg_old))) {
+				for (scaler = 0; scaler < max_scaler; scaler++) {
+					I915_WRITE_FW(SKL_PS_CTRL(phy_pipe, scaler),
+						      disp_path->scaler_cfg.ctrl[scaler]);
+					I915_WRITE_FW(SKL_PS_PWR_GATE(phy_pipe, scaler), 0);
+					I915_WRITE_FW(SKL_PS_WIN_POS(phy_pipe, scaler),
+						      disp_path->scaler_cfg.win_pos[scaler]);
+					I915_WRITE_FW(SKL_PS_WIN_SZ(phy_pipe, scaler),
+						      disp_path->scaler_cfg.win_size[scaler]);
+				}
+			}
+
+			if (memcmp(&wm_cfg_old, &disp_path->wm_cfg, sizeof(wm_cfg_old))) {
+				for (level = 0; level <= max_level; level++) {
+					I915_WRITE_FW(PLANE_WM(phy_pipe, plane, level),
+						      vgpu_calc_wm_level(&disp_path->wm_cfg.planes[plane].wm[level]));
+				}
+				I915_WRITE_FW(PLANE_WM_TRANS(phy_pipe, plane),
+					      vgpu_calc_wm_level(&disp_path->wm_cfg.planes[plane].trans_wm));
+			}
+
+			I915_WRITE_FW(PLANE_CTL(phy_pipe, plane),
+				      vgpu_vreg_t(vgpu, PLANE_CTL(pipe, plane)));
+		} else {
+			I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+		}
+		mmio_hw_access_post(dev_priv);
+		spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+		mutex_unlock(&disp_cfg->sw_lock);
+		drm_modeset_unlock(&intel_crtc->base.mutex);
+	}
+
+	return 0;
+}
+
+static int skl_cursor_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
+				 void *p_data, unsigned int bytes)
+{
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
+	enum pipe pipe = SKL_PLANE_REG_TO_PIPE(offset);
+	enum pipe phy_pipe = INVALID_PIPE;
+	unsigned int phy_offset;
+
+	write_vreg(vgpu, offset, p_data, bytes);
+	if (offset == CURBASE(pipe).reg)
+		vgpu_vreg_t(vgpu, SKL_CURSOR_SURFLIVE(pipe)) =
+			vgpu_vreg(vgpu, offset);
+
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		if (disp_path->pipe == pipe) {
+			phy_pipe = disp_path->p_pipe;
+			break;
+		}
+	}
+
+	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
+	    disp_path &&
+	    phy_pipe != INVALID_PIPE &&
+	    vgpu->gvt->pipe_info[phy_pipe].owner == vgpu->id) {
+		u32 new_val = vgpu_vreg(vgpu, offset);
+
+		phy_offset = offset + (phy_pipe - pipe) * 0x1000;
+
+		mmio_hw_access_pre(dev_priv);
+		if (offset == CURBASE(pipe).reg) {
+			I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+			intel_uncore_posting_read_fw(&dev_priv->uncore,
+						     _MMIO(phy_offset));
+		} else if (offset == CURPOS(pipe).reg) {
+			I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+		} else if (offset == CURCNTR(pipe).reg ||
+			   offset == CUR_FBC_CTL(pipe).reg) {
+			struct drm_device *drm_dev = &dev_priv->drm;
+			struct intel_crtc *intel_crtc = NULL;
+			struct skl_plane_wm cur_wm_old;
+			unsigned long irqflags = 0;
+
+			for_each_intel_crtc(drm_dev, intel_crtc) {
+				drm_modeset_lock(&intel_crtc->base.mutex, NULL);
+				if (disp_path->p_pipe == intel_crtc->pipe)
+					break;
+
+				drm_modeset_unlock(&intel_crtc->base.mutex);
+			}
+
+			if (!intel_crtc) {
+				gvt_dbg_dpy("No active host crtc for cursor mmio update for vgpu:%d, pipe:%d->%d, offset:0x%x->0x%x\n",
+					    vgpu->id, pipe, phy_pipe, offset, phy_offset);
+				return 0;
+			}
+
+			mutex_lock(&disp_cfg->sw_lock);
+			spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
+
+			memcpy(&cur_wm_old, &disp_path->wm_cfg.planes[PLANE_CURSOR], sizeof(cur_wm_old));
+			intel_vgpu_update_plane_wm(vgpu, intel_crtc, PLANE_CURSOR);
+			if (memcmp(&cur_wm_old, &disp_path->wm_cfg.planes[PLANE_CURSOR], sizeof(cur_wm_old))) {
+				int level, max_level = ilk_wm_max_level(dev_priv);
+
+				for (level = 0; level <= max_level; level++) {
+					I915_WRITE_FW(CUR_WM(phy_pipe, level),
+						      vgpu_calc_wm_level(&disp_path->wm_cfg.planes[PLANE_CURSOR].wm[level]));
+				}
+				I915_WRITE_FW(CUR_WM_TRANS(phy_pipe),
+					      vgpu_calc_wm_level(&disp_path->wm_cfg.planes[PLANE_CURSOR].trans_wm));
+			}
+			I915_WRITE_FW(_MMIO(phy_offset), vgpu_vreg(vgpu, offset));
+
+			spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
+			mutex_unlock(&disp_cfg->sw_lock);
+			drm_modeset_unlock(&intel_crtc->base.mutex);
+		}
+		mmio_hw_access_post(dev_priv);
+
+		gvt_dbg_dpy("Cursor mmio update for vgpu:%d, pipe:%d->%d, offset:0x%x->0x%x, val:0x%x->0x%x\n",
+			    vgpu->id, pipe, phy_pipe, offset, phy_offset,
+			    vgpu_vreg(vgpu, offset), new_val);
+	}
+
+	return 0;
+}
+
 static int trigger_aux_channel_interrupt(struct intel_vgpu *vgpu,
 		unsigned int reg)
 {
@@ -904,14 +1279,14 @@ static void dp_aux_ch_ctl_link_training(struct intel_vgpu_dpcd_data *dpcd,
 static int dp_aux_ch_ctl_mmio_write(struct intel_vgpu *vgpu,
 		unsigned int offset, void *p_data, unsigned int bytes)
 {
-	struct intel_vgpu_display *display = &vgpu->display;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
 	int msg, addr, ctrl, op, len;
-	int port_index = OFFSET_TO_DP_AUX_PORT(offset);
+	enum port port = OFFSET_TO_DP_AUX_PORT(offset);
 	struct intel_vgpu_dpcd_data *dpcd = NULL;
-	struct intel_vgpu_port *port = NULL;
 	u32 data;
 
-	if (!dpy_is_valid_port(port_index)) {
+	if (!dpy_is_valid_port(port)) {
 		gvt_vgpu_err("Unsupported DP port access!\n");
 		return 0;
 	}
@@ -919,12 +1294,21 @@ static int dp_aux_ch_ctl_mmio_write(struct intel_vgpu *vgpu,
 	write_vreg(vgpu, offset, p_data, bytes);
 	data = vgpu_vreg(vgpu, offset);
 
-	if ((INTEL_GEN(vgpu->gvt->dev_priv) >= 9)
-		&& offset != _REG_SKL_DP_AUX_CH_CTL(port_index)) {
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list)
+		if (disp_path->port == port)
+			break;
+
+	if (!disp_path) {
+		gvt_err("vgpu-%d invalid vgpu display path\n", vgpu->id);
+		return -EINVAL;
+	}
+
+	if ((INTEL_GEN(vgpu->gvt->dev_priv) >= 9) &&
+	    offset != _REG_SKL_DP_AUX_CH_CTL(port)) {
 		/* SKL DPB/C/D aux ctl register changed */
 		return 0;
 	} else if (IS_BROADWELL(vgpu->gvt->dev_priv) &&
-		   offset != _REG_HSW_DP_AUX_CH_CTL(port_index)) {
+		   offset != _REG_HSW_DP_AUX_CH_CTL(port)) {
 		/* write to the data registers */
 		return 0;
 	}
@@ -935,8 +1319,7 @@ static int dp_aux_ch_ctl_mmio_write(struct intel_vgpu *vgpu,
 		return 0;
 	}
 
-	port = &display->ports[port_index];
-	dpcd = port->dpcd;
+	dpcd = disp_path->dpcd;
 
 	/* read out message from DATA1 register */
 	msg = vgpu_vreg(vgpu, offset + 4);
@@ -1064,7 +1447,7 @@ static int dp_aux_ch_ctl_mmio_write(struct intel_vgpu *vgpu,
 	}
 
 	/* i2c transaction starts */
-	intel_gvt_i2c_handle_aux_ch_write(vgpu, port_index, offset, p_data);
+	intel_gvt_i2c_handle_aux_ch_write(vgpu, port, offset, p_data);
 
 	if (data & DP_AUX_CH_CTL_INTERRUPT)
 		trigger_aux_channel_interrupt(vgpu, offset);
@@ -1095,29 +1478,29 @@ static int vga_control_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 static u32 read_virtual_sbi_register(struct intel_vgpu *vgpu,
 		unsigned int sbi_offset)
 {
-	struct intel_vgpu_display *display = &vgpu->display;
-	int num = display->sbi.number;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	int num = disp_cfg->sbi.number;
 	int i;
 
 	for (i = 0; i < num; ++i)
-		if (display->sbi.registers[i].offset == sbi_offset)
+		if (disp_cfg->sbi.registers[i].offset == sbi_offset)
 			break;
 
 	if (i == num)
 		return 0;
 
-	return display->sbi.registers[i].value;
+	return disp_cfg->sbi.registers[i].value;
 }
 
 static void write_virtual_sbi_register(struct intel_vgpu *vgpu,
 		unsigned int offset, u32 value)
 {
-	struct intel_vgpu_display *display = &vgpu->display;
-	int num = display->sbi.number;
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	int num = disp_cfg->sbi.number;
 	int i;
 
 	for (i = 0; i < num; ++i) {
-		if (display->sbi.registers[i].offset == offset)
+		if (disp_cfg->sbi.registers[i].offset == offset)
 			break;
 	}
 
@@ -1126,11 +1509,11 @@ static void write_virtual_sbi_register(struct intel_vgpu *vgpu,
 			gvt_vgpu_err("SBI caching meets maximum limits\n");
 			return;
 		}
-		display->sbi.number++;
+		disp_cfg->sbi.number++;
 	}
 
-	display->sbi.registers[i].offset = offset;
-	display->sbi.registers[i].value = value;
+	disp_cfg->sbi.registers[i].offset = offset;
+	disp_cfg->sbi.registers[i].value = value;
 }
 
 static int sbi_data_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
@@ -1257,11 +1640,42 @@ static int send_display_ready_uevent(struct intel_vgpu *vgpu, int ready)
 static int pvinfo_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
+	struct intel_gvt *gvt = vgpu->gvt;
 	u32 data = *(u32 *)p_data;
 	bool invalid_write = false;
 
 	switch (offset) {
 	case _vgtif_reg(display_ready):
+		if (data & VGT_DRV_DISPLAY_READY) {
+			struct intel_vgpu_display *disp_cfg;
+			struct intel_vgpu_display_path *disp_path = NULL, *n;
+			bool foreground = false;
+
+			disp_cfg = &vgpu->disp_cfg;
+			list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+				if (disp_path->foreground_state) {
+					foreground = true;
+					break;
+				}
+			}
+
+			intel_vgpu_display_set_foreground(vgpu, true);
+			if (!foreground && READ_ONCE(gvt->disp_auto_switch)) {
+				u32 owner = 0;
+
+				mutex_lock(&gvt->sw_in_progress);
+				owner = intel_vgpu_display_find_owner(vgpu, false, false);
+				if (owner != gvt->disp_owner) {
+					gvt->disp_owner = owner;
+					gvt_dbg_dpy("Schedule display owner changed to 0x%08x "
+						    "due to DISPLAY_READY of vGPU-%d\n",
+						    gvt->disp_owner, vgpu->id);
+					queue_work(system_unbound_wq,
+						   &vgpu->gvt->switch_display_work);
+				}
+				mutex_unlock(&gvt->sw_in_progress);
+			}
+		}
 		send_display_ready_uevent(vgpu, data ? 1 : 0);
 		break;
 	case _vgtif_reg(g2v_notify):
@@ -1298,22 +1712,6 @@ static int pvinfo_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 	return 0;
 }
 
-static int pf_write(struct intel_vgpu *vgpu,
-		unsigned int offset, void *p_data, unsigned int bytes)
-{
-	u32 val = *(u32 *)p_data;
-
-	if ((offset == _PS_1A_CTRL || offset == _PS_2A_CTRL ||
-	   offset == _PS_1B_CTRL || offset == _PS_2B_CTRL ||
-	   offset == _PS_1C_CTRL) && (val & PS_PLANE_SEL_MASK) != 0) {
-		WARN_ONCE(true, "VM(%d): guest is trying to scaling a plane\n",
-			  vgpu->id);
-		return 0;
-	}
-
-	return intel_vgpu_default_mmio_write(vgpu, offset, p_data, bytes);
-}
-
 static int power_well_ctl_mmio_write(struct intel_vgpu *vgpu,
 		unsigned int offset, void *p_data, unsigned int bytes)
 {
@@ -1859,6 +2257,22 @@ static int csfe_chicken1_mmio_write(struct intel_vgpu *vgpu,
 #define MMIO_RING_RO(prefix, d, f, rm, r, w) \
 	MMIO_RING_F(prefix, 4, F_RO | f, 0, rm, d, r, w)
 
+#define MMIO_PIPES_SDH(prefix, plane, s, d, r, w) do { \
+	int pipe; \
+	for_each_pipe(dev_priv, pipe) \
+		MMIO_F(prefix(pipe, plane), s, 0, 0, 0, d, r, w); \
+} while (0)
+
+#define MMIO_PLANES_SDH(prefix, s, d, r, w) do { \
+	int pipe, plane; \
+	for_each_pipe(dev_priv, pipe) \
+		for_each_universal_plane(dev_priv, pipe, plane) \
+			MMIO_F(prefix(pipe, plane), s, 0, 0, 0, d, r, w); \
+} while (0)
+
+#define MMIO_PLANES_DH(prefix, d, r, w) \
+	MMIO_PLANES_SDH(prefix, 4, d, r, w)
+
 static int init_generic_mmio_info(struct intel_gvt *gvt)
 {
 	struct drm_i915_private *dev_priv = gvt->dev_priv;
@@ -1959,9 +2373,9 @@ static int init_generic_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(_MMIO(0xc4040), D_ALL);
 	MMIO_D(DERRMR, D_ALL);
 
-	MMIO_D(PIPEDSL(PIPE_A), D_ALL);
-	MMIO_D(PIPEDSL(PIPE_B), D_ALL);
-	MMIO_D(PIPEDSL(PIPE_C), D_ALL);
+	MMIO_DH(PIPEDSL(PIPE_A), D_ALL, pipe_dsl_mmio_read, NULL);
+	MMIO_DH(PIPEDSL(PIPE_B), D_ALL, pipe_dsl_mmio_read, NULL);
+	MMIO_DH(PIPEDSL(PIPE_C), D_ALL, pipe_dsl_mmio_read, NULL);
 	MMIO_D(PIPEDSL(_PIPE_EDP), D_ALL);
 
 	MMIO_DH(PIPECONF(PIPE_A), D_ALL, NULL, pipeconf_mmio_write);
@@ -1984,147 +2398,138 @@ static int init_generic_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(PIPE_FRMCOUNT_G4X(PIPE_C), D_ALL);
 	MMIO_D(PIPE_FRMCOUNT_G4X(_PIPE_EDP), D_ALL);
 
-	MMIO_D(CURCNTR(PIPE_A), D_ALL);
-	MMIO_D(CURCNTR(PIPE_B), D_ALL);
-	MMIO_D(CURCNTR(PIPE_C), D_ALL);
+	MMIO_D(CURCNTR(PIPE_A), D_BDW);
+	MMIO_D(CURCNTR(PIPE_B), D_BDW);
+	MMIO_D(CURCNTR(PIPE_C), D_BDW);
 
-	MMIO_D(CURPOS(PIPE_A), D_ALL);
-	MMIO_D(CURPOS(PIPE_B), D_ALL);
-	MMIO_D(CURPOS(PIPE_C), D_ALL);
+	MMIO_D(CURPOS(PIPE_A), D_BDW);
+	MMIO_D(CURPOS(PIPE_B), D_BDW);
+	MMIO_D(CURPOS(PIPE_C), D_BDW);
 
-	MMIO_D(CURBASE(PIPE_A), D_ALL);
-	MMIO_D(CURBASE(PIPE_B), D_ALL);
-	MMIO_D(CURBASE(PIPE_C), D_ALL);
+	MMIO_D(CURBASE(PIPE_A), D_BDW);
+	MMIO_D(CURBASE(PIPE_B), D_BDW);
+	MMIO_D(CURBASE(PIPE_C), D_BDW);
 
-	MMIO_D(CUR_FBC_CTL(PIPE_A), D_ALL);
-	MMIO_D(CUR_FBC_CTL(PIPE_B), D_ALL);
-	MMIO_D(CUR_FBC_CTL(PIPE_C), D_ALL);
+	MMIO_D(CUR_FBC_CTL(PIPE_A), D_BDW);
+	MMIO_D(CUR_FBC_CTL(PIPE_B), D_BDW);
+	MMIO_D(CUR_FBC_CTL(PIPE_C), D_BDW);
 
-	MMIO_D(_MMIO(0x700ac), D_ALL);
-	MMIO_D(_MMIO(0x710ac), D_ALL);
-	MMIO_D(_MMIO(0x720ac), D_ALL);
+	MMIO_D(_MMIO(0x700ac), D_BDW);
+	MMIO_D(_MMIO(0x710ac), D_BDW);
+	MMIO_D(_MMIO(0x720ac), D_BDW);
 
 	MMIO_D(_MMIO(0x70090), D_ALL);
 	MMIO_D(_MMIO(0x70094), D_ALL);
 	MMIO_D(_MMIO(0x70098), D_ALL);
 	MMIO_D(_MMIO(0x7009c), D_ALL);
 
-	MMIO_D(DSPCNTR(PIPE_A), D_ALL);
-	MMIO_D(DSPADDR(PIPE_A), D_ALL);
-	MMIO_D(DSPSTRIDE(PIPE_A), D_ALL);
-	MMIO_D(DSPPOS(PIPE_A), D_ALL);
-	MMIO_D(DSPSIZE(PIPE_A), D_ALL);
-	MMIO_DH(DSPSURF(PIPE_A), D_ALL, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_A), D_ALL);
-	MMIO_D(DSPSURFLIVE(PIPE_A), D_ALL);
-	MMIO_DH(REG_50080(PIPE_A, PLANE_PRIMARY), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(DSPCNTR(PIPE_B), D_ALL);
-	MMIO_D(DSPADDR(PIPE_B), D_ALL);
-	MMIO_D(DSPSTRIDE(PIPE_B), D_ALL);
-	MMIO_D(DSPPOS(PIPE_B), D_ALL);
-	MMIO_D(DSPSIZE(PIPE_B), D_ALL);
-	MMIO_DH(DSPSURF(PIPE_B), D_ALL, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_B), D_ALL);
-	MMIO_D(DSPSURFLIVE(PIPE_B), D_ALL);
-	MMIO_DH(REG_50080(PIPE_B, PLANE_PRIMARY), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(DSPCNTR(PIPE_C), D_ALL);
-	MMIO_D(DSPADDR(PIPE_C), D_ALL);
-	MMIO_D(DSPSTRIDE(PIPE_C), D_ALL);
-	MMIO_D(DSPPOS(PIPE_C), D_ALL);
-	MMIO_D(DSPSIZE(PIPE_C), D_ALL);
-	MMIO_DH(DSPSURF(PIPE_C), D_ALL, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_C), D_ALL);
-	MMIO_D(DSPSURFLIVE(PIPE_C), D_ALL);
-	MMIO_DH(REG_50080(PIPE_C, PLANE_PRIMARY), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(SPRCTL(PIPE_A), D_ALL);
-	MMIO_D(SPRLINOFF(PIPE_A), D_ALL);
-	MMIO_D(SPRSTRIDE(PIPE_A), D_ALL);
-	MMIO_D(SPRPOS(PIPE_A), D_ALL);
-	MMIO_D(SPRSIZE(PIPE_A), D_ALL);
-	MMIO_D(SPRKEYVAL(PIPE_A), D_ALL);
-	MMIO_D(SPRKEYMSK(PIPE_A), D_ALL);
-	MMIO_DH(SPRSURF(PIPE_A), D_ALL, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_A), D_ALL);
-	MMIO_D(SPROFFSET(PIPE_A), D_ALL);
-	MMIO_D(SPRSCALE(PIPE_A), D_ALL);
-	MMIO_D(SPRSURFLIVE(PIPE_A), D_ALL);
-	MMIO_DH(REG_50080(PIPE_A, PLANE_SPRITE0), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(SPRCTL(PIPE_B), D_ALL);
-	MMIO_D(SPRLINOFF(PIPE_B), D_ALL);
-	MMIO_D(SPRSTRIDE(PIPE_B), D_ALL);
-	MMIO_D(SPRPOS(PIPE_B), D_ALL);
-	MMIO_D(SPRSIZE(PIPE_B), D_ALL);
-	MMIO_D(SPRKEYVAL(PIPE_B), D_ALL);
-	MMIO_D(SPRKEYMSK(PIPE_B), D_ALL);
-	MMIO_DH(SPRSURF(PIPE_B), D_ALL, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_B), D_ALL);
-	MMIO_D(SPROFFSET(PIPE_B), D_ALL);
-	MMIO_D(SPRSCALE(PIPE_B), D_ALL);
-	MMIO_D(SPRSURFLIVE(PIPE_B), D_ALL);
-	MMIO_DH(REG_50080(PIPE_B, PLANE_SPRITE0), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(SPRCTL(PIPE_C), D_ALL);
-	MMIO_D(SPRLINOFF(PIPE_C), D_ALL);
-	MMIO_D(SPRSTRIDE(PIPE_C), D_ALL);
-	MMIO_D(SPRPOS(PIPE_C), D_ALL);
-	MMIO_D(SPRSIZE(PIPE_C), D_ALL);
-	MMIO_D(SPRKEYVAL(PIPE_C), D_ALL);
-	MMIO_D(SPRKEYMSK(PIPE_C), D_ALL);
-	MMIO_DH(SPRSURF(PIPE_C), D_ALL, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_C), D_ALL);
-	MMIO_D(SPROFFSET(PIPE_C), D_ALL);
-	MMIO_D(SPRSCALE(PIPE_C), D_ALL);
-	MMIO_D(SPRSURFLIVE(PIPE_C), D_ALL);
-	MMIO_DH(REG_50080(PIPE_C, PLANE_SPRITE0), D_ALL, NULL,
-		reg50080_mmio_write);
-
-	MMIO_D(HTOTAL(TRANSCODER_A), D_ALL);
-	MMIO_D(HBLANK(TRANSCODER_A), D_ALL);
-	MMIO_D(HSYNC(TRANSCODER_A), D_ALL);
-	MMIO_D(VTOTAL(TRANSCODER_A), D_ALL);
-	MMIO_D(VBLANK(TRANSCODER_A), D_ALL);
-	MMIO_D(VSYNC(TRANSCODER_A), D_ALL);
-	MMIO_D(BCLRPAT(TRANSCODER_A), D_ALL);
-	MMIO_D(VSYNCSHIFT(TRANSCODER_A), D_ALL);
-	MMIO_D(PIPESRC(TRANSCODER_A), D_ALL);
-
-	MMIO_D(HTOTAL(TRANSCODER_B), D_ALL);
-	MMIO_D(HBLANK(TRANSCODER_B), D_ALL);
-	MMIO_D(HSYNC(TRANSCODER_B), D_ALL);
-	MMIO_D(VTOTAL(TRANSCODER_B), D_ALL);
-	MMIO_D(VBLANK(TRANSCODER_B), D_ALL);
-	MMIO_D(VSYNC(TRANSCODER_B), D_ALL);
-	MMIO_D(BCLRPAT(TRANSCODER_B), D_ALL);
-	MMIO_D(VSYNCSHIFT(TRANSCODER_B), D_ALL);
-	MMIO_D(PIPESRC(TRANSCODER_B), D_ALL);
-
-	MMIO_D(HTOTAL(TRANSCODER_C), D_ALL);
-	MMIO_D(HBLANK(TRANSCODER_C), D_ALL);
-	MMIO_D(HSYNC(TRANSCODER_C), D_ALL);
-	MMIO_D(VTOTAL(TRANSCODER_C), D_ALL);
-	MMIO_D(VBLANK(TRANSCODER_C), D_ALL);
-	MMIO_D(VSYNC(TRANSCODER_C), D_ALL);
-	MMIO_D(BCLRPAT(TRANSCODER_C), D_ALL);
-	MMIO_D(VSYNCSHIFT(TRANSCODER_C), D_ALL);
-	MMIO_D(PIPESRC(TRANSCODER_C), D_ALL);
-
-	MMIO_D(HTOTAL(TRANSCODER_EDP), D_ALL);
-	MMIO_D(HBLANK(TRANSCODER_EDP), D_ALL);
-	MMIO_D(HSYNC(TRANSCODER_EDP), D_ALL);
-	MMIO_D(VTOTAL(TRANSCODER_EDP), D_ALL);
-	MMIO_D(VBLANK(TRANSCODER_EDP), D_ALL);
-	MMIO_D(VSYNC(TRANSCODER_EDP), D_ALL);
-	MMIO_D(BCLRPAT(TRANSCODER_EDP), D_ALL);
-	MMIO_D(VSYNCSHIFT(TRANSCODER_EDP), D_ALL);
+	MMIO_D(DSPCNTR(PIPE_A), D_BDW);
+	MMIO_D(DSPADDR(PIPE_A), D_BDW);
+	MMIO_D(DSPSTRIDE(PIPE_A), D_BDW);
+	MMIO_D(DSPPOS(PIPE_A), D_BDW);
+	MMIO_D(DSPSIZE(PIPE_A), D_BDW);
+	MMIO_DH(DSPSURF(PIPE_A), D_BDW, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_A), D_BDW);
+	MMIO_D(DSPSURFLIVE(PIPE_A), D_BDW);
+
+	MMIO_D(DSPCNTR(PIPE_B), D_BDW);
+	MMIO_D(DSPADDR(PIPE_B), D_BDW);
+	MMIO_D(DSPSTRIDE(PIPE_B), D_BDW);
+	MMIO_D(DSPPOS(PIPE_B), D_BDW);
+	MMIO_D(DSPSIZE(PIPE_B), D_BDW);
+	MMIO_DH(DSPSURF(PIPE_B), D_BDW, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_B), D_BDW);
+	MMIO_D(DSPSURFLIVE(PIPE_B), D_BDW);
+
+	MMIO_D(DSPCNTR(PIPE_C), D_BDW);
+	MMIO_D(DSPADDR(PIPE_C), D_BDW);
+	MMIO_D(DSPSTRIDE(PIPE_C), D_BDW);
+	MMIO_D(DSPPOS(PIPE_C), D_BDW);
+	MMIO_D(DSPSIZE(PIPE_C), D_BDW);
+	MMIO_DH(DSPSURF(PIPE_C), D_BDW, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_C), D_BDW);
+	MMIO_D(DSPSURFLIVE(PIPE_C), D_BDW);
+
+	MMIO_D(SPRCTL(PIPE_A), D_BDW);
+	MMIO_D(SPRLINOFF(PIPE_A), D_BDW);
+	MMIO_D(SPRSTRIDE(PIPE_A), D_BDW);
+	MMIO_D(SPRPOS(PIPE_A), D_BDW);
+	MMIO_D(SPRSIZE(PIPE_A), D_BDW);
+	MMIO_D(SPRKEYVAL(PIPE_A), D_BDW);
+	MMIO_D(SPRKEYMSK(PIPE_A), D_BDW);
+	MMIO_DH(SPRSURF(PIPE_A), D_BDW, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_A), D_BDW);
+	MMIO_D(SPROFFSET(PIPE_A), D_BDW);
+	MMIO_D(SPRSCALE(PIPE_A), D_BDW);
+	MMIO_D(SPRSURFLIVE(PIPE_A), D_BDW);
+
+	MMIO_D(SPRCTL(PIPE_B), D_BDW);
+	MMIO_D(SPRLINOFF(PIPE_B), D_BDW);
+	MMIO_D(SPRSTRIDE(PIPE_B), D_BDW);
+	MMIO_D(SPRPOS(PIPE_B), D_BDW);
+	MMIO_D(SPRSIZE(PIPE_B), D_BDW);
+	MMIO_D(SPRKEYVAL(PIPE_B), D_BDW);
+	MMIO_D(SPRKEYMSK(PIPE_B), D_BDW);
+	MMIO_DH(SPRSURF(PIPE_B), D_BDW, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_B), D_BDW);
+	MMIO_D(SPROFFSET(PIPE_B), D_BDW);
+	MMIO_D(SPRSCALE(PIPE_B), D_BDW);
+	MMIO_D(SPRSURFLIVE(PIPE_B), D_BDW);
+
+	MMIO_D(SPRCTL(PIPE_C), D_BDW);
+	MMIO_D(SPRLINOFF(PIPE_C), D_BDW);
+	MMIO_D(SPRSTRIDE(PIPE_C), D_BDW);
+	MMIO_D(SPRPOS(PIPE_C), D_BDW);
+	MMIO_D(SPRSIZE(PIPE_C), D_BDW);
+	MMIO_D(SPRKEYVAL(PIPE_C), D_BDW);
+	MMIO_D(SPRKEYMSK(PIPE_C), D_BDW);
+	MMIO_DH(SPRSURF(PIPE_C), D_BDW, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_C), D_BDW);
+	MMIO_D(SPROFFSET(PIPE_C), D_BDW);
+	MMIO_D(SPRSCALE(PIPE_C), D_BDW);
+	MMIO_D(SPRSURFLIVE(PIPE_C), D_BDW);
+
+	MMIO_D(HTOTAL(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(HBLANK(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(HSYNC(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(VTOTAL(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(VBLANK(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(VSYNC(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(BCLRPAT(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(VSYNCSHIFT(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(PIPESRC(TRANSCODER_A), D_PRE_SKL);
+	MMIO_D(PIPE_MULT(TRANSCODER_A), D_PRE_SKL);
+
+	MMIO_D(HTOTAL(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(HBLANK(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(HSYNC(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(VTOTAL(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(VBLANK(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(VSYNC(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(BCLRPAT(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(VSYNCSHIFT(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(PIPESRC(TRANSCODER_B), D_PRE_SKL);
+	MMIO_D(PIPE_MULT(TRANSCODER_B), D_PRE_SKL);
+
+	MMIO_D(HTOTAL(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(HBLANK(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(HSYNC(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(VTOTAL(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(VBLANK(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(VSYNC(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(BCLRPAT(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(VSYNCSHIFT(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(PIPESRC(TRANSCODER_C), D_PRE_SKL);
+	MMIO_D(PIPE_MULT(TRANSCODER_C), D_PRE_SKL);
+
+	MMIO_D(HTOTAL(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(HBLANK(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(HSYNC(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(VTOTAL(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(VBLANK(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(VSYNC(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(BCLRPAT(TRANSCODER_EDP), D_PRE_SKL);
+	MMIO_D(VSYNCSHIFT(TRANSCODER_EDP), D_PRE_SKL);
 
 	MMIO_D(PIPE_DATA_M1(TRANSCODER_A), D_ALL);
 	MMIO_D(PIPE_DATA_N1(TRANSCODER_A), D_ALL);
@@ -2430,10 +2835,6 @@ static int init_generic_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(GAMMA_MODE(PIPE_B), D_ALL);
 	MMIO_D(GAMMA_MODE(PIPE_C), D_ALL);
 
-	MMIO_D(PIPE_MULT(PIPE_A), D_ALL);
-	MMIO_D(PIPE_MULT(PIPE_B), D_ALL);
-	MMIO_D(PIPE_MULT(PIPE_C), D_ALL);
-
 	MMIO_D(HSW_TVIDEO_DIP_CTL(TRANSCODER_A), D_ALL);
 	MMIO_D(HSW_TVIDEO_DIP_CTL(TRANSCODER_B), D_ALL);
 	MMIO_D(HSW_TVIDEO_DIP_CTL(TRANSCODER_C), D_ALL);
@@ -2801,9 +3202,52 @@ static int init_broadwell_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(WM_MISC, D_BDW);
 	MMIO_D(_MMIO(_SRD_CTL_EDP), D_BDW);
 
-	MMIO_D(_MMIO(0x6671c), D_BDW_PLUS);
-	MMIO_D(_MMIO(0x66c00), D_BDW_PLUS);
-	MMIO_D(_MMIO(0x66c04), D_BDW_PLUS);
+	MMIO_D(HDCP_KEY_CONF, D_BDW_PLUS);
+	MMIO_D(HDCP_KEY_STATUS, D_BDW_PLUS);
+	MMIO_D(HDCP_AKSV_LO, D_BDW_PLUS);
+	MMIO_D(HDCP_AKSV_HI, D_BDW_PLUS);
+	MMIO_D(HDCP_REP_CTL, D_BDW_PLUS);
+	MMIO_D(HDCP_SHA_V_PRIME_H0, D_BDW_PLUS);
+	MMIO_D(HDCP_SHA_V_PRIME_H1, D_BDW_PLUS);
+	MMIO_D(HDCP_SHA_V_PRIME_H2, D_BDW_PLUS);
+	MMIO_D(HDCP_SHA_V_PRIME_H3, D_BDW_PLUS);
+	MMIO_D(HDCP_SHA_V_PRIME_H4, D_BDW_PLUS);
+
+	MMIO_D(PORT_HDCP_CONF(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANINIT(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANLO(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANHI(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVLO(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVHI(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_RPRIME(PORT_A), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_STATUS(PORT_A), D_BDW_PLUS);
+
+	MMIO_D(PORT_HDCP_CONF(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANINIT(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANLO(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANHI(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVLO(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVHI(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_RPRIME(PORT_B), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_STATUS(PORT_B), D_BDW_PLUS);
+
+	MMIO_D(PORT_HDCP_CONF(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANINIT(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANLO(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANHI(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVLO(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVHI(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_RPRIME(PORT_C), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_STATUS(PORT_C), D_BDW_PLUS);
+
+	MMIO_D(PORT_HDCP_CONF(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANINIT(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANLO(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_ANHI(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVLO(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_BKSVHI(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_RPRIME(PORT_D), D_BDW_PLUS);
+	MMIO_D(PORT_HDCP_STATUS(PORT_D), D_BDW_PLUS);
 
 	MMIO_D(HSW_GTT_CACHE_EN, D_BDW_PLUS);
 
@@ -2908,134 +3352,110 @@ static int init_skl_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(DPLL_CTRL2, D_SKL_PLUS);
 	MMIO_DH(DPLL_STATUS, D_SKL_PLUS, dpll_status_read, NULL);
 
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
-
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
-
-	MMIO_DH(SKL_PS_CTRL(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
-
-	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 3), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 3), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 3), D_SKL_PLUS, NULL, NULL);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_A, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_A, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_B, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_B, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_C, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_POS(PIPE_C, 1), D_SKL_PLUS);
+
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_A, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_A, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_B, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_B, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_C, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_WIN_SZ(PIPE_C, 1), D_SKL_PLUS);
+
+	MMIO_D(SKL_PS_CTRL(PIPE_A, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_CTRL(PIPE_A, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_CTRL(PIPE_B, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_CTRL(PIPE_B, 1), D_SKL_PLUS);
+	MMIO_D(SKL_PS_CTRL(PIPE_C, 0), D_SKL_PLUS);
+	MMIO_D(SKL_PS_CTRL(PIPE_C, 1), D_SKL_PLUS);
+
+	MMIO_DH(CURCNTR(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURCNTR(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURCNTR(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+
+	MMIO_DH(CURPOS(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURPOS(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURPOS(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+
+	MMIO_DH(CURBASE(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURBASE(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CURBASE(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+
+	MMIO_DH(CUR_FBC_CTL(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CUR_FBC_CTL(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_DH(CUR_FBC_CTL(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+
+	MMIO_DH(_MMIO(0x700ac), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(0x710ac), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(0x720ac), D_SKL_PLUS, NULL, NULL);
 
 	MMIO_DH(CUR_BUF_CFG(PIPE_A), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_BUF_CFG(PIPE_B), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_BUF_CFG(PIPE_C), D_SKL_PLUS, NULL, NULL);
 
-	MMIO_F(PLANE_WM(PIPE_A, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_A, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_A, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-
-	MMIO_F(PLANE_WM(PIPE_B, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_B, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_B, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-
-	MMIO_F(PLANE_WM(PIPE_C, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_C, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-	MMIO_F(PLANE_WM(PIPE_C, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
-
 	MMIO_F(CUR_WM(PIPE_A, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
 	MMIO_F(CUR_WM(PIPE_B, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
 	MMIO_F(CUR_WM(PIPE_C, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
 
-	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
-
 	MMIO_DH(CUR_WM_TRANS(PIPE_A), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_WM_TRANS(PIPE_B), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_WM_TRANS(PIPE_C), D_SKL_PLUS, NULL, NULL);
 
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 3), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 3), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 3), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 1)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 2)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 3)), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 4)), D_SKL_PLUS, NULL, NULL);
-
-	MMIO_D(_MMIO(_PLANE_CTL_3_A), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_CTL_3_B), D_SKL_PLUS);
-	MMIO_D(_MMIO(0x72380), D_SKL_PLUS);
-	MMIO_D(_MMIO(0x7239c), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_SURF_3_A), D_SKL_PLUS);
+	/* Registers of pipe timing */
+	MMIO_F(HTOTAL(TRANSCODER_A), 4 * 7, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(HTOTAL(TRANSCODER_B), 4 * 7, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(HTOTAL(TRANSCODER_C), 4 * 7, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(HTOTAL(TRANSCODER_EDP), 4 * 7, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_D(PIPESRC(TRANSCODER_A), D_SKL_PLUS);
+	MMIO_D(PIPESRC(TRANSCODER_B), D_SKL_PLUS);
+	MMIO_D(PIPESRC(TRANSCODER_C), D_SKL_PLUS);
+	MMIO_D(PIPESRC(TRANSCODER_EDP), D_SKL_PLUS);
+
+	MMIO_F(BCLRPAT(TRANSCODER_A), 4 * 4, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(BCLRPAT(TRANSCODER_B), 4 * 4, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(BCLRPAT(TRANSCODER_C), 4 * 4, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(BCLRPAT(TRANSCODER_EDP), 4 * 4, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_D(PLANE_CTL(TRANSCODER_A, PLANE_SPRITE1), D_SKL_PLUS);
+	MMIO_D(PLANE_CTL(TRANSCODER_B, PLANE_SPRITE1), D_SKL_PLUS);
+	MMIO_D(PLANE_CTL(TRANSCODER_C, PLANE_SPRITE1), D_SKL_PLUS);
+
+	MMIO_D(PLANE_SURF(TRANSCODER_A, PLANE_SPRITE1), D_SKL_PLUS);
+	MMIO_D(PLANE_SURF(TRANSCODER_B, PLANE_SPRITE1), D_SKL_PLUS);
+	MMIO_D(PLANE_SURF(TRANSCODER_C, PLANE_SPRITE1), D_SKL_PLUS);
+
+//	MMIO_PLANES_DH(PLANE_COLOR_CTL, D_SKL, NULL, NULL);
+	MMIO_PLANES_DH(PLANE_CTL, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_STRIDE, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_POS, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_SIZE, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_KEYVAL, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_KEYMSK, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+
+	MMIO_PLANES_DH(PLANE_SURF, D_SKL_PLUS, NULL, skl_plane_surf_write);
+	MMIO_PLANES_DH(REG_50080, D_SKL_PLUS, NULL, reg50080_mmio_write);
+
+	MMIO_PLANES_DH(PLANE_KEYMAX, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_OFFSET, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_AUX_DIST, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+	MMIO_PLANES_DH(PLANE_AUX_OFFSET, D_SKL_PLUS, NULL, skl_plane_mmio_write);
+
+	MMIO_PLANES_SDH(PLANE_WM_BASE, 4 * 8, D_SKL_PLUS, NULL, NULL);
+	MMIO_PLANES_DH(PLANE_WM_TRANS, D_SKL_PLUS, NULL, NULL);
+	MMIO_PLANES_DH(PLANE_NV12_BUF_CFG, D_SKL_PLUS, NULL, NULL);
+	MMIO_PLANES_DH(PLANE_BUF_CFG, D_SKL_PLUS, NULL, NULL);
 
 	MMIO_D(CSR_SSP_BASE, D_SKL_PLUS);
 	MMIO_D(CSR_HTP_SKL, D_SKL_PLUS);
 	MMIO_D(CSR_LAST_WRITE, D_SKL_PLUS);
 
 	MMIO_DFH(BDW_SCRATCH1, D_SKL_PLUS, F_CMD_ACCESS, NULL, NULL);
+	MMIO_PLANES_DH(SKL_PLANE_SURFLIVE, D_SKL_PLUS, NULL, NULL);
 
 	MMIO_D(SKL_DFSM, D_SKL_PLUS);
 	MMIO_D(DISPIO_CR_TX_BMU_CR0, D_SKL_PLUS);
@@ -3065,7 +3485,9 @@ static int init_skl_mmio_info(struct intel_gvt *gvt)
 
 	MMIO_D(_MMIO(0x46430), D_SKL_PLUS);
 
-	MMIO_D(_MMIO(0x46520), D_SKL_PLUS);
+	MMIO_D(_MMIO(_CLKGATE_DIS_PSL_A), D_SKL_PLUS);
+	MMIO_D(_MMIO(_CLKGATE_DIS_PSL_B), D_SKL_PLUS);
+	MMIO_D(_MMIO(_CLKGATE_DIS_PSL_C), D_SKL_PLUS);
 
 	MMIO_D(_MMIO(0xc403c), D_SKL_PLUS);
 	MMIO_DFH(GEN8_GARBCNTL, D_SKL_PLUS, F_CMD_ACCESS, NULL, NULL);
@@ -3086,16 +3508,6 @@ static int init_skl_mmio_info(struct intel_gvt *gvt)
 	MMIO_D(_MMIO(0x71034), D_SKL_PLUS);
 	MMIO_D(_MMIO(0x72034), D_SKL_PLUS);
 
-	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_A)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_B)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_C)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_A)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_B)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_C)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_A)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_B)), D_SKL_PLUS);
-	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_C)), D_SKL_PLUS);
-
 	MMIO_D(_MMIO(0x44500), D_SKL_PLUS);
 #define CSFE_CHICKEN1_REG(base) _MMIO((base) + 0xD4)
 	MMIO_RING_DFH(CSFE_CHICKEN1_REG, D_SKL_PLUS, F_MODE_MASK | F_CMD_ACCESS,
@@ -3106,8 +3518,8 @@ static int init_skl_mmio_info(struct intel_gvt *gvt)
 	MMIO_DFH(GEN9_WM_CHICKEN3, D_SKL_PLUS, F_MODE_MASK | F_CMD_ACCESS,
 		 NULL, NULL);
 
-	MMIO_D(GAMT_CHKN_BIT_REG, D_KBL);
-	MMIO_D(GEN9_CTX_PREEMPT_REG, D_KBL | D_SKL);
+	MMIO_D(GAMT_CHKN_BIT_REG, D_KBL | D_CFL);
+	MMIO_D(GEN9_CTX_PREEMPT_REG, D_SKL_PLUS);
 
 	return 0;
 }
@@ -3555,7 +3967,6 @@ int intel_vgpu_mmio_reg_rw(struct intel_vgpu *vgpu, unsigned int offset,
 		u64 ro_mask = mmio_info->ro_mask;
 		u32 old_vreg = 0;
 		u64 data = 0;
-
 		if (intel_gvt_mmio_has_mode_mask(gvt, mmio_info->offset)) {
 			old_vreg = vgpu_vreg(vgpu, offset);
 		}
diff --git a/drivers/gpu/drm/i915/gvt/interrupt.c b/drivers/gpu/drm/i915/gvt/interrupt.c
index 11accd3e1023..dfff53f946b0 100644
--- a/drivers/gpu/drm/i915/gvt/interrupt.c
+++ b/drivers/gpu/drm/i915/gvt/interrupt.c
@@ -593,6 +593,10 @@ static void gen8_init_irq(
 		SET_BIT_INFO(irq, 4, SPRITE_A_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_A);
 		SET_BIT_INFO(irq, 4, SPRITE_B_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_B);
 		SET_BIT_INFO(irq, 4, SPRITE_C_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_C);
+
+		SET_BIT_INFO(irq, 5, PLANE_3_A_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_A);
+		SET_BIT_INFO(irq, 5, PLANE_3_B_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_B);
+		SET_BIT_INFO(irq, 5, PLANE_3_C_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_C);
 	}
 
 	/* GEN8 interrupt PCU events */
@@ -652,6 +656,9 @@ static enum hrtimer_restart vblank_timer_fn(struct hrtimer *data)
 	irq = container_of(vblank_timer, struct intel_gvt_irq, vblank_timer);
 	gvt = container_of(irq, struct intel_gvt, irq);
 
+	/* enable vblank emulation service for vgpus without assigned
+	 * display port.
+	 */
 	intel_gvt_request_service(gvt, INTEL_GVT_REQUEST_EMULATE_VBLANK);
 	hrtimer_add_expires_ns(&vblank_timer->timer, vblank_timer->period);
 	return HRTIMER_RESTART;
diff --git a/drivers/gpu/drm/i915/gvt/interrupt.h b/drivers/gpu/drm/i915/gvt/interrupt.h
index 5313fb1b33e1..f7d7ade4f13c 100644
--- a/drivers/gpu/drm/i915/gvt/interrupt.h
+++ b/drivers/gpu/drm/i915/gvt/interrupt.h
@@ -92,6 +92,9 @@ enum intel_gvt_event_type {
 	SPRITE_A_FLIP_DONE,
 	SPRITE_B_FLIP_DONE,
 	SPRITE_C_FLIP_DONE,
+	PLANE_3_A_FLIP_DONE,
+	PLANE_3_B_FLIP_DONE,
+	PLANE_3_C_FLIP_DONE,
 
 	PCU_THERMAL,
 	PCU_PCODE2DRIVER_MAILBOX,
diff --git a/drivers/gpu/drm/i915/gvt/kvmgt.c b/drivers/gpu/drm/i915/gvt/kvmgt.c
index 04a5a0d90823..7cd63c16113c 100644
--- a/drivers/gpu/drm/i915/gvt/kvmgt.c
+++ b/drivers/gpu/drm/i915/gvt/kvmgt.c
@@ -608,7 +608,8 @@ static int kvmgt_set_opregion(void *p_vgpu)
 static int kvmgt_set_edid(void *p_vgpu, int port_num)
 {
 	struct intel_vgpu *vgpu = (struct intel_vgpu *)p_vgpu;
-	struct intel_vgpu_port *port = intel_vgpu_port(vgpu, port_num);
+	struct intel_vgpu_display *disp_cfg = &vgpu->disp_cfg;
+	struct intel_vgpu_display_path *disp_path = NULL, *n;
 	struct vfio_edid_region *base;
 	int ret;
 
@@ -620,9 +621,12 @@ static int kvmgt_set_edid(void *p_vgpu, int port_num)
 	base->vfio_edid_regs.edid_offset = EDID_BLOB_OFFSET;
 	base->vfio_edid_regs.edid_max_size = EDID_SIZE;
 	base->vfio_edid_regs.edid_size = EDID_SIZE;
-	base->vfio_edid_regs.max_xres = vgpu_edid_xres(port->id);
-	base->vfio_edid_regs.max_yres = vgpu_edid_yres(port->id);
-	base->edid_blob = port->edid->edid_block;
+	list_for_each_entry_safe(disp_path, n, &disp_cfg->path_list, list) {
+		base->vfio_edid_regs.max_xres = vgpu_edid_xres(disp_path->edid_id);
+		base->vfio_edid_regs.max_yres = vgpu_edid_yres(disp_path->edid_id);
+		base->edid_blob = disp_path->edid->edid_block;
+		break;
+	}
 
 	ret = intel_vgpu_register_reg(vgpu,
 			VFIO_REGION_TYPE_GFX,
diff --git a/drivers/gpu/drm/i915/gvt/mmio.c b/drivers/gpu/drm/i915/gvt/mmio.c
index a55178884d67..fc0c54666ead 100644
--- a/drivers/gpu/drm/i915/gvt/mmio.c
+++ b/drivers/gpu/drm/i915/gvt/mmio.c
@@ -234,10 +234,15 @@ int intel_vgpu_emulate_mmio_write(struct intel_vgpu *vgpu, u64 pa,
 void intel_vgpu_reset_mmio(struct intel_vgpu *vgpu, bool dmlr)
 {
 	struct intel_gvt *gvt = vgpu->gvt;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct intel_runtime_info *runtime = RUNTIME_INFO(dev_priv);
 	const struct intel_gvt_device_info *info = &gvt->device_info;
 	void  *mmio = gvt->firmware.mmio;
 
 	if (dmlr) {
+		enum pipe pipe = INVALID_PIPE;
+		int scaler;
+
 		memcpy(vgpu->mmio.vreg, mmio, info->mmio_size);
 
 		vgpu_vreg_t(vgpu, GEN6_GT_THREAD_STATUS_REG) = 0;
@@ -245,7 +250,18 @@ void intel_vgpu_reset_mmio(struct intel_vgpu *vgpu, bool dmlr)
 		/* set the bit 0:2(Core C-State ) to C0 */
 		vgpu_vreg_t(vgpu, GEN6_GT_CORE_STATUS) = 0;
 
-		if (IS_BROXTON(vgpu->gvt->dev_priv)) {
+		if (IS_BROADWELL(dev_priv)) {
+			vgpu_vreg_t(vgpu, PCH_ADPA) &= ~ADPA_CRT_HOTPLUG_MONITOR_MASK;
+			for (pipe = PIPE_A; pipe <= PIPE_C; pipe++) {
+				vgpu_vreg_t(vgpu, PF_CTL(pipe)) = 0;
+				vgpu_vreg_t(vgpu, PF_WIN_SZ(pipe)) = 0;
+				vgpu_vreg_t(vgpu, PF_WIN_POS(pipe)) = 0;
+				vgpu_vreg_t(vgpu, PF_VSCALE(pipe)) = 0;
+				vgpu_vreg_t(vgpu, PF_HSCALE(pipe)) = 0;
+			}
+		}
+
+		if (IS_BROXTON(dev_priv)) {
 			vgpu_vreg_t(vgpu, BXT_P_CR_GT_DISP_PWRON) &=
 				    ~(BIT(0) | BIT(1));
 			vgpu_vreg_t(vgpu, BXT_PORT_CL1CM_DW0(DPIO_PHY0)) &=
@@ -271,6 +287,33 @@ void intel_vgpu_reset_mmio(struct intel_vgpu *vgpu, bool dmlr)
 			vgpu_vreg_t(vgpu, BXT_PHY_CTL(PORT_C)) |=
 				    BXT_PHY_CMNLANE_POWERDOWN_ACK |
 				    BXT_PHY_LANE_POWERDOWN_ACK;
+			vgpu_vreg_t(vgpu, SKL_FUSE_STATUS) |=
+				SKL_FUSE_DOWNLOAD_STATUS |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG0) |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG1) |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG2);
+		}
+		if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
+		    IS_COFFEELAKE(dev_priv)) {
+			vgpu_vreg_t(vgpu, SKL_FUSE_STATUS) |=
+				SKL_FUSE_DOWNLOAD_STATUS |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG0) |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG1) |
+				SKL_FUSE_PG_DIST_STATUS(SKL_PG2);
+			vgpu_vreg_t(vgpu, LCPLL1_CTL) |=
+				LCPLL_PLL_ENABLE |
+				LCPLL_PLL_LOCK;
+			vgpu_vreg_t(vgpu, LCPLL2_CTL) |= LCPLL_PLL_ENABLE;
+		}
+
+		if (INTEL_GEN(dev_priv) >= 9) {
+			for_each_pipe(dev_priv, pipe) {
+				for (scaler = 0; scaler < runtime->num_scalers[pipe]; scaler++) {
+					vgpu_vreg_t(vgpu, SKL_PS_WIN_POS(pipe, scaler)) = 0;
+					vgpu_vreg_t(vgpu, SKL_PS_WIN_SZ(pipe, scaler)) = 0;
+					vgpu_vreg_t(vgpu, SKL_PS_CTRL(pipe, scaler)) = 0;
+				}
+			}
 		}
 	} else {
 #define GVT_GEN8_MMIO_RESET_OFFSET		(0x44200)
diff --git a/drivers/gpu/drm/i915/gvt/reg.h b/drivers/gpu/drm/i915/gvt/reg.h
index 5b66e14c5b7b..6d833cfdafe9 100644
--- a/drivers/gpu/drm/i915/gvt/reg.h
+++ b/drivers/gpu/drm/i915/gvt/reg.h
@@ -57,14 +57,16 @@
 
 #define VGT_SPRSTRIDE(pipe)	_PIPE(pipe, _SPRA_STRIDE, _PLANE_STRIDE_2_B)
 
-#define _REG_701C0(pipe, plane) (0x701c0 + pipe * 0x1000 + (plane - 1) * 0x100)
-#define _REG_701C4(pipe, plane) (0x701c4 + pipe * 0x1000 + (plane - 1) * 0x100)
-
+#define PLANE_WM_BASE(pipe, plane) _MMIO(_PLANE_WM_BASE(pipe, plane))
+#define SKL_PLANE_SURFLIVE(pipe, plane) (_MMIO(_DSPASURFLIVE + (pipe) * 0x1000 + (plane) * 0x100))
+#define SKL_CURSOR_SURFLIVE(pipe) (_MMIO(_CURASURFLIVE + (pipe) * 0x1000))
+#define SKL_PLANE_REG_TO_PIPE(reg) (((reg) >> 12) & 0x3)
+#define SKL_PLANE_REG_TO_PLANE(reg) ((((reg) & 0xFFF) - 0x180) >> 8)
 #define SKL_FLIP_EVENT(pipe, plane) (PRIMARY_A_FLIP_DONE + (plane) * 3 + (pipe))
-
 #define PLANE_CTL_ASYNC_FLIP		(1 << 9)
 #define REG50080_FLIP_TYPE_MASK	0x3
 #define REG50080_FLIP_TYPE_ASYNC	0x1
+#define SKL_CURSOR_MODE_MASK 0x3F
 
 #define REG_50080(_pipe, _plane) ({ \
 	typeof(_pipe) (p) = (_pipe); \
diff --git a/drivers/gpu/drm/i915/gvt/vgpu.c b/drivers/gpu/drm/i915/gvt/vgpu.c
index 8ca5f589edf3..92b4fb22385b 100644
--- a/drivers/gpu/drm/i915/gvt/vgpu.c
+++ b/drivers/gpu/drm/i915/gvt/vgpu.c
@@ -229,6 +229,8 @@ void intel_gvt_activate_vgpu(struct intel_vgpu *vgpu)
  */
 void intel_gvt_deactivate_vgpu(struct intel_vgpu *vgpu)
 {
+	struct intel_gvt *gvt = vgpu->gvt;
+
 	mutex_lock(&vgpu->vgpu_lock);
 	atomic_set(&vgpu->active, false);
 
@@ -238,6 +240,22 @@ void intel_gvt_deactivate_vgpu(struct intel_vgpu *vgpu)
 		mutex_lock(&vgpu->vgpu_lock);
 	}
 
+	intel_vgpu_display_set_foreground(vgpu, false);
+	if (READ_ONCE(gvt->disp_auto_switch)) {
+		u32 owner = 0;
+
+		mutex_lock(&gvt->sw_in_progress);
+		owner = intel_vgpu_display_find_owner(vgpu, true, true);
+		if (owner != gvt->disp_owner) {
+			gvt->disp_owner = owner;
+			gvt_dbg_dpy("Schedule display owner changed to 0x%08x due to "
+				    "deactivate of vGPU-%d\n",
+				    gvt->disp_owner, vgpu->id);
+			queue_work(system_unbound_wq, &gvt->switch_display_work);
+		}
+		mutex_unlock(&gvt->sw_in_progress);
+	}
+
 	intel_vgpu_stop_schedule(vgpu);
 
 	/**
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index cab632791f73..e11e87acd1ef 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -2923,13 +2923,20 @@ static int i915_ddb_info(struct seq_file *m, void *unused)
 
 		for_each_plane_id_on_crtc(crtc, plane_id) {
 			entry = &crtc_state->wm.skl.plane_ddb_y[plane_id];
-			seq_printf(m, "  Plane%-8d%8u%8u%8u\n", plane_id + 1,
+			seq_printf(m, "  YPlane%-8d%8u%8u%8u\n", plane_id + 1,
+				   entry->start, entry->end,
+				   skl_ddb_entry_size(entry));
+			entry = &crtc_state->wm.skl.plane_ddb_uv[plane_id];
+			seq_printf(m, "  UVPlane%-8d%8u%8u%8u\n", plane_id + 1,
 				   entry->start, entry->end,
 				   skl_ddb_entry_size(entry));
 		}
 
 		entry = &crtc_state->wm.skl.plane_ddb_y[PLANE_CURSOR];
-		seq_printf(m, "  %-13s%8u%8u%8u\n", "Cursor", entry->start,
+		seq_printf(m, "  %-13s%8u%8u%8u\n", "YCursor", entry->start,
+			   entry->end, skl_ddb_entry_size(entry));
+		entry = &crtc_state->wm.skl.plane_ddb_uv[PLANE_CURSOR];
+		seq_printf(m, "  %-13s%8u%8u%8u\n", "UVCursor", entry->start,
 			   entry->end, skl_ddb_entry_size(entry));
 	}
 
diff --git a/drivers/gpu/drm/i915/i915_irq.c b/drivers/gpu/drm/i915/i915_irq.c
index 433d2dc1c5f3..d5099f8493a2 100644
--- a/drivers/gpu/drm/i915/i915_irq.c
+++ b/drivers/gpu/drm/i915/i915_irq.c
@@ -52,6 +52,10 @@
 #include "i915_trace.h"
 #include "intel_pm.h"
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+#include "gvt.h"
+#endif
+
 /**
  * DOC: interrupt handling
  *
@@ -1265,6 +1269,24 @@ static void i9xx_pipe_crc_irq_handler(struct drm_i915_private *dev_priv,
 				     res1, res2);
 }
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+static inline void gvt_notify_vblank(struct drm_i915_private *dev_priv,
+				     enum pipe pipe)
+{
+	if (dev_priv->gvt && dev_priv->gvt->pipe_info[pipe].owner)
+		queue_work(system_unbound_wq,
+			   &dev_priv->gvt->pipe_info[pipe].vblank_work);
+}
+
+static inline void gvt_notify_flipdone(struct drm_i915_private *dev_priv,
+				       enum pipe pipe, enum plane_id plane)
+{
+	if (dev_priv->gvt && dev_priv->gvt->pipe_info[pipe].plane_info[plane].owner)
+		queue_work(system_unbound_wq,
+			   &dev_priv->gvt->pipe_info[pipe].plane_info[plane].flipdone_work);
+}
+#endif
+
 static void i9xx_pipestat_irq_reset(struct drm_i915_private *dev_priv)
 {
 	enum pipe pipe;
@@ -2315,8 +2337,12 @@ gen8_de_irq_handler(struct drm_i915_private *dev_priv, u32 master_ctl)
 		ret = IRQ_HANDLED;
 		I915_WRITE(GEN8_DE_PIPE_IIR(pipe), iir);
 
-		if (iir & GEN8_PIPE_VBLANK)
+		if (iir & GEN8_PIPE_VBLANK) {
 			drm_handle_vblank(&dev_priv->drm, pipe);
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+			gvt_notify_vblank(dev_priv, pipe);
+#endif
+		}
 
 		if (iir & GEN8_PIPE_CDCLK_CRC_DONE)
 			hsw_pipe_crc_irq_handler(dev_priv, pipe);
@@ -2324,6 +2350,11 @@ gen8_de_irq_handler(struct drm_i915_private *dev_priv, u32 master_ctl)
 		if (iir & GEN8_PIPE_FIFO_UNDERRUN)
 			intel_cpu_fifo_underrun_irq_handler(dev_priv, pipe);
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		if (iir & GEN9_PIPE_PLANE_FLIP_DONE(PLANE_PRIMARY))
+			gvt_notify_flipdone(dev_priv, pipe, PLANE_PRIMARY);
+#endif
+
 		fault_errors = iir & gen8_de_pipe_fault_mask(dev_priv);
 		if (fault_errors)
 			DRM_ERROR("Fault errors on pipe %c: 0x%08x\n",
@@ -2576,9 +2607,14 @@ int bdw_enable_vblank(struct drm_crtc *crtc)
 	struct drm_i915_private *dev_priv = to_i915(crtc->dev);
 	enum pipe pipe = to_intel_crtc(crtc)->pipe;
 	unsigned long irqflags;
+	unsigned long irq_enable = GEN8_PIPE_VBLANK;
+
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	irq_enable |= GEN9_PIPE_PLANE_FLIP_DONE(PLANE_PRIMARY);
+#endif
 
 	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	bdw_enable_pipe_irq(dev_priv, pipe, GEN8_PIPE_VBLANK);
+	bdw_enable_pipe_irq(dev_priv, pipe, irq_enable);
 	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
 
 	/* Even if there is no DMC, frame counter can get stuck when
@@ -2646,7 +2682,9 @@ void bdw_disable_vblank(struct drm_crtc *crtc)
 	unsigned long irqflags;
 
 	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	bdw_disable_pipe_irq(dev_priv, pipe, GEN8_PIPE_VBLANK);
+	/*since guest will see all the pipes, we don't want it disable vblank*/
+	if (!dev_priv->gvt)
+		bdw_disable_pipe_irq(dev_priv, pipe, GEN8_PIPE_VBLANK);
 	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
 }
 
@@ -3321,6 +3359,9 @@ static void gen8_de_irq_postinstall(struct drm_i915_private *dev_priv)
 		de_pipe_masked |= GEN9_DE_PIPE_IRQ_FAULT_ERRORS;
 		de_port_masked |= GEN9_AUX_CHANNEL_B | GEN9_AUX_CHANNEL_C |
 				  GEN9_AUX_CHANNEL_D;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		de_pipe_masked |= GEN9_PIPE_PLANE_FLIP_DONE(PLANE_PRIMARY);
+#endif
 		if (IS_GEN9_LP(dev_priv))
 			de_port_masked |= BXT_DE_PORT_GMBUS;
 	} else {
diff --git a/drivers/gpu/drm/i915/i915_sysfs.c b/drivers/gpu/drm/i915/i915_sysfs.c
index 65476909d1bf..163e43e70af3 100644
--- a/drivers/gpu/drm/i915/i915_sysfs.c
+++ b/drivers/gpu/drm/i915/i915_sysfs.c
@@ -38,6 +38,10 @@
 #include "intel_pm.h"
 #include "intel_sideband.h"
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+#include "gvt/gvt.h"
+#endif
+
 static inline struct drm_i915_private *kdev_minor_to_i915(struct device *kdev)
 {
 	struct drm_minor *minor = dev_get_drvdata(kdev);
@@ -569,10 +573,328 @@ static void i915_setup_error_capture(struct device *kdev) {}
 static void i915_teardown_error_capture(struct device *kdev) {}
 #endif
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+static ssize_t gvt_disp_ports_mask_show(
+	struct device *kdev, struct device_attribute *attr, char *buf)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+
+	return snprintf(buf, PAGE_SIZE, "Display ports assignment: 0x%016llx\n",
+			dev_priv->gvt->sel_disp_port_mask);
+}
+
+static ssize_t gvt_disp_ports_mask_store(
+	struct device *kdev, struct device_attribute *attr,
+	const char *buf, size_t count)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+	u64 val;
+	ssize_t ret;
+
+	ret = kstrtou64(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	intel_gvt_store_vgpu_display_mask(dev_priv, val);
+
+	return count;
+}
+
+static ssize_t gvt_disp_ports_status_show(
+	struct device *kdev, struct device_attribute *attr, char *buf)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+	struct intel_gvt *gvt = dev_priv->gvt;
+	int id;
+	u32 port_mask;
+	u8 port_sel, port;
+	ssize_t count_total = 0;
+	size_t buf_size = PAGE_SIZE;
+	size_t count = 0;
+
+	count = snprintf(buf, buf_size, "Auto host/vGPU display switch: %s\n",
+			 READ_ONCE(dev_priv->gvt->disp_auto_switch) ?
+			 "Y" : "N");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size, "Available display ports: 0x%08x\n",
+			 gvt->avail_disp_port_mask);
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	for (port = PORT_A; port < I915_MAX_PORTS; port++) {
+		port_sel = intel_gvt_external_disp_id_from_port(port);
+		if (gvt->avail_disp_port_mask & (port_sel << port * 4)) {
+			count = snprintf(buf, buf_size, "  ( PORT_%c(%d) )\n",
+					 port_name(port), port_sel);
+			buf_size -= count;
+			count_total += count;
+			buf += count;
+			if (!buf_size)
+				return count_total;
+		}
+	}
+
+	count = snprintf(buf, buf_size,
+			 "Display ports assignment: 0x%016llx\n",
+			 gvt->sel_disp_port_mask);
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "  Each byte represents the bit mask of assigned port(s) to vGPU 1-8 (low to high)\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "    Bit mask is decoded from LSB to MSB (PORT_A to PORT_A+7):\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "      0: This port isn't assigned to that vGPU.\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "      1: This port is assigned to that vGPU.\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	for (id = 0; id < GVT_MAX_VGPU; id++) {
+		port_mask = (gvt->sel_disp_port_mask >> (id * 8)) & 0xFF;
+		if (port_mask) {
+			count = snprintf(buf, buf_size,
+					 "  vGPU-%d port mask: (0x%02x)\n",
+					 id + 1, port_mask);
+			buf_size -= count;
+			count_total += count;
+			buf += count;
+			if (!buf_size)
+				return count_total;
+
+			for (port = PORT_A; port < I915_MAX_PORTS; port++) {
+				if (port_mask & (1 << port)) {
+					port_sel = intel_gvt_external_disp_id_from_port(port);
+					count = snprintf(buf, buf_size,
+							 "    ( PORT_%c(%d) )\n",
+							 port_name(port), port_sel);
+					buf_size -= count;
+					count_total += count;
+					buf += count;
+					if (!buf_size)
+						return count_total;
+				}
+			}
+		}
+	}
+
+	count = snprintf(buf, buf_size,
+			 "Display ports ownership: 0x%08x\n",
+			 gvt->disp_owner);
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "  x on n-th hex-digit: (port_name(port_id#) <---> owner)\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size,
+			 "    n: port id, PORT_A(1), PORT_B(2), ...\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	count = snprintf(buf, buf_size, "    x: owner: host (0), vGPU id(x)\n");
+	buf_size -= count;
+	count_total += count;
+	buf += count;
+	if (!buf_size)
+		return count_total;
+
+	for (port = PORT_A; port < I915_MAX_PORTS; port++) {
+		port_sel = intel_gvt_external_disp_id_from_port(port);
+		if (gvt->avail_disp_port_mask & (port_sel << port * 4)) {
+			count = snprintf(buf, buf_size, "  ( PORT_%c(%d) ",
+					 port_name(port), port_sel);
+			buf_size -= count;
+			count_total += count;
+			buf += count;
+			if (!buf_size)
+				return count_total;
+
+			id = (gvt->disp_owner >> port * 4) & 0xF;
+			if (id)
+				count = snprintf(buf, buf_size,
+						 "<---> VGPU-%d )\n", id);
+			else
+				count = snprintf(buf, buf_size,
+						 "<---> Host )\n");
+
+			buf_size -= count;
+			count_total += count;
+			buf += count;
+			if (!buf_size)
+				return count_total;
+		}
+	}
+
+	return count_total;
+}
+
+static ssize_t gvt_disp_ports_owner_show(
+	struct device *kdev, struct device_attribute *attr, char *buf)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+
+	return snprintf(buf, PAGE_SIZE,
+			"Display ports ownership: 0x%08x\n",
+			dev_priv->gvt->disp_owner);
+}
+
+static ssize_t gvt_disp_ports_owner_store(
+	struct device *kdev, struct device_attribute *attr,
+	const char *buf, size_t count)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+	u32 val;
+	ssize_t ret;
+
+	ret = kstrtou32(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	intel_gvt_store_vgpu_display_owner(dev_priv, val);
+
+	return count;
+}
+
+static ssize_t gvt_disp_auto_switch_show(
+	struct device *kdev, struct device_attribute *attr, char *buf)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+
+	return snprintf(buf, PAGE_SIZE,
+			"%s\n",
+			READ_ONCE(dev_priv->gvt->disp_auto_switch) ?
+			"Y" : "N");
+}
+
+static ssize_t gvt_disp_auto_switch_store(
+	struct device *kdev, struct device_attribute *attr,
+	const char *buf, size_t count)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+	u32 val;
+	ssize_t ret;
+
+	ret = kstrtou32(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	intel_gvt_store_vgpu_display_switch(dev_priv, val != 0);
+
+	return count;
+}
+
+static ssize_t gvt_disp_edid_filter_show(
+	struct device *kdev, struct device_attribute *attr, char *buf)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+
+	return snprintf(buf, PAGE_SIZE,
+			"%s\n",
+			READ_ONCE(dev_priv->gvt->disp_edid_filter) ?
+			"Y" : "N");
+}
+
+static ssize_t gvt_disp_edid_filter_store(
+	struct device *kdev, struct device_attribute *attr,
+	const char *buf, size_t count)
+{
+	struct drm_i915_private *dev_priv = kdev_minor_to_i915(kdev);
+	u32 val;
+	ssize_t ret;
+
+	ret = kstrtou32(buf, 0, &val);
+	if (ret)
+		return ret;
+
+	if (val != 0)
+		WRITE_ONCE(dev_priv->gvt->disp_edid_filter, true);
+	else
+		WRITE_ONCE(dev_priv->gvt->disp_edid_filter, false);
+
+	return count;
+}
+
+
+static struct device_attribute dev_attr_gvt_disp_ports_mask =
+	__ATTR(gvt_disp_ports_mask, 0644,
+	       gvt_disp_ports_mask_show, gvt_disp_ports_mask_store);
+static struct device_attribute dev_attr_gvt_disp_ports_status =
+	__ATTR(gvt_disp_ports_status, 0444,
+	       gvt_disp_ports_status_show, NULL);
+static struct device_attribute dev_attr_gvt_disp_ports_owner =
+	__ATTR(gvt_disp_ports_owner, 0644,
+	       gvt_disp_ports_owner_show, gvt_disp_ports_owner_store);
+static struct device_attribute dev_attr_gvt_disp_auto_switch =
+	__ATTR(gvt_disp_auto_switch, 0644,
+	       gvt_disp_auto_switch_show, gvt_disp_auto_switch_store);
+static struct device_attribute dev_attr_gvt_disp_edid_filter =
+	__ATTR(gvt_disp_edid_filter, 0644,
+	       gvt_disp_edid_filter_show, gvt_disp_edid_filter_store);
+
+
+static const struct attribute *gvt_attrs[] = {
+	&dev_attr_gvt_disp_ports_mask.attr,
+	&dev_attr_gvt_disp_ports_status.attr,
+	&dev_attr_gvt_disp_ports_owner.attr,
+	&dev_attr_gvt_disp_auto_switch.attr,
+	&dev_attr_gvt_disp_edid_filter.attr,
+	NULL,
+};
+
+#endif
+
 void i915_setup_sysfs(struct drm_i915_private *dev_priv)
 {
 	struct device *kdev = dev_priv->drm.primary->kdev;
 	int ret;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 #ifdef CONFIG_PM
 	if (HAS_RC6(dev_priv)) {
@@ -615,6 +937,13 @@ void i915_setup_sysfs(struct drm_i915_private *dev_priv)
 	if (ret)
 		DRM_ERROR("RPS sysfs setup failed\n");
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt) {
+		if (sysfs_create_files(&kdev->kobj, gvt_attrs))
+			DRM_ERROR("gvt attr setup failed\n");
+	}
+#endif
+
 	i915_setup_error_capture(kdev);
 }
 
diff --git a/drivers/gpu/drm/i915/intel_pm.c b/drivers/gpu/drm/i915/intel_pm.c
index d7a34f710e90..8eadd94e2ec2 100644
--- a/drivers/gpu/drm/i915/intel_pm.c
+++ b/drivers/gpu/drm/i915/intel_pm.c
@@ -46,6 +46,10 @@
 #include "intel_sideband.h"
 #include "../../../platform/x86/intel_ips.h"
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+#include "gvt.h"
+#endif
+
 static void gen9_init_clock_gating(struct drm_i915_private *dev_priv)
 {
 	if (HAS_LLC(dev_priv)) {
@@ -3985,15 +3989,37 @@ skl_ddb_get_hw_plane_state(struct drm_i915_private *dev_priv,
 {
 	u32 val, val2;
 	u32 fourcc = 0;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_gvt_pipe_info *pipe_info = &gvt->pipe_info[pipe];
+	struct intel_dom0_plane_regs *dom0_regs =
+		&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+#endif
 
 	/* Cursor doesn't support NV12/planar, so no extra calculation needed */
 	if (plane_id == PLANE_CURSOR) {
 		val = I915_READ(CUR_BUF_CFG(pipe));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	/* In GVT direct display, we only use the statically allocated ddb */
+		if (gvt) {
+			struct skl_ddb_entry *ddb_y = &pipe_info->ddb_y[PLANE_CURSOR];
+
+			if (ddb_y->end)
+				val = (ddb_y->end - 1) << 16 | ddb_y->start;
+			else
+				val = 0;
+		}
+#endif
 		skl_ddb_entry_init_from_hw(dev_priv, ddb_y, val);
 		return;
 	}
 
 	val = I915_READ(PLANE_CTL(pipe, plane_id));
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	// If pipe is not owned by host, read plane_ctl from cache
+	if (gvt && gvt->pipe_info[pipe].plane_info[plane_id].owner)
+		val = dom0_regs->plane_ctl;
+#endif
 
 	/* No DDB allocated for disabled planes */
 	if (val & PLANE_CTL_ENABLE)
@@ -4008,6 +4034,23 @@ skl_ddb_get_hw_plane_state(struct drm_i915_private *dev_priv,
 		val = I915_READ(PLANE_BUF_CFG(pipe, plane_id));
 		val2 = I915_READ(PLANE_NV12_BUF_CFG(pipe, plane_id));
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		if (gvt) {
+			struct skl_ddb_entry *ddb_y = &pipe_info->ddb_y[plane_id];
+			struct skl_ddb_entry *ddb_uv = &pipe_info->ddb_uv[plane_id];
+
+			if (ddb_y->end)
+				val = (ddb_y->end - 1) << 16 | ddb_y->start;
+			else
+				val = 0;
+
+			if (ddb_uv->end)
+				val2 = (ddb_uv->end - 1) << 16 | ddb_uv->start;
+			else
+				val2 = 0;
+		}
+#endif
+
 		if (fourcc &&
 		    drm_format_info_is_yuv_semiplanar(drm_format_info(fourcc)))
 			swap(val, val2);
@@ -4026,6 +4069,9 @@ void skl_pipe_ddb_get_hw_state(struct intel_crtc *crtc,
 	enum pipe pipe = crtc->pipe;
 	intel_wakeref_t wakeref;
 	enum plane_id plane_id;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	power_domain = POWER_DOMAIN_PIPE(pipe);
 	wakeref = intel_display_power_get_if_enabled(dev_priv, power_domain);
@@ -4037,7 +4083,14 @@ void skl_pipe_ddb_get_hw_state(struct intel_crtc *crtc,
 					   plane_id,
 					   &ddb_y[plane_id],
 					   &ddb_uv[plane_id]);
-
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		// skl_allocate_pipe_ddb will zero out sw ddb for inactive ddb
+		// so we fake hw ddb as well.
+		if (gvt && !crtc->config->hw.active) {
+			memset(&ddb_y[plane_id], 0, sizeof(struct skl_ddb_entry));
+			memset(&ddb_uv[plane_id], 0, sizeof(struct skl_ddb_entry));
+		}
+#endif
 	intel_display_power_put(dev_priv, power_domain, wakeref);
 }
 
@@ -4239,6 +4292,10 @@ skl_allocate_pipe_ddb(struct intel_crtc_state *crtc_state,
 	u64 uv_plane_data_rate[I915_MAX_PLANES] = {};
 	u32 blocks;
 	int level;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+	struct intel_gvt_pipe_info *pipe_info = &gvt->pipe_info[intel_crtc->pipe];
+#endif
 
 	/* Clear the partitioning for disabled planes. */
 	memset(crtc_state->wm.skl.plane_ddb_y, 0, sizeof(crtc_state->wm.skl.plane_ddb_y));
@@ -4275,6 +4332,14 @@ skl_allocate_pipe_ddb(struct intel_crtc_state *crtc_state,
 	crtc_state->wm.skl.plane_ddb_y[PLANE_CURSOR].start =
 		alloc->end - total[PLANE_CURSOR];
 	crtc_state->wm.skl.plane_ddb_y[PLANE_CURSOR].end = alloc->end;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt) {
+		crtc_state->wm.skl.plane_ddb_y[PLANE_CURSOR].start =
+			pipe_info->ddb_y[PLANE_CURSOR].start;
+		crtc_state->wm.skl.plane_ddb_y[PLANE_CURSOR].end =
+			pipe_info->ddb_y[PLANE_CURSOR].end;
+	}
+#endif
 
 	if (total_data_rate == 0)
 		return 0;
@@ -4383,6 +4448,14 @@ skl_allocate_pipe_ddb(struct intel_crtc_state *crtc_state,
 			start += uv_total[plane_id];
 			uv_plane_alloc->end = start;
 		}
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+		if (gvt) {
+			plane_alloc->start = pipe_info->ddb_y[plane_id].start;
+			plane_alloc->end = pipe_info->ddb_y[plane_id].end;
+			uv_plane_alloc->start = pipe_info->ddb_uv[plane_id].start;
+			uv_plane_alloc->end = pipe_info->ddb_uv[plane_id].end;
+		}
+#endif
 	}
 
 	/*
@@ -4446,7 +4519,7 @@ skl_allocate_pipe_ddb(struct intel_crtc_state *crtc_state,
  * should allow pixel_rate up to ~2 GHz which seems sufficient since max
  * 2xcdclk is 1350 MHz and the pixel rate should never exceed that.
 */
-static uint_fixed_16_16_t
+uint_fixed_16_16_t
 skl_wm_method1(const struct drm_i915_private *dev_priv, u32 pixel_rate,
 	       u8 cpp, u32 latency, u32 dbuf_block_size)
 {
@@ -4465,7 +4538,7 @@ skl_wm_method1(const struct drm_i915_private *dev_priv, u32 pixel_rate,
 	return ret;
 }
 
-static uint_fixed_16_16_t
+uint_fixed_16_16_t
 skl_wm_method2(u32 pixel_rate, u32 pipe_htotal, u32 latency,
 	       uint_fixed_16_16_t plane_blocks_per_line)
 {
@@ -4482,7 +4555,7 @@ skl_wm_method2(u32 pixel_rate, u32 pipe_htotal, u32 latency,
 	return ret;
 }
 
-static uint_fixed_16_16_t
+uint_fixed_16_16_t
 intel_get_linetime_us(const struct intel_crtc_state *crtc_state)
 {
 	u32 pixel_rate;
@@ -4810,7 +4883,7 @@ skl_compute_linetime_wm(const struct intel_crtc_state *crtc_state)
 	return linetime_wm;
 }
 
-static void skl_compute_transition_wm(const struct intel_crtc_state *crtc_state,
+void skl_compute_transition_wm(const struct intel_crtc_state *crtc_state,
 				      const struct skl_wm_params *wp,
 				      struct skl_plane_wm *wm)
 {
@@ -5012,9 +5085,7 @@ static void skl_ddb_entry_write(struct drm_i915_private *dev_priv,
 		I915_WRITE_FW(reg, 0);
 }
 
-static void skl_write_wm_level(struct drm_i915_private *dev_priv,
-			       i915_reg_t reg,
-			       const struct skl_wm_level *level)
+static inline uint32_t skl_calc_wm_level(const struct skl_wm_level *level)
 {
 	u32 val = 0;
 
@@ -5025,6 +5096,15 @@ static void skl_write_wm_level(struct drm_i915_private *dev_priv,
 	val |= level->plane_res_b;
 	val |= level->plane_res_l << PLANE_WM_LINES_SHIFT;
 
+	return val;
+}
+
+static void skl_write_wm_level(struct drm_i915_private *dev_priv,
+			       i915_reg_t reg,
+			       const struct skl_wm_level *level)
+{
+	uint32_t val = skl_calc_wm_level(level);
+
 	I915_WRITE_FW(reg, val);
 }
 
@@ -5041,7 +5121,24 @@ void skl_write_plane_wm(struct intel_plane *plane,
 		&crtc_state->wm.skl.plane_ddb_y[plane_id];
 	const struct skl_ddb_entry *ddb_uv =
 		&crtc_state->wm.skl.plane_ddb_uv[plane_id];
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
 
+	if (gvt) {
+		struct intel_dom0_plane_regs *dom0_regs =
+			&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+
+		for (level = 0; level <= max_level; level++) {
+			dom0_regs->plane_wm[level] = skl_calc_wm_level(
+				&wm->wm[level]);
+		}
+		dom0_regs->plane_wm_trans = skl_calc_wm_level(
+			&wm->trans_wm);
+
+		if (gvt->pipe_info[pipe].plane_info[plane_id].owner)
+			return;
+	}
+#endif
 	for (level = 0; level <= max_level; level++) {
 		skl_write_wm_level(dev_priv, PLANE_WM(pipe, plane_id, level),
 				   &wm->wm[level]);
@@ -5049,6 +5146,12 @@ void skl_write_plane_wm(struct intel_plane *plane,
 	skl_write_wm_level(dev_priv, PLANE_WM_TRANS(pipe, plane_id),
 			   &wm->trans_wm);
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	/* In GVT direct display, we only use the statically allocated ddb */
+	if (dev_priv->gvt)
+		return;
+#endif
+
 	if (INTEL_GEN(dev_priv) >= 11) {
 		skl_ddb_entry_write(dev_priv,
 				    PLANE_BUF_CFG(pipe, plane_id), ddb_y);
@@ -5075,6 +5178,21 @@ void skl_write_cursor_wm(struct intel_plane *plane,
 		&crtc_state->wm.skl.optimal.planes[plane_id];
 	const struct skl_ddb_entry *ddb =
 		&crtc_state->wm.skl.plane_ddb_y[plane_id];
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+
+	if (gvt) {
+		struct intel_dom0_plane_regs *dom0_regs =
+			&gvt->pipe_info[pipe].plane_info[PLANE_CURSOR].dom0_regs;
+
+		for (level = 0; level <= max_level; level++)
+			dom0_regs->plane_wm[level] = skl_calc_wm_level(&wm->wm[level]);
+		dom0_regs->plane_wm_trans = skl_calc_wm_level(&wm->trans_wm);
+
+		if (gvt->pipe_info[pipe].owner)
+			return;
+	}
+#endif
 
 	for (level = 0; level <= max_level; level++) {
 		skl_write_wm_level(dev_priv, CUR_WM(pipe, level),
@@ -5082,6 +5200,12 @@ void skl_write_cursor_wm(struct intel_plane *plane,
 	}
 	skl_write_wm_level(dev_priv, CUR_WM_TRANS(pipe), &wm->trans_wm);
 
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	/* In GVT direct display, we only use the statically allocated ddb */
+	if (dev_priv->gvt)
+		return;
+#endif
+
 	skl_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe), ddb);
 }
 
@@ -5626,8 +5750,34 @@ void skl_pipe_wm_get_hw_state(struct intel_crtc *crtc,
 	int level, max_level;
 	enum plane_id plane_id;
 	u32 val;
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	struct intel_gvt *gvt = dev_priv->gvt;
+#endif
 
 	max_level = ilk_wm_max_level(dev_priv);
+#if IS_ENABLED(CONFIG_DRM_I915_GVT)
+	if (gvt && gvt->pipe_info[pipe].owner) {
+		for_each_plane_id_on_crtc(crtc, plane_id) {
+			struct skl_plane_wm *wm = &out->planes[plane_id];
+			struct intel_dom0_plane_regs *dom0_regs =
+				&gvt->pipe_info[pipe].plane_info[plane_id].dom0_regs;
+
+			for (level = 0; level <= max_level; level++) {
+				val = dom0_regs->plane_wm[level];
+				skl_wm_level_from_reg_val(val, &wm->wm[level]);
+			}
+			val = dom0_regs->plane_wm_trans;
+			skl_wm_level_from_reg_val(val, &wm->trans_wm);
+		}
+
+		if (!crtc->active)
+			return;
+
+		out->linetime = I915_READ(PIPE_WM_LINETIME(pipe));
+
+		return;
+	}
+#endif
 
 	for_each_plane_id_on_crtc(crtc, plane_id) {
 		struct skl_plane_wm *wm = &out->planes[plane_id];
-- 
2.17.1

