From 846425f0c049b2d0507497b19ad7ccf0d07f0baa Mon Sep 17 00:00:00 2001
From: Declan Murphy <declan.murphy@intel.com>
Date: Tue, 26 May 2020 15:56:47 +0100
Subject: [PATCH 019/131] crypto: keembay: Add Keem Bay OCS HCU

The Keem Bay offload crypto subsystem (OCS) hashing control unit
(HCU) allows for hardware accelerated hashing of sha224 (default
disabled), sha256, sha384 sm3 and sha512 using the standard kernel
crypto framework.
The HCU driver also implements keyed hashing (HMAC) support for the
above algorithms.

Signed-off-by: Declan Murphy <declan.murphy@intel.com>
---
 MAINTAINERS                                   |    3 +
 drivers/crypto/keembay/Kconfig                |   24 +
 drivers/crypto/keembay/Makefile               |    3 +
 drivers/crypto/keembay/keembay-ocs-hcu-core.c | 1443 +++++++++++++++++
 drivers/crypto/keembay/ocs-hcu.c              |  475 ++++++
 drivers/crypto/keembay/ocs-hcu.h              |  123 ++
 6 files changed, 2071 insertions(+)
 create mode 100644 drivers/crypto/keembay/keembay-ocs-hcu-core.c
 create mode 100644 drivers/crypto/keembay/ocs-hcu.c
 create mode 100644 drivers/crypto/keembay/ocs-hcu.h

diff --git a/MAINTAINERS b/MAINTAINERS
index c046dc33c663..2c80a78fcf56 100644
--- a/MAINTAINERS
+++ b/MAINTAINERS
@@ -8912,6 +8912,9 @@ KEEMBAY OCS HCU
 M:	Declan Murphy <declan.murphy@intel.com>
 S:	Maintained
 F:	Documentation/devicetree/bindings/crypto/intel,keembay-ocs-hcu.yaml
+F:	drivers/crypto/keembay/keembay-ocs-hcu-core.c
+F:	drivers/crypto/keembay/ocs-hcu.c
+F:	drivers/crypto/keembay/ocs-hcu.h
 
 KEEM BAY VPU IPC DRIVER
 M:	Paul Murphy <paul.j.murphy@intel.com>
diff --git a/drivers/crypto/keembay/Kconfig b/drivers/crypto/keembay/Kconfig
index c61112bc90ae..861ddda61e51 100644
--- a/drivers/crypto/keembay/Kconfig
+++ b/drivers/crypto/keembay/Kconfig
@@ -107,6 +107,30 @@ endif # CRYPTO_DEV_KEEMBAY_OCS_SYM_CIPHER_SM4
 
 endif # CRYPTO_DEV_KEEMBAY_OCS_SYM_CIPHER
 
+config CRYPTO_DEV_KEEMBAY_OCS_HCU
+	tristate "Support for Keem Bay OCS HCU HW acceleration"
+	select CRYPTO_ENGINE
+
+	help
+	  Support for Keem Bay Offload and Crypto Subsystem (OCS)
+	  HCU (Hash Control Unit) hardware acceleration for use
+	  with Crypto API.
+
+	  Provides OCS implementation of sha256, sha384, sha512 as well
+	  as the HMAC variant of these algorithms.
+
+config CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224
+	bool "Support for Keem Bay SHA224 and HMAC(SHA224) HW acceleration"
+	default n
+	depends on CRYPTO_DEV_KEEMBAY_OCS_HCU
+
+	help
+	  Enables support for the Keem Bay OCS HCU SHA224 and HMAC(SHA224)
+	  algorithms. These algorithms are not recommended for use.
+
+	  Provides OCS implementation of sha224 as well as the HMAC variant of
+	  this algorithm.
+
 config CRYPTO_DEV_KEEMBAY_OCS_ECC
 	tristate "Support for Keembay OCS ECC HW acceleration"
 	select CRYPTO_ECDH
diff --git a/drivers/crypto/keembay/Makefile b/drivers/crypto/keembay/Makefile
index 43c18eb7b7f2..dc881ebc30f4 100644
--- a/drivers/crypto/keembay/Makefile
+++ b/drivers/crypto/keembay/Makefile
@@ -5,3 +5,6 @@ obj-$(CONFIG_CRYPTO_DEV_KEEMBAY_OCS_SYM_CIPHER) += keembay-ocs-aes.o
 keembay-ocs-aes-objs := keembay-ocs-aes-core.o ocs-aes.o
 
 obj-$(CONFIG_CRYPTO_DEV_KEEMBAY_OCS_ECC) += keembay-ocs-ecc-core.o
+
+obj-$(CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU) += keembay-ocs-hcu.o
+keembay-ocs-hcu-objs := keembay-ocs-hcu-core.o ocs-hcu.o
diff --git a/drivers/crypto/keembay/keembay-ocs-hcu-core.c b/drivers/crypto/keembay/keembay-ocs-hcu-core.c
new file mode 100644
index 000000000000..1fb990ba8779
--- /dev/null
+++ b/drivers/crypto/keembay/keembay-ocs-hcu-core.c
@@ -0,0 +1,1443 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Keem Bay OCS HCU Crypto Driver.
+ *
+ * Copyright (C) 2018-2020 Intel Corporation
+ */
+
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+
+#include <crypto/engine.h>
+#include <crypto/scatterwalk.h>
+#include <crypto/sha.h>
+#include <crypto/sm3.h>
+#include <crypto/hmac.h>
+#include <crypto/internal/hash.h>
+
+#include "ocs-hcu.h"
+
+#define DRV_NAME "keembay-ocs-hcu-driver"
+
+/* Request flags */
+#define REQ_FLAGS_HASH_MASK (REQ_FLAGS_SHA_224 | \
+			     REQ_FLAGS_SHA_256 | \
+			     REQ_FLAGS_SHA_384 | \
+			     REQ_FLAGS_SHA_512 | \
+			     REQ_FLAGS_SM3)
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224
+#define REQ_FLAGS_SHA_224 BIT(0)
+#else /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224 */
+#define REQ_FLAGS_SHA_224 0
+#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224 */
+#define REQ_FLAGS_SHA_256 BIT(1)
+#define REQ_FLAGS_SHA_384 BIT(2)
+#define REQ_FLAGS_SHA_512 BIT(3)
+#define REQ_FLAGS_SM3 BIT(4)
+#define REQ_FLAGS_HMAC BIT(5)
+#define REQ_FLAGS_HASH_KEY BIT(6)
+#define REQ_FLAGS_HMAC_KEY_SET BIT(7)
+#define REQ_FLAGS_HASH_PIO BIT(8)
+#define REQ_FLAGS_INTERMEDIATE_DATA BIT(9)
+#define REQ_FLAGS_FINAL_DATA BIT(10)
+#define REQ_FLAGS_HMAC_HW BIT(12)
+#define REQ_UPDATE BIT(16)
+#define REQ_FINAL BIT(17)
+#define REQ_CMD_MASK (REQ_UPDATE | REQ_FINAL)
+#define REQ_HMAC_NO_SET_KEY_MASK (REQ_FLAGS_HASH_KEY | REQ_FLAGS_HMAC_KEY_SET)
+
+#define REQ_FLAGS_HMAC_TYPE_MASK (REQ_FLAGS_HMAC | REQ_FLAGS_HMAC_HW)
+#define REQ_FLAGS_HMAC_TYPE_SW REQ_FLAGS_HMAC
+#define REQ_FLAGS_HMAC_TYPE_HW (REQ_FLAGS_HMAC | REQ_FLAGS_HMAC_HW)
+
+#define KMB_HCU_BUF_SIZE 256
+#define KMB_OCS_HCU_ALIGN_MASK 0x7
+
+#define KMB_OCS_HCU_MAX_KEYLEN 512
+
+/* Transform context. */
+struct ocs_hcu_ctx {
+	struct crypto_engine_ctx engine_ctx;
+	struct ocs_hcu_dev *hcu_dev;
+	u32 flags;
+	u8 key[KMB_OCS_HCU_MAX_KEYLEN] __aligned(sizeof(u64));
+	u8 opad[SHA512_BLOCK_SIZE + SHA512_DIGEST_SIZE];
+	u8 ipad[SHA512_BLOCK_SIZE] __aligned(sizeof(u64));
+	/* Length ot the key set by the setkey function. */
+	size_t key_len;
+};
+
+/* Context for the request. */
+struct ocs_hcu_rctx {
+	u32 flags;
+	size_t blk_sz;
+
+	/* Head of the unhandled scatterlist entries containing data. */
+	struct scatterlist *sg;
+	unsigned int sg_dma_nents;
+	dma_addr_t ll_dma_addr;
+	/* OCS DMA linked list head. */
+	struct ocs_hcu_dma_desc *dma_list_head;
+	/* OCS DMA linked list tail. */
+	struct ocs_hcu_dma_desc *dma_list_tail;
+	/* The size of the allocated buffer to contain the DMA linked list. */
+	size_t dma_list_size;
+
+	struct ocs_hcu_dev *hcu_dev;
+	struct ocs_hcu_idata_desc idata;
+	u32 algo;
+	u32 dig_sz;
+
+	/* Total data in the SG list at any time. */
+	unsigned int sg_data_total;
+	/* Offset into the data of an individual SG node. */
+	unsigned int sg_data_offset;
+	/* The amount of data in bytes in each buffer. */
+	unsigned int buf_cnt;
+	/* The statig length of each buffer. */
+	unsigned int buf_len;
+
+	u8 buffer[KMB_HCU_BUF_SIZE] __aligned(sizeof(u64));
+};
+
+/* Driver data. */
+struct ocs_hcu_drv {
+	struct list_head dev_list;
+	spinlock_t lock;
+};
+
+static struct ocs_hcu_drv ocs_hcu = {
+	.dev_list = LIST_HEAD_INIT(ocs_hcu.dev_list),
+	.lock = __SPIN_LOCK_UNLOCKED(ocs_hcu.lock),
+};
+
+static inline unsigned int kmb_get_total_data(struct ocs_hcu_rctx *rctx)
+{
+	return rctx->sg_data_total + rctx->buf_cnt;
+}
+
+static void kmb_ocs_hcu_copy_sg(struct ocs_hcu_rctx *rctx,
+				void *buf, size_t count)
+{
+	scatterwalk_map_and_copy(buf, rctx->sg, rctx->sg_data_offset,
+				 count, 0);
+
+	rctx->sg_data_offset += count;
+	rctx->sg_data_total -= count;
+	rctx->buf_cnt += count;
+
+
+	if (rctx->sg_data_offset == rctx->sg->length) {
+		rctx->sg = sg_next(rctx->sg);
+		rctx->sg_data_offset = 0;
+		if (!rctx->sg)
+			rctx->sg_data_total = 0;
+	}
+}
+
+static int kmb_ocs_hcu_append_sg(struct ocs_hcu_rctx *rctx, u32 max_sz)
+{
+	unsigned int count;
+
+	if (unlikely(rctx->buf_cnt > rctx->buf_len) ||
+	    unlikely(rctx->buf_len < max_sz)) {
+		dev_err(rctx->hcu_dev->dev,
+			"No space left in buffer.\n");
+		return -EINVAL;
+	}
+
+	/* Only ever get a DMA block size in the buffer. */
+	while ((rctx->buf_cnt < max_sz) && rctx->sg_data_total) {
+		if (!rctx->sg)
+			return 0;
+		/* Determine the maximum data available to copy from the node.
+		 * Minimum of the length left in the sg node, or the total data
+		 * in the request.
+		 * Then ensure that the buffer has the space available, so
+		 * determine the minimum of the previous minimum with the
+		 * remaining buffer size.
+		 */
+		count = min(rctx->sg->length - rctx->sg_data_offset,
+			    rctx->sg_data_total);
+		count = min(count, max_sz - rctx->buf_cnt);
+
+		if (count <= 0) {
+			if ((rctx->sg->length == 0) && !sg_is_last(rctx->sg)) {
+				rctx->sg = sg_next(rctx->sg);
+				continue;
+			} else {
+				break;
+			}
+		}
+		kmb_ocs_hcu_copy_sg(rctx, rctx->buffer +
+				    rctx->buf_cnt, count);
+	}
+
+	return 0;
+}
+
+static struct ocs_hcu_dev *kmb_ocs_hcu_find_dev(struct ahash_request *req)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+	struct ocs_hcu_ctx *tctx = crypto_ahash_ctx(tfm);
+	struct ocs_hcu_dev *hcu_dev = NULL, *tmp;
+
+	spin_lock_bh(&ocs_hcu.lock);
+	if (!tctx->hcu_dev) {
+		list_for_each_entry(tmp, &ocs_hcu.dev_list, list) {
+			hcu_dev = tmp;
+			break;
+		}
+		tctx->hcu_dev = hcu_dev;
+	} else {
+		hcu_dev = tctx->hcu_dev;
+	}
+
+	spin_unlock_bh(&ocs_hcu.lock);
+
+	return hcu_dev;
+}
+
+static void kmb_ocs_free_dma_list(struct ocs_hcu_rctx *rctx)
+{
+	struct ocs_hcu_dev *hcu_dev = rctx->hcu_dev;
+	struct device *dev = hcu_dev->dev;
+
+	if (!rctx->dma_list_head || !rctx->dma_list_tail)
+		return;
+
+	if (rctx->sg_dma_nents > 0)
+		dma_unmap_sg(dev, hcu_dev->req->src, rctx->sg_dma_nents,
+			     DMA_TO_DEVICE);
+
+	dma_free_coherent(dev, rctx->dma_list_size, rctx->dma_list_head,
+			  rctx->ll_dma_addr);
+
+	rctx->dma_list_head = 0;
+	rctx->dma_list_tail = 0;
+	rctx->ll_dma_addr = 0;
+}
+
+static int kmb_ocs_add_dma_tail(struct ocs_hcu_rctx *rctx,
+				dma_addr_t addr, size_t len)
+{
+	if (addr & KMB_OCS_HCU_ALIGN_MASK || addr > OCS_HCU_DMA_MAX_ADDR_MASK)
+		return -EINVAL;
+
+	if (!len)
+		return 0;
+
+	rctx->dma_list_tail->src_adr = (u32)addr;
+	rctx->dma_list_tail->src_len = (u32)len;
+	rctx->dma_list_tail->ll_flags = 0;
+	rctx->dma_list_tail->nxt_desc = rctx->ll_dma_addr +
+					(virt_to_phys(rctx->dma_list_tail) -
+					virt_to_phys(rctx->dma_list_head)) +
+					sizeof(*rctx->dma_list_tail);
+
+	rctx->dma_list_tail++;
+
+	return 0;
+}
+
+static void kmb_ocs_terminate_dma_tail(struct ocs_hcu_rctx *rctx, int n_nodes)
+{
+	struct ocs_hcu_dma_desc *final = rctx->dma_list_head +
+					 (n_nodes - 1);
+
+	final->nxt_desc = 0;
+	final->ll_flags = OCS_LL_DMA_FLAG_TERMINATE;
+}
+
+/* This function will always have a total DMA size aligned to 64B. */
+static int kmb_ocs_init_dma_list(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ahash_request *req = hcu_dev->req;
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+	struct device *dev = hcu_dev->dev;
+	struct scatterlist *sg = req->src;
+	unsigned int nents = sg_nents(sg);
+	unsigned int total = kmb_get_total_data(rctx);
+	unsigned int remainder = 0;
+	long sgt = (int)rctx->sg_data_total;
+	size_t count;
+	int n_nodes;
+	int rc;
+	int i;
+
+	if (!total)
+		return 0;
+
+	rctx->sg_dma_nents = 0;
+
+	/* If this is not a final DMA (terminated DMA), the data passed to the
+	 * HCU must be algigned to the block size.
+	 */
+	if (!(rctx->flags & REQ_FLAGS_FINAL_DATA))
+		remainder = (total) % rctx->blk_sz;
+
+	/* Determine the number of scatter gather list nodes to map. */
+	for_each_sg(req->src, sg, nents, i) {
+		if (remainder >= sgt)
+			break;
+
+		rctx->sg_dma_nents++;
+
+		if (sg->length > (sgt - remainder))
+			break;
+
+		sgt -= sg->length;
+	}
+
+	/* If there is data in the buffer, increase the number of
+	 * individual nodes in the linked list to be allocated.
+	 */
+	n_nodes = rctx->sg_dma_nents;
+
+	/* Size of the total number of descriptors to allocate. */
+	rctx->dma_list_size = sizeof(*rctx->dma_list_head) * n_nodes;
+
+	rctx->dma_list_head = dma_alloc_coherent(dev, rctx->dma_list_size,
+						 &rctx->ll_dma_addr,
+						 GFP_KERNEL);
+	if (!rctx->dma_list_head)
+		return -ENOMEM;
+
+	rctx->dma_list_tail = rctx->dma_list_head;
+
+	rctx->sg_dma_nents = dma_map_sg(hcu_dev->dev, req->src,
+					rctx->sg_dma_nents, DMA_TO_DEVICE);
+
+	if (!rctx->sg_dma_nents) {
+		rc = -ENOMEM;
+		goto cleanup;
+	}
+
+	/* Add the SG Nodes to the DMA linked list. */
+	for_each_sg(req->src, rctx->sg, rctx->sg_dma_nents, i) {
+		count = min(rctx->sg_data_total - remainder,
+			    rctx->sg->length - rctx->sg_data_offset);
+
+		/* Do not create a zero length DMA descriptor. Check in case of
+		 * zero length SG node.
+		 */
+		if (count > 0) {
+			rc = kmb_ocs_add_dma_tail(rctx, rctx->sg->dma_address,
+						  count);
+			if (rc)
+				goto cleanup;
+
+			rctx->sg_data_offset += count;
+			rctx->sg_data_total -= count;
+
+			if (rctx->sg_data_offset == rctx->sg->length)
+				rctx->sg_data_offset = 0;
+
+			if (rctx->sg_data_total <= remainder)
+				break;
+		}
+	}
+
+	/* Make sure rctx->sg is pointing to the next data. */
+	if (!rctx->sg_data_offset)
+		rctx->sg = sg_next(rctx->sg);
+
+	kmb_ocs_terminate_dma_tail(rctx, n_nodes);
+
+	return 0;
+cleanup:
+	dev_err(dev, "Failed to map DMA buffer.\n");
+	kmb_ocs_free_dma_list(rctx);
+
+	return rc;
+}
+
+static void kmb_ocs_hcu_finish_request(struct ocs_hcu_dev *hcu_dev, int *error)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(hcu_dev->req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+
+	/* Gets the intermediate data (including digest). */
+	ocs_hcu_finish_req(hcu_dev, rctx->algo, &rctx->idata, error);
+
+	/* If the request is not update, it must be either FINAL or FINUP. */
+	if (!(rctx->flags & REQ_UPDATE) || *error) {
+		if (*error == 0 && hcu_dev->req->result)
+			memcpy(hcu_dev->req->result,
+			       rctx->idata.digest, rctx->dig_sz);
+		/* Ensure all key data is deleted. */
+		memzero_explicit(ctx->opad, ARRAY_SIZE(ctx->opad));
+		memzero_explicit(ctx->ipad, ARRAY_SIZE(ctx->ipad));
+		memzero_explicit(ctx->key, ARRAY_SIZE(ctx->key));
+		ctx->key_len = 0;
+		/* Clear buffer of any data. */
+		memzero_explicit(rctx->buffer, ARRAY_SIZE(rctx->buffer));
+		/* Clear the key in HW as well.
+		 * Key size is guaranteed lower than maximum at this point,
+		 * as request completed.
+		 */
+		ocs_hcu_write_key(hcu_dev, ctx->key, HCU_MAX_KEYLEN);
+	}
+
+	ocs_hcu_hw_disable(hcu_dev);
+
+	/* Copy residual data for next transfer. */
+	*error |= kmb_ocs_hcu_append_sg(rctx, KMB_HCU_BUF_SIZE);
+
+	if (!(rctx->flags & REQ_FLAGS_HASH_PIO))
+		kmb_ocs_free_dma_list(rctx);
+
+	/* Clear any flags that are complete at this time. */
+	rctx->flags &= ~REQ_FLAGS_HASH_PIO;
+	crypto_finalize_hash_request(hcu_dev->engine, hcu_dev->req, *error);
+}
+
+static inline int kmb_ocs_set_key(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(hcu_dev->req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+	int rc;
+
+	rc = ocs_hcu_write_key(hcu_dev, ctx->key, HCU_MAX_KEYLEN);
+
+	if (rc)
+		return rc;
+
+	rctx->flags |= REQ_FLAGS_HMAC_KEY_SET;
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_set_hw(struct ocs_hcu_dev *hcu_dev, int algo)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	bool is_hmac_hw = ((rctx->flags & REQ_FLAGS_HMAC_TYPE_MASK) ==
+			   REQ_FLAGS_HMAC_TYPE_HW);
+	bool busy = ocs_hcu_wait_busy(hcu_dev);
+	int rc;
+
+	if (busy)
+		return -EBUSY;
+
+	/* Configure the hardware for the current request. */
+	rc = ocs_hcu_hw_cfg(hcu_dev, algo);
+	if (rc)
+		return rc;
+
+	if (rctx->flags & REQ_FLAGS_INTERMEDIATE_DATA) {
+		ocs_hcu_set_intermediate_data(hcu_dev, &rctx->idata, algo);
+		rctx->flags &= ~REQ_FLAGS_INTERMEDIATE_DATA;
+	}
+
+	if (is_hmac_hw && !(rctx->flags & REQ_HMAC_NO_SET_KEY_MASK))
+		rc = kmb_ocs_set_key(hcu_dev);
+
+	return rc;
+}
+
+static int kmb_ocs_hcu_hash_pio(struct ocs_hcu_dev *hcu_dev,
+				u8 *buf, u32 sz, u32 algo, bool term)
+{
+	int rc = 0;
+
+	/* Configure the hardware for the current request. */
+	rc = kmb_ocs_hcu_set_hw(hcu_dev, algo);
+
+	if (rc)
+		return rc;
+
+	if (sz != ocs_hcu_hash_cpu(hcu_dev, buf, sz, algo, term))
+		return -EIO;
+
+	if (term)
+		ocs_hcu_tx_data_done(hcu_dev);
+
+	return 0;
+}
+
+/* This function will finish the request if the transfer is not terminating. */
+static int kmb_ocs_hcu_tx_pio(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	bool is_final = !!(rctx->flags & REQ_FLAGS_FINAL_DATA);
+	unsigned int total;
+	u32 cpu_tx_sz;
+	int rc = 0;
+
+	/* Configure the hardware for the current request. */
+	rc = kmb_ocs_hcu_set_hw(hcu_dev, rctx->algo);
+
+	if (rc)
+		return rc;
+
+	ocs_hcu_start_hash(hcu_dev);
+
+	if (is_final && kmb_get_total_data(rctx) < KMB_HCU_BUF_SIZE)
+		goto final;
+
+	do {
+		total = kmb_get_total_data(rctx);
+		cpu_tx_sz = total > KMB_HCU_BUF_SIZE ? KMB_HCU_BUF_SIZE :
+						       total - (total %
+						       rctx->blk_sz);
+		rc = kmb_ocs_hcu_append_sg(rctx, cpu_tx_sz);
+		if (rc)
+			return rc;
+
+		/* Hash the contents of the buffer, if not a final transfer
+		 * ensure the buffer does not have any data left.
+		 */
+		rctx->buf_cnt -= ocs_hcu_hash_cpu(hcu_dev, rctx->buffer,
+						  rctx->buf_cnt, rctx->algo,
+						  false);
+		if (rctx->buf_cnt != 0)
+			return -EIO;
+	} while (kmb_get_total_data(rctx) / rctx->blk_sz);
+
+	if (!is_final) {
+		rctx->flags |= REQ_FLAGS_INTERMEDIATE_DATA;
+		if (ocs_hcu_wait_busy(hcu_dev))
+			return -EBUSY;
+		/* No interrupt in this case, indicate it is done. */
+		hcu_dev->flags |= HCU_FLAGS_HCU_DONE;
+		kmb_ocs_hcu_finish_request(hcu_dev, &rc);
+		return 0;
+	}
+
+final:
+	rc = kmb_ocs_hcu_append_sg(rctx, KMB_HCU_BUF_SIZE);
+	if (rc)
+		return rc;
+	rctx->buf_cnt -= ocs_hcu_hash_cpu(hcu_dev, rctx->buffer, rctx->buf_cnt,
+					  rctx->algo, true);
+	if (rctx->buf_cnt != 0) {
+		dev_err(hcu_dev->dev, "%sFAILED final PIO hash\n", __func__);
+		return -EIO;
+	}
+	/* Completing transfer. */
+	ocs_hcu_tx_data_done(hcu_dev);
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_tx_dma(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	bool term = !!(rctx->flags & REQ_FLAGS_FINAL_DATA);
+	int rc;
+
+	if (!rctx->ll_dma_addr)
+		return 0;
+
+	/* Configure the hardware for the current request. */
+	rc = kmb_ocs_hcu_set_hw(hcu_dev, rctx->algo);
+
+	if (rc)
+		return rc;
+
+	/* Start the DMA engine with the descriptor address stored. */
+	ocs_hcu_ll_dma_start(hcu_dev, rctx->ll_dma_addr, term);
+	if (!term)
+		rctx->flags |= REQ_FLAGS_INTERMEDIATE_DATA;
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_tx(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	int rc = 0;
+
+	/* Hash the contents of the buffer rctx buffer, otherwise handle the
+	 * DMA linked list if it has been created.
+	 */
+	if (rctx->flags & REQ_FLAGS_HASH_PIO)
+		rc = kmb_ocs_hcu_tx_pio(hcu_dev);
+	else
+		rc = kmb_ocs_hcu_tx_dma(hcu_dev);
+
+	return rc;
+}
+
+static int kmb_ocs_hcu_handle_queue(struct ahash_request *req)
+{
+	struct ocs_hcu_dev *hcu_dev = kmb_ocs_hcu_find_dev(req);
+
+	if (!hcu_dev)
+		return -ENOENT;
+
+	return crypto_transfer_hash_request_to_engine(hcu_dev->engine,
+						      req);
+}
+
+/* If the data size is below the KMB_HCU_BUF_SIZE threshold, setup PIO hashing.
+ * Otherwise, hash using the DMA engine.
+ */
+static int kmb_ocs_hcu_prepare_request(struct crypto_engine *engine, void *areq)
+{
+	struct ahash_request *req = container_of(areq, struct ahash_request,
+						 base);
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+	struct ocs_hcu_dev *hcu_dev = kmb_ocs_hcu_find_dev(req);
+	unsigned int total = kmb_get_total_data(rctx);
+
+	if (!hcu_dev)
+		return -ENOENT;
+
+	hcu_dev->req = req;
+
+	/* Prepare data for a PIO hash.
+	 * This is for the HMAC 0 size corner case where IPAD and OPAD must
+	 * be hashed.
+	 */
+	if (rctx->buf_cnt || total <= KMB_HCU_BUF_SIZE)
+		rctx->flags |= REQ_FLAGS_HASH_PIO;
+	else if (total)
+		return kmb_ocs_init_dma_list(hcu_dev);
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_do_final(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	int rc;
+	bool is_hmac_sw = ((rctx->flags & REQ_FLAGS_HMAC_TYPE_MASK) ==
+			   REQ_FLAGS_HMAC_TYPE_SW);
+
+	if (is_hmac_sw)
+		rctx->flags |= REQ_FLAGS_FINAL_DATA;
+
+	rc = kmb_ocs_hcu_set_hw(hcu_dev, rctx->algo);
+	if (rc)
+		return rc;
+
+	ocs_hcu_tx_data_done(hcu_dev);
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_do_data(struct ocs_hcu_dev *hcu_dev)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(hcu_dev->req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+	bool is_hmac_sw = ((rctx->flags & REQ_FLAGS_HMAC_TYPE_MASK) ==
+			   REQ_FLAGS_HMAC_TYPE_SW);
+	int rc = 0;
+	int i;
+
+	/* Compute the ipad for HMAC if the key is valid and the first hash. */
+	if (is_hmac_sw && (ctx->key_len <= rctx->blk_sz) &&
+	    !(rctx->flags & REQ_FLAGS_INTERMEDIATE_DATA)) {
+		if (rctx->flags & REQ_FINAL)
+			rctx->flags |= REQ_FLAGS_FINAL_DATA;
+
+		/* Prepare IPAD/OPAD for HMAC. Only done at start and end.
+		 * HMAC(k,m) = H(k ^ opad || H(k ^ ipad || m))
+		 * ipad will be first block of HMAC DMA.
+		 * opad will be calculated in the final request.
+		 * Only needed if not using HW HMAC
+		 */
+		memset(ctx->ipad, HMAC_IPAD_VALUE, rctx->blk_sz);
+		for (i = 0; i < ctx->key_len; i++)
+			ctx->ipad[i] ^= ctx->key[i];
+
+		rc = kmb_ocs_hcu_hash_pio(hcu_dev, ctx->ipad, rctx->blk_sz,
+					    rctx->algo, false);
+
+		if (rc)
+			return rc;
+
+		rc = ocs_hcu_get_intermediate_data(hcu_dev, &rctx->idata,
+						   rctx->algo);
+		if (rc)
+			return rc;
+
+		rctx->flags |= REQ_FLAGS_INTERMEDIATE_DATA;
+	}
+
+	/* If the REQ_FLAGS_FINAL_DATA flag is set the transform is
+	 * terminated or there is data to perform a final hash.
+	 * Otherwise terminate the transform to obtain the final hash.
+	 */
+	if (rctx->flags & (REQ_UPDATE | REQ_FLAGS_FINAL_DATA))
+		rc = kmb_ocs_hcu_tx(hcu_dev);
+	else if (rctx->flags & REQ_FINAL)
+		rc = kmb_ocs_hcu_do_final(hcu_dev);
+
+	return rc;
+}
+
+static int kmb_ocs_hcu_do_one_request(struct crypto_engine *engine, void *areq)
+{
+	struct ahash_request *req = container_of(areq, struct ahash_request,
+						 base);
+	struct ocs_hcu_dev *hcu_dev = kmb_ocs_hcu_find_dev(req);
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+
+	if (!hcu_dev)
+		return -ENOENT;
+
+	hcu_dev->req = req;
+
+	/* Initialize the hardware. */
+	ocs_hcu_hw_init(hcu_dev);
+
+	if (rctx->flags & REQ_FLAGS_HASH_KEY)
+		return kmb_ocs_hcu_hash_pio(hcu_dev, ctx->key,
+					    ctx->key_len,
+					    (rctx->algo &
+					    ~OCS_HCU_ALGO_HMAC_MASK), true);
+
+	return kmb_ocs_hcu_do_data(hcu_dev);
+}
+
+static int kmb_ocs_hcu_init(struct ahash_request *req)
+{
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+	struct ocs_hcu_dev *hcu_dev = kmb_ocs_hcu_find_dev(req);
+
+	if (!hcu_dev)
+		return -ENOENT;
+
+	rctx->flags = ctx->flags;
+	rctx->hcu_dev = hcu_dev;
+	rctx->dig_sz = crypto_ahash_digestsize(tfm);
+
+	switch (rctx->dig_sz) {
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224
+	case SHA224_DIGEST_SIZE:
+		rctx->flags |= REQ_FLAGS_SHA_224;
+		rctx->blk_sz = SHA224_BLOCK_SIZE;
+		rctx->algo = OCS_HCU_ALGO_SHA224;
+		break;
+#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224 */
+	case SHA256_DIGEST_SIZE:
+		rctx->blk_sz = SHA256_BLOCK_SIZE;
+		if (rctx->flags & REQ_FLAGS_SM3) {
+			rctx->algo = OCS_HCU_ALGO_SM3;
+		} else {
+			rctx->flags |= REQ_FLAGS_SHA_256;
+			rctx->algo = OCS_HCU_ALGO_SHA256;
+		}
+		break;
+	case SHA384_DIGEST_SIZE:
+		rctx->flags |= REQ_FLAGS_SHA_384;
+		rctx->blk_sz = SHA384_BLOCK_SIZE;
+		rctx->algo = OCS_HCU_ALGO_SHA384;
+		break;
+	case SHA512_DIGEST_SIZE:
+		rctx->flags |= REQ_FLAGS_SHA_512;
+		rctx->blk_sz = SHA512_BLOCK_SIZE;
+		rctx->algo = OCS_HCU_ALGO_SHA512;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	rctx->buf_len = KMB_HCU_BUF_SIZE;
+	rctx->ll_dma_addr = 0;
+	rctx->dma_list_tail = 0;
+	rctx->dma_list_head = 0;
+	rctx->dma_list_size = 0;
+	/* Clear the total transfer size. */
+	rctx->sg_data_total = 0;
+	rctx->sg_data_offset = 0;
+	rctx->buf_cnt = 0;
+	rctx->sg = NULL;
+
+	if (!(rctx->flags & REQ_FLAGS_HMAC)) {
+		memzero_explicit(ctx->key, KMB_OCS_HCU_MAX_KEYLEN);
+		ctx->key_len = 0;
+	} else if (ctx->key_len > rctx->blk_sz) {
+		/* Set the hash key flag, copied in init function. */
+		rctx->flags |= REQ_FLAGS_HASH_KEY;
+	}
+
+	/* Clear the intermediate data. */
+	memzero_explicit((void *)&rctx->idata, sizeof(rctx->idata));
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_update(struct ahash_request *req)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+
+	if (!req->nbytes)
+		return 0;
+
+	rctx->sg_data_offset = 0;
+
+	/* Check for overflow */
+	if (check_add_overflow(rctx->sg_data_total, req->nbytes,
+	    &rctx->sg_data_total))
+		return -EINVAL;
+
+	rctx->sg = req->src;
+	rctx->flags &= ~REQ_CMD_MASK;
+	rctx->flags |= REQ_UPDATE;
+
+	if (rctx->sg_data_total <= (KMB_HCU_BUF_SIZE - rctx->buf_cnt))
+		return kmb_ocs_hcu_append_sg(rctx, KMB_HCU_BUF_SIZE);
+
+	return kmb_ocs_hcu_handle_queue(req);
+}
+
+static int kmb_ocs_hcu_final(struct ahash_request *req)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+
+	rctx->sg = req->src;
+	rctx->flags &= ~REQ_CMD_MASK;
+	rctx->flags |= REQ_FINAL;
+
+	/* If no intermediate hashes completed, can perform HMAC in one. */
+	if ((rctx->flags & REQ_FLAGS_HMAC) &&
+	    !(rctx->flags & REQ_FLAGS_INTERMEDIATE_DATA) &&
+	    (rctx->flags & REQ_FLAGS_HASH_KEY ||
+	    ctx->key_len < HCU_MAX_KEYLEN) && kmb_get_total_data(rctx)) {
+		rctx->algo |= OCS_HCU_ALGO_HMAC_MASK;
+		rctx->flags |= REQ_FLAGS_HMAC_HW;
+	}
+
+	/* If there is any data left OR we are in an intermediate
+	 * HMAC transfer, there is a final hash to perform.
+	 */
+	if (kmb_get_total_data(rctx))
+		rctx->flags |= REQ_FLAGS_FINAL_DATA;
+
+	return kmb_ocs_hcu_handle_queue(req);
+}
+
+static int kmb_ocs_hcu_finup(struct ahash_request *req)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+
+	/* Check for overflow */
+	if (check_add_overflow(rctx->sg_data_total, req->nbytes,
+	    &rctx->sg_data_total))
+		return -EINVAL;
+
+	return kmb_ocs_hcu_final(req);
+}
+
+static int kmb_ocs_hcu_digest(struct ahash_request *req)
+{
+	int rc = 0;
+
+	rc = kmb_ocs_hcu_init(req);
+	if (rc)
+		return rc;
+
+	rc = kmb_ocs_hcu_finup(req);
+
+	return rc;
+}
+
+static int kmb_ocs_hcu_export(struct ahash_request *req, void *out)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+
+	/* Intermediate data is always stored and applied per request. */
+	memcpy(out, rctx, sizeof(*rctx));
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_import(struct ahash_request *req, const void *in)
+{
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(req);
+
+	/* Intermediate data is always stored and applied per request. */
+	memcpy(rctx, in, sizeof(*rctx));
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_setkey(struct crypto_ahash *tfm, const u8 *key,
+			      unsigned int keylen)
+{
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+
+	if (keylen <= KMB_OCS_HCU_MAX_KEYLEN) {
+		memzero_explicit(ctx->key, KMB_OCS_HCU_MAX_KEYLEN);
+		memcpy(ctx->key, key, keylen);
+		ctx->key_len = keylen;
+	} else {
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_sha_cra_init(struct crypto_tfm *tfm)
+{
+	struct ocs_hcu_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct ocs_hcu_rctx));
+
+	ctx->engine_ctx.op.do_one_request = kmb_ocs_hcu_do_one_request;
+	ctx->engine_ctx.op.prepare_request = kmb_ocs_hcu_prepare_request;
+
+	ctx->flags = 0;
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_sm3_cra_init(struct crypto_tfm *tfm)
+{
+	struct ocs_hcu_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct ocs_hcu_rctx));
+
+	ctx->engine_ctx.op.do_one_request = kmb_ocs_hcu_do_one_request;
+	ctx->engine_ctx.op.prepare_request = kmb_ocs_hcu_prepare_request;
+
+	ctx->flags = REQ_FLAGS_SM3;
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_hmac_sm3_cra_init(struct crypto_tfm *tfm)
+{
+	struct ocs_hcu_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct ocs_hcu_rctx));
+
+	ctx->engine_ctx.op.do_one_request = kmb_ocs_hcu_do_one_request;
+	ctx->engine_ctx.op.prepare_request = kmb_ocs_hcu_prepare_request;
+
+	ctx->flags = REQ_FLAGS_SM3 | REQ_FLAGS_HMAC;
+
+	return 0;
+}
+
+static int kmb_ocs_hcu_hmac_cra_init(struct crypto_tfm *tfm)
+{
+	struct ocs_hcu_ctx *ctx = crypto_tfm_ctx(tfm);
+
+	crypto_ahash_set_reqsize(__crypto_ahash_cast(tfm),
+				 sizeof(struct ocs_hcu_rctx));
+
+	ctx->engine_ctx.op.do_one_request = kmb_ocs_hcu_do_one_request;
+	ctx->engine_ctx.op.prepare_request = kmb_ocs_hcu_prepare_request;
+
+	ctx->flags = REQ_FLAGS_HMAC;
+
+	return 0;
+}
+
+static struct ahash_alg ocs_hcu_algs[] = {
+#ifdef CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.halg = {
+		.digestsize	= SHA224_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "sha224",
+			.cra_driver_name	= "sha224-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA224_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_sha_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.setkey		= kmb_ocs_hcu_setkey,
+	.halg = {
+		.digestsize	= SHA224_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "hmac(sha224)",
+			.cra_driver_name	= "hmac-sha224-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA224_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_hmac_cra_init,
+		}
+	}
+},
+#endif /* CONFIG_CRYPTO_DEV_KEEMBAY_OCS_HCU_HMAC_SHA224 */
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.halg = {
+		.digestsize	= SHA256_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "sha256",
+			.cra_driver_name	= "sha256-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA256_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_sha_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.setkey		= kmb_ocs_hcu_setkey,
+	.halg = {
+		.digestsize	= SHA256_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "hmac(sha256)",
+			.cra_driver_name	= "hmac-sha256-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA256_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_hmac_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.halg = {
+		.digestsize	= SM3_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "sm3",
+			.cra_driver_name	= "sm3-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SM3_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_sm3_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.setkey		= kmb_ocs_hcu_setkey,
+	.halg = {
+		.digestsize	= SM3_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "hmac(sm3)",
+			.cra_driver_name	= "hmac-sm3-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SM3_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_hmac_sm3_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.halg = {
+		.digestsize	= SHA384_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "sha384",
+			.cra_driver_name	= "sha384-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA384_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_sha_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.setkey		= kmb_ocs_hcu_setkey,
+	.halg = {
+		.digestsize	= SHA384_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "hmac(sha384)",
+			.cra_driver_name	= "hmac-sha384-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA384_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_hmac_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.halg = {
+		.digestsize	= SHA512_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "sha512",
+			.cra_driver_name	= "sha512-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA512_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_sha_cra_init,
+		}
+	}
+},
+{
+	.init		= kmb_ocs_hcu_init,
+	.update		= kmb_ocs_hcu_update,
+	.final		= kmb_ocs_hcu_final,
+	.finup		= kmb_ocs_hcu_finup,
+	.digest		= kmb_ocs_hcu_digest,
+	.export		= kmb_ocs_hcu_export,
+	.import		= kmb_ocs_hcu_import,
+	.setkey		= kmb_ocs_hcu_setkey,
+	.halg = {
+		.digestsize	= SHA512_DIGEST_SIZE,
+		.statesize	= sizeof(struct ocs_hcu_rctx),
+		.base	= {
+			.cra_name		= "hmac(sha512)",
+			.cra_driver_name	= "hmac-sha512-keembay-ocs",
+			.cra_priority		= 255,
+			.cra_flags		= CRYPTO_ALG_ASYNC,
+			.cra_blocksize		= SHA512_BLOCK_SIZE,
+			.cra_ctxsize		= sizeof(struct ocs_hcu_ctx),
+			.cra_alignmask		= KMB_OCS_HCU_ALIGN_MASK,
+			.cra_module		= THIS_MODULE,
+			.cra_init		= kmb_ocs_hcu_hmac_cra_init,
+		}
+	}
+}
+};
+
+static irqreturn_t kmb_ocs_hcu_irq_thread(int irq, void *dev_id)
+{
+	struct ocs_hcu_dev *hcu_dev = (struct ocs_hcu_dev *)dev_id;
+	struct ocs_hcu_rctx *rctx = ahash_request_ctx(hcu_dev->req);
+	struct crypto_ahash *tfm = crypto_ahash_reqtfm(hcu_dev->req);
+	struct ocs_hcu_ctx *ctx = crypto_ahash_ctx(tfm);
+	bool is_hmac_sw = ((rctx->flags & REQ_FLAGS_HMAC_TYPE_MASK) ==
+			   REQ_FLAGS_HMAC_TYPE_SW);
+	int rc = 0;
+	int i;
+
+	if (hcu_dev->flags & HCU_FLAGS_HCU_ERROR_MASK) {
+		rc = -EIO;
+		goto finish;
+	}
+
+	/* Check if this is the final HMAC data after a streamed HMAC
+	 * computation. This mode doesn't use the HW HMAC setting and requires
+	 * SW to handle IPAD and OPAD, thus one further hash is required of
+	 * H(k ^ opad || digest) where digest obtained here is H(k ^ ipad || m).
+	 */
+	if (is_hmac_sw && (rctx->flags & REQ_FLAGS_FINAL_DATA)) {
+		/* Gets the intermediate data (including digest). */
+		ocs_hcu_finish_req(hcu_dev, rctx->algo, &rctx->idata, &rc);
+		if (rc)
+			goto finish;
+		/* Prepare OPAD for HMAC.
+		 * HMAC(k,m) = H(k ^ opad || H(k ^ ipad || m))
+		 * We now have the H(k ^ ipad || m), create final data for hash
+		 */
+		memset(ctx->opad, HMAC_OPAD_VALUE, rctx->blk_sz);
+		for (i = 0; i < ctx->key_len; i++)
+			ctx->opad[i] ^= ctx->key[i];
+
+		for (i = 0; (i < rctx->dig_sz); i++)
+			ctx->opad[rctx->blk_sz + i] = rctx->idata.digest[i];
+
+		rctx->flags &= ~(REQ_FLAGS_FINAL_DATA | REQ_FLAGS_HASH_PIO);
+
+		rc = kmb_ocs_hcu_hash_pio(hcu_dev, ctx->opad,
+					  rctx->blk_sz + rctx->dig_sz,
+					  rctx->algo, true);
+		if (rc)
+			goto finish;
+
+		return IRQ_HANDLED;
+	}
+
+	/* Check if the interrupt is due to hashing an oversized key. */
+	if (rctx->flags & REQ_FLAGS_HASH_KEY) {
+		/* Gets the intermediate data (including digest). */
+		ocs_hcu_finish_req(hcu_dev, rctx->algo, &rctx->idata, &rc);
+		/* Copy the result into the sw_key. */
+		memcpy(ctx->key, rctx->idata.digest, rctx->dig_sz);
+		/* Key length will always be greater than digest size. */
+		memzero_explicit(ctx->key + rctx->dig_sz,
+				 ctx->key_len - rctx->dig_sz);
+		ctx->key_len = rctx->dig_sz;
+		rctx->flags &= ~REQ_FLAGS_HASH_KEY;
+
+		/* Continue processing the data. */
+		rc = kmb_ocs_hcu_do_data(hcu_dev);
+		if (rc)
+			goto finish;
+
+		return IRQ_HANDLED;
+	}
+
+finish:
+	kmb_ocs_hcu_finish_request(hcu_dev, &rc);
+
+	return IRQ_HANDLED;
+}
+
+static int kmb_ocs_hcu_unregister_algs(struct ocs_hcu_dev *hcu_dev)
+{
+	int i = 0;
+	int rc = 0;
+	int ret = 0;
+
+	for (; i < ARRAY_SIZE(ocs_hcu_algs); i++) {
+		rc = crypto_unregister_ahash(&ocs_hcu_algs[i]);
+		if (rc) {
+			ret = rc;
+			dev_err(hcu_dev->dev, "Failed to unregister %s.\n",
+				ocs_hcu_algs[i].halg.base.cra_name);
+		}
+	}
+
+	return ret;
+}
+
+static int kmb_ocs_hcu_register_algs(struct ocs_hcu_dev *hcu_dev)
+{
+	int rc = 0;
+	int err_rc = 0;
+	int i;
+	int j;
+
+	for (i = 0; i < ARRAY_SIZE(ocs_hcu_algs); i++) {
+		rc = crypto_register_ahash(&ocs_hcu_algs[i]);
+		if (rc)
+			goto cleanup;
+	}
+
+	return 0;
+cleanup:
+	dev_err(hcu_dev->dev, "Failed to register algo.\n");
+	for (j = 0; j < i; j++) {
+		err_rc = crypto_unregister_ahash(&ocs_hcu_algs[i]);
+		if (err_rc) {
+			dev_err(hcu_dev->dev, "Failed to unregister %s.\n",
+				ocs_hcu_algs[i].halg.base.cra_name);
+		}
+	}
+
+	return rc;
+}
+
+/* Device tree driver match. */
+static const struct of_device_id kmb_ocs_hcu_of_match[] = {
+	{
+		.compatible = "intel,keembay-ocs-hcu",
+	},
+	{}
+};
+
+static int kmb_ocs_hcu_remove(struct platform_device *pdev)
+{
+	struct ocs_hcu_dev *hcu_dev;
+	int rc;
+
+	hcu_dev = platform_get_drvdata(pdev);
+	if (!hcu_dev)
+		return -ENODEV;
+
+	rc = kmb_ocs_hcu_unregister_algs(hcu_dev);
+
+	spin_lock(&ocs_hcu.lock);
+	list_del(&hcu_dev->list);
+	spin_unlock(&ocs_hcu.lock);
+
+	rc |= crypto_engine_exit(hcu_dev->engine);
+
+	return rc;
+}
+
+static int kmb_ocs_hcu_probe(struct platform_device *pdev)
+{
+	int rc;
+	struct device *dev = &pdev->dev;
+	struct resource *hcu_mem;
+	struct ocs_hcu_dev *hcu_dev;
+
+	hcu_dev = devm_kzalloc(dev, sizeof(*hcu_dev),
+				       GFP_KERNEL);
+	if (!hcu_dev)
+		return -ENOMEM;
+
+	hcu_dev->dev = dev;
+
+	platform_set_drvdata(pdev, hcu_dev);
+	rc = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+
+	if (rc)
+		return rc;
+
+	INIT_LIST_HEAD(&hcu_dev->list);
+
+	/* Get the memory address and remap. */
+	hcu_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!hcu_mem) {
+		dev_err(dev, "Could not retrieve io mem resource.\n");
+		rc = -ENODEV;
+		goto list_del;
+	}
+
+	hcu_dev->io_base = devm_ioremap_resource(dev, hcu_mem);
+	if (IS_ERR(hcu_dev->io_base)) {
+		dev_err(dev, "Could not io-remap mem resource.\n");
+		rc = PTR_ERR(hcu_dev->io_base);
+		goto list_del;
+	}
+
+	/* Get and request IRQ. */
+	hcu_dev->irq = platform_get_irq(pdev, 0);
+	if (hcu_dev->irq < 0) {
+		dev_err(dev, "Could not retrieve IRQ.\n");
+		rc = hcu_dev->irq;
+		goto list_del;
+	}
+
+	rc = devm_request_threaded_irq(&pdev->dev, hcu_dev->irq,
+				       ocs_hcu_irq_handler,
+				       kmb_ocs_hcu_irq_thread,
+				       0, "keembay-ocs-hcu", hcu_dev);
+	if (rc < 0) {
+		dev_err(dev, "Could not request IRQ.\n");
+		goto list_del;
+	}
+
+	spin_lock(&ocs_hcu.lock);
+	list_add_tail(&hcu_dev->list, &ocs_hcu.dev_list);
+	spin_unlock(&ocs_hcu.lock);
+
+	/* Initialize crypto engine */
+	hcu_dev->engine = crypto_engine_alloc_init(dev, 1);
+	if (!hcu_dev->engine)
+		goto list_del;
+
+	rc = crypto_engine_start(hcu_dev->engine);
+	if (rc) {
+		dev_err(dev, "Could not start engine.\n");
+		goto cleanup;
+	}
+
+	/* Security infrastructure guarantees OCS clock is enabled. */
+
+	rc = kmb_ocs_hcu_register_algs(hcu_dev);
+	if (rc) {
+		dev_err(dev, "Could not register algorithms.\n");
+		goto cleanup;
+	}
+
+	return 0;
+cleanup:
+	crypto_engine_exit(hcu_dev->engine);
+list_del:
+	spin_lock(&ocs_hcu.lock);
+	list_del(&hcu_dev->list);
+	spin_unlock(&ocs_hcu.lock);
+
+	return rc;
+
+}
+
+/* The OCS driver is a platform device. */
+static struct platform_driver kmb_ocs_hcu_driver = {
+	.probe = kmb_ocs_hcu_probe,
+	.remove = kmb_ocs_hcu_remove,
+	.driver = {
+			.name = DRV_NAME,
+			.of_match_table = kmb_ocs_hcu_of_match,
+		},
+};
+
+module_platform_driver(kmb_ocs_hcu_driver);
+
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/crypto/keembay/ocs-hcu.c b/drivers/crypto/keembay/ocs-hcu.c
new file mode 100644
index 000000000000..fff2b6ed58a9
--- /dev/null
+++ b/drivers/crypto/keembay/ocs-hcu.c
@@ -0,0 +1,475 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Keem Bay OCS Hash control unit Crypto Driver.
+ *
+ * Copyright (C) 2018-2020 Intel Corporation
+ */
+
+#include <linux/delay.h>
+#include <linux/irq.h>
+#include <linux/module.h>
+
+#include <crypto/sha.h>
+
+#include "ocs-hcu.h"
+
+#define OCS_HCU_MODE 0x00
+#define OCS_HCU_CHAIN 0x04
+#define OCS_HCU_OPERATION 0x08
+#define OCS_HCU_KEY_0 0x0C
+#define OCS_HCU_KEY_1 0x10
+#define OCS_HCU_KEY_2 0x14
+#define OCS_HCU_KEY_3 0x18
+#define OCS_HCU_KEY_4 0x1C
+#define OCS_HCU_KEY_5 0x20
+#define OCS_HCU_KEY_6 0x24
+#define OCS_HCU_KEY_7 0x28
+#define OCS_HCU_KEY_8 0x2C
+#define OCS_HCU_KEY_9 0x30
+#define OCS_HCU_KEY_10 0x34
+#define OCS_HCU_KEY_11 0x38
+#define OCS_HCU_KEY_12 0x3C
+#define OCS_HCU_KEY_13 0x40
+#define OCS_HCU_KEY_14 0x44
+#define OCS_HCU_KEY_15 0x48
+#define OCS_HCU_ISR 0x50
+#define OCS_HCU_IER 0x54
+#define OCS_HCU_STATUS 0x58
+#define OCS_HCU_MSG_LEN_LO 0x60
+#define OCS_HCU_MSG_LEN_HI 0x64
+#define OCS_HCU_KEY_BYTE_ORDER_CFG 0x80
+#define OCS_HCU_DMA_SRC_ADDR 0x400
+#define OCS_HCU_DMA_DST_ADDR 0x404
+#define OCS_HCU_DMA_SRC_SIZE 0x408
+#define OCS_HCU_DMA_DST_SIZE 0x40C
+#define OCS_HCU_DMA_DMA_MODE 0x410
+#define OCS_HCU_DMA_OTHER_MODE 0x414
+#define OCS_HCU_DMA_NEXT_SRC_DESCR 0x418
+#define OCS_HCU_DMA_NEXT_DST_DESCR 0x41C
+#define OCS_HCU_DMA_WHILE_ACTIVE_MODE 0x420
+#define OCS_HCU_DMA_LOG 0x424
+#define OCS_HCU_DMA_STATUS 0x428
+#define OCS_HCU_DMA_PERF_CNTR 0x42C
+#define OCS_HCU_DMA_VALID_SAI_31_0 0x440
+#define OCS_HCU_DMA_VALID_SAI_63_32 0x444
+#define OCS_HCU_DMA_VALID_SAI_95_64 0x448
+#define OCS_HCU_DMA_VALID_SAI_127_96 0x44C
+#define OCS_HCU_DMA_VALID_SAI_159_128 0x450
+#define OCS_HCU_DMA_VALID_SAI_191_160 0x454
+#define OCS_HCU_DMA_VALID_SAI_223_192 0x458
+#define OCS_HCU_DMA_VALID_SAI_255_224 0x45C
+#define OCS_HCU_DMA_MSI_ISR 0x480
+#define OCS_HCU_DMA_MSI_IER 0x484
+#define OCS_HCU_DMA_MSI_MASK 0x488
+#define OCS_HCU_DMA_MSI_MA 0x800
+#define OCS_HCU_DMA_MSI_DA 0x804
+#define OCS_HCU_DMA_MSI_EN 0x808
+#define OCS_HCU_DMA_INBUFFER_WRITE_FIFO 0x600
+#define OCS_HCU_DMA_OUTBUFFER_READ_FIFO 0x700
+
+/* Register bit definitions. */
+#define HCU_STATUS_BUSY_MASK BIT(0)
+
+#define HCU_BYTE_ORDER_SWAP BIT(0)
+
+#define HCU_IRQ_HASH_DONE BIT(2)
+#define HCU_IRQ_HASH_ERR (BIT(3) | BIT(1) | BIT(0))
+
+#define HCU_DMA_IRQ_SRC_DONE BIT(0)
+#define HCU_DMA_IRQ_DST_DONE BIT(1)
+#define HCU_DMA_IRQ_SAI_ERR BIT(2)
+#define HCU_DMA_IRQ_BAD_COMP_ERR BIT(3)
+#define HCU_DMA_IRQ_INBUF_RD_ERR BIT(4)
+#define HCU_DMA_IRQ_INBUF_WD_ERR BIT(5)
+#define HCU_DMA_IRQ_OUTBUF_WR_ERR BIT(6)
+#define HCU_DMA_IRQ_OUTBUF_RD_ERR BIT(7)
+#define HCU_DMA_IRQ_CRD_ERR BIT(8)
+#define HCU_DMA_IRQ_ERR_MASK (HCU_DMA_IRQ_SAI_ERR | \
+			HCU_DMA_IRQ_BAD_COMP_ERR | \
+			HCU_DMA_IRQ_INBUF_RD_ERR | \
+			HCU_DMA_IRQ_INBUF_WD_ERR | \
+			HCU_DMA_IRQ_OUTBUF_WR_ERR | \
+			HCU_DMA_IRQ_OUTBUF_RD_ERR | \
+			HCU_DMA_IRQ_CRD_ERR)
+
+#define HCU_DMA_SNOOP_MASK (0x7 << 28)
+#define HCU_DMA_SRC_LL_EN BIT(25)
+#define HCU_DMA_EN BIT(31)
+#define HCU_DMA_STAT_SRC_DONE BIT(15)
+
+#define HCU_HMAC_OFFSET 22
+#define HCU_HMAC_MASK BIT(HCU_HMAC_OFFSET)
+#define HCU_ALGO_OFFSET 16
+
+#define HCU_CHAIN_WRITE_ENDIANNESS_BIT 30
+#define HCU_CHAIN_READ_ENDIANNESS_BIT 28
+#define HCU_DATA_WRITE_ENDIANNESS_BIT 26
+
+#define HCU_DMA_MSI_UNMASK BIT(0)
+#define HCU_DMA_MSI_DISABLE 0
+#define HCU_IRQ_DISABLE 0
+
+#define OCS_HCU_NUM_CHAINS_SHA256_224_SM3 8
+#define OCS_HCU_NUM_CHAINS_SHA384_512 16
+
+#define OCS_HCU_START BIT(0)
+#define OCS_HCU_TERMINATE BIT(1)
+
+static inline u32 ocs_hcu_digest_size(u32 algo)
+{
+	switch (algo & OCS_HCU_ALGO_MASK) {
+	case OCS_HCU_ALGO_SHA224:
+		return SHA224_DIGEST_SIZE;
+	case OCS_HCU_ALGO_SHA256:
+	case OCS_HCU_ALGO_SM3:
+		/* SM3 shares the same digest size. */
+		return SHA256_DIGEST_SIZE;
+	case OCS_HCU_ALGO_SHA384:
+		return SHA384_DIGEST_SIZE;
+	case OCS_HCU_ALGO_SHA512:
+		return SHA512_DIGEST_SIZE;
+	default:
+		return 0;
+	}
+}
+
+static inline u32 ocs_hcu_num_chains(u32 algo)
+{
+	switch (algo & OCS_HCU_ALGO_MASK) {
+	case OCS_HCU_ALGO_SHA224:
+	case OCS_HCU_ALGO_SHA256:
+	case OCS_HCU_ALGO_SM3:
+		return OCS_HCU_NUM_CHAINS_SHA256_224_SM3;
+	case OCS_HCU_ALGO_SHA384:
+	case OCS_HCU_ALGO_SHA512:
+		return OCS_HCU_NUM_CHAINS_SHA384_512;
+	default:
+		return 0;
+	};
+}
+
+static inline u32 ocs_hcu_block_size(u32 algo)
+{
+	switch (algo & OCS_HCU_ALGO_MASK) {
+	case OCS_HCU_ALGO_SHA224:
+		return SHA224_BLOCK_SIZE;
+	case OCS_HCU_ALGO_SHA256:
+	case OCS_HCU_ALGO_SM3:
+		/* SM3 shares the same block size. */
+		return SHA256_BLOCK_SIZE;
+	case OCS_HCU_ALGO_SHA384:
+		return SHA384_BLOCK_SIZE;
+	case OCS_HCU_ALGO_SHA512:
+		return SHA512_BLOCK_SIZE;
+	default:
+		return 0;
+	}
+}
+
+bool ocs_hcu_wait_busy(struct ocs_hcu_dev *hcu_dev)
+{
+	int retries = 100000;
+
+	do {
+		if (!(readl(hcu_dev->io_base + OCS_HCU_STATUS) &
+		    HCU_STATUS_BUSY_MASK))
+			return false;
+		usleep_range(100, 200);
+	} while (retries--);
+
+	return true;
+}
+
+void ocs_hcu_irq_en(struct ocs_hcu_dev *hcu_dev)
+{
+	/* Clear any pending interrupts. */
+	writel(0xFFFFFFFF, hcu_dev->io_base + OCS_HCU_ISR);
+	writel(0xFFFFFFFF, hcu_dev->io_base + OCS_HCU_DMA_MSI_ISR);
+	/* Enable error and HCU done interrupts. */
+	writel(HCU_IRQ_HASH_DONE | HCU_IRQ_HASH_ERR,
+	       hcu_dev->io_base + OCS_HCU_IER);
+	/* Only operating on DMA source completion and error interrupts. */
+	writel(HCU_DMA_IRQ_ERR_MASK | HCU_DMA_IRQ_SRC_DONE,
+		   hcu_dev->io_base + OCS_HCU_DMA_MSI_IER);
+	/* Unmask */
+	writel(HCU_DMA_MSI_UNMASK, hcu_dev->io_base + OCS_HCU_DMA_MSI_MASK);
+}
+
+void ocs_hcu_irq_dis(struct ocs_hcu_dev *hcu_dev)
+{
+	writel(HCU_IRQ_DISABLE, hcu_dev->io_base + OCS_HCU_IER);
+	writel(HCU_DMA_MSI_DISABLE, hcu_dev->io_base + OCS_HCU_DMA_MSI_IER);
+}
+
+int ocs_hcu_get_intermediate_data(struct ocs_hcu_dev *hcu_dev,
+				  struct ocs_hcu_idata_desc *data, u32 algo)
+{
+	int i;
+	bool busy;
+	const int n = ocs_hcu_num_chains(algo);
+	u32 *chain;
+
+	/* Data not requested. */
+	if (!data)
+		return -EINVAL;
+
+	chain = (u32 *)data->digest;
+
+	/* Ensure that the OCS is no longer busy before reading the chains. */
+	busy = ocs_hcu_wait_busy(hcu_dev);
+
+	if (busy)
+		return -EBUSY;
+
+	for (i = 0; i < n; i++)
+		chain[i] = readl(hcu_dev->io_base + OCS_HCU_CHAIN);
+
+	data->msg_len_lo = readl(hcu_dev->io_base + OCS_HCU_MSG_LEN_LO);
+	data->msg_len_hi = readl(hcu_dev->io_base + OCS_HCU_MSG_LEN_HI);
+
+	return 0;
+}
+
+void ocs_hcu_set_intermediate_data(struct ocs_hcu_dev *hcu_dev,
+				   struct ocs_hcu_idata_desc *data, u32 algo)
+{
+	int i;
+	const int n = ocs_hcu_num_chains(algo);
+	u32 *chain = (u32 *)data->digest;
+
+	for (i = 0; i < n; i++)
+		writel(chain[i], hcu_dev->io_base + OCS_HCU_CHAIN);
+
+	writel(data->msg_len_lo, hcu_dev->io_base + OCS_HCU_MSG_LEN_LO);
+	writel(data->msg_len_hi, hcu_dev->io_base + OCS_HCU_MSG_LEN_HI);
+}
+
+void ocs_hcu_hw_init(struct ocs_hcu_dev *hcu_dev)
+{
+	u32 cfg = 0;
+
+	if ((hcu_dev->flags & HCU_FLAGS_HCU_INIT) == 0) {
+		/* Initialize hardware. */
+		cfg = (KMB_HCU_ENDIANNESS_MASK <<
+		       HCU_DATA_WRITE_ENDIANNESS_BIT);
+		writel(cfg, hcu_dev->io_base + OCS_HCU_MODE);
+		ocs_hcu_irq_en(hcu_dev);
+		hcu_dev->flags |= HCU_FLAGS_HCU_INIT;
+	}
+}
+
+/* To be called after ocs_hcu_hw_init */
+int ocs_hcu_hw_cfg(struct ocs_hcu_dev *hcu_dev, u32 algo)
+{
+	u32 cfg = readl(hcu_dev->io_base + OCS_HCU_MODE);
+	u32 ocs_algo = algo & OCS_HCU_ALGO_MASK;
+
+	if (ocs_algo != OCS_HCU_ALGO_SHA256 &&
+	    ocs_algo != OCS_HCU_ALGO_SHA224 &&
+	    ocs_algo != OCS_HCU_ALGO_SHA384 &&
+	    ocs_algo != OCS_HCU_ALGO_SHA512 &&
+	    ocs_algo != OCS_HCU_ALGO_SM3)
+		return -EINVAL;
+
+	if ((hcu_dev->flags & HCU_FLAGS_HCU_INIT) == 0)
+		return -EPERM;
+
+	cfg |= ocs_algo << HCU_ALGO_OFFSET;
+	cfg &= ~HCU_HMAC_MASK;
+	cfg |= ((algo & OCS_HCU_ALGO_HMAC_MASK) >>
+	       OCS_HCU_ALGO_HMAC_SHIFT) << HCU_HMAC_OFFSET;
+
+	writel(cfg, hcu_dev->io_base + OCS_HCU_MODE);
+
+	return 0;
+}
+
+void ocs_hcu_hw_disable(struct ocs_hcu_dev *hcu_dev)
+{
+	if ((hcu_dev->flags & HCU_FLAGS_HCU_INIT) == HCU_FLAGS_HCU_INIT) {
+		/* Clear hardware. */
+		writel(0, hcu_dev->io_base + OCS_HCU_MODE);
+		ocs_hcu_irq_dis(hcu_dev);
+		hcu_dev->flags &= ~HCU_FLAGS_HCU_INIT;
+	}
+}
+
+void ocs_hcu_tx_data_done(struct ocs_hcu_dev *hcu_dev)
+{
+	writel(OCS_HCU_TERMINATE, hcu_dev->io_base + OCS_HCU_OPERATION);
+}
+
+static unsigned int ocs_hcu_hash_final_cpu(struct ocs_hcu_dev *hcu_dev,
+					   u8 *buf, u32 sz)
+{
+	u32 *buf_32 = (u32 *)buf;
+	u32 sz_32 = sz / sizeof(u32);
+	int i;
+	int retries = 10000;
+
+	/* Write in using full register size. */
+	for (i = 0; i < sz_32; i++)
+		writel(buf_32[i], hcu_dev->io_base +
+		       OCS_HCU_DMA_INBUFFER_WRITE_FIFO);
+
+	/* Write final bytes into buffer. */
+	for (i = sz_32 * sizeof(u32); i < sz; i++)
+		writeb(buf[i], hcu_dev->io_base +
+		       OCS_HCU_DMA_INBUFFER_WRITE_FIFO);
+
+	/* Wait until the writes are complete. */
+	do {
+		if ((readl(hcu_dev->io_base + OCS_HCU_DMA_STATUS) &
+		    HCU_DMA_STAT_SRC_DONE))
+			break;
+
+		if ((readl(hcu_dev->io_base + OCS_HCU_DMA_MSI_ISR) &
+		    HCU_DMA_IRQ_ERR_MASK))
+			return 0;
+
+		usleep_range(100, 200);
+	} while (retries--);
+
+	return (unsigned int)sz;
+}
+
+static unsigned int ocs_hcu_hash_block_aligned_cpu(struct ocs_hcu_dev *hcu_dev,
+				   u8 *buf, u32 sz, u32 algo)
+{
+	u32 blk_sz = ocs_hcu_block_size(algo);
+	u32 blk_sz_32;
+	u32 num_blks;
+	u32 *buf_32 = (u32 *)buf;
+	int i;
+
+	if (blk_sz == 0)
+		return 0;
+
+	blk_sz_32 = blk_sz / sizeof(u32);
+	num_blks = sz / blk_sz;
+
+	for (i = 0; i < (blk_sz_32 * num_blks); i++)
+		writel(buf_32[i], hcu_dev->io_base +
+			   OCS_HCU_DMA_INBUFFER_WRITE_FIFO);
+
+	if (readl(hcu_dev->io_base + OCS_HCU_DMA_MSI_ISR) &
+	    HCU_DMA_IRQ_ERR_MASK)
+		return 0;
+
+	return (unsigned int)(i * sizeof(u32));
+}
+
+void ocs_hcu_start_hash(struct ocs_hcu_dev *hcu_dev)
+{
+	writel(OCS_HCU_START, hcu_dev->io_base + OCS_HCU_OPERATION);
+}
+
+unsigned int ocs_hcu_hash_cpu(struct ocs_hcu_dev *hcu_dev,
+		      u8 *buf, u32 sz, u32 algo, bool terminate)
+{
+	unsigned int written;
+
+	if (!buf)
+		return 0;
+
+	written = ocs_hcu_hash_block_aligned_cpu(hcu_dev, buf, sz, algo);
+
+	if (terminate)
+		written += ocs_hcu_hash_final_cpu(hcu_dev, buf + written,
+						  sz - written);
+
+	return written;
+}
+
+void ocs_hcu_ll_dma_start(struct ocs_hcu_dev *hcu_dev, dma_addr_t head,
+			  bool terminate)
+{
+	u32 cfg = HCU_DMA_SNOOP_MASK | HCU_DMA_SRC_LL_EN | HCU_DMA_EN;
+
+	if (!head || head > OCS_HCU_DMA_MAX_ADDR_MASK)
+		return;
+
+	hcu_dev->flags |= HCU_FLAGS_HCU_ACTIVE;
+
+	writel(head, hcu_dev->io_base + OCS_HCU_DMA_NEXT_SRC_DESCR);
+	writel(0, hcu_dev->io_base + OCS_HCU_DMA_SRC_SIZE);
+	writel(0, hcu_dev->io_base + OCS_HCU_DMA_DST_SIZE);
+	ocs_hcu_start_hash(hcu_dev);
+	writel(cfg, hcu_dev->io_base + OCS_HCU_DMA_DMA_MODE);
+	if (terminate)
+		writel(OCS_HCU_TERMINATE,
+		       hcu_dev->io_base + OCS_HCU_OPERATION);
+}
+
+void ocs_hcu_finish_req(struct ocs_hcu_dev *hcu_dev, u32 algo,
+			struct ocs_hcu_idata_desc *data, int *error)
+{
+	if (hcu_dev->flags & HCU_FLAGS_HCU_DONE && *error == 0) {
+		/* Get the digest and message length if data
+		 * buffer provided.
+		 */
+		*error = ocs_hcu_get_intermediate_data(hcu_dev, data, algo);
+		/* Clear the HCU flags for the next request. */
+		hcu_dev->flags &= ~(HCU_FLAGS_FINISH_REQ_MASK);
+	}
+
+	if (*error)
+		hcu_dev->flags |= HCU_FLAGS_HCU_OP_ERR;
+}
+
+int ocs_hcu_write_key(struct ocs_hcu_dev *hcu_dev, u8 *key, unsigned int len)
+{
+	u32 *key_32 = (u32 *)key;
+	int i = 0;
+
+	if (len != HCU_MAX_KEYLEN)
+		return -EINVAL;
+
+	/* Swap the byte order of the memory. */
+	writel(HCU_BYTE_ORDER_SWAP,
+	       hcu_dev->io_base + OCS_HCU_KEY_BYTE_ORDER_CFG);
+
+	for (i = 0; i < (HCU_MAX_KEYLEN / sizeof(u32)); i++)
+		writel(key_32[(HCU_MAX_KEYLEN / sizeof(u32)) - (i + 1)],
+		       hcu_dev->io_base + (OCS_HCU_KEY_0 + (sizeof(u32) * i)));
+
+	return 0;
+}
+
+irqreturn_t ocs_hcu_irq_handler(int irq, void *dev_id)
+{
+	struct ocs_hcu_dev *hcu_dev = dev_id;
+	irqreturn_t rc = IRQ_NONE;
+	u32 hcu_irq = readl(hcu_dev->io_base + OCS_HCU_ISR);
+	u32 dma_irq = readl(hcu_dev->io_base + OCS_HCU_DMA_MSI_ISR);
+
+	/* Check the HCU status. */
+	if (hcu_irq & HCU_IRQ_HASH_ERR) {
+		hcu_dev->flags |= HCU_FLAGS_HCU_ERR;
+		rc = IRQ_WAKE_THREAD;
+	} else if (hcu_irq & HCU_IRQ_HASH_DONE) {
+		hcu_dev->flags |= HCU_FLAGS_HCU_DONE;
+		rc = IRQ_WAKE_THREAD;
+	}
+	/* Clear the HCU interrupt. */
+	writel(hcu_irq, hcu_dev->io_base + OCS_HCU_ISR);
+
+	/* Check the DMA status. */
+	if (dma_irq & HCU_DMA_IRQ_ERR_MASK) {
+		hcu_dev->flags |= HCU_FLAGS_HCU_DMA_ERR;
+		rc = IRQ_WAKE_THREAD;
+	} else if (dma_irq & HCU_DMA_IRQ_SRC_DONE) {
+		/* DMA is complete, indicate that the HCU is done this
+		 * transaction.
+		 */
+		hcu_dev->flags |= HCU_FLAGS_HCU_DONE;
+		rc = IRQ_WAKE_THREAD;
+	}
+
+	/* Clear the HCU DMA interrupt. */
+	writel(dma_irq, hcu_dev->io_base + OCS_HCU_DMA_MSI_ISR);
+
+	return rc;
+}
+
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/crypto/keembay/ocs-hcu.h b/drivers/crypto/keembay/ocs-hcu.h
new file mode 100644
index 000000000000..862d71429f0a
--- /dev/null
+++ b/drivers/crypto/keembay/ocs-hcu.h
@@ -0,0 +1,123 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Keem Bay OCS HCU Crypto Driver.
+ *
+ * Copyright (C) 2018-2020 Intel Corporation
+ */
+
+#ifndef _CRYPTO_OCS_HCU_H
+#define _CRYPTO_OCS_HCU_H
+
+#define OCS_LL_DMA_FLAG_TERMINATE BIT(31)
+#define OCS_LL_DMA_FLAG_FREEZE BIT(30)
+#define OCS_LL_DMA_FLAG_RESERVED (BIT(30) - 1)
+
+#define OCS_HCU_ALGO_SHA256 2
+#define OCS_HCU_ALGO_SHA224 3
+#define OCS_HCU_ALGO_SHA384 4
+#define OCS_HCU_ALGO_SHA512 5
+#define OCS_HCU_ALGO_SM3 6
+#define OCS_HCU_ALGO_MASK (BIT(3) - 1)
+#define OCS_HCU_ALGO_HMAC_SHIFT 4
+#define OCS_HCU_ALGO_HMAC_MASK BIT(OCS_HCU_ALGO_HMAC_SHIFT)
+
+#define OCS_HCU_DMA_NO_SNOOP 1
+#define OCS_HCU_DMA_SNOOP 0
+#define OCS_HCU_DMA_BTF_SWAP 1
+#define OCS_HCU_DMA_BTF_NO_SWAP 0
+#define OCS_HCU_DMA_ADDR_MODE_FIXED 1
+#define OCS_HCU_DMA_ADDR_MODE_LINEAR 0
+#define OCS_HCU_DMA_MAX_ADDR_MASK (BIT(32) - 1)
+
+#define OCS_HCU_MAX_CHAIN_NUM 16
+
+/* Device flags */
+#define HCU_FLAGS_HCU_INIT BIT(0)
+#define HCU_FLAGS_HCU_ACTIVE BIT(1)
+#define HCU_FLAGS_HCU_DONE BIT(16)
+#define HCU_FLAGS_HCU_ERR BIT(17)
+#define HCU_FLAGS_HCU_DMA_ERR BIT(18)
+#define HCU_FLAGS_HCU_OP_ERR BIT(19)
+#define HCU_FLAGS_HCU_ERROR_MASK (HCU_FLAGS_HCU_DMA_ERR | \
+				  HCU_FLAGS_HCU_ERR | \
+				  HCU_FLAGS_HCU_OP_ERR)
+#define HCU_FLAGS_FINISH_REQ_MASK (HCU_FLAGS_HCU_ACTIVE | \
+				   HCU_FLAGS_HCU_DONE)
+
+#define HCU_MAX_KEYLEN 64
+
+#define KMB_HCU_ENDIANNESS_MASK (0x2A)
+
+/**
+ * HCU device context.
+ * @list: List of device contexts.
+ * @dev: OCS HCU device.
+ * @irq: IRQ number.
+ * @io_base: IO Base of HCU.
+ * @flags: HW flags indicating state.
+ * @req: Request being operated on.
+ * @engine: Crypto engine for the device.
+ */
+struct ocs_hcu_dev {
+	struct list_head list;
+	struct device *dev;
+	int irq;
+	/* Base address of OCS HCU */
+	void __iomem *io_base;
+	/* Status of the OCS HCU device */
+	u32 flags;
+	/* Active request. */
+	struct ahash_request *req;
+	struct crypto_engine *engine;
+};
+
+/**
+ * Structure of HW required linked list descriptor.
+ * @src_addr: Source address of the data.
+ * @src_len: Length of data to be fetched.
+ * @nxt_desc: Next descriptor to fetch.
+ * @ll_flags: Flags (Freeze @ terminate) for the DMA engine.
+ */
+struct ocs_hcu_dma_desc {
+	u32 src_adr;
+	u32 src_len;
+	u32 nxt_desc;
+	u32 ll_flags;
+};
+
+/**
+ * Structure to contain the intermediate data generated by the HCU.
+ * @msg_len_lo: Length of data the HCU has operated on in bits, low 32b.
+ * @msg_len_hi: Length of data the HCU has operated on in bits, high 32b.
+ * @digest: The digest read from the HCU. If the HCU is terminated, it will
+ *	    contain the actual hash digest. Otherwise it is the intermediate
+ *	    state.
+ */
+struct ocs_hcu_idata_desc {
+	u32 msg_len_lo;
+	u32 msg_len_hi;
+	u8 digest[SHA512_DIGEST_SIZE];
+};
+
+irqreturn_t ocs_hcu_irq_handler(int irq, void *dev_id);
+int ocs_hcu_get_intermediate_data(struct ocs_hcu_dev *hcu_dev,
+				  struct ocs_hcu_idata_desc *data, u32 algo);
+void ocs_hcu_set_intermediate_data(struct ocs_hcu_dev *hcu_dev,
+				   struct ocs_hcu_idata_desc *data, u32 algo);
+void ocs_hcu_hw_init(struct ocs_hcu_dev *hcu_dev);
+void ocs_hcu_hw_disable(struct ocs_hcu_dev *hcu_dev);
+int ocs_hcu_hw_cfg(struct ocs_hcu_dev *hcu_dev, u32 algo);
+void ocs_hcu_start_hash(struct ocs_hcu_dev *hcu_dev);
+unsigned int ocs_hcu_hash_cpu(struct ocs_hcu_dev *hcu_dev,
+		      u8 *buf, u32 sz, u32 algo, bool terminate);
+void ocs_hcu_ll_dma_start(struct ocs_hcu_dev *hcu_dev, dma_addr_t head,
+			  bool terminate);
+void ocs_hcu_tx_data_done(struct ocs_hcu_dev *hcu_dev);
+void ocs_hcu_finish_req(struct ocs_hcu_dev *hcu_dev, u32 algo,
+			struct ocs_hcu_idata_desc *data, int *error);
+void ocs_hcu_irq_en(struct ocs_hcu_dev *hcu_dev);
+void ocs_hcu_irq_dis(struct ocs_hcu_dev *hcu_dev);
+bool ocs_hcu_wait_busy(struct ocs_hcu_dev *hcu_dev);
+int ocs_hcu_write_key(struct ocs_hcu_dev *hcu_dev, u8 *key, unsigned int len);
+
+#endif /* _CRYPTO_OCS_HCU_H */
-- 
2.17.1

