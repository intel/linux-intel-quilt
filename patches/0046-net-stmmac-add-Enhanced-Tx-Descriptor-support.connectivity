From 01b3841d2bd466535cfe64070e240ad1f011924b Mon Sep 17 00:00:00 2001
From: Kweh Hock Leong <hock.leong.kweh@intel.com>
Date: Fri, 9 Aug 2019 00:22:00 +0800
Subject: [PATCH 046/105] net: stmmac: add Enhanced Tx Descriptor support in
 main flow

Add support for Enhanced Tx Descriptor in stmmac_main.c for all
Tx related functions.

For TSO, it is available in DWMAC v4.0 and above, so it is not
applicable for extended descriptor which is used in DWMAC v3.5.

The tail pointer for supporting enhanced and extended descriptor
is made to be updated correctly.

Signed-off-by: Kweh Hock Leong <hock.leong.kweh@intel.com>
Signed-off-by: Ong Boon Leong <boon.leong.ong@intel.com>
Signed-off-by: Tan, Tee Min
---
 drivers/net/ethernet/stmicro/stmmac/stmmac.h  |   2 +
 .../net/ethernet/stmicro/stmmac/stmmac_main.c | 129 ++++++++++++++++--
 2 files changed, 116 insertions(+), 15 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac.h b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
index 703d87d0c178..78d6c5dbda15 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
@@ -50,6 +50,7 @@ struct stmmac_tx_queue {
 	struct timer_list txtimer;
 	u32 queue_index;
 	struct stmmac_priv *priv_data;
+	struct dma_enhanced_tx_desc *dma_enhtx ____cacheline_aligned_in_smp;
 	struct dma_extended_desc *dma_etx ____cacheline_aligned_in_smp;
 	struct dma_desc *dma_tx;
 	struct sk_buff **tx_skbuff;
@@ -205,6 +206,7 @@ struct stmmac_priv {
 	int tx_lpi_enabled;
 	unsigned int mode;
 	unsigned int chain_mode;
+	int enhanced_tx_desc;
 	int extend_desc;
 	struct hwtstamp_config tstamp_config;
 	struct ptp_clock *ptp_clock;
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 3bd271ccb787..93d3a6766d7f 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -1115,6 +1115,8 @@ static void stmmac_display_tx_rings(struct stmmac_priv *priv)
 
 		if (priv->extend_desc)
 			head_tx = (void *)tx_q->dma_etx;
+		else if (priv->enhanced_tx_desc)
+			head_tx = (void *)tx_q->dma_enhtx;
 		else
 			head_tx = (void *)tx_q->dma_tx;
 
@@ -1191,7 +1193,12 @@ static void stmmac_clear_tx_descriptors(struct stmmac_priv *priv, u32 queue)
 	for (i = 0; i < DMA_TX_SIZE; i++)
 		if (priv->extend_desc)
 			stmmac_init_tx_desc(priv, &tx_q->dma_etx[i].basic,
-					priv->mode, (i == DMA_TX_SIZE - 1));
+					    priv->mode,
+					    (i == DMA_TX_SIZE - 1));
+		else if (priv->enhanced_tx_desc)
+			stmmac_init_tx_desc(priv, &tx_q->dma_enhtx[i].basic,
+					    priv->mode,
+					    (i == DMA_TX_SIZE - 1));
 		else
 			stmmac_init_tx_desc(priv, &tx_q->dma_tx[i],
 					priv->mode, (i == DMA_TX_SIZE - 1));
@@ -1407,7 +1414,12 @@ static int init_dma_tx_desc_rings(struct net_device *dev)
 		if (priv->mode == STMMAC_CHAIN_MODE) {
 			if (priv->extend_desc)
 				stmmac_mode_init(priv, tx_q->dma_etx,
-						tx_q->dma_tx_phy, DMA_TX_SIZE, 1);
+						 tx_q->dma_tx_phy,
+						 DMA_TX_SIZE, 1);
+			else if (priv->enhanced_tx_desc)
+				stmmac_mode_init(priv, tx_q->dma_enhtx,
+						 tx_q->dma_tx_phy,
+						 DMA_TX_SIZE, 1);
 			else
 				stmmac_mode_init(priv, tx_q->dma_tx,
 						tx_q->dma_tx_phy, DMA_TX_SIZE, 0);
@@ -1417,6 +1429,8 @@ static int init_dma_tx_desc_rings(struct net_device *dev)
 			struct dma_desc *p;
 			if (priv->extend_desc)
 				p = &((tx_q->dma_etx + i)->basic);
+			else if (priv->enhanced_tx_desc)
+				p = &((tx_q->dma_enhtx + i)->basic);
 			else
 				p = tx_q->dma_tx + i;
 
@@ -1541,14 +1555,18 @@ static void free_dma_tx_desc_resources(struct stmmac_priv *priv)
 		dma_free_tx_skbufs(priv, queue);
 
 		/* Free DMA regions of consistent memory previously allocated */
-		if (!priv->extend_desc)
-			dma_free_coherent(priv->device,
-					  DMA_TX_SIZE * sizeof(struct dma_desc),
-					  tx_q->dma_tx, tx_q->dma_tx_phy);
-		else
+		if (priv->extend_desc)
 			dma_free_coherent(priv->device, DMA_TX_SIZE *
 					  sizeof(struct dma_extended_desc),
 					  tx_q->dma_etx, tx_q->dma_tx_phy);
+		else if (priv->enhanced_tx_desc)
+			dma_free_coherent(priv->device, DMA_TX_SIZE *
+					  sizeof(struct dma_enhanced_tx_desc),
+					  tx_q->dma_enhtx, tx_q->dma_tx_phy);
+		else
+			dma_free_coherent(priv->device, DMA_TX_SIZE *
+					  sizeof(struct dma_desc),
+					  tx_q->dma_tx, tx_q->dma_tx_phy);
 
 		kfree(tx_q->tx_skbuff_dma);
 		kfree(tx_q->tx_skbuff);
@@ -1664,6 +1682,15 @@ static int alloc_dma_tx_desc_resources(struct stmmac_priv *priv)
 							   GFP_KERNEL);
 			if (!tx_q->dma_etx)
 				goto err_dma;
+		} else if (priv->enhanced_tx_desc) {
+			tx_q->dma_enhtx = dma_alloc_coherent(priv->device,
+							     DMA_TX_SIZE *
+							     sizeof(struct
+							     dma_enhanced_tx_desc),
+							     &tx_q->dma_tx_phy,
+							     GFP_KERNEL);
+			if (!tx_q->dma_enhtx)
+				goto err_dma;
 		} else {
 			tx_q->dma_tx = dma_alloc_coherent(priv->device,
 							  DMA_TX_SIZE * sizeof(struct dma_desc),
@@ -1910,6 +1937,8 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 
 		if (priv->extend_desc)
 			p = (struct dma_desc *)(tx_q->dma_etx + entry);
+		else if (priv->enhanced_tx_desc)
+			p = &(tx_q->dma_enhtx + entry)->basic;
 		else
 			p = tx_q->dma_tx + entry;
 
@@ -2017,7 +2046,12 @@ static void stmmac_tx_err(struct stmmac_priv *priv, u32 chan)
 	for (i = 0; i < DMA_TX_SIZE; i++)
 		if (priv->extend_desc)
 			stmmac_init_tx_desc(priv, &tx_q->dma_etx[i].basic,
-					priv->mode, (i == DMA_TX_SIZE - 1));
+					    priv->mode,
+					    (i == DMA_TX_SIZE - 1));
+		else if (priv->enhanced_tx_desc)
+			stmmac_init_tx_desc(priv, &tx_q->dma_enhtx[i].basic,
+					    priv->mode,
+					    (i == DMA_TX_SIZE - 1));
 		else
 			stmmac_init_tx_desc(priv, &tx_q->dma_tx[i],
 					priv->mode, (i == DMA_TX_SIZE - 1));
@@ -3107,7 +3141,11 @@ static bool stmmac_vlan_insert(struct stmmac_priv *priv, struct sk_buff *skb,
 
 	tag = skb_vlan_tag_get(skb);
 
-	p = tx_q->dma_tx + tx_q->cur_tx;
+	if (priv->enhanced_tx_desc)
+		p = &tx_q->dma_enhtx[tx_q->cur_tx].basic;
+	else
+		p = tx_q->dma_tx + tx_q->cur_tx;
+
 	if (stmmac_set_desc_vlan_tag(priv, p, tag, inner_tag, inner_type))
 		return false;
 
@@ -3142,7 +3180,11 @@ static void stmmac_tso_allocator(struct stmmac_priv *priv, dma_addr_t des,
 
 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
 		WARN_ON(tx_q->tx_skbuff[tx_q->cur_tx]);
-		desc = tx_q->dma_tx + tx_q->cur_tx;
+		/* TSO is not available in DWMAC v3.5  */
+		if (priv->enhanced_tx_desc)
+			desc = &(tx_q->dma_enhtx + tx_q->cur_tx)->basic;
+		else
+			desc = tx_q->dma_tx + tx_q->cur_tx;
 
 		curr_addr = des + (total_len - tmp_len);
 		if (priv->dma_cap.addr64 <= 32)
@@ -3236,7 +3278,11 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	/* set new MSS value if needed */
 	if (mss != tx_q->mss) {
-		mss_desc = tx_q->dma_tx + tx_q->cur_tx;
+		/* TSO is not available in DWMAC v3.5  */
+		if (priv->enhanced_tx_desc)
+			mss_desc = &(tx_q->dma_enhtx + tx_q->cur_tx)->basic;
+		else
+			mss_desc = tx_q->dma_tx + tx_q->cur_tx;
 		stmmac_set_mss(priv, mss_desc, mss);
 		tx_q->mss = mss;
 		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx, DMA_TX_SIZE);
@@ -3255,8 +3301,12 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	first_entry = tx_q->cur_tx;
 	WARN_ON(tx_q->tx_skbuff[first_entry]);
+	/* TSO is not available in DWMAC v3.5  */
+	if (priv->enhanced_tx_desc)
+		desc = &(tx_q->dma_enhtx + first_entry)->basic;
+	else
+		desc = tx_q->dma_tx + first_entry;
 
-	desc = tx_q->dma_tx + first_entry;
 	first = desc;
 
 	if (has_vlan)
@@ -3394,13 +3444,29 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		stmmac_display_ring(priv, (void *)tx_q->dma_tx, DMA_TX_SIZE, 0);
 
+		/* TSO is not available in DWMAC v3.5  */
+		if (priv->enhanced_tx_desc)
+			stmmac_display_ring(priv, (void *)tx_q->dma_enhtx,
+					    DMA_TX_SIZE, 0);
+		else
+			stmmac_display_ring(priv, (void *)tx_q->dma_tx,
+					    DMA_TX_SIZE, 0);
 		pr_info(">>> frame to be transmitted: ");
 		print_pkt(skb->data, skb_headlen(skb));
 	}
 
 	netdev_tx_sent_queue(netdev_get_tx_queue(dev, queue), skb->len);
 
-	tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * sizeof(*desc));
+	/* TSO is not available in DWMAC v3.5  */
+	if (priv->enhanced_tx_desc)
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy +
+					(tx_q->cur_tx *
+					sizeof(struct dma_enhanced_tx_desc));
+	else
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy +
+					(tx_q->cur_tx *
+					sizeof(struct dma_desc));
+
 	stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);
 	stmmac_tx_timer_arm(priv, queue);
 
@@ -3473,6 +3539,8 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	if (likely(priv->extend_desc))
 		desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+	else if (priv->enhanced_tx_desc)
+		desc = &tx_q->dma_enhtx[entry].basic;
 	else
 		desc = tx_q->dma_tx + entry;
 
@@ -3502,6 +3570,8 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		if (likely(priv->extend_desc))
 			desc = (struct dma_desc *)(tx_q->dma_etx + entry);
+		else if (priv->enhanced_tx_desc)
+			desc = &tx_q->dma_enhtx[entry].basic;
 		else
 			desc = tx_q->dma_tx + entry;
 
@@ -3548,6 +3618,8 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 	if (set_ic) {
 		if (likely(priv->extend_desc))
 			desc = &tx_q->dma_etx[entry].basic;
+		else if (priv->enhanced_tx_desc)
+			desc = &tx_q->dma_enhtx[entry].basic;
 		else
 			desc = &tx_q->dma_tx[entry];
 
@@ -3574,6 +3646,8 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 		if (priv->extend_desc)
 			tx_head = (void *)tx_q->dma_etx;
+		else if (priv->enhanced_tx_desc)
+			tx_head = (void *)tx_q->dma_enhtx;
 		else
 			tx_head = (void *)tx_q->dma_tx;
 
@@ -3640,7 +3714,19 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	stmmac_enable_dma_transmission(priv, priv->ioaddr);
 
-	tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx * sizeof(*desc));
+	if (priv->extend_desc)
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy +
+					(tx_q->cur_tx *
+					sizeof(struct dma_extended_desc));
+	else if (priv->enhanced_tx_desc)
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy +
+					(tx_q->cur_tx *
+					sizeof(struct dma_enhanced_tx_desc));
+	else
+		tx_q->tx_tail_addr = tx_q->dma_tx_phy +
+					(tx_q->cur_tx *
+					sizeof(struct dma_desc));
+
 	stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr, queue);
 	stmmac_tx_timer_arm(priv, queue);
 
@@ -4553,11 +4639,20 @@ static void sysfs_display_ring(void *head, int size, int extend_desc,
 			       struct seq_file *seq)
 {
 	int i;
+	struct dma_enhanced_tx_desc *enhp = (struct dma_enhanced_tx_desc *)head;
 	struct dma_extended_desc *ep = (struct dma_extended_desc *)head;
 	struct dma_desc *p = (struct dma_desc *)head;
 
 	for (i = 0; i < size; i++) {
-		if (extend_desc) {
+		if (extend_desc == 2) {
+			seq_printf(seq, "%d [0x%x]: 0x%x 0x%x 0x%x 0x%x\n",
+				   i, (unsigned int)virt_to_phys(enhp),
+				   le32_to_cpu(enhp->basic.des0),
+				   le32_to_cpu(enhp->basic.des1),
+				   le32_to_cpu(enhp->basic.des2),
+				   le32_to_cpu(enhp->basic.des3));
+			enhp++;
+		} else if (extend_desc == 1) {
 			seq_printf(seq, "%d [0x%x]: 0x%x 0x%x 0x%x 0x%x\n",
 				   i, (unsigned int)virt_to_phys(ep),
 				   le32_to_cpu(ep->basic.des0),
@@ -4612,6 +4707,10 @@ static int stmmac_rings_status_show(struct seq_file *seq, void *v)
 			seq_printf(seq, "Extended descriptor ring:\n");
 			sysfs_display_ring((void *)tx_q->dma_etx,
 					   DMA_TX_SIZE, 1, seq);
+		} else if (priv->enhanced_tx_desc) {
+			seq_printf(seq, "Enhanced descriptor ring:\n");
+			sysfs_display_ring((void *)tx_q->dma_enhtx,
+					   DMA_TX_SIZE, 2, seq);
 		} else {
 			seq_printf(seq, "Descriptor ring:\n");
 			sysfs_display_ring((void *)tx_q->dma_tx,
-- 
2.17.1

