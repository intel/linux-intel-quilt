From 1843e77535dacf5c986f5fcd0e1b2bfc2f5c7f0d Mon Sep 17 00:00:00 2001
From: Ira Weiny <ira.weiny@intel.com>
Date: Thu, 9 Jul 2020 23:18:12 -0700
Subject: [PATCH 86/96] x86/pks: Preserve the PKRS MSR on context switch

The PKRS MSR is defined as a per-logical-processor register.  This
isolates memory access by CPU.  Unfortunately, the MSR is not managed
by XSAVE.  Therefore, We must preserve the protections for individual
tasks even if they are context switched out and placed on another cpu
later.

Define a saved PKRS value in the task struct, as well as a cached
per-logical-processor MSR value which mirrors the MSR value of the
current CPU.  Initialize, all tasks with the default MSR value.  Then,
on schedule in, check the saved task MSR vs the per-cpu value.  If
different proceed to write the MSR.  If not we avoid the overhead of the
MSR write and continue.

Follow on patches will update the saved PKRS as well as the MSR if
needed.

Co-developed-by: Fenghua Yu <fenghua.yu@intel.com>
Signed-off-by: Fenghua Yu <fenghua.yu@intel.com>
Signed-off-by: Ira Weiny <ira.weiny@intel.com>
---
 arch/x86/include/asm/msr-index.h      |  1 +
 arch/x86/include/asm/pkeys_internal.h | 20 ++++++++++++++++++++
 arch/x86/include/asm/processor.h      | 12 ++++++++++++
 arch/x86/kernel/cpu/common.c          |  2 ++
 arch/x86/kernel/process.c             | 21 +++++++++++++++++++++
 arch/x86/mm/pkeys.c                   | 17 +++++++++++++++++
 6 files changed, 73 insertions(+)

Index: kernel-staging/arch/x86/include/asm/msr-index.h
===================================================================
--- kernel-staging.orig/arch/x86/include/asm/msr-index.h
+++ kernel-staging/arch/x86/include/asm/msr-index.h
@@ -755,6 +755,7 @@
 
 #define MSR_IA32_TSC_DEADLINE		0x000006E0
 
+#define MSR_IA32_PKRS			0x000006E1
 
 #define MSR_TSX_FORCE_ABORT		0x0000010F
 
Index: kernel-staging/arch/x86/include/asm/pkeys_internal.h
===================================================================
--- kernel-staging.orig/arch/x86/include/asm/pkeys_internal.h
+++ kernel-staging/arch/x86/include/asm/pkeys_internal.h
@@ -8,4 +8,24 @@
 
 #define PKR_AD_KEY(pkey)	(PKR_AD_BIT << ((pkey) * PKR_BITS_PER_PKEY))
 
+/*
+ * Define a default PKRS value for each task.
+ *
+ * Key 0 has no restriction.  All other keys are set to the most restrictive
+ * value which is access disabled (AD=1).
+ *
+ * NOTE: This needs to be a macro to be used as part of the INIT_THREAD macro.
+ */
+#define INIT_PKRS_VALUE (PKR_AD_KEY(1) | PKR_AD_KEY(2) | PKR_AD_KEY(3) | \
+			 PKR_AD_KEY(4) | PKR_AD_KEY(5) | PKR_AD_KEY(6) | \
+			 PKR_AD_KEY(7) | PKR_AD_KEY(8) | PKR_AD_KEY(9) | \
+			 PKR_AD_KEY(10) | PKR_AD_KEY(11) | PKR_AD_KEY(12) | \
+			 PKR_AD_KEY(13) | PKR_AD_KEY(14) | PKR_AD_KEY(15))
+
+#ifdef CONFIG_ARCH_HAS_SUPERVISOR_PKEYS
+void write_pkrs(u32 new_pkrs);
+#else
+static inline void write_pkrs(u32 new_pkrs) { }
+#endif
+
 #endif /*_ASM_X86_PKEYS_INTERNAL_H */
Index: kernel-staging/arch/x86/include/asm/processor.h
===================================================================
--- kernel-staging.orig/arch/x86/include/asm/processor.h
+++ kernel-staging/arch/x86/include/asm/processor.h
@@ -548,6 +548,11 @@ struct thread_struct {
 	struct cet_status	cet;
 #endif
 
+#ifdef	CONFIG_ARCH_HAS_SUPERVISOR_PKEYS
+	/* Saved Protection key register for supervisor mappings */
+	u32			saved_pkrs;
+#endif
+
 	/* Floating point and extended processor state */
 	struct fpu		fpu;
 	/*
@@ -856,8 +861,15 @@ static inline void spin_lock_prefetch(co
  */
 #define ARCH_SHADOW_STACK_GUARD_GAP PAGE_SIZE
 
+#ifdef CONFIG_ARCH_HAS_SUPERVISOR_PKEYS
+#define INIT_THREAD_PKRS	.saved_pkrs = INIT_PKRS_VALUE,
+#else
+#define INIT_THREAD_PKRS
+#endif
+
 #define INIT_THREAD  {						\
 	.addr_limit		= KERNEL_DS,			\
+	INIT_THREAD_PKRS					\
 }
 
 extern unsigned long KSTK_ESP(struct task_struct *task);
Index: kernel-staging/arch/x86/kernel/cpu/common.c
===================================================================
--- kernel-staging.orig/arch/x86/kernel/cpu/common.c
+++ kernel-staging/arch/x86/kernel/cpu/common.c
@@ -58,6 +58,7 @@
 #include <asm/cpu_device_id.h>
 #include <asm/cet.h>
 #include <asm/uv/uv.h>
+#include <asm/pkeys_internal.h>
 
 #include "cpu.h"
 
@@ -1506,6 +1507,7 @@ static void setup_pks(void)
 	if (!cpu_feature_enabled(X86_FEATURE_PKS))
 		return;
 
+	write_pkrs(INIT_PKRS_VALUE);
 	cr4_set_bits(X86_CR4_PKS);
 }
 
Index: kernel-staging/arch/x86/kernel/process.c
===================================================================
--- kernel-staging.orig/arch/x86/kernel/process.c
+++ kernel-staging/arch/x86/kernel/process.c
@@ -44,6 +44,7 @@
 #include <asm/proto.h>
 #include <asm/frame.h>
 #include <asm/cet.h>
+#include <asm/pkeys_internal.h>
 
 #include "process.h"
 
@@ -195,6 +196,22 @@ int copy_thread(unsigned long clone_flag
 	return ret;
 }
 
+#ifdef CONFIG_ARCH_HAS_SUPERVISOR_PKEYS
+DECLARE_PER_CPU(u32, pkrs_cache);
+static inline void pks_init_task(struct task_struct *tsk)
+{
+	/* New tasks get the most restrictive PKRS value */
+	tsk->thread.saved_pkrs = INIT_PKRS_VALUE;
+}
+static inline void pks_sched_in(void)
+{
+	write_pkrs(current->thread.saved_pkrs);
+}
+#else
+static inline void pks_init_task(struct task_struct *tsk) { }
+static inline void pks_sched_in(void) { }
+#endif
+
 void flush_thread(void)
 {
 	struct task_struct *tsk = current;
@@ -203,6 +220,8 @@ void flush_thread(void)
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));
 
 	fpu__clear_all(&tsk->thread.fpu);
+
+	pks_init_task(tsk);
 }
 
 void disable_TSC(void)
@@ -652,6 +671,8 @@ void __switch_to_xtra(struct task_struct
 
 	if ((tifp ^ tifn) & _TIF_SLD)
 		switch_to_sld(tifn);
+
+	pks_sched_in();
 }
 
 /*
Index: kernel-staging/arch/x86/mm/pkeys.c
===================================================================
--- kernel-staging.orig/arch/x86/mm/pkeys.c
+++ kernel-staging/arch/x86/mm/pkeys.c
@@ -229,3 +229,20 @@ u32 update_pkey_val(u32 pk_reg, int pkey
 
 	return pk_reg;
 }
+
+DEFINE_PER_CPU(u32, pkrs_cache);
+
+void write_pkrs(u32 new_pkrs)
+{
+	u32 *pkrs;
+
+	if (!static_cpu_has(X86_FEATURE_PKS))
+		return;
+
+	pkrs = get_cpu_ptr(&pkrs_cache);
+	if (*pkrs != new_pkrs) {
+		*pkrs = new_pkrs;
+		wrmsrl(MSR_IA32_PKRS, new_pkrs);
+	}
+	put_cpu_ptr(pkrs);
+}
