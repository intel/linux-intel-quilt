From a9d3931257c927df409a753fabaad47d34915467 Mon Sep 17 00:00:00 2001
From: Tomasz Lis <tomasz.lis@intel.com>
Date: Mon, 20 May 2024 22:18:47 +0200
Subject: [PATCH 2/2] drm/i915: SR-IOV Save-Restore Feature support

SR-IOV Save-Restore Feature support. Ported from dii client
(6.6) and dii server(5.15)

Signed-off-by: Kalyan Alle <kalyan.alle@intel.com>
Signed-off-by: Tomasz Lis <tomasz.lis@intel.com>
Signed-off-by: Kooran Paul, Princy <princy.kooran.paul@intel.com>
---
 drivers/gpu/drm/i915/Makefile                 |   1 +
 drivers/gpu/drm/i915/gt/gen8_engine_cs.c      | 206 +++++-
 drivers/gpu/drm/i915/gt/gen8_engine_cs.h      |   7 +
 drivers/gpu/drm/i915/gt/intel_context_sseu.c  |   5 +-
 drivers/gpu/drm/i915/gt/intel_engine_cs.c     |   3 +
 .../gpu/drm/i915/gt/intel_engine_heartbeat.c  |  39 ++
 .../gpu/drm/i915/gt/intel_engine_heartbeat.h  |   5 +
 drivers/gpu/drm/i915/gt/intel_engine_types.h  |   1 +
 .../drm/i915/gt/intel_execlists_submission.c  |   1 -
 drivers/gpu/drm/i915/gt/intel_ggtt.c          | 187 +++++-
 drivers/gpu/drm/i915/gt/intel_gt.c            |   6 +
 drivers/gpu/drm/i915/gt/intel_gt.h            |   1 +
 drivers/gpu/drm/i915/gt/intel_gt_irq.c        |  36 ++
 drivers/gpu/drm/i915/gt/intel_gt_types.h      |  16 +
 drivers/gpu/drm/i915/gt/intel_gtt.h           |  24 +
 drivers/gpu/drm/i915/gt/intel_lrc.c           |  42 +-
 drivers/gpu/drm/i915/gt/intel_lrc.h           |   1 +
 drivers/gpu/drm/i915/gt/intel_reset.c         |  33 +-
 drivers/gpu/drm/i915/gt/intel_reset.h         |   4 +
 drivers/gpu/drm/i915/gt/intel_ring.c          |  97 ++-
 drivers/gpu/drm/i915/gt/intel_ring.h          |  27 +-
 drivers/gpu/drm/i915/gt/intel_sa_media.c      |   5 +
 drivers/gpu/drm/i915/gt/intel_timeline.c      |  41 +-
 drivers/gpu/drm/i915/gt/intel_timeline.h      |   7 +
 drivers/gpu/drm/i915/gt/intel_workarounds.c   |   5 +-
 .../gpu/drm/i915/gt/iov/intel_iov_debugfs.c   |  29 +
 .../gpu/drm/i915/gt/iov/intel_iov_memirq.c    |  17 +-
 .../gpu/drm/i915/gt/iov/intel_iov_migration.c | 181 ++++++
 .../gpu/drm/i915/gt/iov/intel_iov_migration.h |  14 +
 .../drm/i915/gt/iov/intel_iov_provisioning.c  | 153 +++++
 .../drm/i915/gt/iov/intel_iov_provisioning.h  |   6 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_query.c | 122 +++-
 drivers/gpu/drm/i915/gt/iov/intel_iov_query.h |   2 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c |   7 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.c | 101 ++-
 drivers/gpu/drm/i915/gt/iov/intel_iov_state.h |   3 +
 drivers/gpu/drm/i915/gt/iov/intel_iov_types.h |   3 +
 drivers/gpu/drm/i915/gt/selftest_execlists.c  |   1 +
 drivers/gpu/drm/i915/gt/selftest_lrc.c        |   6 +-
 .../gpu/drm/i915/gt/uc/abi/guc_actions_abi.h  |   1 +
 .../drm/i915/gt/uc/abi/guc_actions_vf_abi.h   |  38 ++
 .../gpu/drm/i915/gt/uc/abi/guc_errors_abi.h   |   1 +
 drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h |  11 +
 drivers/gpu/drm/i915/gt/uc/intel_guc.c        |   8 +
 drivers/gpu/drm/i915/gt/uc/intel_guc.h        |  11 +
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c     | 137 ++++
 drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h     |   2 +
 .../gpu/drm/i915/gt/uc/intel_guc_submission.c | 237 +++++--
 .../gpu/drm/i915/gt/uc/intel_guc_submission.h |   3 +
 drivers/gpu/drm/i915/gt/uc/intel_uc.c         |   6 +-
 drivers/gpu/drm/i915/i915_drv.h               |   6 +-
 drivers/gpu/drm/i915/i915_params.c            |   7 +
 drivers/gpu/drm/i915/i915_params.h            |   1 +
 drivers/gpu/drm/i915/i915_pci.c               |   5 +
 drivers/gpu/drm/i915/i915_perf.c              |   3 +
 drivers/gpu/drm/i915/i915_request.c           |  24 +
 drivers/gpu/drm/i915/i915_request.h           |  26 +
 drivers/gpu/drm/i915/i915_sriov.c             | 608 +++++++++++++++++-
 drivers/gpu/drm/i915/i915_sriov.h             |   5 +
 drivers/gpu/drm/i915/i915_sriov_types.h       |  16 +-
 drivers/gpu/drm/i915/intel_device_info.h      |   4 +-
 drivers/gpu/drm/i915/intel_pcode.c            |   2 +-
 drivers/gpu/drm/i915/selftests/mock_gtt.c     |   3 +
 drivers/vfio/pci/i915/i915_vfio_pci.h         |   5 +-
 drivers/vfio/pci/i915/main.c                  |   3 +-
 drivers/vfio/pci/i915/test/data_test.c        |   2 +-
 include/drm/drm_mm.h                          |  19 +
 include/drm/i915_sriov.h                      |  11 +-
 68 files changed, 2494 insertions(+), 156 deletions(-)
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_migration.c
 create mode 100644 drivers/gpu/drm/i915/gt/iov/intel_iov_migration.h

diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 31e85ff15de0..898b14ab6b22 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -218,6 +218,7 @@ iov-y += \
        gt/iov/intel_iov.o \
        gt/iov/intel_iov_debugfs.o \
        gt/iov/intel_iov_event.o \
+       gt/iov/intel_iov_migration.o \
        gt/iov/intel_iov_ggtt.o \
        gt/iov/intel_iov_memirq.o \
        gt/iov/intel_iov_provisioning.o \
diff --git a/drivers/gpu/drm/i915/gt/gen8_engine_cs.c b/drivers/gpu/drm/i915/gt/gen8_engine_cs.c
index e9f65f27b53f..4d4dd0ebdb54 100644
--- a/drivers/gpu/drm/i915/gt/gen8_engine_cs.c
+++ b/drivers/gpu/drm/i915/gt/gen8_engine_cs.c
@@ -10,6 +10,8 @@
 #include "intel_lrc.h"
 #include "intel_ring.h"
 
+#define GEN8_EMIT_INIT_BREADCRUMB_NUM_DWORDS 6
+
 int gen8_emit_flush_rcs(struct i915_request *rq, u32 mode)
 {
 	bool vf_flush_wa = false, dc_flush_wa = false;
@@ -439,7 +441,7 @@ int gen8_emit_init_breadcrumb(struct i915_request *rq)
 	if (!i915_request_timeline(rq)->has_initial_breadcrumb)
 		return 0;
 
-	cs = intel_ring_begin(rq, 6);
+	cs = intel_ring_begin(rq, GEN8_EMIT_INIT_BREADCRUMB_NUM_DWORDS);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
@@ -478,21 +480,80 @@ int gen8_emit_init_breadcrumb(struct i915_request *rq)
 	return 0;
 }
 
+/**
+ * Retrieve the ring position initial breadcrumb for given request.
+ * @rq: the request instance
+ *
+ * Return: Ring position of commands emited by emit_init_breadcrumb().
+ */
+u32 get_init_breadcrumb_pos(struct i915_request *rq)
+{
+	if (!rq->engine->emit_init_breadcrumb)
+		return rq->infix;
+
+	GEM_BUG_ON(rq->engine->emit_init_breadcrumb != gen8_emit_init_breadcrumb);
+
+	if (!intel_timeline_has_initial_breadcrumb(rcu_access_pointer(rq->timeline)))
+		return rq->infix;
+
+	if (__intel_ring_count(rq->head, rq->infix, rq->ring->size) >
+			GEN8_EMIT_INIT_BREADCRUMB_NUM_DWORDS * sizeof(u32))
+		return intel_ring_wrap(rq->ring, rq->infix -
+				       GEN8_EMIT_INIT_BREADCRUMB_NUM_DWORDS * sizeof(u32));
+
+	return rq->head;
+}
+
+static int xehp_get_params_for_emit_bb_start(struct i915_request *rq,
+				u64 *offset, u32 *len, u32 *flags)
+{
+	struct intel_ring *ring = rq->ring;
+	u32 *cs;
+
+	cs = ring->vaddr + rq->infix;
+	*flags = 0;
+	*len = 0;
+
+	if (__intel_ring_count(rq->infix, rq->advance, ring->size) < 9 * sizeof(u32))
+		return -ENOMSG;
+
+	if ((*cs & MI_INSTR(0x3F, 0)) != MI_ARB_ON_OFF)
+		return -EILSEQ;
+	cs++;
+
+	if (*cs != (MI_LOAD_REGISTER_MEM_GEN8  |
+		   MI_SRM_LRM_GLOBAL_GTT |
+		   MI_LRI_LRM_CS_MMIO))
+		return -EILSEQ;
+	cs += 4;
+
+	if ((*cs & MI_INSTR(0x3F, 3)) != MI_BATCH_BUFFER_START_GEN8)
+		return -EILSEQ;
+	*flags |= (*cs++ & BIT(8) ? 0 : I915_DISPATCH_SECURE);
+	*offset = *cs++;
+	*offset |= ((u64)*cs++) << 32;
+
+	return 0;
+}
+
 static int __xehp_emit_bb_start(struct i915_request *rq,
 				u64 offset, u32 len,
 				const unsigned int flags,
 				u32 arb)
 {
 	struct intel_context *ce = rq->context;
-	u32 wa_offset = lrc_indirect_bb(ce);
+	u32 wa_offset;
+	int srcu;
 	u32 *cs;
 
 	GEM_BUG_ON(!ce->wa_bb_page);
 
-	cs = intel_ring_begin(rq, 12);
+	cs = intel_ring_begin_ggtt(rq, &srcu, 12);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
+	wa_offset = lrc_indirect_bb(ce);
+
 	*cs++ = MI_ARB_ON_OFF | arb;
 
 	*cs++ = MI_LOAD_REGISTER_MEM_GEN8 |
@@ -514,7 +575,7 @@ static int __xehp_emit_bb_start(struct i915_request *rq,
 
 	*cs++ = MI_ARB_ON_OFF | MI_ARB_DISABLE;
 
-	intel_ring_advance(rq, cs);
+	intel_ring_advance_ggtt(rq, srcu, cs);
 
 	return 0;
 }
@@ -539,6 +600,8 @@ int gen8_emit_bb_start_noarb(struct i915_request *rq,
 {
 	u32 *cs;
 
+	GEM_BUG_ON(IS_SRIOV_VF(rq->i915) && (flags & I915_DISPATCH_SECURE));
+
 	cs = intel_ring_begin(rq, 4);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
@@ -569,12 +632,40 @@ int gen8_emit_bb_start_noarb(struct i915_request *rq,
 	return 0;
 }
 
+static int gen8_get_params_for_emit_bb_start(struct i915_request *rq,
+				u64 *offset, u32 *len, u32 *flags)
+{
+	struct intel_ring *ring = rq->ring;
+	u32 *cs;
+
+	cs = ring->vaddr + rq->infix;
+	*flags = 0;
+	*len = 0;
+
+	if (rq->postfix - rq->infix < 4 * sizeof(u32))
+		return -ENOMSG;
+
+	if ((*cs & MI_INSTR(0x3F, 3)) != (MI_ARB_ON_OFF | MI_ARB_ENABLE))
+		return -EILSEQ;
+	cs++;
+
+	if ((*cs & MI_INSTR(0x3F, 3)) != MI_BATCH_BUFFER_START_GEN8)
+		return -EILSEQ;
+	*flags |= (*cs++ & BIT(8) ? 0 : I915_DISPATCH_SECURE);
+	*offset = *cs++;
+	*offset |= ((u64)*cs++) << 32;
+
+	return 0;
+}
+
 int gen8_emit_bb_start(struct i915_request *rq,
 		       u64 offset, u32 len,
 		       const unsigned int flags)
 {
 	u32 *cs;
 
+	GEM_BUG_ON(IS_SRIOV_VF(rq->i915) && (flags & I915_DISPATCH_SECURE));
+
 	if (unlikely(i915_request_has_nopreempt(rq)))
 		return gen8_emit_bb_start_noarb(rq, offset, len, flags);
 
@@ -605,6 +696,103 @@ static void assert_request_valid(struct i915_request *rq)
 	GEM_BUG_ON(intel_ring_direction(ring, rq->wa_tail, rq->head) <= 0);
 }
 
+/**
+ * Fill an area of a ring with NOOP instructions.
+ * @ring: the ring struct instance
+ * @start: start position on the ring, qword-aligned
+ * @end: end position on the ring, qword-aligned (the content at this position
+ *    will not get NOOPed)
+ */
+void ring_range_emit_noop(struct intel_ring *ring, u32 start, u32 end)
+{
+	int i, num_dwords;
+	u32 *cs;
+	void *vaddr = ring->vaddr;
+
+	cs = vaddr + start;
+	num_dwords = __intel_ring_count(start, end, ring->size) / sizeof(u32);
+	GEM_BUG_ON(num_dwords & 1);
+	GEM_BUG_ON(num_dwords < 0);
+
+	for (i = num_dwords/2 + 1; i > 0; i--) {
+		*cs++ = MI_NOOP;
+		*cs++ = MI_NOOP;
+		cs = vaddr + intel_ring_wrap(ring, ptrdiff(cs, vaddr));
+	}
+}
+
+/**
+ * Refreshes commands on the ring associated to init_breadcrumb command packet.
+ * @rq: the request instance
+ */
+int reemit_init_breadcrumb(struct i915_request *rq)
+{
+	u32 advance;
+	u32 dwlen;
+
+	if (!test_bit(I915_FENCE_FLAG_INITIAL_BREADCRUMB, &rq->fence.flags))
+		return -ENOMSG;
+
+	if (rq->engine->emit_init_breadcrumb == gen8_emit_init_breadcrumb)
+		dwlen = GEN8_EMIT_INIT_BREADCRUMB_NUM_DWORDS;
+	else
+		dwlen = 0;
+
+	/* ring emit position is expected to be already correctly set for reemit */
+	if (__intel_ring_count(rq->ring->emit, rq->advance, rq->ring->size) < dwlen * sizeof(u32))
+		return -EPROTO;
+
+	advance = rq->advance;
+
+	if (rq->engine->emit_init_breadcrumb) {
+		__clear_bit(I915_FENCE_FLAG_INITIAL_BREADCRUMB, &rq->fence.flags);
+		rq->engine->emit_init_breadcrumb(rq);
+	}
+
+	rq->advance = advance;
+
+	return 0;
+}
+
+static int get_params_for_emit_bb_start(struct i915_request *rq,
+				u64 *offset, u32 *len, u32 *flags)
+{
+	if (rq->engine->emit_bb_start == xehp_emit_bb_start)
+		return xehp_get_params_for_emit_bb_start(rq, offset, len, flags);
+	if (rq->engine->emit_bb_start == gen8_emit_bb_start)
+		return gen8_get_params_for_emit_bb_start(rq, offset, len, flags);
+
+	return -EOPNOTSUPP;
+}
+
+/**
+ * Refreshes commands on the ring associated to bb_start command packet.
+ * @rq: the request instance
+ *
+ * If the old ring content was lost, it is not possible to refresh it as
+ * some parameters of the request were stored only there. But if the ring
+ * data is ok and we just want to re-emit after update to GGTT addresses,
+ * this should do it.
+ */
+int reemit_bb_start(struct i915_request *rq)
+{
+	u64 offset;
+	u32 emlen, emflags, advance;
+	int err;
+
+	advance = rq->advance;
+
+	err = get_params_for_emit_bb_start(rq, &offset, &emlen, &emflags);
+	if (err)
+		return err;
+
+	err = rq->engine->emit_bb_start(rq, offset, emlen, emflags);
+
+	rq->advance = advance;
+
+	return err;
+}
+
 /*
  * Reserve space for 2 NOOPs at the end of each request to be
  * used as a workaround for not being allowed to do lite
@@ -726,6 +914,8 @@ u32 *gen11_emit_fini_breadcrumb_rcs(struct i915_request *rq, u32 *cs)
 
 static u32 *gen12_emit_preempt_busywait(struct i915_request *rq, u32 *cs)
 {
+	GEM_BUG_ON(IS_SRIOV_VF(rq->i915));
+
 	*cs++ = MI_ARB_CHECK; /* trigger IDLE->ACTIVE first */
 	*cs++ = MI_SEMAPHORE_WAIT_TOKEN |
 		MI_SEMAPHORE_GLOBAL_GTT |
@@ -754,8 +944,9 @@ static u32 hold_switchout_semaphore_offset(struct i915_request *rq)
 /* Wa_14019159160 */
 static u32 *hold_switchout_emit_wa_busywait(struct i915_request *rq, u32 *cs)
 {
-	int i;
+	int srcu;
 
+	intel_ring_fini_begin_ggtt(rq, &srcu);
 	*cs++ = MI_ATOMIC_INLINE | MI_ATOMIC_GLOBAL_GTT | MI_ATOMIC_CS_STALL |
 		MI_ATOMIC_MOVE;
 	*cs++ = hold_switchout_semaphore_offset(rq);
@@ -766,8 +957,8 @@ static u32 *hold_switchout_emit_wa_busywait(struct i915_request *rq, u32 *cs)
 	 * When MI_ATOMIC_INLINE_DATA set this command must be 11 DW + (1 NOP)
 	 * to align. 4 DWs above + 8 filler DWs here.
 	 */
-	for (i = 0; i < 8; ++i)
-		*cs++ = 0;
+	memset32(cs, 0, 8);
+	cs += 8;
 
 	*cs++ = MI_SEMAPHORE_WAIT |
 		MI_SEMAPHORE_GLOBAL_GTT |
@@ -776,6 +967,7 @@ static u32 *hold_switchout_emit_wa_busywait(struct i915_request *rq, u32 *cs)
 	*cs++ = 0;
 	*cs++ = hold_switchout_semaphore_offset(rq);
 	*cs++ = 0;
+	intel_ring_fini_advance_ggtt(rq, srcu, cs);
 
 	return cs;
 }
diff --git a/drivers/gpu/drm/i915/gt/gen8_engine_cs.h b/drivers/gpu/drm/i915/gt/gen8_engine_cs.h
index 867ba697aceb..8319d57c95c8 100644
--- a/drivers/gpu/drm/i915/gt/gen8_engine_cs.h
+++ b/drivers/gpu/drm/i915/gt/gen8_engine_cs.h
@@ -15,6 +15,7 @@
 
 struct intel_engine_cs;
 struct intel_gt;
+struct intel_ring;
 struct i915_request;
 
 int gen8_emit_flush_rcs(struct i915_request *rq, u32 mode);
@@ -24,7 +25,11 @@ int gen12_emit_flush_rcs(struct i915_request *rq, u32 mode);
 int gen8_emit_flush_xcs(struct i915_request *rq, u32 mode);
 int gen12_emit_flush_xcs(struct i915_request *rq, u32 mode);
 
+void ring_range_emit_noop(struct intel_ring *ring, u32 start, u32 end);
+
 int gen8_emit_init_breadcrumb(struct i915_request *rq);
+u32 get_init_breadcrumb_pos(struct i915_request *rq);
+int reemit_init_breadcrumb(struct i915_request *rq);
 
 int gen8_emit_bb_start_noarb(struct i915_request *rq,
 			     u64 offset, u32 len,
@@ -40,6 +45,8 @@ int xehp_emit_bb_start(struct i915_request *rq,
 		       u64 offset, u32 len,
 		       const unsigned int flags);
 
+int reemit_bb_start(struct i915_request *rq);
+
 u32 *gen8_emit_fini_breadcrumb_xcs(struct i915_request *rq, u32 *cs);
 u32 *gen12_emit_fini_breadcrumb_xcs(struct i915_request *rq, u32 *cs);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_context_sseu.c b/drivers/gpu/drm/i915/gt/intel_context_sseu.c
index ece16c2b5b8e..3fda68899935 100644
--- a/drivers/gpu/drm/i915/gt/intel_context_sseu.c
+++ b/drivers/gpu/drm/i915/gt/intel_context_sseu.c
@@ -18,9 +18,10 @@ static int gen8_emit_rpcs_config(struct i915_request *rq,
 				 const struct intel_sseu sseu)
 {
 	u64 offset;
+	int srcu;
 	u32 *cs;
 
-	cs = intel_ring_begin(rq, 4);
+	cs = intel_ring_begin_ggtt(rq, &srcu, 4);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
@@ -32,7 +33,7 @@ static int gen8_emit_rpcs_config(struct i915_request *rq,
 	*cs++ = upper_32_bits(offset);
 	*cs++ = intel_sseu_make_rpcs(rq->engine->gt, &sseu);
 
-	intel_ring_advance(rq, cs);
+	intel_ring_advance_ggtt(rq, srcu, cs);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_cs.c b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
index 6aaaa976bc80..5104e1dc48f9 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_cs.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_cs.c
@@ -537,6 +537,9 @@ static int intel_engine_setup(struct intel_gt *gt, enum intel_engine_id id,
 	if (engine->class == RENDER_CLASS || engine->class == COMPUTE_CLASS) {
 		engine->flags |= I915_ENGINE_HAS_RCS_REG_STATE;
 		engine->flags |= I915_ENGINE_HAS_EU_PRIORITY;
+		/* EU attention is not available on VFs */
+		if (!IS_SRIOV_VF(gt->i915))
+			engine->flags |= I915_ENGINE_HAS_EU_ATTENTION;
 	}
 
 	engine->props.heartbeat_interval_ms =
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.c b/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.c
index 8d4bb95f8424..abb3b05f7378 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.c
+++ b/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.c
@@ -272,6 +272,45 @@ void intel_engine_init_heartbeat(struct intel_engine_cs *engine)
 	INIT_DELAYED_WORK(&engine->heartbeat.work, heartbeat);
 }
 
+static void intel_gt_pm_get_all_engines(struct intel_gt *gt)
+{
+	struct intel_engine_cs *engine;
+	unsigned int eid;
+
+	for_each_engine(engine, gt, eid) {
+		intel_engine_pm_get(engine);
+	}
+}
+
+static void intel_gt_pm_put_all_engines(struct intel_gt *gt)
+{
+	struct intel_engine_cs *engine;
+	unsigned int eid;
+
+	for_each_engine(engine, gt, eid) {
+		intel_engine_pm_put(engine);
+	}
+}
+
+void intel_gt_heartbeats_disable(struct intel_gt *gt)
+{
+	/*
+	 * Heartbeat re-enables automatically when an engine is being unparked.
+	 * So to disable the heartbeat and make sure it stays disabled, we
+	 * need to bump PM wakeref for every engine, so that unpark will
+	 * not be called due to changes in PM states.
+	*/
+	intel_gt_pm_get_all_engines(gt);
+	intel_gt_park_heartbeats(gt);
+}
+
+void intel_gt_heartbeats_restore(struct intel_gt *gt, bool unpark)
+{
+	intel_gt_pm_put_all_engines(gt);
+	if (unpark)
+		intel_gt_unpark_heartbeats(gt);
+}
+
 static int __intel_engine_pulse(struct intel_engine_cs *engine)
 {
 	struct i915_sched_attr attr = { .priority = I915_PRIORITY_BARRIER };
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.h b/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.h
index 5da6d809a87a..8ef457ddf253 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_heartbeat.h
@@ -6,6 +6,8 @@
 #ifndef INTEL_ENGINE_HEARTBEAT_H
 #define INTEL_ENGINE_HEARTBEAT_H
 
+#include <linux/types.h>
+
 struct intel_engine_cs;
 struct intel_gt;
 
@@ -20,6 +22,9 @@ void intel_engine_unpark_heartbeat(struct intel_engine_cs *engine);
 void intel_gt_park_heartbeats(struct intel_gt *gt);
 void intel_gt_unpark_heartbeats(struct intel_gt *gt);
 
+void intel_gt_heartbeats_disable(struct intel_gt *gt);
+void intel_gt_heartbeats_restore(struct intel_gt *gt, bool unpark);
+
 int intel_engine_pulse(struct intel_engine_cs *engine);
 int intel_engine_flush_barriers(struct intel_engine_cs *engine);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_engine_types.h b/drivers/gpu/drm/i915/gt/intel_engine_types.h
index 1ca1cd0a21ba..fff739e81ab2 100644
--- a/drivers/gpu/drm/i915/gt/intel_engine_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_engine_types.h
@@ -594,6 +594,7 @@ struct intel_engine_cs {
 #define I915_ENGINE_HAS_EU_PRIORITY    BIT(10)
 #define I915_ENGINE_FIRST_RENDER_COMPUTE BIT(11)
 #define I915_ENGINE_USES_WA_HOLD_SWITCHOUT BIT(12)
+#define I915_ENGINE_HAS_EU_ATTENTION   BIT(14)
 	unsigned int flags;
 
 	/*
diff --git a/drivers/gpu/drm/i915/gt/intel_execlists_submission.c b/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
index f8086634729c..0b7bda489972 100644
--- a/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
+++ b/drivers/gpu/drm/i915/gt/intel_execlists_submission.c
@@ -2783,7 +2783,6 @@ static int emit_pdps(struct i915_request *rq)
 		*cs++ = lower_32_bits(pd_daddr);
 	}
 	*cs++ = MI_ARB_ON_OFF | MI_ARB_ENABLE;
-	intel_ring_advance(rq, cs);
 
 	intel_ring_advance(rq, cs);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index 64d6035d08d4..809950f2bf23 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -822,6 +822,25 @@ static int ggtt_reserve_guc_top(struct i915_ggtt *ggtt)
 	return ret;
 }
 
+/**
+ * i915_ggtt_address_lock_init - initialize the SRCU for GGTT address computation lock
+ * @i915: i915 device instance struct
+ */
+void i915_ggtt_address_lock_init(struct i915_ggtt *ggtt)
+{
+	init_waitqueue_head(&ggtt->queue);
+	init_srcu_struct(&ggtt->blocked_srcu);
+}
+
+/**
+ * i915_ggtt_address_lock_fini - finalize the SRCU for GGTT address computation lock
+ * @i915: i915 device instance struct
+ */
+void i915_ggtt_address_lock_fini(struct i915_ggtt *ggtt)
+{
+	cleanup_srcu_struct(&ggtt->blocked_srcu);
+}
+
 static void ggtt_release_guc_top(struct i915_ggtt *ggtt)
 {
 	if (drm_mm_node_allocated(&ggtt->uc_fw))
@@ -831,11 +850,142 @@ static void ggtt_release_guc_top(struct i915_ggtt *ggtt)
 static void cleanup_init_ggtt(struct i915_ggtt *ggtt)
 {
 	ggtt_release_guc_top(ggtt);
+	i915_ggtt_address_lock_fini(ggtt);
 	if (drm_mm_node_allocated(&ggtt->error_capture))
 		drm_mm_remove_node(&ggtt->error_capture);
 	mutex_destroy(&ggtt->error_mutex);
 }
 
+static void ggtt_address_write_lock(struct i915_ggtt *ggtt)
+{
+	/*
+	 * We are just setting the bit, without the usual checks whether it is
+	 * already set. Such checks are unneccessary if the blocked code is
+	 * running in a worker and the caller function just schedules it.
+	 * But the worker must be aware of re-schedules and know when to skip
+	 * finishing the locking.
+	 */
+	set_bit(GGTT_ADDRESS_COMPUTE_BLOCKED, &ggtt->flags);
+	/*
+	 * After switching our GGTT_ADDRESS_COMPUTE_BLOCKED bit, we should wait for
+	 * all related critical sections to finish. First make sure any read-side
+	 * locking currently in progress either got the lock or noticed the BLOCKED
+	 * flag and is waiting for it to clear. Then wait for all read-side unlocks.
+	 */
+	synchronize_rcu_expedited();
+	synchronize_srcu(&ggtt->blocked_srcu);
+}
+
+static void ggtt_address_write_unlock(struct i915_ggtt *ggtt)
+{
+	clear_bit_unlock(GGTT_ADDRESS_COMPUTE_BLOCKED, &ggtt->flags);
+	smp_mb__after_atomic();
+	wake_up_all(&ggtt->queue);
+}
+
+/**
+ * i915_ggtt_address_write_lock - enter the ggtt address computation fixups section
+ * @i915: i915 device instance struct
+ */
+void i915_ggtt_address_write_lock(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_ggtt(gt, i915, id)
+		ggtt_address_write_lock(gt->ggtt);
+}
+
+static int ggtt_address_read_lock_sync(struct i915_ggtt *ggtt, int *srcu)
+__acquires(&ggtt->blocked_srcu)
+{
+	might_sleep();
+
+	rcu_read_lock();
+	while (test_bit(GGTT_ADDRESS_COMPUTE_BLOCKED, &ggtt->flags)) {
+		rcu_read_unlock();
+
+		if (wait_event_interruptible(ggtt->queue,
+					     !test_bit(GGTT_ADDRESS_COMPUTE_BLOCKED,
+						       &ggtt->flags)))
+			return -EINTR;
+
+		rcu_read_lock();
+	}
+	*srcu = __srcu_read_lock(&ggtt->blocked_srcu);
+	rcu_read_unlock();
+
+	return 0;
+}
+
+static int ggtt_address_read_lock_interruptible(struct i915_ggtt *ggtt, int *srcu)
+__acquires(&ggtt->blocked_srcu)
+{
+	rcu_read_lock();
+	while (test_bit(GGTT_ADDRESS_COMPUTE_BLOCKED, &ggtt->flags)) {
+		rcu_read_unlock();
+
+		cpu_relax();
+		if (signal_pending(current))
+			return -EINTR;
+
+		rcu_read_lock();
+	}
+	*srcu = __srcu_read_lock(&ggtt->blocked_srcu);
+	rcu_read_unlock();
+
+	return 0;
+}
+
+static void ggtt_address_read_lock(struct i915_ggtt *ggtt, int *srcu)
+__acquires(&ggtt->blocked_srcu)
+{
+	rcu_read_lock();
+	while (test_bit(GGTT_ADDRESS_COMPUTE_BLOCKED, &ggtt->flags))
+		cpu_relax();
+	*srcu = __srcu_read_lock(&ggtt->blocked_srcu);
+	rcu_read_unlock();
+}
+
+int gt_ggtt_address_read_lock_sync(struct intel_gt *gt, int *srcu)
+{
+	return ggtt_address_read_lock_sync(gt->ggtt, srcu);
+}
+
+int gt_ggtt_address_read_lock_interruptible(struct intel_gt *gt, int *srcu)
+{
+	return ggtt_address_read_lock_interruptible(gt->ggtt, srcu);
+}
+
+void gt_ggtt_address_read_lock(struct intel_gt *gt, int *srcu)
+{
+	ggtt_address_read_lock(gt->ggtt, srcu);
+}
+
+static void ggtt_address_read_unlock(struct i915_ggtt *ggtt, int tag)
+__releases(&ggtt->blocked_srcu)
+{
+	__srcu_read_unlock(&ggtt->blocked_srcu, tag);
+}
+
+void gt_ggtt_address_read_unlock(struct intel_gt *gt, int srcu)
+{
+	ggtt_address_read_unlock(gt->ggtt, srcu);
+}
+
+/**
+ * i915_ggtt_address_write_unlock - finish the ggtt address computation fixups section
+ * @i915: i915 device instance struct
+ */
+void i915_ggtt_address_write_unlock(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_ggtt(gt, i915, id)
+		ggtt_address_write_unlock(gt->ggtt);
+}
+
 static int init_ggtt(struct i915_ggtt *ggtt)
 {
 	/*
@@ -865,6 +1015,8 @@ static int init_ggtt(struct i915_ggtt *ggtt)
 	if (ret)
 		return ret;
 
+	i915_ggtt_address_lock_init(ggtt);
+
 	ret = intel_iov_init_ggtt(&ggtt->vm.gt->iov);
 	if (ret)
 		return ret;
@@ -1809,6 +1961,11 @@ int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
 	return 0;
 }
 
+bool i915_ggtt_has_xehpsdv_pte_vfid_mask(struct i915_ggtt *ggtt)
+{
+	return GRAPHICS_VER_FULL(ggtt->vm.i915) < IP_VER(12, 50);
+}
+
 void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node)
 {
 	if (!drm_mm_node_allocated(node))
@@ -1909,6 +2066,26 @@ static gen8_pte_t tgl_prepare_vf_pte_vfid(u16 vfid)
 	return FIELD_PREP(TGL_GGTT_PTE_VFID_MASK, vfid);
 }
 
+static gen8_pte_t xehpsdv_prepare_vf_pte_vfid(u16 vfid)
+{
+	GEM_BUG_ON(!FIELD_FIT(XEHPSDV_GGTT_PTE_VFID_MASK, vfid));
+
+	return FIELD_PREP(XEHPSDV_GGTT_PTE_VFID_MASK, vfid);
+}
+
+static gen8_pte_t prepare_vf_pte_vfid(struct i915_ggtt *ggtt, u16 vfid)
+{
+	if (i915_ggtt_has_xehpsdv_pte_vfid_mask(ggtt))
+		return tgl_prepare_vf_pte_vfid(vfid);
+	else
+		return xehpsdv_prepare_vf_pte_vfid(vfid);
+}
+
+static gen8_pte_t prepare_vf_pte(struct i915_ggtt *ggtt, u16 vfid)
+{
+	return prepare_vf_pte_vfid(ggtt, vfid) | GEN8_PAGE_PRESENT;
+}
+
 gen8_pte_t i915_ggtt_prepare_vf_pte(u16 vfid)
 {
 	return tgl_prepare_vf_pte_vfid(vfid) | GEN8_PAGE_PRESENT;
@@ -1918,7 +2095,7 @@ void i915_ggtt_set_space_owner(struct i915_ggtt *ggtt, u16 vfid,
 			       const struct drm_mm_node *node)
 {
 	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
-	const gen8_pte_t pte = i915_ggtt_prepare_vf_pte(vfid);
+	const gen8_pte_t pte = prepare_vf_pte(ggtt, vfid);
 	u64 base = node->start;
 	u64 size = node->size;
 
@@ -1967,7 +2144,7 @@ void ggtt_pte_clear_vfid(void *buf, u64 size)
  * @ggtt: the &struct i915_ggtt
  * @node: the &struct drm_mm_node - the @node->start is used as the start offset for save
  * @buf: preallocated buffer in which PTEs will be saved
- * @size: size of preallocated buffer (in bytes)
+ * @size: size of prealocated buffer (in bytes)
  *        - must be sizeof(gen8_pte_t) aligned
  * @flags: function flags:
  *         - #I915_GGTT_SAVE_PTES_NO_VFID BIT - save PTEs without VFID
@@ -1980,9 +2157,6 @@ int i915_ggtt_save_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *node,
 {
 	gen8_pte_t __iomem *gtt_entries = ggtt->gsm;
 
-	if (i915_ggtt_require_binder(ggtt->vm.i915))
-		return -EOPNOTSUPP;
-
 	if (!buf && !size)
 		return ggtt_size_to_ptes_size(node->size);
 
@@ -2027,9 +2201,6 @@ int i915_ggtt_restore_ptes(struct i915_ggtt *ggtt, const struct drm_mm_node *nod
 	u32 vfid = FIELD_GET(I915_GGTT_RESTORE_PTES_VFID_MASK, flags);
 	gen8_pte_t pte;
 
-	if (i915_ggtt_require_binder(ggtt->vm.i915))
-		return -EOPNOTSUPP;
-
 	GEM_BUG_ON(!size);
 	GEM_BUG_ON(!IS_ALIGNED(size, sizeof(gen8_pte_t)));
 
diff --git a/drivers/gpu/drm/i915/gt/intel_gt.c b/drivers/gpu/drm/i915/gt/intel_gt.c
index c5c4716cecb3..6b7096422587 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt.c
@@ -1039,6 +1039,12 @@ int intel_gt_tiles_init(struct drm_i915_private *i915)
 	int ret;
 
 	for_each_gt(gt, i915, id) {
+		 if (!i915->gt[id])
+			  break;
+
+		if (GRAPHICS_VER(i915) >= 8)
+			setup_private_pat(gt);
+
 		ret = intel_gt_probe_lmem(gt);
 		if (ret)
 			return ret;
diff --git a/drivers/gpu/drm/i915/gt/intel_gt.h b/drivers/gpu/drm/i915/gt/intel_gt.h
index 9394f059a5c4..2f0edf3b27b4 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt.h
@@ -198,6 +198,7 @@ int intel_gt_tiles_init(struct drm_i915_private *i915);
 
 void intel_gt_info_print(const struct intel_gt_info *info,
 			 struct drm_printer *p);
+void intel_boost_fake_int_timer(struct intel_gt *gt, bool on_off);
 
 void intel_gt_watchdog_work(struct work_struct *work);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_irq.c b/drivers/gpu/drm/i915/gt/intel_gt_irq.c
index ad4c51f18d3a..3c717c870aaf 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_irq.c
+++ b/drivers/gpu/drm/i915/gt/intel_gt_irq.c
@@ -273,6 +273,13 @@ void gen11_gt_irq_postinstall(struct intel_gt *gt)
 			GT_CONTEXT_SWITCH_INTERRUPT |
 			GT_WAIT_SEMAPHORE_INTERRUPT;
 
+	/* Wa:16014207253 */
+	if (gt->fake_int.enabled)
+		irqs = 0;
+
+	if (gt->fake_int.enabled)
+		drm_info(&gt->i915->drm, "Using fake interrupt w/a, gt = %d\n", gt->info.id);
+
 	dmask = irqs << 16 | irqs;
 	smask = irqs << 16;
 
@@ -554,3 +561,32 @@ void gen5_gt_irq_postinstall(struct intel_gt *gt)
 		GEN3_IRQ_INIT(uncore, GEN6_PM, gt->pm_imr, pm_irqs);
 	}
 }
+
+void intel_boost_fake_int_timer(struct intel_gt *gt, bool on_off)
+{
+	u32 new_delay;
+	bool boost;
+
+	if (!gt->fake_int.enabled)
+		return;
+
+	if (on_off) {
+		atomic_inc(&gt->fake_int.boost);
+		boost = true;
+	} else {
+		boost = !atomic_dec_and_test(&gt->fake_int.boost);
+	}
+
+	if (!gt->fake_int.delay)
+		return;
+
+	new_delay = boost ? gt->fake_int.delay_fast : gt->fake_int.delay_slow;
+	if (new_delay == gt->fake_int.delay)
+		return;
+
+	gt->fake_int.delay = new_delay;
+	hrtimer_cancel(&gt->fake_int.timer);
+	hrtimer_start(&gt->fake_int.timer, ns_to_ktime(gt->fake_int.delay),
+			HRTIMER_MODE_REL);
+}
+
diff --git a/drivers/gpu/drm/i915/gt/intel_gt_types.h b/drivers/gpu/drm/i915/gt/intel_gt_types.h
index aaf4c354d251..deaf87b57251 100644
--- a/drivers/gpu/drm/i915/gt/intel_gt_types.h
+++ b/drivers/gpu/drm/i915/gt/intel_gt_types.h
@@ -144,6 +144,13 @@ struct intel_gt {
 		struct delayed_work retire_work;
 	} requests;
 
+	/**
+	 * pinned_contexts: List of pinned contexts. This list is only
+	 * assumed to be manipulated during driver load- or unload time and
+	 * does therefore not have any additional protection.
+	 */
+	struct list_head pinned_contexts;
+
 	struct {
 		struct llist_head list;
 		struct work_struct work;
@@ -158,6 +165,15 @@ struct intel_gt {
 	ktime_t last_init_time;
 	struct intel_reset reset;
 
+	struct {
+		bool enabled;
+		struct hrtimer timer;
+		atomic_t boost;
+		u32 delay;
+		u32 delay_fast, delay_slow;
+		bool int_enabled;
+	} fake_int;
+
 	/**
 	 * Is the GPU currently considered idle, or busy executing
 	 * userspace requests? Whilst idle, we allow runtime power
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index c91023e55b14..735acb5cc0a5 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -117,6 +117,7 @@ typedef u64 gen8_pte_t;
 #define MTL_GGTT_PTE_PAT0		BIT_ULL(52)
 #define MTL_GGTT_PTE_PAT1		BIT_ULL(53)
 #define TGL_GGTT_PTE_VFID_MASK		GENMASK_ULL(4, 2)
+#define XEHPSDV_GGTT_PTE_VFID_MASK      GENMASK_ULL(11, 2)
 #define GEN12_GGTT_PTE_ADDR_MASK	GENMASK_ULL(45, 12)
 #define MTL_GGTT_PTE_PAT_MASK		GENMASK_ULL(53, 52)
 
@@ -194,6 +195,11 @@ struct intel_gt;
 #define for_each_sgt_daddr_next(__dp, __iter) \
 	__for_each_daddr_next(__dp, __iter, I915_GTT_PAGE_SIZE)
 
+/* iterate through those GTs which contain a unique GGTT reference */
+#define for_each_ggtt(gt__, i915__, __id__) \
+	for_each_gt(gt__, i915__, __id__) \
+	for_each_if((gt__)->type != GT_MEDIA)
+
 struct i915_page_table {
 	struct drm_i915_gem_object *base;
 	union {
@@ -424,6 +430,13 @@ struct i915_ggtt {
 
 	/** List of GTs mapping this GGTT */
 	struct list_head gt_list;
+
+	/* Sleepable RCU for blocking on address computations. */
+	struct srcu_struct blocked_srcu;
+	unsigned long flags;
+#define GGTT_ADDRESS_COMPUTE_BLOCKED	0
+	/** Waitqueue to signal when the blocking has completed. */
+	wait_queue_head_t queue;
 };
 
 struct i915_ppgtt {
@@ -627,6 +640,8 @@ int i915_ggtt_balloon(struct i915_ggtt *ggtt, u64 start, u64 end,
 		      struct drm_mm_node *node);
 void i915_ggtt_deballoon(struct i915_ggtt *ggtt, struct drm_mm_node *node);
 
+bool i915_ggtt_has_xehpsdv_pte_vfid_mask(struct i915_ggtt *ggtt);
+
 int i915_ggtt_sgtable_update_ptes(struct i915_ggtt *ggtt, unsigned int vfid, u64 ggtt_addr,
 				  struct sg_table *st, u32 num_entries,
 				  const gen8_pte_t pte_pattern);
@@ -705,6 +720,15 @@ release_pd_entry(struct i915_page_directory * const pd,
 		 const struct drm_i915_gem_object * const scratch);
 void gen6_ggtt_invalidate(struct i915_ggtt *ggtt);
 
+void i915_ggtt_address_lock_init(struct i915_ggtt *ggtt);
+void i915_ggtt_address_lock_fini(struct i915_ggtt *ggtt);
+int gt_ggtt_address_read_lock_sync(struct intel_gt *gt, int *srcu);
+int gt_ggtt_address_read_lock_interruptible(struct intel_gt *gt, int *srcu);
+void gt_ggtt_address_read_lock(struct intel_gt *gt, int *srcu);
+void gt_ggtt_address_read_unlock(struct intel_gt *gt, int srcu);
+void i915_ggtt_address_write_lock(struct drm_i915_private *i915);
+void i915_ggtt_address_write_unlock(struct drm_i915_private *i915);
+
 void ppgtt_bind_vma(struct i915_address_space *vm,
 		    struct i915_vm_pt_stash *stash,
 		    struct i915_vma_resource *vma_res,
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc.c b/drivers/gpu/drm/i915/gt/intel_lrc.c
index 31efe2094527..141944d99069 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.c
@@ -1204,13 +1204,17 @@ int lrc_alloc(struct intel_context *ce, struct intel_engine_cs *engine)
 
 void lrc_reset(struct intel_context *ce)
 {
+	int srcu;
+
 	GEM_BUG_ON(!intel_context_is_pinned(ce));
 
 	intel_ring_reset(ce->ring, ce->ring->emit);
 
 	/* Scrub away the garbage */
+	gt_ggtt_address_read_lock(ce->engine->gt, &srcu);
 	lrc_init_regs(ce, ce->engine, true);
 	ce->lrc.lrca = lrc_update_regs(ce, ce->engine, ce->ring->tail);
+	gt_ggtt_address_read_unlock(ce->engine->gt, srcu);
 }
 
 int
@@ -1597,6 +1601,30 @@ u32 lrc_update_regs(const struct intel_context *ce,
 	return lrc_descriptor(ce) | CTX_DESC_FORCE_RESTORE;
 }
 
+void lrc_update_regs_with_address(struct intel_context *ce)
+{
+	struct intel_ring *ring = ce->ring;
+	u32 *regs = ce->lrc_reg_state;
+
+	regs[CTX_RING_START] = i915_ggtt_offset(ring->vma);
+
+	init_wa_bb_regs(regs, ce->engine);
+
+	if (ce->wa_bb_page) {
+		u32 *(*fn)(const struct intel_context *ce, u32 *cs);
+
+		fn = gen12_emit_indirect_ctx_xcs;
+		if (ce->engine->class == RENDER_CLASS)
+			fn = gen12_emit_indirect_ctx_rcs;
+
+		/* Mutually exclusive wrt to global indirect bb */
+		GEM_BUG_ON(ce->engine->wa_ctx.indirect_ctx.size);
+		setup_indirect_ctx_bb(ce, ce->engine, fn);
+	}
+
+	ce->lrc.lrca = lrc_descriptor(ce) | CTX_DESC_FORCE_RESTORE;
+}
+
 void lrc_update_offsets(struct intel_context *ce,
 			struct intel_engine_cs *engine)
 {
@@ -1609,10 +1637,22 @@ void lrc_check_regs(const struct intel_context *ce,
 {
 	const struct intel_ring *ring = ce->ring;
 	u32 *regs = ce->lrc_reg_state;
+	u32 regs_ring_start, vma_ring_start;
 	bool valid = true;
 	int x;
 
-	if (regs[CTX_RING_START] != i915_ggtt_offset(ring->vma)) {
+	regs_ring_start = regs[CTX_RING_START];
+	vma_ring_start = i915_ggtt_offset(ring->vma);
+	/*
+	 * The RING_START check is less strict on VFs, due to expected
+	 * inconsistency if unpin happens during post-migration recovery.
+	 * Only offsets within pages are guaranteed to always match.
+	 */
+	if (IS_SRIOV_VF(engine->i915)) {
+		regs_ring_start = offset_in_page(regs_ring_start);
+		vma_ring_start = offset_in_page(vma_ring_start);
+	}
+	if (regs_ring_start != vma_ring_start) {
 		pr_err("%s: context submitted with incorrect RING_START [%08x], expected %08x\n",
 		       engine->name,
 		       regs[CTX_RING_START],
diff --git a/drivers/gpu/drm/i915/gt/intel_lrc.h b/drivers/gpu/drm/i915/gt/intel_lrc.h
index 7111bae759f3..3849505f2a4a 100644
--- a/drivers/gpu/drm/i915/gt/intel_lrc.h
+++ b/drivers/gpu/drm/i915/gt/intel_lrc.h
@@ -64,6 +64,7 @@ void lrc_reset_regs(const struct intel_context *ce,
 u32 lrc_update_regs(const struct intel_context *ce,
 		    const struct intel_engine_cs *engine,
 		    u32 head);
+void lrc_update_regs_with_address(struct intel_context *ce);
 void lrc_update_offsets(struct intel_context *ce,
 			struct intel_engine_cs *engine);
 
diff --git a/drivers/gpu/drm/i915/gt/intel_reset.c b/drivers/gpu/drm/i915/gt/intel_reset.c
index 2dd9f341cf39..b6f3d031e4c3 100644
--- a/drivers/gpu/drm/i915/gt/intel_reset.c
+++ b/drivers/gpu/drm/i915/gt/intel_reset.c
@@ -811,7 +811,7 @@ wa_14015076503_end(struct intel_gt *gt, intel_engine_mask_t engine_mask)
 			 HECI_H_GS1_ER_PREP, 0);
 }
 
-static int __intel_gt_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask)
+int __intel_gt_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask)
 {
 	const int retries = engine_mask == ALL_ENGINES ? RESET_MAX_RETRIES : 1;
 	reset_func reset;
@@ -1584,6 +1584,37 @@ void intel_gt_handle_error(struct intel_gt *gt,
 	intel_runtime_pm_put(gt->uncore->rpm, wakeref);
 }
 
+/**
+ * intel_gt_reset_backoff_raise - make any reset calls to back off and resign
+ * @gt: #intel_gt to mark for reset backoff
+ *
+ * In some driver states, we want reset procedure to not be called. It does
+ * not mean the reset should be just blocked until later, but that it should
+ * be skipped completely. This function waits for any previous backoff to
+ * release, and then sets the backoff flag.
+ *
+ * The BACKOFF flag has an associated Sleepable RCU, so the blocking is a two
+ * point procedure. After setting the flag by this call, gt->reset.backoff_srcu
+ * should be synchronized, to make sure all other uses have truly ended.
+ */
+void intel_gt_reset_backoff_raise(struct intel_gt *gt)
+{
+	while (test_and_set_bit(I915_RESET_BACKOFF, &gt->reset.flags))
+		wait_event(gt->reset.queue,
+			   !test_bit(I915_RESET_BACKOFF, &gt->reset.flags));
+}
+
+/**
+ * intel_gt_reset_backoff_clear - unset the previously raised back off flag
+ * @gt: #intel_gt to clear reset backoff
+ */
+void intel_gt_reset_backoff_clear(struct intel_gt *gt)
+{
+	clear_bit_unlock(I915_RESET_BACKOFF, &gt->reset.flags);
+	smp_mb__after_atomic();
+	wake_up_all(&gt->reset.queue);
+}
+
 static int _intel_gt_reset_lock(struct intel_gt *gt, int *srcu, bool retry)
 {
 	might_lock(&gt->reset.backoff_srcu);
diff --git a/drivers/gpu/drm/i915/gt/intel_reset.h b/drivers/gpu/drm/i915/gt/intel_reset.h
index c00de353075c..1acd70248219 100644
--- a/drivers/gpu/drm/i915/gt/intel_reset.h
+++ b/drivers/gpu/drm/i915/gt/intel_reset.h
@@ -41,6 +41,8 @@ void __i915_request_reset(struct i915_request *rq, bool guilty);
 int __must_check intel_gt_reset_trylock(struct intel_gt *gt, int *srcu);
 int __must_check intel_gt_reset_lock_interruptible(struct intel_gt *gt, int *srcu);
 void intel_gt_reset_unlock(struct intel_gt *gt, int tag);
+void intel_gt_reset_backoff_raise(struct intel_gt *gt);
+void intel_gt_reset_backoff_clear(struct intel_gt *gt);
 
 void intel_gt_set_wedged(struct intel_gt *gt);
 bool intel_gt_unset_wedged(struct intel_gt *gt);
@@ -57,6 +59,8 @@ void intel_gt_set_wedged_on_fini(struct intel_gt *gt);
 int intel_gt_reset_engine(struct intel_engine_cs *engine);
 int intel_gt_reset_all_engines(struct intel_gt *gt);
 
+int __intel_gt_reset(struct intel_gt *gt, intel_engine_mask_t engine_mask);
+
 int intel_reset_guc(struct intel_gt *gt);
 
 struct intel_wedge_me {
diff --git a/drivers/gpu/drm/i915/gt/intel_ring.c b/drivers/gpu/drm/i915/gt/intel_ring.c
index 59da4b7bd262..31d6720d5df5 100644
--- a/drivers/gpu/drm/i915/gt/intel_ring.c
+++ b/drivers/gpu/drm/i915/gt/intel_ring.c
@@ -8,6 +8,7 @@
 #include "gem/i915_gem_object.h"
 
 #include "i915_drv.h"
+#include "i915_sriov.h"
 #include "i915_vma.h"
 #include "intel_engine.h"
 #include "intel_engine_regs.h"
@@ -16,6 +17,8 @@
 #include "intel_gt.h"
 #include "intel_timeline.h"
 
+#define INVALID_SRCU -1
+
 unsigned int intel_ring_update_space(struct intel_ring *ring)
 {
 	unsigned int space;
@@ -227,7 +230,12 @@ wait_for_space(struct intel_ring *ring,
 	return 0;
 }
 
-u32 *intel_ring_begin(struct i915_request *rq, unsigned int num_dwords)
+static bool need_ggtt_srcu(struct drm_i915_private *i915)
+{
+	return IS_SRIOV_VF(i915) && !i915_sriov_current_is_vf_migration_recovery(i915);
+}
+
+static u32 *ring_packet_begin(struct i915_request *rq, int *srcu, unsigned int num_dwords)
 {
 	struct intel_ring *ring = rq->ring;
 	const unsigned int remain_usable = ring->effective_size - ring->emit;
@@ -285,6 +293,17 @@ u32 *intel_ring_begin(struct i915_request *rq, unsigned int num_dwords)
 			return ERR_PTR(ret);
 	}
 
+	if (unlikely(srcu)) {
+		*srcu = INVALID_SRCU;
+		if (unlikely(need_ggtt_srcu(rq->i915))) {
+			int ret;
+
+			ret = gt_ggtt_address_read_lock_interruptible(rq->engine->gt, srcu);
+			if (unlikely(ret))
+				return ERR_PTR(ret);
+		}
+	}
+
 	if (unlikely(need_wrap)) {
 		need_wrap &= ~1;
 		GEM_BUG_ON(need_wrap > ring->space);
@@ -308,28 +327,72 @@ u32 *intel_ring_begin(struct i915_request *rq, unsigned int num_dwords)
 	return cs;
 }
 
-/* Align the ring tail to a cacheline boundary */
-int intel_ring_cacheline_align(struct i915_request *rq)
+static void ring_packet_advance(struct i915_request *rq, int srcu, u32 *cs)
 {
-	int num_dwords;
-	void *cs;
+	/*
+	 * Compare current state against what was provided to the preceding
+	 * intel_ring_begin() by checking whether the number of dwords
+	 * emitted matches the space reserved for the command packet (i.e.
+	 * the value passed to intel_ring_begin()).
+	 */
+	GEM_BUG_ON((rq->ring->vaddr + rq->ring->emit) != cs);
+	GEM_BUG_ON(!IS_ALIGNED(rq->ring->emit, 8)); /* RING_TAIL qword align */
 
-	num_dwords = (rq->ring->emit & (CACHELINE_BYTES - 1)) / sizeof(u32);
-	if (num_dwords == 0)
-		return 0;
+	rq->advance = intel_ring_offset(rq, cs);
+	if (srcu != INVALID_SRCU) {
+		set_bit(I915_FENCE_FLAG_GGTT_EMITTED, &rq->fence.flags);
+		gt_ggtt_address_read_unlock(rq->engine->gt, srcu);
+	}
+}
 
-	num_dwords = CACHELINE_DWORDS - num_dwords;
-	GEM_BUG_ON(num_dwords & 1);
+static void ring_fini_packet_begin(struct i915_request *rq, int *srcu)
+{
+	if (!srcu)
+		return;
 
-	cs = intel_ring_begin(rq, num_dwords);
-	if (IS_ERR(cs))
-		return PTR_ERR(cs);
+	*srcu = INVALID_SRCU;
+	if (unlikely(need_ggtt_srcu(rq->i915)))
+		gt_ggtt_address_read_lock(rq->engine->gt, srcu);
+}
 
-	memset64(cs, (u64)MI_NOOP << 32 | MI_NOOP, num_dwords / 2);
-	intel_ring_advance(rq, cs + num_dwords);
+static void ring_fini_packet_advance(struct i915_request *rq, int srcu, u32 *cs)
+{
+	rq->tail = intel_ring_offset(rq, cs);
+	if (srcu != INVALID_SRCU) {
+		set_bit(I915_FENCE_FLAG_GGTT_EMITTED, &rq->fence.flags);
+		gt_ggtt_address_read_unlock(rq->engine->gt, srcu);
+	}
+}
 
-	GEM_BUG_ON(rq->ring->emit & (CACHELINE_BYTES - 1));
-	return 0;
+/**
+ * intel_ring_begin - prepare for ring command packet emission
+ * @rq: request which starts the command packet
+ * @num_dwords: length of the packet
+ * Return: pointer to ring position where the packet starts
+ */
+u32 *intel_ring_begin(struct i915_request *rq, unsigned int num_dwords)
+{
+	return ring_packet_begin(rq, NULL, num_dwords);
+}
+
+u32 *intel_ring_begin_ggtt(struct i915_request *rq, int *srcu, unsigned int num_dwords)
+{
+	return ring_packet_begin(rq, srcu, num_dwords);
+}
+
+void intel_ring_advance_ggtt(struct i915_request *rq, int srcu, u32 *cs)
+{
+	ring_packet_advance(rq, srcu, cs);
+}
+
+void intel_ring_fini_begin_ggtt(struct i915_request *rq, int *srcu)
+{
+	ring_fini_packet_begin(rq, srcu);
+}
+
+void intel_ring_fini_advance_ggtt(struct i915_request *rq, int srcu, u32 *cs)
+{
+	ring_fini_packet_advance(rq, srcu, cs);
 }
 
 #if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
diff --git a/drivers/gpu/drm/i915/gt/intel_ring.h b/drivers/gpu/drm/i915/gt/intel_ring.h
index 1b32dadfb8c3..a77716059105 100644
--- a/drivers/gpu/drm/i915/gt/intel_ring.h
+++ b/drivers/gpu/drm/i915/gt/intel_ring.h
@@ -16,8 +16,10 @@ struct intel_ring *
 intel_engine_create_ring(struct intel_engine_cs *engine, int size);
 
 u32 *intel_ring_begin(struct i915_request *rq, unsigned int num_dwords);
-int intel_ring_cacheline_align(struct i915_request *rq);
-
+u32 *intel_ring_begin_ggtt(struct i915_request *rq, int *srcu, unsigned int num_dwords);
+void intel_ring_advance_ggtt(struct i915_request *rq, int srcu, u32 *cs);
+void intel_ring_fini_begin_ggtt(struct i915_request *rq, int *srcu);
+void intel_ring_fini_advance_ggtt(struct i915_request *rq, int srcu, u32 *cs);
 unsigned int intel_ring_update_space(struct intel_ring *ring);
 
 void __intel_ring_pin(struct intel_ring *ring);
@@ -41,13 +43,13 @@ static inline void intel_ring_put(struct intel_ring *ring)
 static inline void intel_ring_advance(struct i915_request *rq, u32 *cs)
 {
 	/* Dummy function.
-	 *
-	 * This serves as a placeholder in the code so that the reader
-	 * can compare against the preceding intel_ring_begin() and
-	 * check that the number of dwords emitted matches the space
-	 * reserved for the command packet (i.e. the value passed to
-	 * intel_ring_begin()).
-	 */
+	*
+	* This serves as a placeholder in the code so that the reader
+	* can compare against the preceding intel_ring_begin() and
+	* check that the number of dwords emitted matches the space
+	* reserved for the command packet (i.e. the value passed to
+	* intel_ring_begin()).
+	*/
 	GEM_BUG_ON((rq->ring->vaddr + rq->ring->emit) != cs);
 	GEM_BUG_ON(!IS_ALIGNED(rq->ring->emit, 8)); /* RING_TAIL qword align */
 }
@@ -127,6 +129,13 @@ intel_ring_set_tail(struct intel_ring *ring, unsigned int tail)
 	return tail;
 }
 
+static inline unsigned int
+__intel_ring_count(unsigned int head, unsigned int tail, unsigned int size)
+{
+	GEM_BUG_ON(!is_power_of_2(size));
+	return (tail - head) & (size - 1);
+}
+
 static inline unsigned int
 __intel_ring_space(unsigned int head, unsigned int tail, unsigned int size)
 {
diff --git a/drivers/gpu/drm/i915/gt/intel_sa_media.c b/drivers/gpu/drm/i915/gt/intel_sa_media.c
index 8cc3c71b3ea4..3a9912b60eb3 100644
--- a/drivers/gpu/drm/i915/gt/intel_sa_media.c
+++ b/drivers/gpu/drm/i915/gt/intel_sa_media.c
@@ -15,6 +15,7 @@ int intel_sa_mediagt_setup(struct intel_gt *gt, phys_addr_t phys_addr,
 {
 	struct drm_i915_private *i915 = gt->i915;
 	struct intel_uncore *uncore;
+	int err;
 
 	uncore = drmm_kzalloc(&i915->drm, sizeof(*uncore), GFP_KERNEL);
 	if (!uncore)
@@ -38,6 +39,10 @@ int intel_sa_mediagt_setup(struct intel_gt *gt, phys_addr_t phys_addr,
 	gt->uncore = uncore;
 	gt->phys_addr = phys_addr;
 
+	err = intel_iov_init_mmio(&gt->iov);
+	if (unlikely(err))
+		return err;
+
 	/*
 	 * For current platforms we can assume there's only a single
 	 * media GT and cache it for quick lookup.
diff --git a/drivers/gpu/drm/i915/gt/intel_timeline.c b/drivers/gpu/drm/i915/gt/intel_timeline.c
index b9640212d659..7a910e1adb5a 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline.c
+++ b/drivers/gpu/drm/i915/gt/intel_timeline.c
@@ -190,8 +190,13 @@ void __intel_timeline_pin(struct intel_timeline *tl)
 
 int intel_timeline_pin(struct intel_timeline *tl, struct i915_gem_ww_ctx *ww)
 {
-	int err;
+	int err, srcu;
 
+	/*
+	 * if already pinned, only increment the count to allow recursive
+	 * pinning; if not pinned yet, do nothing - the count should be then
+	 * incremented at the end of the pinning procedure, not here
+	 */
 	if (atomic_add_unless(&tl->pin_count, 1, 0))
 		return 0;
 
@@ -205,6 +210,12 @@ int intel_timeline_pin(struct intel_timeline *tl, struct i915_gem_ww_ctx *ww)
 	if (err)
 		return err;
 
+	err = gt_ggtt_address_read_lock_sync(tl->gt, &srcu);
+	if (unlikely(err)) {
+		__i915_vma_unpin(tl->hwsp_ggtt);
+		return err;
+	}
+
 	tl->hwsp_offset =
 		i915_ggtt_offset(tl->hwsp_ggtt) +
 		offset_in_page(tl->hwsp_offset);
@@ -216,10 +227,30 @@ int intel_timeline_pin(struct intel_timeline *tl, struct i915_gem_ww_ctx *ww)
 		i915_active_release(&tl->active);
 		__i915_vma_unpin(tl->hwsp_ggtt);
 	}
+	gt_ggtt_address_read_unlock(tl->gt, srcu);
 
 	return 0;
 }
 
+/**
+ * intel_timeline_rebase_hwsp - Recompute hwsp_offset cached within the pinned timeline.
+ * @tl: context timeline instance struct
+ */
+void intel_timeline_rebase_hwsp(struct intel_timeline *tl)
+{
+	if (!atomic_read(&tl->pin_count))
+		return; /* the offset will get updated while pinning */
+
+	GEM_BUG_ON(!tl->hwsp_map);
+	GEM_BUG_ON(!tl->hwsp_ggtt);
+
+	tl->hwsp_offset =
+		i915_ggtt_offset(tl->hwsp_ggtt) +
+		offset_in_page(tl->hwsp_offset);
+	GT_TRACE(tl->gt, "timeline:%llx using HWSP offset:%x\n",
+		 tl->fence_context, tl->hwsp_offset);
+}
+
 void intel_timeline_reset_seqno(const struct intel_timeline *tl)
 {
 	u32 *hwsp_seqno = (u32 *)tl->hwsp_seqno;
@@ -309,16 +340,24 @@ __intel_timeline_get_seqno(struct intel_timeline *tl,
 			   u32 *seqno)
 {
 	u32 next_ofs = offset_in_page(tl->hwsp_offset + TIMELINE_SEQNO_BYTES);
+	int err, srcu;
 
 	/* w/a: bit 5 needs to be zero for MI_FLUSH_DW address. */
 	if (TIMELINE_SEQNO_BYTES <= BIT(5) && (next_ofs & BIT(5)))
 		next_ofs = offset_in_page(next_ofs + BIT(5));
 
+	err = gt_ggtt_address_read_lock_sync(tl->gt, &srcu);
+	if (unlikely(err))
+		return err;
+
 	tl->hwsp_offset = i915_ggtt_offset(tl->hwsp_ggtt) + next_ofs;
 	tl->hwsp_seqno = tl->hwsp_map + next_ofs;
 	intel_timeline_reset_seqno(tl);
 
 	*seqno = timeline_advance(tl);
+
+	gt_ggtt_address_read_unlock(tl->gt, srcu);
+
 	GEM_BUG_ON(i915_seqno_passed(*tl->hwsp_seqno, *seqno));
 	return 0;
 }
diff --git a/drivers/gpu/drm/i915/gt/intel_timeline.h b/drivers/gpu/drm/i915/gt/intel_timeline.h
index 57308c4d664a..38ced03a429e 100644
--- a/drivers/gpu/drm/i915/gt/intel_timeline.h
+++ b/drivers/gpu/drm/i915/gt/intel_timeline.h
@@ -42,6 +42,12 @@ static inline void intel_timeline_put(struct intel_timeline *timeline)
 	kref_put(&timeline->kref, __intel_timeline_free);
 }
 
+static inline bool
+intel_timeline_has_initial_breadcrumb(const struct intel_timeline *tl)
+{
+	return true;
+}
+
 static inline int __intel_timeline_sync_set(struct intel_timeline *tl,
 					    u64 context, u32 seqno)
 {
@@ -80,6 +86,7 @@ void intel_timeline_reset_seqno(const struct intel_timeline *tl);
 int intel_timeline_read_hwsp(struct i915_request *from,
 			     struct i915_request *until,
 			     u32 *hwsp_offset);
+void intel_timeline_rebase_hwsp(struct intel_timeline *tl);
 
 void intel_gt_init_timelines(struct intel_gt *gt);
 void intel_gt_fini_timelines(struct intel_gt *gt);
diff --git a/drivers/gpu/drm/i915/gt/intel_workarounds.c b/drivers/gpu/drm/i915/gt/intel_workarounds.c
index 441036840e69..ed51ccefe907 100644
--- a/drivers/gpu/drm/i915/gt/intel_workarounds.c
+++ b/drivers/gpu/drm/i915/gt/intel_workarounds.c
@@ -2994,6 +2994,7 @@ wa_list_srm(struct i915_request *rq,
 	unsigned int i, count = 0;
 	const struct i915_wa *wa;
 	u32 srm, *cs;
+	int srcu;
 
 	srm = MI_STORE_REGISTER_MEM | MI_SRM_LRM_GLOBAL_GTT;
 	if (GRAPHICS_VER(i915) >= 8)
@@ -3004,7 +3005,7 @@ wa_list_srm(struct i915_request *rq,
 			count++;
 	}
 
-	cs = intel_ring_begin(rq, 4 * count);
+	cs = intel_ring_begin_ggtt(rq, &srcu, 4 * count + 4);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
@@ -3019,7 +3020,7 @@ wa_list_srm(struct i915_request *rq,
 		*cs++ = i915_ggtt_offset(vma) + sizeof(u32) * i;
 		*cs++ = 0;
 	}
-	intel_ring_advance(rq, cs);
+	intel_ring_advance_ggtt(rq, srcu, cs);
 
 	return 0;
 }
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
index f201eb1364b8..672fc3d5bbe3 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_debugfs.c
@@ -84,6 +84,35 @@ static int vf_self_config_show(struct seq_file *m, void *data)
 }
 DEFINE_INTEL_GT_DEBUGFS_ATTRIBUTE(vf_self_config);
 
+#if 0
+static ssize_t relocate_ggtt_write(struct file *file, const char __user *user,
+				    size_t count, loff_t *ppos)
+{
+	struct intel_iov *iov = &((struct intel_gt *)file->private_data)->iov;
+	u32 vfid;
+	int ret;
+
+	if (*ppos)
+		return 0;
+
+	ret = kstrtou32_from_user(user, count, 0, &vfid);
+	if (ret < 0)
+		return ret;
+
+	if (!vfid || vfid > pf_get_totalvfs(iov))
+		return -EINVAL;
+
+	ret = intel_iov_provisioning_move_ggtt(iov, vfid);
+	if (ret < 0)
+		return ret;
+
+	return count;
+}
+
+DEFINE_I915_GT_RAW_ATTRIBUTE(relocate_ggtt_fops, simple_open,
+				NULL, NULL, relocate_ggtt_write, default_llseek);
+#endif
+
 /**
  * intel_iov_debugfs_register - Register IOV specific entries in GT debugfs.
  * @iov: the IOV struct
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
index e11d5c876ecf..5ec98eca996e 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_memirq.c
@@ -49,6 +49,7 @@
 static int vf_create_memirq_data(struct intel_iov *iov)
 {
 	struct drm_i915_private *i915 = iov_to_i915(iov);
+	struct intel_gt *gt = iov_to_gt(iov);
 	struct drm_i915_gem_object *obj;
 	void *vaddr;
 	int err;
@@ -76,8 +77,15 @@ static int vf_create_memirq_data(struct intel_iov *iov)
 	iov->vf.irq.vaddr = vaddr;
 
 	enable_vector = (u32 *)(vaddr + I915_VF_IRQ_ENABLE);
-	/*XXX: we should start with all irqs disabled: 0xffff0000 */
-	*enable_vector = 0xffff;
+
+	/* Wa:16014207253 */
+	if (gt->fake_int.enabled) {
+		drm_info(&gt->i915->drm, "Using fake interrupt w/a, gt = %d\n", gt->info.id);
+		*enable_vector = 0x0;
+	} else {
+		/*XXX: we should start with all irqs disabled: 0xffff0000 */
+		*enable_vector = 0xffff;
+	}
 
 	return 0;
 
@@ -260,6 +268,11 @@ static void __guc_mem_irq_handler(struct intel_guc *guc, u8 *status)
 
 	MEMIRQ_DEBUG(gt, "STATUS %s %*ph\n", "GUC", 16, status);
 
+	if (READ_ONCE(status[ilog2(GUC_INTR_SW_INT_0)]) == 0xFF) {
+		WRITE_ONCE(status[ilog2(GUC_INTR_SW_INT_0)], 0x00);
+		intel_sriov_vf_migrated_event_handler(guc);
+	}
+
 	if (READ_ONCE(status[ilog2(GUC_INTR_GUC2HOST)]) == 0xFF) {
 		WRITE_ONCE(status[ilog2(GUC_INTR_GUC2HOST)], 0x00);
 		intel_guc_to_host_event_handler(guc);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.c
new file mode 100644
index 000000000000..2bfca0b2e4ba
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.c
@@ -0,0 +1,181 @@
+// SPDX-License-Identifier: MIT
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#include "gt/intel_gtt.h"
+#include "intel_iov_migration.h"
+#include "intel_iov_query.h"
+#include "intel_iov_utils.h"
+#include "intel_iov.h"
+
+/**
+ * intel_iov_migration_reinit_guc - Re-initialize GuC communication.
+ * @iov: the iov struct
+ *
+ * After migration, we need to reestablish communication with GuC and
+ * re-query all VF configuration to make sure they match previous
+ * provisioning. Note that most of VF provisioning shall be the same,
+ * except GGTT range, since GGTT is not virtualized per-VF.
+ *
+ * Returns: 0 if the operation completed successfully, or a negative error
+ * code otherwise.
+ */
+int intel_iov_migration_reinit_guc(struct intel_iov *iov)
+{
+	int err;
+	const char *where;
+
+	err = intel_iov_query_config(iov);
+	if (unlikely(err)) {
+		where = "query config";
+		goto fail;
+	}
+
+	return 0;
+
+fail:
+	IOV_ERROR(iov, "GuC re-init failed on %s (%pe)\n",
+		  where, ERR_PTR(err));
+	return err;
+}
+
+static u64 drm_mm_node_end(struct drm_mm_node *node)
+{
+	return node->start + node->size;
+}
+
+static s64 vf_get_post_migration_ggtt_shift(struct intel_iov *iov)
+{
+	u64 old_base;
+	s64 ggtt_shift;
+
+	old_base = drm_mm_node_end(&iov->vf.ggtt_balloon[0]);
+	ggtt_shift = iov->vf.config.ggtt_base - (s64)old_base;
+	iov->vf.config.ggtt_shift = ggtt_shift;
+
+	IOV_DEBUG(iov, "GGTT base shifted from %#llx to %#llx\n",
+		  old_base, old_base + ggtt_shift);
+
+	return ggtt_shift;
+}
+
+static void i915_ggtt_shift_nodes(struct i915_ggtt *ggtt, struct drm_mm_node balloon_nodes[2],
+				   s64 shift)
+{
+	struct drm_mm_node *node, *tmpn;
+	int err;
+	LIST_HEAD(temp_list_head);
+
+	lockdep_assert_held(&ggtt->vm.mutex);
+
+	/*
+	 * Move nodes, from range previously assigned to this VF, into temp list.
+	 *
+	 * The balloon_nodes array contains two nodes: first which reserves the GGTT area
+	 * below the range for current VF, and second which reserves area above. There
+	 * may also exist extra nodes at the bottom or top of GGTT range, as long as
+	 * there are no free spaces inbetween. Such extra nodes will be left unchanged.
+	 *
+	 * Below is a GGTT layout of example VF, with a certain address range assigned to
+	 * said VF, and inaccessible areas above and below:
+	 *
+	 *  0                                                                   vm->total
+	 *  |<--------------------------- Total GGTT size ----------------------------->|
+	 *
+	 *  +-----------+-------------------------+----------+--------------+-----------+
+	 *  |\\\\\\\\\\\|/////////////////////////|  VF mem  |//////////////|\\\\\\\\\\\|
+	 *  +-----------+-------------------------+----------+--------------+-----------+
+	 *
+	 * Hardware enforced access rules before migration:
+	 *
+	 *  |<------- inaccessible for VF ------->|<VF owned>|<-- inaccessible for VF ->|
+	 *
+	 * drm_mm nodes used for tracking allocations:
+	 *
+	 *  |<- extra ->|<------- balloon ------->|<- nodes->|<-- balloon ->|<- extra ->|
+	 *
+	 * After the migration, GGTT area assigned to the VF might have shifted, either
+	 * to lower or to higher address. But we expect the total size and extra areas to
+	 * be identical, as migration can only happen between matching platforms.
+	 * Below is an example of GGTT layout of the VF after migration. Content of the
+	 * GGTT for VF has been moved to a new area, and we receive its address from GuC:
+	 *
+	 *  +-----------+--------------+----------+-------------------------+-----------+
+	 *  |\\\\\\\\\\\|//////////////|  VF mem  |/////////////////////////|\\\\\\\\\\\|
+	 *  +-----------+--------------+----------+-------------------------+-----------+
+	 *
+	 * Hardware enforced access rules after migration:
+	 *
+	 *  |<- inaccessible for VF -->|<VF owned>|<------- inaccessible for VF ------->|
+	 *
+	 * So the VF has a new slice of GGTT assigned, and during migration process, the
+	 * memory content was copied to that new area. But the drm_mm nodes within i915
+	 * are still tracking allocations using the old addresses. The nodes within VF
+	 * owned area have to be shifted, and balloon nodes need to be resized to
+	 * properly mask out areas not owned by the VF.
+	 *
+	 * Fixed drm_mm nodes used for tracking allocations:
+	 *
+	 *  |<- extra  ->|<- balloon ->|<-- VF -->|<-------- balloon ------>|<- extra ->|
+	 *
+	 * Due to use of GPU profiles, we do not expect the old and new GGTT ares to
+	 * overlap; but our node shifting will fix addresses properly regardless.
+	 *
+	 */
+	drm_mm_for_each_node_in_range_safe(node, tmpn, &ggtt->vm.mm,
+					   drm_mm_node_end(&balloon_nodes[0]),
+					   balloon_nodes[1].start) {
+		drm_mm_remove_node(node);
+		list_add(&node->node_list, &temp_list_head);
+	}
+
+	/* shift and re-add ballooning nodes */
+	for (node = &balloon_nodes[0]; node <= &balloon_nodes[1]; node++) {
+		if (!drm_mm_node_allocated(node))
+			continue;
+		drm_mm_remove_node(node);
+	}
+	balloon_nodes[0].size += shift;
+	balloon_nodes[1].start += shift;
+	balloon_nodes[1].size -= shift;
+	for (node = &balloon_nodes[0]; node <= &balloon_nodes[1]; node++) {
+		if (node->size == 0)
+			continue;
+		err = drm_mm_reserve_node(&ggtt->vm.mm, node);
+		GEM_BUG_ON(err);
+	}
+
+	/*
+	 * Now the GGTT VM contains only nodes outside of area assigned to this VF.
+	 * We can re-add all VF nodes with shifted offsets.
+	 */
+	list_for_each_entry_safe(node, tmpn, &temp_list_head, node_list) {
+		list_del(&node->node_list);
+		node->start += shift;
+		err = drm_mm_reserve_node(&ggtt->vm.mm, node);
+		GEM_BUG_ON(err);
+	}
+}
+
+/**
+ * intel_iov_migration_fixup_ggtt_nodes - Shift GGTT allocations to match assigned range.
+ * @iov: the iov struct
+ *
+ * Since Global GTT is not virtualized, each VF has an assigned range
+ * within the global space. This range might have changed during migration,
+ * which requires all memory addresses pointing to GGTT to be shifted.
+ */
+void intel_iov_migration_fixup_ggtt_nodes(struct intel_iov *iov)
+{
+	struct intel_gt *gt = iov_to_gt(iov);
+	struct i915_ggtt *ggtt = gt->ggtt;
+	s64 ggtt_shift;
+
+	mutex_lock(&ggtt->vm.mutex);
+
+	ggtt_shift = vf_get_post_migration_ggtt_shift(iov);
+	i915_ggtt_shift_nodes(ggtt, iov->vf.ggtt_balloon, ggtt_shift);
+
+	mutex_unlock(&ggtt->vm.mutex);
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.h
new file mode 100644
index 000000000000..29abd58f0d62
--- /dev/null
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_migration.h
@@ -0,0 +1,14 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright  2022 Intel Corporation
+ */
+
+#ifndef __INTEL_IOV_MIGRATION_H__
+#define __INTEL_IOV_MIGRATION_H__
+
+struct intel_iov;
+
+int intel_iov_migration_reinit_guc(struct intel_iov *iov);
+void intel_iov_migration_fixup_ggtt_nodes(struct intel_iov *iov);
+
+#endif /* __INTEL_IOV_MIGRATION_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
index afb0eb37899d..a9911bc87c21 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.c
@@ -2160,6 +2160,50 @@ int intel_iov_provisioning_verify(struct intel_iov *iov, unsigned int num_vfs)
 	return 0;
 }
 
+static u32 pf_get_vf_tile_mask(struct intel_iov *iov, unsigned int vfid)
+{
+	struct intel_gt *gt;
+	unsigned int gtid;
+	u32 tile_mask  = 0;
+	int err;
+
+	GEM_BUG_ON(iov_is_remote(iov));
+
+	for_each_gt(gt, iov_to_i915(iov), gtid) {
+		err = pf_validate_config(&gt->iov, vfid);
+		if (!err)
+			tile_mask |= BIT(gtid);
+	}
+
+	IOV_DEBUG(iov, "VF%d tile_mask=%#x\n", vfid, tile_mask);
+	GEM_BUG_ON(tile_mask & ~GENMASK(iov_to_i915(iov)->remote_tiles, 0));
+
+	return tile_mask;
+}
+
+/**
+ * intel_iov_provisioning_get_tile_mask() - Query tile mask of the VF.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function shall be called only on PF.
+ *
+ * Return: tile mask.
+ */
+u32 intel_iov_provisioning_get_tile_mask(struct intel_iov *iov, unsigned int id)
+{
+	u32 tile_mask;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+	tile_mask = pf_get_vf_tile_mask(iov_get_root(iov), id);
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return tile_mask;
+}
+
 /* Return: number of configuration dwords written */
 static u32 encode_config_ggtt(u32 *cfg, const struct intel_iov_config *config)
 {
@@ -2213,6 +2257,19 @@ static u32 encode_config(u32 *cfg, const struct intel_iov_config *config)
 	return n;
 }
 
+/* Return: number of configuration dwords written */
+static u32 encode_tile_mask(u32 *cfg, u32 tile_mask)
+{
+	u32 n = 0;
+
+	if (tile_mask) {
+		cfg[n++] = MAKE_GUC_KLV(VF_CFG_TILE_MASK);
+		cfg[n++] = tile_mask;
+	}
+
+	return n;
+}
+
 static int pf_verify_config_klvs(struct intel_iov *iov, const u32 *cfg, u32 cfg_size)
 {
 	while (cfg_size) {
@@ -2273,6 +2330,12 @@ static int pf_push_configs(struct intel_iov *iov, unsigned int num)
 		if (err != -ENODATA)
 			cfg_size = encode_config(cfg, &provisioning->configs[n]);
 
+		if (iov_is_root(iov) && HAS_REMOTE_TILES(iov_to_i915(iov))) {
+			u32 tile_mask = pf_get_vf_tile_mask(iov, n);
+
+			cfg_size += encode_tile_mask(cfg + cfg_size, tile_mask);
+		}
+
 		if (iov_to_gt(iov)->type == GT_MEDIA) {
 			struct intel_iov *root = iov_get_root(iov);
 			struct intel_iov_config *config = &root->pf.provisioning.configs[n];
@@ -2648,6 +2711,96 @@ int intel_iov_provisioning_print_dbs(struct intel_iov *iov, struct drm_printer *
 	return 0;
 }
 
+#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV)
+#if 0
+static int pf_reprovision_ggtt(struct intel_iov *iov, unsigned int id)
+{
+	struct i915_ggtt *ggtt = iov_to_gt(iov)->ggtt;
+	struct intel_iov_provisioning *provisioning = &iov->pf.provisioning;
+	struct intel_iov_config *config = &provisioning->configs[id];
+	struct drm_mm_node *node = &config->ggtt_region;
+	struct drm_mm_node new_node = {};
+	u64 alignment = pf_get_ggtt_alignment(iov);
+	u64 node_size = node->size;
+	unsigned int ptes_size;
+	void *ptes;
+	int err;
+
+	if (!drm_mm_node_allocated(node))
+		return -ENODATA;
+
+	/* save PTEs */
+	ptes_size = i915_ggtt_save_ptes(ggtt, node, NULL, 0, 0);
+	ptes = kmalloc(ptes_size, GFP_KERNEL);
+	if (!ptes)
+		return -ENOMEM;
+	err = i915_ggtt_save_ptes(ggtt, node, ptes, ptes_size, 0);
+	if (err < 0)
+		goto out;
+
+	/* allocate new block */
+	mutex_lock(&ggtt->vm.mutex);
+	err = i915_gem_gtt_insert(&ggtt->vm, &new_node, node_size, alignment,
+		I915_COLOR_UNEVICTABLE,
+		0, ggtt->vm.total,
+		PIN_HIGH);
+	mutex_unlock(&ggtt->vm.mutex);
+	if (err)
+		goto out;
+	GEM_WARN_ON(node_size != new_node.size);
+
+	/* reprovision */
+	err = pf_push_config_ggtt(iov, id, new_node.start, new_node.size);
+	if (err) {
+		mutex_lock(&ggtt->vm.mutex);
+		drm_mm_remove_node(&new_node);
+		mutex_unlock(&ggtt->vm.mutex);
+		goto out;
+	}
+
+	/* replace node */
+	mutex_lock(&ggtt->vm.mutex);
+	drm_mm_remove_node(node);
+	drm_mm_replace_node(&new_node, node);
+	mutex_unlock(&ggtt->vm.mutex);
+
+	/* restore PTEs */
+	err = i915_ggtt_restore_ptes(ggtt, node, ptes, ptes_size, 0);
+	if (err)
+		i915_ggtt_set_space_owner(ggtt, id, node);
+
+out:
+	kfree(ptes);
+	return err;
+}
+
+/**
+ * intel_iov_provisioning_move_ggtt - Move existing GGTT allocation to other location.
+ * @iov: the IOV struct
+ * @id: VF identifier
+ *
+ * This function is for internal testing of VF migration scenarios.
+ * This function can only be called on PF.
+ */
+int intel_iov_provisioning_move_ggtt(struct intel_iov *iov, unsigned int id)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	intel_wakeref_t wakeref;
+	int err = -ENONET;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+	GEM_BUG_ON(id > pf_get_totalvfs(iov));
+	GEM_BUG_ON(id == PFID);
+
+	with_intel_runtime_pm(rpm, wakeref)
+		err = pf_reprovision_ggtt(iov, id);
+
+	return err;
+}
+
+#endif /* CONFIG_DRM_I915_DEBUG_IOV */
+#endif
+
 static int pf_push_self_config(struct intel_iov *iov)
 {
 	struct intel_guc *guc = iov_to_guc(iov);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
index 6cecb6d84ae9..1cbb2d83a8a3 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_provisioning.h
@@ -29,6 +29,8 @@ int intel_iov_provisioning_auto(struct intel_iov *iov, unsigned int num_vfs);
 int intel_iov_provisioning_verify(struct intel_iov *iov, unsigned int num_vfs);
 int intel_iov_provisioning_push(struct intel_iov *iov, unsigned int num);
 
+u32 intel_iov_provisioning_get_tile_mask(struct intel_iov *iov, unsigned int vfid);
+
 int intel_iov_provisioning_set_ggtt(struct intel_iov *iov, unsigned int id, u64 size);
 u64 intel_iov_provisioning_get_ggtt(struct intel_iov *iov, unsigned int id);
 int intel_iov_provisioning_set_spare_ggtt(struct intel_iov *iov, u64 size);
@@ -69,6 +71,10 @@ int intel_iov_provisioning_print_dbs(struct intel_iov *iov, struct drm_printer *
 
 int intel_iov_provisioning_print_available_ggtt(struct intel_iov *iov, struct drm_printer *p);
 
+#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV)
+int intel_iov_provisioning_move_ggtt(struct intel_iov *iov, unsigned int id);
+#endif /* CONFIG_DRM_I915_DEBUG_IOV */
+
 int intel_iov_provisioning_force_vgt_mode(struct intel_iov *iov);
 
 #endif /* __INTEL_IOV_PROVISIONING_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
index 83239e3e4cde..c9599049a5ed 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.c
@@ -50,6 +50,39 @@ static int vf_reset_guc_state(struct intel_iov *iov)
 	return err;
 }
 
+static int guc_action_vf_notify_resfix_done(struct intel_guc *guc)
+{
+	u32 request[GUC_HXG_REQUEST_MSG_MIN_LEN] = {
+		FIELD_PREP(GUC_HXG_MSG_0_ORIGIN, GUC_HXG_ORIGIN_HOST) |
+		FIELD_PREP(GUC_HXG_MSG_0_TYPE, GUC_HXG_TYPE_REQUEST) |
+		FIELD_PREP(GUC_HXG_REQUEST_MSG_0_ACTION, GUC_ACTION_VF2GUC_NOTIFY_RESFIX_DONE),
+	};
+	int ret;
+
+	ret = intel_guc_send_mmio(guc, request, ARRAY_SIZE(request), NULL, 0);
+
+	return ret > 0 ? -EPROTO : ret;
+}
+
+/**
+ * intel_iov_notify_resfix_done - Notify GuC about resource fixups apply completed.
+ * @iov: the IOV struct instance
+ */
+int intel_iov_notify_resfix_done(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	err = guc_action_vf_notify_resfix_done(guc);
+	if (unlikely(err))
+		IOV_PROBE_ERROR(iov, "Failed to notify GuC about resource fixup done (%pe)\n",
+				ERR_PTR(err));
+
+	return err;
+}
+
 static int guc_action_match_version(struct intel_guc *guc, u32 *branch,
 				    u32 *major, u32 *minor, u32 *patch)
 {
@@ -102,6 +135,18 @@ static int vf_handshake_with_guc(struct intel_iov *iov)
 	if (unlikely(err))
 		goto fail;
 
+	/* XXX we don't support interface version change */
+	if ((iov->vf.config.guc_abi.major || iov->vf.config.guc_abi.minor) &&
+	     (iov->vf.config.guc_abi.branch != branch ||
+	     iov->vf.config.guc_abi.major != major ||
+	     iov->vf.config.guc_abi.minor != minor)) {
+		IOV_ERROR(iov, "Unexpected interface version change: %u.%u.%u.%u != %u.%u.%u.%u\n",
+			  branch, major, minor, patch,
+			  iov->vf.config.guc_abi.branch, iov->vf.config.guc_abi.major,
+			  iov->vf.config.guc_abi.minor, iov->vf.config.guc_abi.patch);
+		return -EREMCHG;
+	}
+
 	/* we shouldn't get anything newer than requested */
 	if (major > GUC_VF_VERSION_LATEST_MAJOR) {
 		err = -EPROTO;
@@ -284,6 +329,37 @@ static int vf_get_ipver(struct intel_iov *iov)
 
 }
 
+static int vf_get_tiles(struct intel_iov *iov)
+{
+	struct intel_guc *guc = iov_to_guc(iov);
+	u32 tile_mask;
+	int err;
+
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+	GEM_BUG_ON(!iov_is_root(iov));
+
+	err = guc_action_query_single_klv32(guc, GUC_KLV_VF_CFG_TILE_MASK_KEY, &tile_mask);
+	if (unlikely(err))
+		return err;
+
+	if (!tile_mask) {
+		IOV_ERROR(iov, "Invalid GT assignment: %#x\n", tile_mask);
+		return -ENODATA;
+	}
+
+	IOV_DEBUG(iov, "tile mask %#x\n", tile_mask);
+
+	if (iov->vf.config.tile_mask && iov->vf.config.tile_mask != tile_mask) {
+		IOV_ERROR(iov, "Unexpected GT reassignment: %#x != %#x\n",
+			  tile_mask, iov->vf.config.tile_mask);
+		return -EREMCHG;
+	}
+
+	iov->vf.config.tile_mask = tile_mask;
+
+	return 0;
+}
+
 static int vf_get_ggtt_info(struct intel_iov *iov)
 {
 	struct intel_guc *guc = iov_to_guc(iov);
@@ -291,7 +367,6 @@ static int vf_get_ggtt_info(struct intel_iov *iov)
 	int err;
 
 	GEM_BUG_ON(!intel_iov_is_vf(iov));
-	GEM_BUG_ON(iov->vf.config.ggtt_size);
 
 	err = guc_action_query_single_klv64(guc, GUC_KLV_VF_CFG_GGTT_START_KEY, &start);
 	if (unlikely(err))
@@ -304,6 +379,12 @@ static int vf_get_ggtt_info(struct intel_iov *iov)
 	IOV_DEBUG(iov, "GGTT %#llx-%#llx = %lluK\n",
 		  start, start + size - 1, size / SZ_1K);
 
+	if (iov->vf.config.ggtt_size && iov->vf.config.ggtt_size != size) {
+		IOV_ERROR(iov, "Unexpected GGTT reassignment: %lluK != %lluK\n",
+			  size / SZ_1K, iov->vf.config.ggtt_size / SZ_1K);
+		return -EREMCHG;
+	}
+
 	iov->vf.config.ggtt_base = start;
 	iov->vf.config.ggtt_size = size;
 
@@ -317,7 +398,6 @@ static int vf_get_submission_cfg(struct intel_iov *iov)
 	int err;
 
 	GEM_BUG_ON(!intel_iov_is_vf(iov));
-	GEM_BUG_ON(iov->vf.config.num_ctxs);
 
 	err = guc_action_query_single_klv32(guc, GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY, &num_ctxs);
 	if (unlikely(err))
@@ -329,12 +409,33 @@ static int vf_get_submission_cfg(struct intel_iov *iov)
 
 	IOV_DEBUG(iov, "CTXs %u DBs %u\n", num_ctxs, num_dbs);
 
+	if (iov->vf.config.num_ctxs && iov->vf.config.num_ctxs != num_ctxs) {
+		IOV_ERROR(iov, "Unexpected CTXs reassignment: %u != %u\n",
+			  num_ctxs, iov->vf.config.num_ctxs);
+		return -EREMCHG;
+	}
+	if (iov->vf.config.num_dbs && iov->vf.config.num_dbs != num_dbs) {
+		IOV_ERROR(iov, "Unexpected DBs reassignment: %u != %u\n",
+			  num_dbs, iov->vf.config.num_dbs);
+		return -EREMCHG;
+	}
+
 	iov->vf.config.num_ctxs = num_ctxs;
 	iov->vf.config.num_dbs = num_dbs;
 
 	return iov->vf.config.num_ctxs ? 0 : -ENODATA;
 }
 
+static bool vf_in_tile_mask(struct intel_iov *iov)
+{
+	GEM_BUG_ON(!intel_iov_is_vf(iov));
+
+	if (!HAS_REMOTE_TILES(iov_to_i915(iov)))
+		return true;
+
+	return iov_get_root(iov)->vf.config.tile_mask & BIT(iov_to_gt(iov)->info.id);
+}
+
 /**
  * intel_iov_query_config - Query IOV config data over MMIO.
  * @iov: the IOV struct
@@ -353,6 +454,16 @@ int intel_iov_query_config(struct intel_iov *iov)
 	if (unlikely(err))
 		return err;
 
+	if (HAS_REMOTE_TILES(iov_to_i915(iov)) && iov_is_root(iov)) {
+		err = vf_get_tiles(iov);
+		if (unlikely(err))
+			return err;
+
+		if (!vf_in_tile_mask(iov))
+			return 0;
+	}
+
+
 	err = vf_get_ggtt_info(iov);
 	if (unlikely(err))
 		return err;
@@ -883,6 +994,9 @@ int intel_iov_query_runtime(struct intel_iov *iov, bool early)
 
 	GEM_BUG_ON(!intel_iov_is_vf(iov));
 
+	if (!vf_in_tile_mask(iov))
+		return 0;
+
 	if (early) {
 		err = vf_handshake_with_pf_mmio(iov);
 		if (unlikely(err))
@@ -929,6 +1043,10 @@ void intel_iov_query_print_config(struct intel_iov *iov, struct drm_printer *p)
 {
 	GEM_BUG_ON(!intel_iov_is_vf(iov));
 
+	/* tile_mask is valid on root GT only, report it once on primary GT */
+	if (HAS_REMOTE_TILES(iov_to_i915(iov)) && iov_to_gt(iov) == to_gt(iov_to_i915(iov)))
+		drm_printf(p, "tile mask:\t%#x\n", iov_get_root(iov)->vf.config.tile_mask);
+
 	drm_printf(p, "GGTT range:\t%#08llx-%#08llx\n",
 			iov->vf.config.ggtt_base,
 			iov->vf.config.ggtt_base + iov->vf.config.ggtt_size - 1);
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
index 58fb1f01b193..963e869a6fe4 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_query.h
@@ -20,4 +20,6 @@ void intel_iov_query_fini(struct intel_iov *iov);
 
 void intel_iov_query_print_config(struct intel_iov *iov, struct drm_printer *p);
 
+int intel_iov_notify_resfix_done(struct intel_iov *iov);
+
 #endif /* __INTEL_IOV_QUERY_H__ */
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
index 339391fc1c5e..9f844091f37c 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_relay.c
@@ -352,6 +352,9 @@ static int relay_send_and_wait(struct intel_iov_relay *relay, u32 target,
 	pending.response = buf;
 	pending.response_size = buf_size;
 
+	/* Wa:16014207253 */
+	intel_boost_fake_int_timer(relay_to_gt(relay), true);
+
 	/* list ordering does not need to match fence ordering */
 	spin_lock(&relay->lock);
 	list_add_tail(&pending.link, &relay->pending_relays);
@@ -392,6 +395,9 @@ static int relay_send_and_wait(struct intel_iov_relay *relay, u32 target,
 	list_del(&pending.link);
 	spin_unlock(&relay->lock);
 
+	/* Wa:16014207253 */
+	intel_boost_fake_int_timer(relay_to_gt(relay), false);
+
 	if (unlikely(ret < 0)) {
 		RELAY_PROBE_ERROR(relay, "Unsuccessful %s.%u %#x:%u to %u (%pe) %*ph\n",
 				  hxg_type_to_string(FIELD_GET(GUC_HXG_MSG_0_TYPE, msg[0])),
@@ -474,6 +480,7 @@ int intel_iov_relay_send_to_pf(struct intel_iov_relay *relay,
 	GEM_BUG_ON(relay_type != GUC_HXG_TYPE_REQUEST);
 	return relay_send_and_wait(relay, 0, relay_id, msg, len, buf, buf_size);
 }
+ALLOW_ERROR_INJECTION(intel_iov_relay_send_to_pf, ERRNO);
 
 static int relay_handle_reply(struct intel_iov_relay *relay, u32 origin,
 			      u32 relay_id, int reply, const u32 *msg, u32 len)
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
index 82009bbada9f..a695fbaef7b9 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.c
@@ -11,6 +11,7 @@
 #include "intel_iov_utils.h"
 #include "gt/intel_gt.h"
 #include "gt/uc/abi/guc_actions_pf_abi.h"
+#include "gt/iov/intel_iov_reg.h"
 
 static void pf_state_worker_func(struct work_struct *w);
 
@@ -173,8 +174,19 @@ static void pf_clear_vf_ggtt_entries(struct intel_iov *iov, u32 vfid)
 	i915_ggtt_set_space_owner(gt->ggtt, vfid, &config->ggtt_region);
 }
 
+static bool pf_vfs_flr_enabled(struct intel_iov *iov, u32 vfid)
+{
+	return iov_to_i915(iov)->params.vfs_flr_mask & BIT(vfid);
+}
+
 static int pf_process_vf_flr_finish(struct intel_iov *iov, u32 vfid)
 {
+	if (!pf_vfs_flr_enabled(iov, vfid)) {
+		IOV_DEBUG(iov, "VF%u FLR processing skipped\n", vfid);
+		goto skip;
+	}
+	IOV_DEBUG(iov, "processing VF%u FLR\n", vfid);
+
 	/* Wa_14017568299:mtl - Clear Unsupported Request Detected status*/
 	wa_14017568299(iov, vfid);
 
@@ -184,6 +196,7 @@ static int pf_process_vf_flr_finish(struct intel_iov *iov, u32 vfid)
 	pf_clear_vf_ggtt_entries(iov, vfid);
 	mutex_unlock(pf_provisioning_mutex(iov));
 
+skip:
 	return pf_trigger_vf_flr_finish(iov, vfid);
 }
 
@@ -364,8 +377,10 @@ static void pf_handle_vf_flr(struct intel_iov *iov, u32 vfid)
 	unsigned int gtid;
 
 	if (!iov_is_root(iov)) {
-		if (iov_to_gt(iov)->type == GT_MEDIA)
+		if (iov_to_gt(iov)->type == GT_MEDIA) {
+			iov->pf.state.data[vfid].paused = false;
 			return;
+		}
 		IOV_ERROR(iov, "Unexpected VF%u FLR notification\n", vfid);
 		return;
 	}
@@ -400,6 +415,13 @@ static void pf_handle_vf_pause_done(struct intel_iov *iov, u32 vfid)
 	dev_info(dev, "VF%u %s\n", vfid, "paused");
 }
 
+static void pf_handle_vf_fixup_done(struct intel_iov *iov, u32 vfid)
+{
+	struct device *dev = iov_to_dev(iov);
+
+	dev_info(dev, "VF%u %s\n", vfid, "has completed migration");
+}
+
 static int pf_handle_vf_event(struct intel_iov *iov, u32 vfid, u32 eventid)
 {
 	switch (eventid) {
@@ -412,6 +434,9 @@ static int pf_handle_vf_event(struct intel_iov *iov, u32 vfid, u32 eventid)
 	case GUC_PF_NOTIFY_VF_PAUSE_DONE:
 		pf_handle_vf_pause_done(iov, vfid);
 		break;
+	case GUC_PF_NOTIFY_VF_FIXUP_DONE:
+		pf_handle_vf_fixup_done(iov, vfid);
+		break;
 	default:
 		return -ENOPKG;
 	}
@@ -665,15 +690,8 @@ ssize_t intel_iov_state_save_ggtt(struct intel_iov *iov, u32 vfid, void *buf, si
 		goto out;
 	}
 
-	with_intel_runtime_pm(rpm, wakeref) {
-		unsigned int flags = I915_GGTT_SAVE_PTES_NO_VFID;
-
-		/* Wa_22018453856 */
-		if (i915_ggtt_require_binder(iov_to_i915(iov)))
-			ret = intel_iov_ggtt_shadow_save(iov, vfid, buf, size, flags);
-		else
-			ret = i915_ggtt_save_ptes(ggtt, node, buf, size, flags);
-	}
+	with_intel_runtime_pm(rpm, wakeref)
+		ret = i915_ggtt_save_ptes(ggtt, node, buf, size, I915_GGTT_SAVE_PTES_NO_VFID);
 
 out:
 	mutex_unlock(pf_provisioning_mutex(iov));
@@ -681,6 +699,7 @@ ssize_t intel_iov_state_save_ggtt(struct intel_iov *iov, u32 vfid, void *buf, si
 	return ret;
 }
 
+
 /**
  * intel_iov_state_restore_ggtt - Restore VF GGTT.
  * @iov: the IOV struct
@@ -704,16 +723,10 @@ int intel_iov_state_restore_ggtt(struct intel_iov *iov, u32 vfid, const void *bu
 
 	mutex_lock(pf_provisioning_mutex(iov));
 
-	with_intel_runtime_pm(rpm, wakeref) {
-		unsigned int flags = FIELD_PREP(I915_GGTT_RESTORE_PTES_VFID_MASK, vfid) |
-						I915_GGTT_RESTORE_PTES_NEW_VFID;
-
-		/* Wa_22018453856 */
-		if (i915_ggtt_require_binder(iov_to_i915(iov)))
-			ret = intel_iov_ggtt_shadow_restore(iov, vfid, buf, size, flags);
-		else
-			ret = i915_ggtt_restore_ptes(ggtt, node, buf, size, flags);
-	}
+	with_intel_runtime_pm(rpm, wakeref)
+		ret = i915_ggtt_restore_ptes(ggtt, node, buf, size,
+				FIELD_PREP(I915_GGTT_RESTORE_PTES_VFID_MASK, vfid) |
+				I915_GGTT_RESTORE_PTES_NEW_VFID);
 
 	mutex_unlock(pf_provisioning_mutex(iov));
 
@@ -825,6 +838,14 @@ int intel_iov_state_save_vf_size(struct intel_iov *iov, u32 vfid)
 	return ret;
 }
 
+int intel_iov_state_save_mmio_size(struct intel_iov *iov, u32 vfid)
+{
+	struct drm_i915_private *i915 = iov_to_i915(iov);
+
+	return (GRAPHICS_VER_FULL(i915) < IP_VER(12, 50)) ?
+		GEN12_VF_REGISTERS_STRIDE :
+		XEHPSDV_VF_REGISTERS_STRIDE;
+}
 /**
  * intel_iov_state_save_vf - Save VF state.
  * @iov: the IOV struct
@@ -932,3 +953,43 @@ int intel_iov_state_store_guc_migration_state(struct intel_iov *iov, u32 vfid,
 		return ret;
 	return 0;
 }
+
+ssize_t intel_iov_state_save_mmio(struct intel_iov *iov, u32 vfid, void *buf, size_t size)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	struct intel_gt *gt = iov_to_gt(iov);
+	intel_wakeref_t wakeref;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref) {
+		memcpy_fromio(buf, gt->uncore->regs + 0x190000 + (vfid * GEN12_VF_REGISTERS_STRIDE),
+				size);
+	}
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return size;
+}
+
+int intel_iov_state_restore_mmio(struct intel_iov *iov, u32 vfid, const void *buf, size_t size)
+{
+	struct intel_runtime_pm *rpm = iov_to_gt(iov)->uncore->rpm;
+	struct intel_gt *gt = iov_to_gt(iov);
+	intel_wakeref_t wakeref;
+
+	GEM_BUG_ON(!intel_iov_is_pf(iov));
+
+	mutex_lock(pf_provisioning_mutex(iov));
+
+	with_intel_runtime_pm(rpm, wakeref) {
+		memcpy_toio(gt->uncore->regs + 0x190000 + (vfid * GEN12_VF_REGISTERS_STRIDE), buf,
+				size);
+	}
+
+	mutex_unlock(pf_provisioning_mutex(iov));
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
index 802b558acb00..fea50ae4ea9f 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_state.h
@@ -25,6 +25,9 @@ int intel_iov_state_stop_vf(struct intel_iov *iov, u32 vfid);
 int intel_iov_state_save_vf_size(struct intel_iov *iov, u32 vfid);
 ssize_t intel_iov_state_save_ggtt(struct intel_iov *iov, u32 vfid, void *buf, size_t size);
 int intel_iov_state_restore_ggtt(struct intel_iov *iov, u32 vfid, const void *buf, size_t size);
+int intel_iov_state_save_mmio_size(struct intel_iov *iov, u32 vfid);
+ssize_t intel_iov_state_save_mmio(struct intel_iov *iov, u32 vfid, void *buf, size_t size);
+int intel_iov_state_restore_mmio(struct intel_iov *iov, u32 vfid, const void *buf, size_t size);
 int intel_iov_state_save_vf(struct intel_iov *iov, u32 vfid, void *buf, size_t size);
 int intel_iov_state_restore_vf(struct intel_iov *iov, u32 vfid, const void *buf, size_t size);
 int intel_iov_state_store_guc_migration_state(struct intel_iov *iov, u32 vfid,
diff --git a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
index 5a86850dfa10..c46de35fa67b 100644
--- a/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
+++ b/drivers/gpu/drm/i915/gt/iov/intel_iov_types.h
@@ -280,6 +280,7 @@ struct intel_iov_relay {
  * @ggtt_size: size of GGTT region.
  * @num_ctxs: number of GuC submission contexts.
  * @num_dbs: number of GuC doorbells.
+ * @tile_mask: assigned tiles (as bitmask with tile0 = BIT(0)).
  */
 struct intel_iov_vf_config {
 	struct {
@@ -289,9 +290,11 @@ struct intel_iov_vf_config {
 		u8 patch;
 	} guc_abi;
 	u64 ggtt_base;
+	s64 ggtt_shift;
 	u64 ggtt_size;
 	u16 num_ctxs;
 	u16 num_dbs;
+	u32 tile_mask;
 };
 
 /**
diff --git a/drivers/gpu/drm/i915/gt/selftest_execlists.c b/drivers/gpu/drm/i915/gt/selftest_execlists.c
index 222ca7c44951..95cab86c6b3e 100644
--- a/drivers/gpu/drm/i915/gt/selftest_execlists.c
+++ b/drivers/gpu/drm/i915/gt/selftest_execlists.c
@@ -754,6 +754,7 @@ static int live_error_interrupt(void *arg)
 					*cs++ = MI_NOOP;
 					*cs++ = MI_NOOP;
 				}
+				intel_ring_advance(rq, cs);
 
 				client[i] = i915_request_get(rq);
 				i915_request_add(rq);
diff --git a/drivers/gpu/drm/i915/gt/selftest_lrc.c b/drivers/gpu/drm/i915/gt/selftest_lrc.c
index e17b8777d21d..2c93c7e7515f 100644
--- a/drivers/gpu/drm/i915/gt/selftest_lrc.c
+++ b/drivers/gpu/drm/i915/gt/selftest_lrc.c
@@ -433,7 +433,7 @@ static int __live_lrc_state(struct intel_engine_cs *engine,
 		goto err_unpin;
 	}
 
-	cs = intel_ring_begin(rq, 4 * MAX_IDX);
+	cs = intel_ring_begin(rq, 4 * 2);
 	if (IS_ERR(cs)) {
 		err = PTR_ERR(cs);
 		i915_request_add(rq);
@@ -452,6 +452,8 @@ static int __live_lrc_state(struct intel_engine_cs *engine,
 	*cs++ = i915_ggtt_offset(scratch) + RING_TAIL_IDX * sizeof(u32);
 	*cs++ = 0;
 
+	intel_ring_advance(rq, cs);
+
 	err = i915_vma_move_to_active(scratch, rq, EXEC_OBJECT_WRITE);
 
 	i915_request_get(rq);
@@ -599,6 +601,8 @@ __gpr_read(struct intel_context *ce, struct i915_vma *scratch, u32 *slot)
 		*cs++ = 0;
 	}
 
+	intel_ring_advance(rq, cs);
+
 	err = igt_vma_move_to_active_unlocked(scratch, rq, EXEC_OBJECT_WRITE);
 
 	i915_request_get(rq);
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
index e44255c00f1d..c12746304e8d 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_abi.h
@@ -227,6 +227,7 @@ enum intel_guc_action {
 	INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC = 0x4601,
 	INTEL_GUC_ACTION_CLIENT_SOFT_RESET = 0x5507,
 	INTEL_GUC_ACTION_SET_ENG_UTIL_BUFF = 0x550A,
+	INTEL_GUC_ACTION_SET_DEVICE_ENGINE_UTILIZATION_V2 = 0x550C,
 	INTEL_GUC_ACTION_TLB_INVALIDATION = 0x7000,
 	INTEL_GUC_ACTION_TLB_INVALIDATION_DONE = 0x7001,
 	INTEL_GUC_ACTION_STATE_CAPTURE_NOTIFICATION = 0x8002,
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
index 93e4c504f45f..c457b62db2f0 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_actions_vf_abi.h
@@ -110,6 +110,44 @@
 #define VF2GUC_VF_RESET_RESPONSE_MSG_LEN		GUC_HXG_RESPONSE_MSG_MIN_LEN
 #define VF2GUC_VF_RESET_RESPONSE_MSG_0_MBZ		GUC_HXG_RESPONSE_MSG_0_DATA0
 
+/**
+ * DOC: VF2GUC_NOTIFY_RESFIX_DONE
+ *
+ * This action is used by VF to notify the GuC that the VF KMD has completed
+ * post-migration recovery steps.
+ *
+ * This message must be sent as `MMIO HXG Message`_.
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_HOST_                                |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_REQUEST_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 27:16 | DATA0 = MBZ                                                  |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  15:0 | ACTION = _`GUC_ACTION_VF2GUC_NOTIFY_RESFIX_DONE` = 0x5508    |
+ *  +---+-------+--------------------------------------------------------------+
+ *
+ *  +---+-------+--------------------------------------------------------------+
+ *  |   | Bits  | Description                                                  |
+ *  +===+=======+==============================================================+
+ *  | 0 |    31 | ORIGIN = GUC_HXG_ORIGIN_GUC_                                 |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   | 30:28 | TYPE = GUC_HXG_TYPE_RESPONSE_SUCCESS_                        |
+ *  |   +-------+--------------------------------------------------------------+
+ *  |   |  27:0 | DATA0 = MBZ                                                  |
+ *  +---+-------+--------------------------------------------------------------+
+ */
+#define GUC_ACTION_VF2GUC_NOTIFY_RESFIX_DONE		0x5508
+
+#define VF2GUC_NOTIFY_RESFIX_DONE_REQUEST_MSG_LEN	GUC_HXG_REQUEST_MSG_MIN_LEN
+#define VF2GUC_NOTIFY_RESFIX_DONE_REQUEST_MSG_0_MBZ	GUC_HXG_REQUEST_MSG_0_DATA0
+
+#define VF2GUC_NOTIFY_RESFIX_DONE_RESPONSE_MSG_LEN	GUC_HXG_RESPONSE_MSG_MIN_LEN
+#define VF2GUC_NOTIFY_RESFIX_DONE_RESPONSE_MSG_0_MBZ	GUC_HXG_RESPONSE_MSG_0_DATA0
+
 /**
  * DOC: VF2GUC_QUERY_SINGLE_KLV
  *
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
index 00d6402333f8..ae2b3c5bb438 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_errors_abi.h
@@ -9,6 +9,7 @@
 enum intel_guc_response_status {
 	INTEL_GUC_RESPONSE_STATUS_SUCCESS = 0x0,
 	INTEL_GUC_RESPONSE_NOT_SUPPORTED = 0x20,
+	INTEL_GUC_RESPONSE_VF_MIGRATED = 0x107,
 	INTEL_GUC_RESPONSE_NO_ATTRIBUTE_TABLE = 0x201,
 	INTEL_GUC_RESPONSE_NO_DECRYPTION_KEY = 0x202,
 	INTEL_GUC_RESPONSE_DECRYPTION_FAILED = 0x204,
diff --git a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
index 0ce5f0570e4c..77a2fc3d3acd 100644
--- a/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
+++ b/drivers/gpu/drm/i915/gt/uc/abi/guc_klvs_abi.h
@@ -185,6 +185,14 @@ enum {
  *      :0: no contexts (default)
  *      :1-65535: number of contexts (Gen12)
  *
+ * _`GUC_KLV_VF_CFG_TILE_MASK` : 0x0005
+ *      For multi-tiled products, this field contains the bitwise-OR of tiles
+ *      assigned to the VF. Bit-0-set means VF has access to Tile-0,
+ *      Bit-31-set means VF has access to Tile-31, and etc.
+ *      At least one tile will always be allocated.
+ *      If all bits are zero, VF KMD should treat this as a fatal error.
+ *      For, single-tile products this KLV config is ignored.
+ *
  * _`GUC_KLV_VF_CFG_NUM_DOORBELLS` : 0x0006
  *      Refers to the number of doorbells allocated to this VF.
  *
@@ -284,6 +292,9 @@ enum {
 #define GUC_KLV_VF_CFG_NUM_CONTEXTS_KEY		0x0004
 #define GUC_KLV_VF_CFG_NUM_CONTEXTS_LEN		1u
 
+#define GUC_KLV_VF_CFG_TILE_MASK_KEY		0x0005
+#define GUC_KLV_VF_CFG_TILE_MASK_LEN		1u
+
 #define GUC_KLV_VF_CFG_NUM_DOORBELLS_KEY	0x0006
 #define GUC_KLV_VF_CFG_NUM_DOORBELLS_LEN	1u
 
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.c b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
index 1a1c31a43ced..41a8f4e5d607 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.c
@@ -635,6 +635,13 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 		u32 hint = FIELD_GET(GUC_HXG_FAILURE_MSG_0_HINT, header);
 		u32 error = FIELD_GET(GUC_HXG_FAILURE_MSG_0_ERROR, header);
 
+		if (error == INTEL_GUC_RESPONSE_VF_MIGRATED) {
+			ret = intel_sriov_vf_migrated_event_handler(guc);
+			if (ret == -EAGAIN)
+				goto retry;
+			goto out;
+		}
+
 		guc_err(guc, "mmio request %#x: failure %x/%u\n",
 			request[0], error, hint);
 		ret = -ENXIO;
@@ -676,6 +683,7 @@ int intel_guc_send_mmio(struct intel_guc *guc, const u32 *request, u32 len,
 
 	return ret;
 }
+ALLOW_ERROR_INJECTION(intel_guc_send_mmio, ERRNO);
 
 int intel_guc_crash_process_msg(struct intel_guc *guc, u32 action)
 {
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc.h b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
index f06b2b7c9b0a..f20ad52da81e 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc.h
@@ -362,6 +362,17 @@ intel_guc_send_and_receive(struct intel_guc *guc, const u32 *action, u32 len,
 				 response_buf, response_buf_size, 0);
 }
 
+static inline void intel_guc_send_wait(unsigned int *sleep_period_us,
+					   bool not_atomic)
+{
+	if (likely(not_atomic)) {
+		usleep_range(*sleep_period_us, 2 * *sleep_period_us);
+		*sleep_period_us = min(*sleep_period_us << 1, 1000u);
+	} else {
+		cpu_relax();
+	}
+}
+
 static inline int intel_guc_send_busy_loop(struct intel_guc *guc,
 					   const u32 *action,
 					   u32 len,
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
index 140a89fae42f..f8d1edbff525 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.c
@@ -932,6 +932,7 @@ int intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,
 
 	return ret;
 }
+ALLOW_ERROR_INJECTION(intel_guc_ct_send, ERRNO);
 
 static struct ct_incoming_msg *ct_alloc_msg(u32 num_dwords)
 {
@@ -1453,6 +1454,142 @@ void intel_guc_ct_event_handler(struct intel_guc_ct *ct)
 	ct_try_receive_message(ct);
 }
 
+/*
+ * ct_update_addresses_in_message - Shift any GGTT addresses within
+ * a single message left within CTB from before post-migration recovery.
+ * @ct: pointer to CT struct of the target GuC
+ * @cmds: the buffer containing CT messages
+ * @head: start of the target message within the buffer
+ * @len: length of the target message
+ * @size: size of the commands buffer
+ * @shift: the address shift to be added to each GGTT reference
+ */
+static void ct_update_addresses_in_message(struct intel_guc_ct *ct,
+					    u32 *cmds, u32 head, u32 len,
+					    u32 size, s64 shift)
+{
+	u32 action, i, n;
+	u64 offset;
+
+#define msg(p) cmds[(head + (p)) % size]
+#define fixup64(p)                             \
+	offset = make_u64(msg(p+1), msg(p+0));  \
+	offset += shift;                        \
+	msg(p+0) = lower_32_bits(offset);       \
+	msg(p+1) = upper_32_bits(offset)
+
+	action = FIELD_GET(GUC_HXG_REQUEST_MSG_0_ACTION, msg(0));
+	switch (action) {
+	case INTEL_GUC_ACTION_SET_DEVICE_ENGINE_UTILIZATION_V2:
+		fixup64(1);
+		break;
+	case INTEL_GUC_ACTION_REGISTER_CONTEXT:
+	case INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC:
+		/* field wq_desc */
+		fixup64(5);
+		/* field wq_base */
+		fixup64(7);
+		if (action == INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC) {
+			/* field number_children */
+			n = msg(10);
+			/* field hwlrca and child lrcas */
+			for (i = 0; i < n; i++) {
+				fixup64(11 + 2 * i);
+			}
+		} else {
+			/* field hwlrca */
+			fixup64(10);
+		}
+		break;
+	default:
+		break;
+	}
+#undef fixup64
+#undef msg
+}
+
+static int ct_update_addresses_in_buffer(struct intel_guc_ct *ct,
+					 struct intel_guc_ct_buffer *ctb,
+					 s64 shift, u32 *mhead, s32 available)
+{
+	u32 head = *mhead;
+	u32 size = ctb->size;
+	u32 *cmds = ctb->cmds;
+	u32 header, len;
+
+	header = cmds[head];
+	head = (head + 1) % size;
+
+	/* message len with header */
+	len = FIELD_GET(GUC_CTB_MSG_0_NUM_DWORDS, header) + GUC_CTB_MSG_MIN_LEN;
+
+	if (unlikely(len > (u32)available)) {
+		CT_ERROR(ct, "Incomplete message %*ph %*ph %*ph\n",
+			 4, &header,
+			 4 * (head + available - 1 > size ?
+				size - head : available - 1), &cmds[head],
+			 4 * (head + available - 1 > size ?
+				available - 1 - size + head : 0), &cmds[0]);
+		return 0;
+	}
+	ct_update_addresses_in_message(ct, cmds, head, len - 1, size, shift);
+	*mhead = (head + len - 1) % size;
+
+	return available - len;
+}
+
+/**
+ * intel_guc_ct_update_addresses - Shifts any GGTT addresses left
+ * within CTB from before post-migration recovery.
+ * @ct: pointer to CT struct of the target GuC
+ */
+int intel_guc_ct_update_addresses(struct intel_guc_ct *ct)
+{
+	struct intel_guc *guc = ct_to_guc(ct);
+	struct intel_gt *gt = guc_to_gt(guc);
+	struct intel_guc_ct_buffer *ctb = &ct->ctbs.send;
+	struct guc_ct_buffer_desc *desc = ctb->desc;
+	u32 head = ctb->head;
+	u32 tail = READ_ONCE(desc->tail);
+	u32 size = ctb->size;
+	s32 available;
+	s64 ggtt_shift;
+
+	if (unlikely(ctb->broken))
+		return -EPIPE;
+
+	GEM_BUG_ON(head > size);
+
+	if (unlikely(tail >= size)) {
+		CT_ERROR(ct, "Invalid tail offset %u >= %u)\n",
+			 tail, size);
+		desc->status |= GUC_CTB_STATUS_OVERFLOW;
+		goto corrupted;
+	}
+
+	available = tail - head;
+
+	/* beware of buffer wrap case */
+	if (unlikely(available < 0))
+		available += size;
+	CT_DEBUG(ct, "available %d (%u:%u:%u)\n", available, head, tail, size);
+	GEM_BUG_ON(available < 0);
+
+	ggtt_shift = gt->iov.vf.config.ggtt_shift;
+
+	while (available > 0)
+		available = ct_update_addresses_in_buffer(ct, ctb, ggtt_shift, &head, available);
+
+	return 0;
+
+corrupted:
+	CT_ERROR(ct, "Corrupted descriptor head=%u tail=%u status=%#x\n",
+		 head, tail, desc->status);
+	ctb->broken = true;
+	CT_DEAD(ct, READ);
+	return -EPIPE;
+}
+
 /* FIXME: There is an known H2G loss issue that may be
  * caused by MTL cache coherence issue. This temporary WA will:
  * 1. Detect H2G loss and print to dmesg.
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
index fc74b1c8037f..8fa9579d4d80 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_ct.h
@@ -144,6 +144,8 @@ int intel_guc_ct_send(struct intel_guc_ct *ct, const u32 *action, u32 len,
 		      u32 *response_buf, u32 response_buf_size, u32 flags);
 void intel_guc_ct_event_handler(struct intel_guc_ct *ct);
 
+int intel_guc_ct_update_addresses(struct intel_guc_ct *ct);
+
 void intel_guc_ct_print_info(struct intel_guc_ct *ct, struct drm_printer *p);
 
 #endif /* _INTEL_GUC_CT_H_ */
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
index 9a4171ba3209..17b8c33e6038 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.c
@@ -1729,6 +1729,96 @@ void intel_guc_submission_reset_prepare(struct intel_guc *guc)
 	scrub_guc_desc_for_outstanding_g2h(guc);
 }
 
+static void guc_submission_refresh_request_ring_content(struct i915_request *rq)
+{
+	u32 rhead, remit, rspace;
+	int err;
+
+	if (!test_bit(I915_FENCE_FLAG_GGTT_EMITTED, &rq->fence.flags))
+		return;
+
+	/*
+	 * Pretend we have an empty, uninitialized request, being added at
+	 * end of the ring. This allows us to re-use the emit callbacks,
+	 * despite them being designed for exec only during request creation.
+	*/
+	rhead = rq->ring->head;
+	remit = rq->ring->emit;
+	rspace = rq->ring->space;
+	rq->ring->emit = get_init_breadcrumb_pos(rq);
+	rq->ring->head = rq->head;
+	intel_ring_update_space(rq->ring);
+	rq->reserved_space =
+		2 * rq->engine->emit_fini_breadcrumb_dw * sizeof(u32);
+
+	err = reemit_init_breadcrumb(rq);
+	if (err)
+		DRM_DEBUG_DRIVER("Request prefix ring content not recognized, fence %llx:%lld, err=%pe\n",
+				  rq->fence.context, rq->fence.seqno, ERR_PTR(err));
+
+	err = reemit_bb_start(rq);
+
+	if (err)
+		DRM_DEBUG_DRIVER("Request infix ring content not recognized, fence %llx:%lld, err=%pe\n",
+				  rq->fence.context, rq->fence.seqno, ERR_PTR(err));
+
+	rq->ring->head = rhead;
+	rq->ring->emit = remit;
+	rq->ring->space = rspace;
+	rq->reserved_space = 0;
+
+	if (test_bit(I915_FENCE_FLAG_ACTIVE, &rq->fence.flags))
+		rq->engine->emit_fini_breadcrumb(rq, rq->ring->vaddr + rq->postfix);
+}
+
+static void guc_submission_noop_request_ring_content(struct i915_request *rq)
+{
+	ring_range_emit_noop(rq->ring, rq->head, rq->tail);
+}
+
+void guc_submission_refresh_ctx_rings_content(struct intel_context *ce)
+{
+	struct intel_timeline *tl;
+	struct i915_request *rq;
+
+	if (unlikely(!test_bit(CONTEXT_ALLOC_BIT, &ce->flags)))
+		return;
+
+	tl = ce->timeline;
+
+	list_for_each_entry_rcu(rq, &tl->requests, link) {
+		if (i915_request_completed(rq))
+			guc_submission_noop_request_ring_content(rq);
+		else
+			guc_submission_refresh_request_ring_content(rq);
+	}
+}
+
+/**
+ * intel_guc_submission_pause - temporarily stop GuC submission mechanics
+ * @guc: intel_guc struct instance for the target tile
+ */
+void intel_guc_submission_pause(struct intel_guc *guc)
+{
+	struct i915_sched_engine * const sched_engine = guc->sched_engine;
+
+	tasklet_disable_nosync(&sched_engine->tasklet);
+}
+
+/**
+ * intel_guc_submission_restore - unpause GuC submission mechanics
+ * @guc: intel_guc struct instance for the target tile
+ */
+void intel_guc_submission_restore(struct intel_guc *guc)
+{
+	/*
+	 * If the submissions were only paused, there should be no need
+	 * to perform all the enabling operations; but since other threads
+	 * could have disabled the submissions fully, we need a full enable.
+	*/
+	enable_submission(guc);
+}
+
 static struct intel_engine_cs *
 guc_virtual_get_sibling(struct intel_engine_cs *ve, unsigned int sibling)
 {
@@ -1757,12 +1847,14 @@ __context_to_physical_engine(struct intel_context *ce)
 static void guc_reset_state(struct intel_context *ce, u32 head, bool scrub)
 {
 	struct intel_engine_cs *engine = __context_to_physical_engine(ce);
+	int srcu;
 
 	if (!intel_context_is_schedulable(ce))
 		return;
 
 	GEM_BUG_ON(!intel_context_is_pinned(ce));
 
+	gt_ggtt_address_read_lock(ce->engine->gt, &srcu);
 	/*
 	 * We want a simple context + ring to execute the breadcrumb update.
 	 * We cannot rely on the context being intact across the GPU hang,
@@ -1776,6 +1868,7 @@ static void guc_reset_state(struct intel_context *ce, u32 head, bool scrub)
 
 	/* Rerun the request; its payload has been neutered (if guilty). */
 	lrc_update_regs(ce, engine, head);
+	gt_ggtt_address_read_unlock(ce->engine->gt, srcu);
 }
 
 static void guc_engine_reset_prepare(struct intel_engine_cs *engine)
@@ -2520,33 +2613,36 @@ static int __guc_action_register_multi_lrc_v69(struct intel_guc *guc,
 	return guc_submission_send_busy_loop(guc, action, len, 0, loop);
 }
 
-static int __guc_action_register_multi_lrc_v70(struct intel_guc *guc,
-					       struct intel_context *ce,
-					       struct guc_ctxt_registration_info *info,
-					       bool loop)
+static void prepare_context_registration_info_v69(struct intel_context *ce);
+static void prepare_context_registration_info_v70(struct intel_context *ce,
+						  struct guc_ctxt_registration_info *info);
+
+static int __prepare_context_registration_action_multi_lrc_v70(struct intel_context *ce, u32 *action)
 {
+	struct guc_ctxt_registration_info info;
 	struct intel_context *child;
-	u32 action[13 + (MAX_ENGINE_INSTANCE * 2)];
 	int len = 0;
 	u32 next_id;
 
 	GEM_BUG_ON(ce->parallel.number_children > MAX_ENGINE_INSTANCE);
 
+	prepare_context_registration_info_v70(ce, &info);
+
 	action[len++] = INTEL_GUC_ACTION_REGISTER_CONTEXT_MULTI_LRC;
-	action[len++] = info->flags;
-	action[len++] = info->context_idx;
-	action[len++] = info->engine_class;
-	action[len++] = info->engine_submit_mask;
-	action[len++] = info->wq_desc_lo;
-	action[len++] = info->wq_desc_hi;
-	action[len++] = info->wq_base_lo;
-	action[len++] = info->wq_base_hi;
-	action[len++] = info->wq_size;
+	action[len++] = info.flags;
+	action[len++] = info.context_idx;
+	action[len++] = info.engine_class;
+	action[len++] = info.engine_submit_mask;
+	action[len++] = info.wq_desc_lo;
+	action[len++] = info.wq_desc_hi;
+	action[len++] = info.wq_base_lo;
+	action[len++] = info.wq_base_hi;
+	action[len++] = info.wq_size;
 	action[len++] = ce->parallel.number_children + 1;
-	action[len++] = info->hwlrca_lo;
-	action[len++] = info->hwlrca_hi;
+	action[len++] = info.hwlrca_lo;
+	action[len++] = info.hwlrca_hi;
 
-	next_id = info->context_idx + 1;
+	next_id = info.context_idx + 1;
 	for_each_child(ce, child) {
 		GEM_BUG_ON(next_id++ != child->guc_id.id);
 
@@ -2558,9 +2654,7 @@ static int __guc_action_register_multi_lrc_v70(struct intel_guc *guc,
 		action[len++] = upper_32_bits(child->lrc.lrca);
 	}
 
-	GEM_BUG_ON(len > ARRAY_SIZE(action));
-
-	return guc_submission_send_busy_loop(guc, action, len, 0, loop);
+	return len;
 }
 
 static int __guc_action_register_context_v69(struct intel_guc *guc,
@@ -2578,32 +2672,31 @@ static int __guc_action_register_context_v69(struct intel_guc *guc,
 					     0, loop);
 }
 
-static int __guc_action_register_context_v70(struct intel_guc *guc,
-					     struct guc_ctxt_registration_info *info,
-					     bool loop)
+
+static int __prepare_context_registration_action_single_v70(struct intel_context *ce, u32 *action)
 {
-	u32 action[] = {
-		INTEL_GUC_ACTION_REGISTER_CONTEXT,
-		info->flags,
-		info->context_idx,
-		info->engine_class,
-		info->engine_submit_mask,
-		info->wq_desc_lo,
-		info->wq_desc_hi,
-		info->wq_base_lo,
-		info->wq_base_hi,
-		info->wq_size,
-		info->hwlrca_lo,
-		info->hwlrca_hi,
-	};
+	struct guc_ctxt_registration_info info;
+	int len = 0;
 
-	return guc_submission_send_busy_loop(guc, action, ARRAY_SIZE(action),
-					     0, loop);
-}
+	GEM_BUG_ON(ce->parallel.number_children > MAX_ENGINE_INSTANCE);
 
-static void prepare_context_registration_info_v69(struct intel_context *ce);
-static void prepare_context_registration_info_v70(struct intel_context *ce,
-						  struct guc_ctxt_registration_info *info);
+	prepare_context_registration_info_v70(ce, &info);
+
+	action[len++] = INTEL_GUC_ACTION_REGISTER_CONTEXT;
+	action[len++] = info.flags;
+	action[len++] = info.context_idx;
+	action[len++] = info.engine_class;
+	action[len++] = info.engine_submit_mask;
+	action[len++] = info.wq_desc_lo;
+	action[len++] = info.wq_desc_hi;
+	action[len++] = info.wq_base_lo;
+	action[len++] = info.wq_base_hi;
+	action[len++] = info.wq_size;
+	action[len++] = info.hwlrca_lo;
+	action[len++] = info.hwlrca_hi;
+
+	return len;
+}
 
 static int
 register_context_v69(struct intel_guc *guc, struct intel_context *ce, bool loop)
@@ -2624,14 +2717,34 @@ register_context_v69(struct intel_guc *guc, struct intel_context *ce, bool loop)
 static int
 register_context_v70(struct intel_guc *guc, struct intel_context *ce, bool loop)
 {
-	struct guc_ctxt_registration_info info;
+	u32 action[13 + (MAX_ENGINE_INSTANCE * 2)];
+	bool not_atomic = !in_atomic() && !rcu_preempt_depth() && !irqs_disabled();
+	unsigned int sleep_period_us = 1;
+	int srcu, len, err;
 
-	prepare_context_registration_info_v70(ce, &info);
+	/* No sleeping with spin locks, just busy loop */
+	might_sleep_if(loop && not_atomic);
+
+retry:
+	err = gt_ggtt_address_read_lock_interruptible(guc_to_gt(guc), &srcu);
+	if (unlikely(err))
+		return err;
 
 	if (intel_context_is_parent(ce))
-		return __guc_action_register_multi_lrc_v70(guc, ce, &info, loop);
+		len = __prepare_context_registration_action_multi_lrc_v70(ce, action);
 	else
-		return __guc_action_register_context_v70(guc, &info, loop);
+		len = __prepare_context_registration_action_single_v70(ce, action);
+
+	GEM_BUG_ON(len > ARRAY_SIZE(action));
+
+	err = intel_guc_send_nb(guc, action, len, 0);
+	gt_ggtt_address_read_unlock(guc_to_gt(guc), srcu);
+	if (unlikely(err == -EBUSY && loop)) {
+		intel_guc_send_wait(&sleep_period_us, not_atomic);
+		goto retry;
+	}
+
+	return err;
 }
 
 static int register_context(struct intel_context *ce, bool loop)
@@ -3044,6 +3157,12 @@ static int __guc_context_pin(struct intel_context *ce,
 			     struct intel_engine_cs *engine,
 			     void *vaddr)
 {
+	int ret, srcu;
+
+	ret = gt_ggtt_address_read_lock_sync(engine->gt, &srcu);
+	if (unlikely(ret))
+		return ret;
+
 	if (i915_ggtt_offset(ce->state) !=
 	    (ce->lrc.lrca & CTX_GTT_ADDRESS_MASK))
 		set_bit(CONTEXT_LRCA_DIRTY, &ce->flags);
@@ -3052,8 +3171,10 @@ static int __guc_context_pin(struct intel_context *ce,
 	 * GuC context gets pinned in guc_request_alloc. See that function for
 	 * explaination of why.
 	 */
+	ret = lrc_pin(ce, engine, vaddr);
 
-	return lrc_pin(ce, engine, vaddr);
+	gt_ggtt_address_read_unlock(engine->gt, srcu);
+	return ret;
 }
 
 static int guc_context_pre_pin(struct intel_context *ce,
@@ -5724,12 +5845,13 @@ static int emit_bb_start_parent_no_preempt_mid_batch(struct i915_request *rq,
 						     const unsigned int flags)
 {
 	struct intel_context *ce = rq->context;
+	int srcu;
 	u32 *cs;
 	u8 i;
 
 	GEM_BUG_ON(!intel_context_is_parent(ce));
 
-	cs = intel_ring_begin(rq, 10 + 4 * ce->parallel.number_children);
+	cs = intel_ring_begin_ggtt(rq, &srcu, 10 + 4 * ce->parallel.number_children);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
@@ -5761,7 +5883,7 @@ static int emit_bb_start_parent_no_preempt_mid_batch(struct i915_request *rq,
 	*cs++ = upper_32_bits(offset);
 	*cs++ = MI_NOOP;
 
-	intel_ring_advance(rq, cs);
+	intel_ring_advance_ggtt(rq, srcu, cs);
 
 	return 0;
 }
@@ -5772,11 +5894,12 @@ static int emit_bb_start_child_no_preempt_mid_batch(struct i915_request *rq,
 {
 	struct intel_context *ce = rq->context;
 	struct intel_context *parent = intel_context_to_parent(ce);
+	int srcu;
 	u32 *cs;
 
 	GEM_BUG_ON(!intel_context_is_child(ce));
 
-	cs = intel_ring_begin(rq, 12);
+	cs = intel_ring_begin_ggtt(rq, &srcu, 12);
 	if (IS_ERR(cs))
 		return PTR_ERR(cs);
 
@@ -5805,7 +5928,7 @@ static int emit_bb_start_child_no_preempt_mid_batch(struct i915_request *rq,
 	*cs++ = lower_32_bits(offset);
 	*cs++ = upper_32_bits(offset);
 
-	intel_ring_advance(rq, cs);
+	intel_ring_advance_ggtt(rq, srcu, cs);
 
 	return 0;
 }
@@ -5867,9 +5990,12 @@ emit_fini_breadcrumb_parent_no_preempt_mid_batch(struct i915_request *rq,
 	struct intel_context *ce = rq->context;
 	__maybe_unused u32 *before_fini_breadcrumb_user_interrupt_cs;
 	__maybe_unused u32 *start_fini_breadcrumb_cs = cs;
+	int srcu;
 
 	GEM_BUG_ON(!intel_context_is_parent(ce));
 
+	intel_ring_fini_begin_ggtt(rq, &srcu);
+
 	if (unlikely(skip_handshake(rq))) {
 		/*
 		 * NOP everything in __emit_fini_breadcrumb_parent_no_preempt_mid_batch,
@@ -5899,7 +6025,7 @@ emit_fini_breadcrumb_parent_no_preempt_mid_batch(struct i915_request *rq,
 	GEM_BUG_ON(start_fini_breadcrumb_cs +
 		   ce->engine->emit_fini_breadcrumb_dw != cs);
 
-	rq->tail = intel_ring_offset(rq, cs);
+	intel_ring_fini_advance_ggtt(rq, srcu, cs);
 
 	return cs;
 }
@@ -5943,9 +6069,12 @@ emit_fini_breadcrumb_child_no_preempt_mid_batch(struct i915_request *rq,
 	struct intel_context *ce = rq->context;
 	__maybe_unused u32 *before_fini_breadcrumb_user_interrupt_cs;
 	__maybe_unused u32 *start_fini_breadcrumb_cs = cs;
+	int srcu;
 
 	GEM_BUG_ON(!intel_context_is_child(ce));
 
+	intel_ring_fini_begin_ggtt(rq, &srcu);
+
 	if (unlikely(skip_handshake(rq))) {
 		/*
 		 * NOP everything in __emit_fini_breadcrumb_child_no_preempt_mid_batch,
@@ -5975,7 +6104,7 @@ emit_fini_breadcrumb_child_no_preempt_mid_batch(struct i915_request *rq,
 	GEM_BUG_ON(start_fini_breadcrumb_cs +
 		   ce->engine->emit_fini_breadcrumb_dw != cs);
 
-	rq->tail = intel_ring_offset(rq, cs);
+	intel_ring_fini_advance_ggtt(rq, srcu, cs);
 
 	return cs;
 }
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
index 2e99a645b892..6b9cfc6e4228 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
+++ b/drivers/gpu/drm/i915/gt/uc/intel_guc_submission.h
@@ -18,6 +18,8 @@ int intel_guc_submission_limit_ids(struct intel_guc *guc, u32 limit);
 int intel_guc_submission_init(struct intel_guc *guc);
 int intel_guc_submission_enable(struct intel_guc *guc);
 void intel_guc_submission_disable(struct intel_guc *guc);
+void intel_guc_submission_pause(struct intel_guc *guc);
+void intel_guc_submission_restore(struct intel_guc *guc);
 void intel_guc_submission_fini(struct intel_guc *guc);
 int intel_guc_preempt_work_create(struct intel_guc *guc);
 void intel_guc_preempt_work_destroy(struct intel_guc *guc);
@@ -26,6 +28,7 @@ void intel_guc_submission_print_info(struct intel_guc *guc,
 				     struct drm_printer *p);
 void intel_guc_submission_print_context_info(struct intel_guc *guc,
 					     struct drm_printer *p);
+void guc_submission_refresh_ctx_rings_content(struct intel_context *ce);
 void intel_guc_dump_active_requests(struct intel_engine_cs *engine,
 				    struct i915_request *hung_rq,
 				    struct drm_printer *m);
diff --git a/drivers/gpu/drm/i915/gt/uc/intel_uc.c b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
index ef33c2e7bf39..1e96a353bf56 100644
--- a/drivers/gpu/drm/i915/gt/uc/intel_uc.c
+++ b/drivers/gpu/drm/i915/gt/uc/intel_uc.c
@@ -227,7 +227,7 @@ static int guc_enable_communication(struct intel_guc *guc)
 {
 	struct intel_gt *gt = guc_to_gt(guc);
 	struct drm_i915_private *i915 = gt->i915;
-	int ret;
+	int srcu, ret;
 
 	GEM_BUG_ON(intel_guc_ct_enabled(&guc->ct));
 
@@ -235,7 +235,11 @@ static int guc_enable_communication(struct intel_guc *guc)
 	if (ret)
 		return ret;
 
+	ret = gt_ggtt_address_read_lock_sync(gt, &srcu);
+	if (unlikely(ret))
+		return ret;
 	ret = intel_guc_ct_enable(&guc->ct);
+	gt_ggtt_address_read_unlock(gt, srcu);
 	if (ret)
 		return ret;
 
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index 4f7f095e42e8..63babd19fa9e 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -216,6 +216,8 @@ struct drm_i915_private {
 	struct intel_uncore uncore;
 	struct intel_uncore_mmio_debug mmio_debug;
 
+	unsigned int remote_tiles;
+
 	struct i915_sriov sriov;
 	struct i915_virtual_gpu vgpu;
 
@@ -727,6 +729,7 @@ IS_SUBPLATFORM(const struct drm_i915_private *i915,
 	(INTEL_INFO(i915)->has_oa_slice_contrib_limits)
 #define HAS_OAM(i915) \
 	(INTEL_INFO(i915)->has_oam)
+#define HAS_REMOTE_TILES(dev_priv)   (INTEL_INFO(dev_priv)->has_remote_tiles)
 
 /*
  * Set this flag, when platform requires 64K GTT page sizes or larger for
@@ -745,7 +748,8 @@ IS_SUBPLATFORM(const struct drm_i915_private *i915,
 
 #define HAS_MEMORY_IRQ(dev_priv) (INTEL_INFO(dev_priv)->has_memirq)
 
-#define HAS_MEMORY_IRQ_STATUS(dev_priv) (HAS_MEMORY_IRQ(dev_priv) && IS_SRIOV_VF(dev_priv))
+#define HAS_MEMORY_IRQ_STATUS(dev_priv) \
+	(INTEL_INFO(dev_priv)->has_iov_memirq && IS_SRIOV_VF(dev_priv))
 
 /*
  * Platform has the dedicated compression control state for each lmem surfaces
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index 7a625aeda76d..8243033ec7ed 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -140,6 +140,13 @@ i915_param_named_unsafe(request_timeout_ms, uint, 0600,
 			"Default request/fence/batch buffer expiration timeout.");
 #endif
 
+#if IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV)
+i915_param_named_unsafe(vfs_flr_mask, ulong, 0600,
+	"Bitmask to enable (1) or disable (0) cleaning by PF VF's resources "
+	"(GGTT and LMEM) after FLR (default: ~0 - cleaning enable for all VFs) "
+	"Bit number indicates VF number, e.g. bit 1 indicates VF1");
+#endif
+
 i915_param_named_unsafe(lmem_size, uint, 0400,
 			"Set the lmem size(in MiB) for each region. (default: 0, all memory)");
 i915_param_named_unsafe(lmem_bar_size, uint, 0400,
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index 6adc1cb8ed0c..1a53250ac2a5 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -62,6 +62,7 @@ struct drm_printer;
 	param(unsigned int, lmem_size, 0, 0400) \
 	param(unsigned int, lmem_bar_size, 0, 0400) \
 	param(unsigned int, max_vfs, 0, 0400) \
+	param(unsigned long, vfs_flr_mask, ~0, IS_ENABLED(CONFIG_DRM_I915_DEBUG_IOV) ? 0600 : 0) \
 	param(int, force_disable_ccs, 0, 0400) \
 	/* leave bools at the end to not create holes */ \
 	param(bool, enable_mtl_rcs_ccs_wa, true, 0x400) \
diff --git a/drivers/gpu/drm/i915/i915_pci.c b/drivers/gpu/drm/i915/i915_pci.c
index b5562d86eb07..8aff711aa887 100644
--- a/drivers/gpu/drm/i915/i915_pci.c
+++ b/drivers/gpu/drm/i915/i915_pci.c
@@ -728,6 +728,10 @@ static const struct intel_device_info adl_p_info = {
 	.__runtime.ppgtt_size = 48, \
 	.__runtime.ppgtt_type = INTEL_PPGTT_FULL
 
+#define REMOTE_TILE_FEATURES \
+	.has_remote_tiles = 1, \
+	.memory_regions = (REGION_SMEM | REGION_STOLEN | REGION_LMEM)
+
 #define DG2_FEATURES \
 	XE_HP_FEATURES, \
 	DGFX_FEATURES, \
@@ -780,6 +784,7 @@ static const struct intel_device_info mtl_info = {
 	.has_flat_ccs = 0,
 	.has_gmd_id = 1,
 	.has_guc_deprivilege = 1,
+	.has_iov_memirq = 1,
 	.has_guc_tlb_invalidation = 1,
 	.has_llc = 0,
 	.has_memirq = 1,
diff --git a/drivers/gpu/drm/i915/i915_perf.c b/drivers/gpu/drm/i915/i915_perf.c
index eb9bca7289a4..09c641189a05 100644
--- a/drivers/gpu/drm/i915/i915_perf.c
+++ b/drivers/gpu/drm/i915/i915_perf.c
@@ -1321,6 +1321,9 @@ __store_reg_to_mem(struct i915_request *rq, i915_reg_t reg, u32 ggtt_offset)
 {
 	u32 *cs, cmd;
 
+	/* GGTT address cannot be transferred unlocked on VF */
+	GEM_BUG_ON(IS_SRIOV_VF(rq->i915));
+
 	cmd = MI_STORE_REGISTER_MEM | MI_SRM_LRM_GLOBAL_GTT;
 	if (GRAPHICS_VER(rq->i915) >= 8)
 		cmd++;
diff --git a/drivers/gpu/drm/i915/i915_request.c b/drivers/gpu/drm/i915/i915_request.c
index 2dd4595936f9..a416ffa93fdf 100644
--- a/drivers/gpu/drm/i915/i915_request.c
+++ b/drivers/gpu/drm/i915/i915_request.c
@@ -551,6 +551,8 @@ static bool fatal_error(int error)
 
 void __i915_request_skip(struct i915_request *rq)
 {
+	int srcu;
+
 	GEM_BUG_ON(!fatal_error(rq->fence.error));
 
 	if (rq->infix == rq->postfix)
@@ -558,6 +560,7 @@ void __i915_request_skip(struct i915_request *rq)
 
 	RQ_TRACE(rq, "error: %d\n", rq->fence.error);
 
+	gt_ggtt_address_read_lock(rq->engine->gt, &srcu);
 	/*
 	 * As this request likely depends on state from the lost
 	 * context, clear out all the user operations leaving the
@@ -565,6 +568,8 @@ void __i915_request_skip(struct i915_request *rq)
 	 */
 	__i915_request_fill(rq, 0);
 	rq->infix = rq->postfix;
+	set_bit(I915_FENCE_FLAG_GGTT_EMITTED, &rq->fence.flags);
+	gt_ggtt_address_read_unlock(rq->engine->gt, srcu);
 }
 
 bool i915_request_set_error_once(struct i915_request *rq, int error)
@@ -888,6 +893,18 @@ static void __i915_request_ctor(void *arg)
 	init_llist_head(&rq->execute_cb);
 }
 
+static int wait_for_space(struct i915_request *rq)
+{
+	void *ptr;
+
+	ptr = intel_ring_begin(rq, 0);
+	if (IS_ERR(ptr))
+		return PTR_ERR(ptr);
+	intel_ring_advance(rq, ptr);
+
+	return 0;
+}
+
 #if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
 #define clear_batch_ptr(_rq) ((_rq)->batch = NULL)
 #else
@@ -995,6 +1012,10 @@ __i915_request_create(struct intel_context *ce, gfp_t gfp)
 	rq->reserved_space =
 		2 * rq->engine->emit_fini_breadcrumb_dw * sizeof(u32);
 
+	ret = wait_for_space(rq);
+	if (unlikely(ret))
+		goto err_free;
+
 	/*
 	 * Record the position of the start of the request so that
 	 * should we detect the updated seqno part-way through the
@@ -1155,6 +1176,7 @@ __emit_semaphore_wait(struct i915_request *to,
 
 	GEM_BUG_ON(GRAPHICS_VER(to->engine->i915) < 8);
 	GEM_BUG_ON(i915_request_has_initial_breadcrumb(to));
+	GEM_BUG_ON(IS_SRIOV_VF(to->i915));
 
 	/* We need to pin the signaler's HWSP until we are finished reading. */
 	err = intel_timeline_read_hwsp(from, to, &hwsp_offset);
@@ -1813,6 +1835,8 @@ struct i915_request *__i915_request_commit(struct i915_request *rq)
 	cs = intel_ring_begin(rq, engine->emit_fini_breadcrumb_dw);
 	GEM_BUG_ON(IS_ERR(cs));
 	rq->postfix = intel_ring_offset(rq, cs);
+	/* postfix commands are not emitted yet, but the space is reserved */
+	intel_ring_advance(rq, cs + engine->emit_fini_breadcrumb_dw);
 
 	return __i915_request_add_to_timeline(rq);
 }
diff --git a/drivers/gpu/drm/i915/i915_request.h b/drivers/gpu/drm/i915/i915_request.h
index 0ac55b2e4223..7e7f12be06ef 100644
--- a/drivers/gpu/drm/i915/i915_request.h
+++ b/drivers/gpu/drm/i915/i915_request.h
@@ -170,6 +170,29 @@ enum {
 	 * fence (dma_fence_array) and i915 generated for parallel submission.
 	 */
 	I915_FENCE_FLAG_COMPOSITE,
+
+	/*
+	 * I915_FENCE_FLAG_LR - This fence represents a request on long running
+	 * (LR) context. Can't wait on this under a reservation object, and
+	 * can't wait in reclaim. This fence doesn't signal until the LR request
+	 * is done, and is thus different from a preempt fence.
+	 */
+	I915_FENCE_FLAG_LR,
+
+	/*
+	 * I915_FENCE_FLAG_GGTT_EMITTED - The request represented by this fence
+	 * has emitted at least one packet of commands to the ring which contained
+	 * GGTT address reference. This flag indicates that there may be GGTT
+	 * address references within the ring area associated to this request.
+	 * Only command packets which are used on SRIOV VF execution are obligated
+	 * to be mared with this flag.
+	 */
+	I915_FENCE_FLAG_GGTT_EMITTED,
+
+	I915_FENCE_FLAG_UFENCE,
+
+	__I915_FENCE_FLAG_LAST__
+
 };
 
 /*
@@ -296,6 +319,9 @@ struct i915_request {
 	/* Position in the ring of the end of any workarounds after the tail */
 	u32 wa_tail;
 
+	/** Position in the ring of the end of last packet of emitted commands */
+	u32 advance;
+
 	/* Preallocate space in the ring for the emitting the request */
 	u32 reserved_space;
 
diff --git a/drivers/gpu/drm/i915/i915_sriov.c b/drivers/gpu/drm/i915/i915_sriov.c
index b4f62c400541..6305c655f10a 100644
--- a/drivers/gpu/drm/i915/i915_sriov.c
+++ b/drivers/gpu/drm/i915/i915_sriov.c
@@ -8,14 +8,21 @@
 #include "i915_sriov.h"
 #include "i915_sriov_sysfs.h"
 #include "i915_drv.h"
+#include "i915_irq.h"
 #include "i915_pci.h"
 #include "i915_utils.h"
 #include "i915_reg.h"
 #include "intel_pci_config.h"
+#include "gem/i915_gem_context.h"
+#include "gem/i915_gem_pm.h"
 
+#include "gt/intel_engine_heartbeat.h"
 #include "gt/intel_gt.h"
 #include "gt/intel_gt_pm.h"
+#include "gt/intel_lrc.h"
+#include "gt/iov/intel_iov_migration.h"
 #include "gt/iov/intel_iov_provisioning.h"
+#include "gt/iov/intel_iov_query.h"
 #include "gt/iov/intel_iov_service.h"
 #include "gt/iov/intel_iov_reg.h"
 #include "gt/iov/intel_iov_state.h"
@@ -23,6 +30,90 @@
 
 #include "pxp/intel_pxp.h"
 
+/**
+ * DOC: VM Migration with SR-IOV
+ *
+ * Most VMM applications allow to save state of a VM, and restore it
+ * at different time or on another machine. To allow proper migration of a
+ * VM which configuration includes directly attached VF device, we need to
+ * assure that VF state is part of the VM image being migrated.
+ *
+ * Storing complete state of any hardware is hard. Doing so in a manner
+ * which allows restoring back such state is even harder. Since the migrated
+ * VF state might contain configuration or provisioning which was specific
+ * to the source machine, we need to do proper re-initialization of VF
+ * device on the target machine. This initialization is done within
+ * `VF Post-migration worker`_.
+ */
+
+/**
+ * DOC: VF Post-migration worker
+ *
+ * After `VM Migration with SR-IOV`_, i915 ends up running on a new VF device
+ * which had it GuC state restored. While the platform model and memory sizes
+ * assigned to this new VF must match the previous, address of Global GTT chunk
+ * assigned to the new VF might be different. Both GuC and VF KMD are expected
+ * to update the GGTT references in the objects they own.
+ *
+ * The new GuC informs the VF driver that migration just happened, by triggering
+ * MIGRATED interrupt. After that, GuC enters a state where it waits for the VF
+ * KMD to perform all the necessary fixups. Communication with the GuC is limited
+ * at that point, allowing only a few MMIO commands. CTB communication is not
+ * working, because GuC is not allowed to read any messages from H2G CT buffer.
+ *
+ * On receiving the MIGRATED IRQ, VF KMD schedules post-migration worker. The
+ * worker makes sure it is executed at most once per migration, by limiting its
+ * operations in case it was scheduled again before finishing.  Normal work of
+ * GuC is restored only after VF KMD sends RESFIX_DONE or RESET message to the
+ * GuC, of which the latter is used in exceptional flow only.
+ *
+ * The post-migration worker has two main goals:
+ *
+ * * Update driver state to prepare work on a new hardware (treated as new
+ *   even if the VM got restored at the place where it worked before).
+ *
+ * * Provide users with seamless experience in terms of GPU execution (no failed
+ *   kernel calls nor corrupted buffers).
+ *
+ * To achieve these goals, the following operations need to be performed:
+ *
+ * * Get new provisioning information from GuC. While count of the provisioned
+ *   resources must match the previous VM instance, the start point might be
+ *   different, and for non-virtualized ones that is significant.
+ *
+ * * Apply fixups to prepare work on new ranges of non-virtualized resources.
+ *   This really only concerns Global GTT, as it only has one address space
+ *   shared between PF and all VFs.
+ *
+ * * Update state information which depended on the previous hardware and is no
+ *   longer fully valid. This currently only concerns references to the old GGTT
+ *   address range within context state and on the context ring.
+ *
+ * * Prevent any kernel workers from trying to use resources before fixups, as
+ *   that would propagate references which are no longer valid, or interfere with
+ *   the applying of fixups. These workers operate as separate threads, so they
+ *   could try to access various driver structures before they are ready.
+ *
+ * * Provide seamless switch for the user space, by honoring all the requests
+ *   from before the finalization of post-migration recovery process.
+ *
+ * The post-migration worker performs the operations above in proper order to
+ * ensure safe transition. First it does a shutdown of some driver operations
+ * to avoid waiting for any locks taken there. Then it does handshake for
+ * `GuC MMIO based communication`_, and receives new provisioning data through
+ * that channel. With the new GGTT range taken from provisioning, the worker
+ * rebases 'Virtual Memory Address'_ structures used for tracking GGTT allocations,
+ * by shifting addresses of the underlying `drm_mm`_ nodes to range newly
+ * assigned to this VF. Similar adjustments are then applied to places where
+ * address from these nodes was stored. These are hardware states of contexts,
+ * commands emited on rings linked to these contexts, and messages expected to be
+ * sent to GuC via H2G CT buffer. After the fixups are applied, a message to GuC
+ * is sent confirming that everything is ready to continue GPU execution. The
+ * previously stopped VF driver operations are then kickstarted. If there are
+ * any requests which were preempted while pausing, they are re-submitted by
+ * the tasklet soon after post-migration worker ends.
+ */
+
 #if IS_ENABLED(CONFIG_DRM_I915_DEBUG)
 /* XXX: can't use drm_WARN() as we are still using preliminary IP versions at few locations */
 void assert_graphics_ip_ver_ready(const struct drm_i915_private *i915)
@@ -177,6 +268,13 @@ enum i915_iov_mode i915_sriov_probe(struct drm_i915_private *i915)
 	return I915_IOV_MODE_NONE;
 }
 
+static void migration_worker_func(struct work_struct *w);
+
+static void vf_init_early(struct drm_i915_private *i915)
+{
+	INIT_WORK(&i915->sriov.vf.migration_worker, migration_worker_func);
+}
+
 static int vf_check_guc_submission_support(struct drm_i915_private *i915)
 {
 	if (!intel_guc_submission_is_wanted(&to_gt(i915)->uc.guc)) {
@@ -218,6 +316,7 @@ int i915_sriov_early_tweaks(struct drm_i915_private *i915)
 	int err;
 
 	if (IS_SRIOV_VF(i915)) {
+		vf_init_early(i915);
 		err = vf_check_guc_submission_support(i915);
 		if (unlikely(err))
 			return err;
@@ -1190,7 +1289,7 @@ sriov_to_gt(struct pci_dev *pdev, unsigned int tile)
  *
  * Return: Size in bytes.
  */
-size_t
+ssize_t
 i915_sriov_ggtt_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
 {
 	struct intel_gt *gt;
@@ -1269,6 +1368,64 @@ i915_sriov_ggtt_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
 }
 EXPORT_SYMBOL_NS_GPL(i915_sriov_ggtt_load, I915_SRIOV_NS);
 
+static struct intel_iov *sriov_save_restore_get_iov_or_error(struct pci_dev *pdev, unsigned int id)
+{
+	struct intel_gt *gt;
+
+	gt = sriov_to_gt(pdev, id);
+	if (!gt)
+		return  ERR_PTR(-ENODEV);
+
+	if (!guc_supports_save_restore_v2(&gt->uc.guc)) {
+		IOV_ERROR(&gt->iov, "No save/restore support in loaded GuC FW\n");
+		return  ERR_PTR(-EOPNOTSUPP);
+	}
+
+	return &gt->iov;
+}
+
+ssize_t
+i915_sriov_mmio_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+		     void *buf, size_t size)
+{
+	struct intel_iov *iov;
+
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
+
+	WARN_ON(buf == NULL && size == 0);
+
+	return intel_iov_state_save_mmio(iov, vfid, buf, size);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_mmio_save, I915_SRIOV_NS);
+
+int i915_sriov_mmio_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			 const void *buf, size_t size)
+{
+	struct intel_iov *iov;
+
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
+
+	return intel_iov_state_restore_mmio(iov, vfid, buf, size);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_mmio_load, I915_SRIOV_NS);
+
+ssize_t
+i915_sriov_mmio_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
+{
+	struct intel_iov *iov;
+
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
+
+	return intel_iov_state_save_mmio_size(iov, vfid);
+}
+EXPORT_SYMBOL_NS_GPL(i915_sriov_mmio_size, I915_SRIOV_NS);
+
 /**
  * i915_sriov_fw_state_size - Get size needed to store GuC FW state.
  * @pdev: PF pci device
@@ -1279,17 +1436,17 @@ EXPORT_SYMBOL_NS_GPL(i915_sriov_ggtt_load, I915_SRIOV_NS);
  *
  * Return: size in bytes on success or a negative error code on failure.
  */
-size_t
+ssize_t
 i915_sriov_fw_state_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
 {
-	struct intel_gt *gt;
+	struct intel_iov *iov;
 	int ret;
 
-	gt = sriov_to_gt(pdev, tile);
-	if (!gt)
-		return -ENODEV;
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
 
-	ret = intel_iov_state_save_vf_size(&gt->iov, vfid);
+	ret = intel_iov_state_save_vf_size(iov, vfid);
 
 	return ret;
 }
@@ -1311,14 +1468,14 @@ ssize_t
 i915_sriov_fw_state_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
 			 void *buf, size_t size)
 {
-	struct intel_gt *gt;
+	struct intel_iov *iov;
 	int ret;
 
-	gt = sriov_to_gt(pdev, tile);
-	if (!gt)
-		return -ENODEV;
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
 
-	ret = intel_iov_state_save_vf(&gt->iov, vfid, buf, size);
+	ret = intel_iov_state_save_vf(iov, vfid, buf, size);
 
 	return ret;
 }
@@ -1340,13 +1497,13 @@ int
 i915_sriov_fw_state_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
 			 const void *buf, size_t size)
 {
-	struct intel_gt *gt;
+	struct intel_iov *iov;
 
-	gt = sriov_to_gt(pdev, tile);
-	if (!gt)
-		return -ENODEV;
+	iov = sriov_save_restore_get_iov_or_error(pdev, tile);
+	if (IS_ERR(iov))
+		return PTR_ERR(iov);
 
-	return intel_iov_state_store_guc_migration_state(&gt->iov, vfid, buf, size);
+	return intel_iov_state_store_guc_migration_state(iov, vfid, buf, size);
 }
 EXPORT_SYMBOL_NS_GPL(i915_sriov_fw_state_load, I915_SRIOV_NS);
 
@@ -1442,3 +1599,420 @@ int i915_sriov_resume(struct drm_i915_private *i915)
 
 	return 0;
 }
+
+static void intel_gt_default_contexts_ring_restore(struct intel_gt *gt)
+{
+	struct intel_context *ce;
+
+	list_for_each_entry(ce, &gt->pinned_contexts, pinned_contexts_link) {
+	if (!ce)
+		continue;
+
+	if (!ce->timeline)
+		continue;
+
+	guc_submission_refresh_ctx_rings_content(ce);
+	}
+}
+
+static void user_contexts_ring_restore(struct drm_i915_private *i915)
+{
+	struct i915_gem_context *ctx;
+
+	spin_lock_irq(&i915->gem.contexts.lock);
+	rcu_read_lock();
+	list_for_each_entry_rcu(ctx, &i915->gem.contexts.list, link) {
+		struct i915_gem_engines_iter it;
+		struct intel_context *ce;
+
+		if (!kref_get_unless_zero(&ctx->ref))
+			continue;
+		spin_unlock_irq(&i915->gem.contexts.lock);
+
+		for_each_gem_engine(ce, rcu_dereference(ctx->engines), it) {
+			guc_submission_refresh_ctx_rings_content(ce);
+		}
+
+		spin_lock_irq(&i915->gem.contexts.lock);
+		i915_gem_context_put(ctx);
+	}
+	rcu_read_unlock();
+	spin_unlock_irq(&i915->gem.contexts.lock);
+}
+
+static void user_contexts_hwsp_rebase(struct drm_i915_private *i915)
+{
+	struct i915_gem_context *ctx;
+
+	spin_lock_irq(&i915->gem.contexts.lock);
+	rcu_read_lock();
+	list_for_each_entry_rcu(ctx, &i915->gem.contexts.list, link) {
+		struct i915_gem_engines_iter it;
+		struct intel_context *ce;
+
+		if (!kref_get_unless_zero(&ctx->ref))
+			continue;
+		spin_unlock_irq(&i915->gem.contexts.lock);
+
+		for_each_gem_engine(ce, rcu_dereference(ctx->engines), it) {
+			if (intel_context_is_pinned(ce)) {
+				intel_timeline_rebase_hwsp(ce->timeline);
+				lrc_update_regs_with_address(ce);
+			}
+		}
+
+		spin_lock_irq(&i915->gem.contexts.lock);
+		i915_gem_context_put(ctx);
+	}
+	rcu_read_unlock();
+	spin_unlock_irq(&i915->gem.contexts.lock);
+}
+
+static void intel_gt_default_contexts_hwsp_rebase(struct intel_gt *gt)
+{
+	struct intel_context *ce;
+
+	list_for_each_entry(ce, &gt->pinned_contexts, pinned_contexts_link) {
+		if (intel_context_is_pinned(ce)) {
+			intel_timeline_rebase_hwsp(ce->timeline);
+			lrc_update_regs_with_address(ce);
+		}
+	}
+}
+
+static void vf_post_migration_fixup_contexts(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id) {
+	if (!gt)
+		continue;
+
+	if (!gt->pinned_contexts.next || !gt->pinned_contexts.prev)
+		continue;
+
+	if (list_empty(&gt->pinned_contexts))
+		continue;
+
+	intel_gt_default_contexts_hwsp_rebase(gt);
+	intel_gt_default_contexts_ring_restore(gt);
+	}
+
+	user_contexts_hwsp_rebase(i915);
+	user_contexts_ring_restore(i915);
+}
+
+static void vf_post_migration_fixup_ctb(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_guc_ct_update_addresses(&gt->uc.guc.ct);
+}
+
+static void heartbeats_disable(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_gt_heartbeats_disable(gt);
+}
+
+static void heartbeats_restore(struct drm_i915_private *i915, bool unpark)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_gt_heartbeats_restore(gt, unpark);
+}
+
+/**
+ * submissions_disable - Turn off advancing with execution of scheduled submissions.
+ * @i915: the i915 struct
+ *
+ * When the hardware is not ready to accept submissions, continuing to push
+ * the scheduled requests would only lead to a series of errors, and aborting
+ * requests which could be successfully executed if submitted after the pipeline
+ * is back to ready state.
+ */
+static void submissions_disable(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_guc_submission_pause(&gt->uc.guc);
+}
+
+/**
+ * submissions_restore - Re-enable advancing with execution of scheduled submissions.
+ * @i915: the i915 struct
+ *
+ * We possibly unwinded some requests which did not finished before migration; now
+ * we can allow these requests to be re-submitted.
+ */
+static void submissions_restore(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_guc_submission_restore(&gt->uc.guc);
+}
+
+/**
+ * vf_post_migration_shutdown - Stop the driver activities after VF migration.
+ * @i915: the i915 struct instance
+ *
+ * After this VM is migrated and assigned to a new VF, it is running on a new
+ * hardware, and therefore many hardware-dependent states and related structures
+ * require fixups. Without fixups, the hardware cannot do any work, and therefore
+ * all GPU pipelines are stalled.
+ * Stop some of kernel acivities to make the fixup process faster.
+ */
+static void vf_post_migration_shutdown(struct drm_i915_private *i915)
+{
+	heartbeats_disable(i915);
+	submissions_disable(i915);
+}
+
+/**
+ * vf_post_migration_reset_guc_state - Reset GuC state.
+ * @i915: the i915 struct
+ *
+ * This function sends VF state reset to GuC, as a way of exiting RESFIX
+ * state if a proper post-migration recovery procedure has failed.
+ */
+static void vf_post_migration_reset_guc_state(struct drm_i915_private *i915)
+{
+	intel_wakeref_t wakeref;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id)
+			__intel_gt_reset(gt, ALL_ENGINES);
+	}
+	drm_notice(&i915->drm, "VF migration recovery reset sent\n");
+}
+
+static bool vf_post_migration_is_scheduled(struct drm_i915_private *i915)
+{
+	return work_pending(&i915->sriov.vf.migration_worker);
+}
+
+static int vf_post_migration_reinit_guc(struct drm_i915_private *i915)
+{
+	intel_wakeref_t wakeref;
+	int err;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id) {
+			err = intel_iov_migration_reinit_guc(&gt->iov);
+			if (unlikely(err))
+				break;
+		}
+	}
+	return err;
+}
+
+static void vf_post_migration_fixup_ggtt_nodes(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id) {
+		/* media doesn't have its own ggtt */
+		if (gt->type == GT_MEDIA)
+			continue;
+		intel_iov_migration_fixup_ggtt_nodes(&gt->iov);
+	}
+}
+
+/*
+ * vf_post_migration_notify_resfix_done - Notify all GuCs about resource fixups apply finished.
+ * @i915: i915 device instance struct
+ */
+static void vf_post_migration_notify_resfix_done(struct drm_i915_private *i915)
+{
+	intel_wakeref_t wakeref;
+
+	with_intel_runtime_pm(&i915->runtime_pm, wakeref) {
+		struct intel_gt *gt;
+		unsigned int id;
+
+		for_each_gt(gt, i915, id)
+			intel_iov_notify_resfix_done(&gt->iov);
+	}
+	drm_dbg(&i915->drm, "VF resource fixups done notification sent\n");
+}
+
+/**
+ * vf_post_migration_kickstart - Re-start the driver activities under new hardware.
+ * @i915: the i915 struct instance
+ *
+ * After we have finished with all post-migration fixups, restart the driver
+ * activities to continue feeding the GPU with workloads.
+ */
+static void vf_post_migration_kickstart(struct drm_i915_private *i915)
+{
+	intel_runtime_pm_enable_interrupts(i915);
+	submissions_restore(i915);
+	heartbeats_restore(i915, true);
+}
+
+static void i915_reset_backoff_enter(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	/* Raise flag for any other resets to back off and resign. */
+	for_each_gt(gt, i915, id)
+		intel_gt_reset_backoff_raise(gt);
+
+	/* Make sure intel_gt_reset_trylock() sees the I915_RESET_BACKOFF. */
+	synchronize_rcu_expedited();
+
+	/*
+	 * Wait for any operations already in progress which state could be
+	 * skewed by post-migration actions.
+	 */
+	for_each_gt(gt, i915, id)
+		synchronize_srcu_expedited(&gt->reset.backoff_srcu);
+}
+
+static void i915_reset_backoff_leave(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id)
+		intel_gt_reset_backoff_clear(gt);
+}
+
+static void vf_post_migration_recovery(struct drm_i915_private *i915)
+{
+	int err;
+
+	i915_reset_backoff_enter(i915);
+
+	drm_dbg(&i915->drm, "migration recovery in progress\n");
+	vf_post_migration_shutdown(i915);
+
+	if (vf_post_migration_is_scheduled(i915))
+		goto defer;
+	i915_ggtt_address_write_lock(i915);
+	err = vf_post_migration_reinit_guc(i915);
+	if (unlikely(err))
+		goto fail;
+
+	vf_post_migration_fixup_ggtt_nodes(i915);
+	vf_post_migration_fixup_contexts(i915);
+	vf_post_migration_fixup_ctb(i915);
+
+	if (!vf_post_migration_is_scheduled(i915)) {
+		vf_post_migration_notify_resfix_done(i915);
+		i915_ggtt_address_write_unlock(i915);
+	}
+	vf_post_migration_kickstart(i915);
+	i915_reset_backoff_leave(i915);
+	drm_notice(&i915->drm, "migration recovery completed\n");
+	return;
+
+defer:
+	drm_dbg(&i915->drm, "migration recovery deferred\n");
+	/* We bumped wakerefs when disabling heartbeat. Put them back. */
+	heartbeats_restore(i915, false);
+	i915_reset_backoff_leave(i915);
+	return;
+
+fail:
+	drm_err(&i915->drm, "migration recovery failed (%pe)\n", ERR_PTR(err));
+	intel_gt_set_wedged(to_gt(i915));
+	if (!vf_post_migration_is_scheduled(i915))
+		i915_ggtt_address_write_unlock(i915);
+	i915_reset_backoff_leave(i915);
+}
+
+static void migration_worker_func(struct work_struct *w)
+{
+	struct drm_i915_private *i915 = container_of(w, struct drm_i915_private,
+						     sriov.vf.migration_worker);
+
+	vf_post_migration_recovery(i915);
+}
+
+/**
+ * i915_sriov_vf_start_migration_recovery - Start VF migration recovery.
+ * @i915: the i915 struct
+ *
+ * This function shall be called only by VF.
+ */
+void i915_sriov_vf_start_migration_recovery(struct drm_i915_private *i915)
+{
+	bool started;
+
+	GEM_BUG_ON(!IS_SRIOV_VF(i915));
+
+	WRITE_ONCE(i915->sriov.vf.migration_gt_flags, 0);
+	smp_mb();
+
+	started = queue_work(system_unbound_wq, &i915->sriov.vf.migration_worker);
+	dev_info(i915->drm.dev, "VF migration recovery %s\n", started ?
+		 "scheduled" : "already in progress");
+}
+
+/**
+ * i915_sriov_current_is_vf_migration_recovery - returns if current worker is the
+ *   VF post-migration recovery worker
+ * @i915: the i915 struct instance
+ * Return: True if the current cpu context is the post-migration recovery worker
+ */
+bool i915_sriov_current_is_vf_migration_recovery(struct drm_i915_private *i915)
+{
+	return current_work() == &i915->sriov.vf.migration_worker;
+}
+
+static bool vf_ready_to_recovery_on_all_tiles(struct drm_i915_private *i915)
+{
+	struct intel_gt *gt;
+	unsigned int id;
+
+	for_each_gt(gt, i915, id) {
+		if (!test_bit(id, &i915->sriov.vf.migration_gt_flags))
+			return false;
+	}
+	return true;
+}
+
+int intel_sriov_vf_migrated_event_handler(struct intel_guc *guc)
+{
+	struct intel_gt *gt = guc_to_gt(guc);
+	struct drm_i915_private *i915 = gt->uncore->i915;
+
+	if (!guc->submission_initialized) {
+		/*
+		 * If at driver init, ignore migration which happened
+		 * before the driver was loaded.
+		 */
+		vf_post_migration_reset_guc_state(i915);
+		return -EAGAIN;
+	}
+
+	set_bit(gt->info.id, &i915->sriov.vf.migration_gt_flags);
+	smp_mb__after_atomic();
+	dev_info(i915->drm.dev, "VF migration recovery ready on gt%d\n",
+		 gt->info.id);
+	if (vf_ready_to_recovery_on_all_tiles(i915))
+		i915_sriov_vf_start_migration_recovery(i915);
+
+	return -EREMOTEIO;
+}
diff --git a/drivers/gpu/drm/i915/i915_sriov.h b/drivers/gpu/drm/i915/i915_sriov.h
index fdeae8c0e6ea..76d7b84b8247 100644
--- a/drivers/gpu/drm/i915/i915_sriov.h
+++ b/drivers/gpu/drm/i915/i915_sriov.h
@@ -53,4 +53,9 @@ static inline void assert_graphics_ip_ver_ready(const struct drm_i915_private *i
 static inline void assert_media_ip_ver_ready(const struct drm_i915_private *i915) { }
 #endif
 
+/* VF only */
+void i915_sriov_vf_start_migration_recovery(struct drm_i915_private *i915);
+int intel_sriov_vf_migrated_event_handler(struct intel_guc *guc);
+bool i915_sriov_current_is_vf_migration_recovery(struct drm_i915_private *i915);
+
 #endif /* __I915_SRIOV_H__ */
diff --git a/drivers/gpu/drm/i915/i915_sriov_types.h b/drivers/gpu/drm/i915/i915_sriov_types.h
index c58a4a5723a6..635e137e8af5 100644
--- a/drivers/gpu/drm/i915/i915_sriov_types.h
+++ b/drivers/gpu/drm/i915/i915_sriov_types.h
@@ -31,12 +31,26 @@ struct i915_sriov_pf {
 	bool disable_auto_provisioning;
 };
 
+/**
+ * struct i915_sriov_vf - i915 SR-IOV VF data.
+ */
+struct i915_sriov_vf {
+
+	/** @migration_worker: migration recovery worker */
+	struct work_struct migration_worker;
+	unsigned long migration_gt_flags;
+};
+
 /**
  * struct i915_sriov - i915 SR-IOV data.
  * @pf: PF only data.
  */
 struct i915_sriov {
-	struct i915_sriov_pf pf;
+	union {
+		struct i915_sriov_pf pf;
+		struct i915_sriov_vf vf;
+	};
+
 };
 
 #endif /* __I915_SRIOV_TYPES_H__ */
diff --git a/drivers/gpu/drm/i915/intel_device_info.h b/drivers/gpu/drm/i915/intel_device_info.h
index 4afb40711e05..736b63362601 100644
--- a/drivers/gpu/drm/i915/intel_device_info.h
+++ b/drivers/gpu/drm/i915/intel_device_info.h
@@ -157,6 +157,7 @@ enum intel_ppgtt_type {
 	func(has_heci_gscfi); \
 	func(has_guc_deprivilege); \
 	func(has_guc_tlb_invalidation); \
+	func(has_iov_memirq); \
 	func(has_l3_ccs_read); \
 	func(has_l3_dpf); \
 	func(has_llc); \
@@ -179,7 +180,8 @@ enum intel_ppgtt_type {
 	func(has_coherent_ggtt); \
 	func(tuning_thread_rr_after_dep); \
 	func(unfenced_needs_alignment); \
-	func(hws_needs_physical);
+	func(hws_needs_physical); \
+	func(has_remote_tiles);
 
 struct intel_ip_version {
 	u8 ver;
diff --git a/drivers/gpu/drm/i915/intel_pcode.c b/drivers/gpu/drm/i915/intel_pcode.c
index 3db2ba439bb5..de4632c55514 100644
--- a/drivers/gpu/drm/i915/intel_pcode.c
+++ b/drivers/gpu/drm/i915/intel_pcode.c
@@ -224,7 +224,7 @@ int intel_pcode_init(struct intel_uncore *uncore)
 {
 	int err;
 
-	if (!IS_DGFX(uncore->i915))
+	if (!IS_DGFX(uncore->i915) || IS_SRIOV_VF(uncore->i915))
 		return 0;
 
 	/*
diff --git a/drivers/gpu/drm/i915/selftests/mock_gtt.c b/drivers/gpu/drm/i915/selftests/mock_gtt.c
index a516c0aa88fd..a42dc1aa230b 100644
--- a/drivers/gpu/drm/i915/selftests/mock_gtt.c
+++ b/drivers/gpu/drm/i915/selftests/mock_gtt.c
@@ -108,6 +108,8 @@ void mock_init_ggtt(struct intel_gt *gt)
 {
 	struct i915_ggtt *ggtt = gt->ggtt;
 
+	i915_ggtt_address_lock_init(ggtt);
+
 	ggtt->vm.gt = gt;
 	ggtt->vm.i915 = gt->i915;
 	ggtt->vm.is_ggtt = true;
@@ -133,4 +135,5 @@ void mock_init_ggtt(struct intel_gt *gt)
 void mock_fini_ggtt(struct i915_ggtt *ggtt)
 {
 	i915_address_space_fini(&ggtt->vm);
+	i915_ggtt_address_lock_fini(ggtt);
 }
diff --git a/drivers/vfio/pci/i915/i915_vfio_pci.h b/drivers/vfio/pci/i915/i915_vfio_pci.h
index 483fc3945afd..0daa17ea9e41 100644
--- a/drivers/vfio/pci/i915/i915_vfio_pci.h
+++ b/drivers/vfio/pci/i915/i915_vfio_pci.h
@@ -9,6 +9,7 @@
 #include <drm/i915_sriov.h>
 
 #define I915_VFIO_MAX_DATA_SIZE SZ_32M
+#define I915_VFIO_MIGRATION_DATA_SIZE SZ_1M
 #define I915_VFIO_MAX_TILE 2
 
 /**
@@ -80,13 +81,13 @@ struct i915_vfio_pci_core_device {
 };
 
 struct i915_vfio_pci_mappable_resource_ops {
-	size_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+	ssize_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
 	void * (*map)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
 	void (*unmap)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
 };
 
 struct i915_vfio_pci_resource_ops {
-	size_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
+	ssize_t (*size)(struct pci_dev *pf, unsigned int vfid, unsigned int tile);
 	ssize_t (*save)(struct pci_dev *pf, unsigned int vfid, unsigned int tile,
 			void *buf, size_t size);
 	int (*load)(struct pci_dev *pf, unsigned int vfid, unsigned int tile,
diff --git a/drivers/vfio/pci/i915/main.c b/drivers/vfio/pci/i915/main.c
index 5ca058952c1a..b56f9b1f47b1 100644
--- a/drivers/vfio/pci/i915/main.c
+++ b/drivers/vfio/pci/i915/main.c
@@ -291,6 +291,7 @@ static int i915_vfio_pci_get_device_state(struct vfio_device *core_vdev,
 static int i915_vfio_pci_get_data_size(struct vfio_device *vdev,
 				       unsigned long *stop_copy_length)
 {
+	*stop_copy_length = I915_VFIO_MIGRATION_DATA_SIZE;
 	return 0;
 }
 
@@ -398,4 +399,4 @@ module_pci_driver(i915_vfio_pci_driver);
 MODULE_LICENSE("GPL");
 MODULE_AUTHOR("Intel Corporation");
 MODULE_DESCRIPTION("VFIO PCI driver with migration support for Intel Graphics");
-MODULE_IMPORT_NS(DRM_I915);
+MODULE_IMPORT_NS(I915_SRIOV_NS);
diff --git a/drivers/vfio/pci/i915/test/data_test.c b/drivers/vfio/pci/i915/test/data_test.c
index 13150801572e..59e50913a67c 100644
--- a/drivers/vfio/pci/i915/test/data_test.c
+++ b/drivers/vfio/pci/i915/test/data_test.c
@@ -20,7 +20,7 @@ struct i915_vfio_pci_data_test {
 	struct i915_vfio_pci_core_device *i915_vdev;
 };
 
-static size_t
+static ssize_t
 i915_vfio_test_res_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile)
 {
 	struct i915_vfio_pci_data_test *priv = pci_get_drvdata(pdev);
diff --git a/include/drm/drm_mm.h b/include/drm/drm_mm.h
index f654874c4ce6..2f5d7d8b5f7f 100644
--- a/include/drm/drm_mm.h
+++ b/include/drm/drm_mm.h
@@ -504,6 +504,25 @@ __drm_mm_interval_first(const struct drm_mm *mm, u64 start, u64 last);
 	     node__->start < (end__);					\
 	     node__ = list_next_entry(node__, node_list))
 
+/**
+ * drm_mm_for_each_node_in_range_safe - iterator to walk over a range of
+ * allocated nodes
+ * @node__: drm_mm_node structure to assign to in each iteration step
+ * @next__: &struct drm_mm_node to store the next step
+ * @mm__: drm_mm allocator to walk
+ * @start__: starting offset, the first node will overlap this
+ * @end__: ending offset, the last node will start before this (but may overlap)
+ *
+ * This iterator walks over all nodes in the range allocator that lie
+ * between @start and @end. It is implemented similarly to list_for_each_safe(),
+ * so save against removal of elements.
+ */
+#define drm_mm_for_each_node_in_range_safe(node__, next__, mm__, start__, end__)	\
+	for (node__ = __drm_mm_interval_first((mm__), (start__), (end__)-1), \
+		next__ = list_next_entry(node__, node_list); \
+	     node__->start < (end__);					\
+	     node__ = next__, next__ = list_next_entry(next__, node_list))
+
 void drm_mm_scan_init_with_range(struct drm_mm_scan *scan,
 				 struct drm_mm *mm,
 				 u64 size, u64 alignment, unsigned long color,
diff --git a/include/drm/i915_sriov.h b/include/drm/i915_sriov.h
index 2912a60a16a1..e7475772a5ef 100644
--- a/include/drm/i915_sriov.h
+++ b/include/drm/i915_sriov.h
@@ -8,7 +8,7 @@ int i915_sriov_resume_vf(struct pci_dev *pdev, unsigned int vfid);
 
 int i915_sriov_wait_vf_flr_done(struct pci_dev *pdev, unsigned int vfid);
 
-size_t
+ssize_t
 i915_sriov_ggtt_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
 ssize_t i915_sriov_ggtt_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
 			     void *buf, size_t size);
@@ -16,7 +16,14 @@ int
 i915_sriov_ggtt_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
 		     const void *buf, size_t size);
 
-size_t
+ssize_t
+i915_sriov_mmio_size(struct pci_dev *pdev, unsigned int vfid, unsigned int tile);
+ssize_t i915_sriov_mmio_save(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+			     void *buf, size_t size);
+int
+i915_sriov_mmio_load(struct pci_dev *pdev, unsigned int vfid, unsigned int tile,
+		     const void *buf, size_t size);
+ssize_t
 i915_sriov_fw_state_size(struct pci_dev *pdev, unsigned int vfid,
 			 unsigned int tile);
 ssize_t
-- 
2.34.1

