From 94463f95ff1c561791465643502f5f0cb38bfc2e Mon Sep 17 00:00:00 2001
From: Adrian Hunter <adrian.hunter@intel.com>
Date: Tue, 26 Feb 2019 14:49:18 +0200
Subject: [PATCH 03/65] scsi: ufs: Add UME support

Add support for UFS Unified Memory Extension.

This patch assumes memory is allocated which means using CMA e.g.

	Kernel config:
		CONFIG_CMA
		CONFIG_DMA_CMA

	Kernel command line:
		cma=128M@4G

Signed-off-by: Adrian Hunter <adrian.hunter@intel.com>
---
 drivers/scsi/ufs/ufs-sysfs.c |   2 +-
 drivers/scsi/ufs/ufs.h       |  17 ++-
 drivers/scsi/ufs/ufshcd.c    | 251 +++++++++++++++++++++++++++++++++--
 drivers/scsi/ufs/ufshcd.h    |  10 ++
 drivers/scsi/ufs/ufshci.h    |  13 ++
 5 files changed, 281 insertions(+), 12 deletions(-)

diff --git a/drivers/scsi/ufs/ufs-sysfs.c b/drivers/scsi/ufs/ufs-sysfs.c
index 969a36b15897..0926b1c0c3f7 100644
--- a/drivers/scsi/ufs/ufs-sysfs.c
+++ b/drivers/scsi/ufs/ufs-sysfs.c
@@ -129,7 +129,7 @@ static void ufshcd_auto_hibern8_update(struct ufs_hba *hba, u32 ahit)
 	if (hba->ahit == ahit)
 		goto out_unlock;
 	hba->ahit = ahit;
-	if (!pm_runtime_suspended(hba->dev))
+	if (!pm_runtime_suspended(hba->dev) && !hba->ahit_disabled)
 		ufshcd_writel(hba, hba->ahit, REG_AUTO_HIBERNATE_IDLE_TIMER);
 out_unlock:
 	spin_unlock_irqrestore(hba->host->host_lock, flags);
diff --git a/drivers/scsi/ufs/ufs.h b/drivers/scsi/ufs/ufs.h
index 3327981ef894..ed54eac4e6d7 100644
--- a/drivers/scsi/ufs/ufs.h
+++ b/drivers/scsi/ufs/ufs.h
@@ -139,8 +139,10 @@ enum flag_idn {
 	QUERY_FLAG_IDN_RESERVED2			= 0x07,
 	QUERY_FLAG_IDN_FPHYRESOURCEREMOVAL		= 0x08,
 	QUERY_FLAG_IDN_BUSY_RTC				= 0x09,
-	QUERY_FLAG_IDN_RESERVED3			= 0x0A,
+	QUERY_FLAG_IDN_UNIFIED_MEMORY			= 0x0A,
 	QUERY_FLAG_IDN_PERMANENTLY_DISABLE_FW_UPDATE	= 0x0B,
+	QUERY_FLAG_IDN_SUSPEND_UM			= 0x0C,
+	QUERY_FLAG_IDN_UM_SUSPENDED			= 0x0D,
 };
 
 /* Attribute idn for Query requests */
@@ -163,8 +165,8 @@ enum attr_idn {
 	QUERY_ATTR_IDN_SECONDS_PASSED		= 0x0F,
 	QUERY_ATTR_IDN_CNTX_CONF		= 0x10,
 	QUERY_ATTR_IDN_CORR_PRG_BLK_NUM		= 0x11,
-	QUERY_ATTR_IDN_RESERVED2		= 0x12,
-	QUERY_ATTR_IDN_RESERVED3		= 0x13,
+	QUERY_ATTR_IDN_UM_AREA_SIZE		= 0x12,
+	QUERY_ATTR_IDN_MAX_UMPIU_REQS		= 0x13,
 	QUERY_ATTR_IDN_FFU_STATUS		= 0x14,
 	QUERY_ATTR_IDN_PSA_STATE		= 0x15,
 	QUERY_ATTR_IDN_PSA_DATA_SIZE		= 0x16,
@@ -258,8 +260,13 @@ enum device_desc_param {
 	DEVICE_DESC_PARAM_PSA_MAX_DATA		= 0x25,
 	DEVICE_DESC_PARAM_PSA_TMT		= 0x29,
 	DEVICE_DESC_PARAM_PRDCT_REV		= 0x2A,
+	DEVICE_DESC_PARAM_MIN_UMA_SZ		= 0x31,
 };
 
+#define UFS_DEVICE_SUB_CLASS_NONBOOTABLE	0x01
+#define UFS_DEVICE_SUB_CLASS_REMOVABLE		0x02
+#define UFS_DEVICE_SUB_CLASS_UM_SUPPORT		0x04
+
 /* Interconnect descriptor parameters offsets in bytes*/
 enum interconnect_desc_param {
 	INTERCONNECT_DESC_PARAM_LEN		= 0x0,
@@ -536,10 +543,14 @@ struct ufs_dev_info {
 /**
  * ufs_dev_desc - ufs device details from the device descriptor
  *
+ * @subclass: device subclass
+ * @min_uma_sz: minimum UM area size
  * @wmanufacturerid: card details
  * @model: card model
  */
 struct ufs_dev_desc {
+	u8 subclass;
+	u32 min_uma_sz;
 	u16 wmanufacturerid;
 	u8 *model;
 };
diff --git a/drivers/scsi/ufs/ufshcd.c b/drivers/scsi/ufs/ufshcd.c
index 11a87f51c442..7789b6f9ad1b 100644
--- a/drivers/scsi/ufs/ufshcd.c
+++ b/drivers/scsi/ufs/ufshcd.c
@@ -403,7 +403,13 @@ static void ufshcd_print_err_hist(struct ufs_hba *hba,
 
 static void ufshcd_print_host_regs(struct ufs_hba *hba)
 {
-	ufshcd_dump_regs(hba, 0, UFSHCI_REG_SPACE_SIZE, "host_regs: ");
+	unsigned int sz;
+
+	sz = hba->capabilities & MASK_DEVICE_BUS_MASTER_MODE_SUPPORT ?
+		UFSHCI_UMA_REG_SPACE_SIZE :
+		UFSHCI_REG_SPACE_SIZE;
+
+	ufshcd_dump_regs(hba, 0, sz, "host_regs: ");
 	dev_err(hba->dev, "hba->ufs_version = 0x%x, hba->capabilities = 0x%x\n",
 		hba->ufs_version, hba->capabilities);
 	dev_err(hba->dev,
@@ -2839,11 +2845,13 @@ int ufshcd_query_flag(struct ufs_hba *hba, enum query_opcode opcode,
  * @index: index field
  * @selector: selector field
  * @attr_val: the attribute value after the query request completes
+ * @quiet: suppress error message
  *
  * Returns 0 for success, non-zero in case of failure
 */
-int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
-		      enum attr_idn idn, u8 index, u8 selector, u32 *attr_val)
+int __ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
+			enum attr_idn idn, u8 index, u8 selector, u32 *attr_val,
+			bool quiet)
 {
 	struct ufs_query_req *request = NULL;
 	struct ufs_query_res *response = NULL;
@@ -2881,7 +2889,8 @@ int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
 	err = ufshcd_exec_dev_cmd(hba, DEV_CMD_TYPE_QUERY, QUERY_REQ_TIMEOUT);
 
 	if (err) {
-		dev_err(hba->dev, "%s: opcode 0x%.2x for idn %d failed, index %d, err = %d\n",
+		if (!quiet)
+			dev_err(hba->dev, "%s: opcode 0x%.2x for idn %d failed, index %d, err = %d\n",
 				__func__, opcode, idn, index, err);
 		goto out_unlock;
 	}
@@ -2895,6 +2904,13 @@ int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
 	return err;
 }
 
+int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
+		      enum attr_idn idn, u8 index, u8 selector, u32 *attr_val)
+{
+	return __ufshcd_query_attr(hba, opcode, idn, index, selector, attr_val,
+				   false);
+}
+
 /**
  * ufshcd_query_attr_retry() - API function for sending query
  * attribute with retries
@@ -3871,6 +3887,39 @@ static int ufshcd_link_recovery(struct ufs_hba *hba)
 	return ret;
 }
 
+static int ufshcd_uma_suspend(struct ufs_hba *hba)
+{
+	bool flag_res = 0;
+	int ret;
+	int i;
+
+	ret = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_SET_FLAG,
+				      QUERY_FLAG_IDN_SUSPEND_UM, NULL);
+
+	/* Poll for max. 1000 iterations for fUMSuspended flag to set */
+	for (i = 0; i < 1000 && !ret && !flag_res; i++)
+		ret = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_READ_FLAG,
+					      QUERY_FLAG_IDN_UM_SUSPENDED, &flag_res);
+	if (!ret && !flag_res)
+		ret = -ETIMEDOUT;
+	if (ret)
+		dev_err(hba->dev, "UMA suspend failed. ret = %d\n", ret);
+
+	return ret;
+}
+
+static int ufshcd_uma_unsuspend(struct ufs_hba *hba)
+{
+	int ret;
+
+	ret = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_CLEAR_FLAG,
+				      QUERY_FLAG_IDN_SUSPEND_UM, NULL);
+	if (ret)
+		dev_err(hba->dev, "UMA unsuspend failed. ret = %d\n", ret);
+
+	return ret;
+}
+
 static int __ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
 {
 	int ret;
@@ -3904,6 +3953,15 @@ static int __ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
 static int ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
 {
 	int ret = 0, retries;
+	bool uma_suspended = false;
+
+	/* UMA must be suspended prior to hibernate */
+	if (hba->uma && ufshcd_is_ufs_dev_active(hba)) {
+		ret = ufshcd_uma_suspend(hba);
+		if (ret)
+			return ret;
+		uma_suspended = true;
+	}
 
 	for (retries = UIC_HIBERN8_ENTER_RETRIES; retries > 0; retries--) {
 		ret = __ufshcd_uic_hibern8_enter(hba);
@@ -3911,6 +3969,9 @@ static int ufshcd_uic_hibern8_enter(struct ufs_hba *hba)
 			goto out;
 	}
 out:
+	if (ret && ret != -ENOLINK && uma_suspended)
+		ufshcd_uma_unsuspend(hba);
+
 	return ret;
 }
 
@@ -3931,28 +3992,50 @@ static int ufshcd_uic_hibern8_exit(struct ufs_hba *hba)
 		dev_err(hba->dev, "%s: hibern8 exit failed. ret = %d\n",
 			__func__, ret);
 		ret = ufshcd_link_recovery(hba);
+
+		if (!ret && hba->uma && ufshcd_is_ufs_dev_active(hba))
+			ufshcd_uma_unsuspend(hba);
 	} else {
 		ufshcd_vops_hibern8_notify(hba, UIC_CMD_DME_HIBER_EXIT,
 								POST_CHANGE);
 		hba->ufs_stats.last_hibern8_exit_tstamp = ktime_get();
 		hba->ufs_stats.hibern8_exit_cnt++;
+
+		/* UMA is suspended prior to hibernate and unsuspended here */
+		if (hba->uma && ufshcd_is_ufs_dev_active(hba))
+			ufshcd_uma_unsuspend(hba);
 	}
 
 	return ret;
 }
 
-static void ufshcd_auto_hibern8_enable(struct ufs_hba *hba)
+static void __ufshcd_auto_hibern8_disable(struct ufs_hba *hba, bool disable)
 {
 	unsigned long flags;
+	u32 val;
 
-	if (!ufshcd_is_auto_hibern8_supported(hba) || !hba->ahit)
+	if (!ufshcd_is_auto_hibern8_supported(hba))
 		return;
 
 	spin_lock_irqsave(hba->host->host_lock, flags);
-	ufshcd_writel(hba, hba->ahit, REG_AUTO_HIBERNATE_IDLE_TIMER);
+	hba->ahit_disabled = disable;
+	if (hba->ahit) {
+		val = disable ? 0 : hba->ahit;
+		ufshcd_writel(hba, val, REG_AUTO_HIBERNATE_IDLE_TIMER);
+	}
 	spin_unlock_irqrestore(hba->host->host_lock, flags);
 }
 
+static void ufshcd_auto_hibern8_enable(struct ufs_hba *hba)
+{
+	__ufshcd_auto_hibern8_disable(hba, false);
+}
+
+static void ufshcd_auto_hibern8_disable(struct ufs_hba *hba)
+{
+	__ufshcd_auto_hibern8_disable(hba, true);
+}
+
  /**
  * ufshcd_init_pwr_info - setting the POR (power on reset)
  * values in hba power info
@@ -6499,13 +6582,14 @@ static int ufs_get_device_desc(struct ufs_hba *hba,
 	size_t buff_len;
 	u8 model_index;
 	u8 *desc_buf;
+	__be32 val;
 
 	if (!dev_desc)
 		return -EINVAL;
 
 	buff_len = max_t(size_t, hba->desc_size.dev_desc,
 			 QUERY_DESC_MAX_SIZE + 1);
-	desc_buf = kmalloc(buff_len, GFP_KERNEL);
+	desc_buf = kzalloc(buff_len, GFP_KERNEL);
 	if (!desc_buf) {
 		err = -ENOMEM;
 		goto out;
@@ -6518,6 +6602,11 @@ static int ufs_get_device_desc(struct ufs_hba *hba,
 		goto out;
 	}
 
+	dev_desc->subclass = desc_buf[DEVICE_DESC_PARAM_DEVICE_SUB_CLASS];
+
+	memcpy(&val, desc_buf + DEVICE_DESC_PARAM_MIN_UMA_SZ, sizeof(val));
+	dev_desc->min_uma_sz = be32_to_cpu(val);
+
 	/*
 	 * getting vendor (manufacturerID) and Bank Index in big endian
 	 * format
@@ -6855,6 +6944,129 @@ static int ufshcd_set_dev_ref_clk(struct ufs_hba *hba)
 	return err;
 }
 
+static int __ufshcd_uma_reenable(struct ufs_hba *hba, bool init_dev)
+{
+	u32 max_umpiu_reqs;
+	u32 config;
+	int err;
+
+	config = ufshcd_readl(hba, REG_UMA_CONFIG);
+
+	if (!(config & UFSHCI_UMA_ENABLE)) {
+		/* Set UMA address and size for the host controller */
+		ufshcd_writel(hba, lower_32_bits(hba->uma_addr),
+			      REG_UMA_BASE_ADDR_L);
+		ufshcd_writel(hba, upper_32_bits(hba->uma_addr),
+			      REG_UMA_BASE_ADDR_H);
+		ufshcd_writel(hba, hba->uma_size, REG_UMA_OFFSET_MAX);
+	}
+
+	if (init_dev) {
+		u32 retries;
+
+		/*
+		 * Set UMA size for the device. This is valid only after a power
+		 * cycle or hardware reset. Here it is assumed that if it has
+		 * already been written, then it has the correct value.
+		 */
+		for (retries = QUERY_REQ_RETRIES; retries > 0; retries--) {
+			err = __ufshcd_query_attr(hba,
+				UPIU_QUERY_OPCODE_WRITE_ATTR,
+				QUERY_ATTR_IDN_UM_AREA_SIZE, 0, 0,
+				&hba->uma_size, true);
+			if (err == QUERY_RESULT_ALREADY_WRITTEN) {
+				err = 0;
+				break;
+			}
+		}
+		if (err)
+			return err;
+
+		/* Set max. UMPIU requests */
+		max_umpiu_reqs = ufshcd_readl(hba, REG_UMA_CAP) &
+				 UFSHCI_UMA_MNOOUR;
+		err = ufshcd_query_attr_retry(hba, UPIU_QUERY_OPCODE_WRITE_ATTR,
+					      QUERY_ATTR_IDN_MAX_UMPIU_REQS, 0,
+					      0, &max_umpiu_reqs);
+		if (err)
+			return err;
+	}
+
+	/* Enable UMA for the host controller */
+	if (!(config & UFSHCI_UMA_ENABLE)) {
+		config |= UFSHCI_UMA_ENABLE;
+		ufshcd_writel(hba, config, REG_UMA_CONFIG);
+	}
+
+	if (init_dev) {
+		bool flag_res = 1;
+		int i;
+
+		/* Enable UMA for the device */
+		err = ufshcd_query_flag_retry(hba, UPIU_QUERY_OPCODE_SET_FLAG,
+					QUERY_FLAG_IDN_UNIFIED_MEMORY, NULL);
+
+		/* Poll for max. 1000 iterations for fUM flag to clear */
+		for (i = 0; i < 1000 && !err && flag_res; i++)
+			err = ufshcd_query_flag_retry(hba,
+					UPIU_QUERY_OPCODE_READ_FLAG,
+					QUERY_FLAG_IDN_UNIFIED_MEMORY,
+					&flag_res);
+		if (err)
+			return err;
+
+		if (flag_res)
+			dev_err(hba->dev, "fUM was not cleared by the device\n");
+	}
+
+	return 0;
+}
+
+static void ufshcd_uma_reenable(struct ufs_hba *hba)
+{
+	if (!hba->uma)
+		return;
+
+	__ufshcd_uma_reenable(hba, false);
+}
+
+static int ufshcd_uma_enable(struct ufs_hba *hba, struct ufs_dev_desc *dev_desc)
+{
+	int err;
+
+	if (!(hba->capabilities & MASK_DEVICE_BUS_MASTER_MODE_SUPPORT) ||
+	    !(dev_desc->subclass & UFS_DEVICE_SUB_CLASS_UM_SUPPORT))
+		return 0;
+
+	if (!hba->uma_size) {
+		gfp_t flags = GFP_KERNEL | __GFP_NOWARN | __GFP_NORETRY;
+		u32 sz = PAGE_ALIGN(dev_desc->min_uma_sz);
+
+		if (!sz)
+			return 0;
+
+		hba->uma_base_addr = dmam_alloc_coherent(hba->dev, sz,
+							 &hba->uma_addr, flags);
+		if (!hba->uma_base_addr) {
+			dev_err(hba->dev, "Failed to allocate unified memory area\n");
+			return -ENOMEM;
+		}
+
+		hba->uma_size = sz;
+	}
+
+	err = __ufshcd_uma_reenable(hba, true);
+	if (err)
+		return err;
+
+	hba->uma = true;
+
+	dev_info(hba->dev, "Enabled %u KiB unified memory area\n",
+		 hba->uma_size / 1024);
+
+	return 0;
+}
+
 /**
  * ufshcd_probe_hba - probe hba to detect device and initialize
  * @hba: per-adapter instance
@@ -6907,6 +7119,12 @@ static int ufshcd_probe_hba(struct ufs_hba *hba)
 
 	ufshcd_tune_unipro_params(hba);
 
+	ret = ufshcd_uma_enable(hba, &card);
+	if (ret) {
+		dev_err(hba->dev, "Failed to enable unified memory area\n");
+		goto out;
+	}
+
 	/* UFS device is also active now */
 	ufshcd_set_ufs_dev_active(hba);
 	ufshcd_force_reset_auto_bkops(hba);
@@ -7537,6 +7755,9 @@ ufshcd_send_request_sense(struct ufs_hba *hba, struct scsi_device *sdp)
  *
  * Returns 0 if requested power mode is set successfully
  * Returns non-zero if failed to set the requested power mode
+ *
+ * Note for UMA, power transitions have IMMED==0 so no polling is needed to
+ * prevent subsequently entering HIBERNATE while there are outstanding UMPIUs.
  */
 static int ufshcd_set_dev_pwr_mode(struct ufs_hba *hba,
 				     enum ufs_dev_pwr_mode pwr_mode)
@@ -7811,6 +8032,13 @@ static int ufshcd_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op)
 	       !ufshcd_is_runtime_pm(pm_op))) {
 		/* ensure that bkops is disabled */
 		ufshcd_disable_auto_bkops(hba);
+		/*
+		 * If UMA is enabled, auto-hibernate is not permitted during
+		 * power mode transitions.
+		 */
+		if (hba->uma)
+			ufshcd_auto_hibern8_disable(hba);
+
 		ret = ufshcd_set_dev_pwr_mode(hba, req_dev_pwr_mode);
 		if (ret)
 			goto enable_gating;
@@ -7865,6 +8093,8 @@ static int ufshcd_suspend(struct ufs_hba *hba, enum ufs_pm_op pm_op)
 		ufshcd_resume_clkscaling(hba);
 	hba->clk_gating.is_suspended = false;
 	ufshcd_release(hba);
+	if (hba->uma)
+		ufshcd_auto_hibern8_enable(hba);
 out:
 	hba->pm_op_in_progress = 0;
 	if (ret)
@@ -7886,6 +8116,7 @@ static int ufshcd_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)
 {
 	int ret;
 	enum uic_link_state old_link_state;
+	bool reinit = true;
 
 	hba->pm_op_in_progress = 1;
 	old_link_state = hba->uic_link_state;
@@ -7928,8 +8159,12 @@ static int ufshcd_resume(struct ufs_hba *hba, enum ufs_pm_op pm_op)
 		 */
 		if (ret || !ufshcd_is_link_active(hba))
 			goto vendor_suspend;
+		reinit = false;
 	}
 
+	if (reinit)
+		ufshcd_uma_reenable(hba);
+
 	if (!ufshcd_is_ufs_dev_active(hba)) {
 		ret = ufshcd_set_dev_pwr_mode(hba, UFS_ACTIVE_PWR_MODE);
 		if (ret)
diff --git a/drivers/scsi/ufs/ufshcd.h b/drivers/scsi/ufs/ufshcd.h
index c94cfda52829..3b9a3b56c6b9 100644
--- a/drivers/scsi/ufs/ufshcd.h
+++ b/drivers/scsi/ufs/ufshcd.h
@@ -559,6 +559,13 @@ struct ufs_hba {
 
 	/* Auto-Hibernate Idle Timer register value */
 	u32 ahit;
+	bool ahit_disabled;
+
+	/* Unified memory area */
+	void *uma_base_addr;
+	dma_addr_t uma_addr;
+	u32 uma_size;
+	bool uma;
 
 	struct ufshcd_lrb *lrb;
 	unsigned long lrb_in_use;
@@ -911,6 +918,9 @@ int ufshcd_read_desc_param(struct ufs_hba *hba,
 			   u8 param_offset,
 			   u8 *param_read_buf,
 			   u8 param_size);
+int __ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
+		      enum attr_idn idn, u8 index, u8 selector, u32 *attr_val,
+		      bool quiet);
 int ufshcd_query_attr(struct ufs_hba *hba, enum query_opcode opcode,
 		      enum attr_idn idn, u8 index, u8 selector, u32 *attr_val);
 int ufshcd_query_flag(struct ufs_hba *hba, enum query_opcode opcode,
diff --git a/drivers/scsi/ufs/ufshci.h b/drivers/scsi/ufs/ufshci.h
index dbb75cd28dc8..e7e86ba12cb6 100644
--- a/drivers/scsi/ufs/ufshci.h
+++ b/drivers/scsi/ufs/ufshci.h
@@ -45,6 +45,7 @@ enum {
 /* UFSHCI Registers */
 enum {
 	REG_CONTROLLER_CAPABILITIES		= 0x00,
+	REG_UMA_CAP				= 0x04,
 	REG_UFS_VERSION				= 0x08,
 	REG_CONTROLLER_DEV_ID			= 0x10,
 	REG_CONTROLLER_PROD_ID			= 0x14,
@@ -76,6 +77,13 @@ enum {
 
 	UFSHCI_REG_SPACE_SIZE			= 0xA0,
 
+	REG_UMA_BASE_ADDR_L			= 0xB0,
+	REG_UMA_BASE_ADDR_H			= 0xB4,
+	REG_UMA_OFFSET_MAX			= 0xB8,
+	REG_UMA_CONFIG				= 0xBC,
+
+	UFSHCI_UMA_REG_SPACE_SIZE		= 0xC0,
+
 	REG_UFS_CCAP				= 0x100,
 	REG_UFS_CRYPTOCAP			= 0x104,
 
@@ -90,6 +98,7 @@ enum {
 	MASK_64_ADDRESSING_SUPPORT		= 0x01000000,
 	MASK_OUT_OF_ORDER_DATA_DELIVERY_SUPPORT	= 0x02000000,
 	MASK_UIC_DME_TEST_MODE_SUPPORT		= 0x04000000,
+	MASK_DEVICE_BUS_MASTER_MODE_SUPPORT	= 0x08000000,
 };
 
 #define UFS_MASK(mask, offset)		((mask) << (offset))
@@ -244,6 +253,10 @@ enum {
 #define COMMAND_OPCODE_MASK		0xFF
 #define GEN_SELECTOR_INDEX_MASK		0xFFFF
 
+/* UMA - Unified Memory Area */
+#define UFSHCI_UMA_MNOOUR		GENMASK(2, 0)
+#define UFSHCI_UMA_ENABLE		0x1
+
 #define MIB_ATTRIBUTE_MASK		UFS_MASK(0xFFFF, 16)
 #define RESET_LEVEL			0xFF
 
-- 
2.17.1

