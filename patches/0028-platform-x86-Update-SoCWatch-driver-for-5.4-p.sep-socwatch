From 5c97ae6f4bef66d409e92ebbc6f712a5143465d6 Mon Sep 17 00:00:00 2001
From: Faycal Benmlih <faycal.benmlih@intel.com>
Date: Tue, 15 Oct 2019 15:10:45 -0500
Subject: [PATCH 28/32] platform/x86: Update SoCWatch driver for 5.4 pull

Update to SoCWatch driver version 2.11 which has initial
CTA driver support for the 5.4 pull

Signed-off-by: Faycal Benmlih <faycal.benmlih@intel.com>
---
 drivers/platform/x86/socwatch/Makefile        |    4 +-
 .../platform/x86/socwatch/inc/sw_collector.h  |  268 +-
 drivers/platform/x86/socwatch/inc/sw_cta.h    |   69 +
 .../platform/x86/socwatch/inc/sw_defines.h    |  316 +-
 .../platform/x86/socwatch/inc/sw_file_ops.h   |  138 +-
 .../x86/socwatch/inc/sw_hardware_io.h         |  243 +-
 .../platform/x86/socwatch/inc/sw_internal.h   |  296 +-
 drivers/platform/x86/socwatch/inc/sw_ioctl.h  |  614 +--
 .../x86/socwatch/inc/sw_kernel_defines.h      |  328 +-
 drivers/platform/x86/socwatch/inc/sw_list.h   |  152 +-
 .../platform/x86/socwatch/inc/sw_lock_defs.h  |  208 +-
 drivers/platform/x86/socwatch/inc/sw_mem.h    |  162 +-
 .../x86/socwatch/inc/sw_ops_provider.h        |  122 +-
 .../x86/socwatch/inc/sw_output_buffer.h       |  284 +-
 .../socwatch/inc/sw_overhead_measurements.h   |  368 +-
 .../platform/x86/socwatch/inc/sw_structs.h    | 1129 ++--
 drivers/platform/x86/socwatch/inc/sw_telem.h  |  152 +-
 .../socwatch/inc/sw_trace_notifier_provider.h |  162 +-
 .../x86/socwatch/inc/sw_tracepoint_handlers.h |  310 +-
 drivers/platform/x86/socwatch/inc/sw_types.h  |  302 +-
 .../platform/x86/socwatch/inc/sw_version.h    |  139 +-
 drivers/platform/x86/socwatch/sw_collector.c  | 1368 ++---
 drivers/platform/x86/socwatch/sw_cta.c        |  324 ++
 drivers/platform/x86/socwatch/sw_driver.c     | 3118 +++++------
 drivers/platform/x86/socwatch/sw_file_ops.c   |  674 +--
 .../platform/x86/socwatch/sw_hardware_io.c    |  368 +-
 drivers/platform/x86/socwatch/sw_internal.c   |  500 +-
 drivers/platform/x86/socwatch/sw_mem.c        |  644 +--
 .../platform/x86/socwatch/sw_ops_provider.c   | 2307 ++++----
 .../platform/x86/socwatch/sw_output_buffer.c  | 1676 +++---
 drivers/platform/x86/socwatch/sw_reader.c     |  318 +-
 drivers/platform/x86/socwatch/sw_telem.c      | 1746 +++---
 .../x86/socwatch/sw_trace_notifier_provider.c | 4676 ++++++++---------
 .../x86/socwatch/sw_tracepoint_handlers.c     |  812 +--
 34 files changed, 12395 insertions(+), 11902 deletions(-)
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_cta.h
 create mode 100644 drivers/platform/x86/socwatch/sw_cta.c

diff --git a/drivers/platform/x86/socwatch/Makefile b/drivers/platform/x86/socwatch/Makefile
index 63af91b57e80..327e20577bca 100644
--- a/drivers/platform/x86/socwatch/Makefile
+++ b/drivers/platform/x86/socwatch/Makefile
@@ -4,7 +4,7 @@
 
 DRIVER_BASE=socwatch
 DRIVER_MAJOR=2
-DRIVER_MINOR=10
+DRIVER_MINOR=11
 # basic name of driver
 DRIVER_NAME=${DRIVER_BASE}${DRIVER_MAJOR}_${DRIVER_MINOR}
 
@@ -19,4 +19,4 @@ $(DRIVER_NAME)-objs	:= sw_driver.o sw_hardware_io.o \
 			sw_output_buffer.o sw_tracepoint_handlers.o \
 			sw_mem.o sw_collector.o sw_telem.o \
 			sw_file_ops.o sw_internal.o sw_ops_provider.o \
-			sw_reader.o sw_trace_notifier_provider.o
+			sw_reader.o sw_trace_notifier_provider.o sw_cta.o
diff --git a/drivers/platform/x86/socwatch/inc/sw_collector.h b/drivers/platform/x86/socwatch/inc/sw_collector.h
index 69a7a4833b1d..ab042abfe73a 100644
--- a/drivers/platform/x86/socwatch/inc/sw_collector.h
+++ b/drivers/platform/x86/socwatch/inc/sw_collector.h
@@ -1,134 +1,134 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_COLLECTOR_H__
-
-#include "sw_internal.h"
-
-/*
- * Forward declaration
- */
-struct sw_hw_ops;
-
-/* TODO: convert from 'list_head' to 'hlist_head' */
-/**
- * struct - sw_collector_data
- * Information about the collector to be invoked at collection time.
- *
- * The collector_lists array holds linked lists of collectors to
- * be exercised at specific points in time during the collection
- * (e.g. begin, poll, end, etc.).  At a trigger time, the driver walks
- * that time's list of nodes, and exercises the collectors on that list.
- *
- * @list:                   List/link implementation
- * @cpumask:                Collect if cpu matches mask
- * @info:                   Ptr to metric info
- * @ops:                    Ptr to collector's operations
- * @last_update_jiffies:    Indicates when this node was last exercised.
- * @per_msg_payload_size:   Data size
- * @msg:                    Ptr to collected data
- */
-struct sw_collector_data {
-	SW_LIST_ENTRY(list, sw_collector_data);
-	struct cpumask                  cpumask;
-	struct sw_driver_interface_info *info;
-	const struct sw_hw_ops          **ops;
-	size_t                          per_msg_payload_size;
-	u64                             last_update_jiffies;
-	struct sw_driver_msg *msg;
-};
-
-#define GET_MSG_SLOT_FOR_CPU(msgs, cpu, size) ((struct sw_driver_msg *) & \
-	(((char *)(msgs))[(cpu) * (sizeof(struct sw_driver_msg) + (size))]))
-
-struct sw_collector_data *sw_alloc_collector_node(void);
-void sw_free_collector_node(struct sw_collector_data *node);
-int sw_handle_collector_node(struct sw_collector_data *data);
-int sw_handle_collector_node_on_cpu(struct sw_collector_data *data, int cpu);
-int sw_write_collector_node(struct sw_collector_data *data);
-
-void sw_init_collector_list(void *list_head);
-void sw_destroy_collector_list(void *list_head);
-int sw_handle_collector_list(void *list_head,
-	int (*func)(struct sw_collector_data *data));
-int sw_handle_collector_list_on_cpu(void *list_head,
-	int (*func)(struct sw_collector_data *data, int cpu),
-	int cpu);
-
-int sw_handle_driver_io_descriptor(char *dst_vals,
-	int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	const struct sw_hw_ops *hw_ops);
-int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
-int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
-
-int sw_add_driver_info(void *list_head,
-	const struct sw_driver_interface_info *info);
-
-void sw_handle_per_cpu_msg(void *info);
-void sw_handle_per_cpu_msg_no_sched(void *info);
-void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info);
-
-void sw_set_collector_ops(const struct sw_hw_ops *hw_ops);
-
-/**
- * Process all messages for the given time.
- * @param[in]   when    The time period e.g. 'BEGIN' or 'END'
- *
- * @returns     0   on success, non-zero on error
- */
-extern int sw_process_snapshot(enum sw_when_type when);
-extern int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu);
-#endif /* __SW_COLLECTOR_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_COLLECTOR_H__
+
+#include "sw_internal.h"
+
+/*
+ * Forward declaration
+ */
+struct sw_hw_ops;
+
+/* TODO: convert from 'list_head' to 'hlist_head' */
+/**
+ * struct - sw_collector_data
+ * Information about the collector to be invoked at collection time.
+ *
+ * The collector_lists array holds linked lists of collectors to
+ * be exercised at specific points in time during the collection
+ * (e.g. begin, poll, end, etc.).  At a trigger time, the driver walks
+ * that time's list of nodes, and exercises the collectors on that list.
+ *
+ * @list:                   List/link implementation
+ * @cpumask:                Collect if cpu matches mask
+ * @info:                   Ptr to metric info
+ * @ops:                    Ptr to collector's operations
+ * @last_update_jiffies:    Indicates when this node was last exercised.
+ * @per_msg_payload_size:   Data size
+ * @msg:                    Ptr to collected data
+ */
+struct sw_collector_data {
+	SW_LIST_ENTRY(list, sw_collector_data);
+	struct cpumask                  cpumask;
+	struct sw_driver_interface_info *info;
+	const struct sw_hw_ops          **ops;
+	size_t                          per_msg_payload_size;
+	u64                             last_update_jiffies;
+	struct sw_driver_msg *msg;
+};
+
+#define GET_MSG_SLOT_FOR_CPU(msgs, cpu, size) ((struct sw_driver_msg *) & \
+	(((char *)(msgs))[(cpu) * (sizeof(struct sw_driver_msg) + (size))]))
+
+struct sw_collector_data *sw_alloc_collector_node(void);
+void sw_free_collector_node(struct sw_collector_data *node);
+int sw_handle_collector_node(struct sw_collector_data *data);
+int sw_handle_collector_node_on_cpu(struct sw_collector_data *data, int cpu);
+int sw_write_collector_node(struct sw_collector_data *data);
+
+void sw_init_collector_list(void *list_head);
+void sw_destroy_collector_list(void *list_head);
+int sw_handle_collector_list(void *list_head,
+	int (*func)(struct sw_collector_data *data));
+int sw_handle_collector_list_on_cpu(void *list_head,
+	int (*func)(struct sw_collector_data *data, int cpu),
+	int cpu);
+
+int sw_handle_driver_io_descriptor(char *dst_vals,
+	int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	const struct sw_hw_ops *hw_ops);
+int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
+int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
+
+int sw_add_driver_info(void *list_head,
+	const struct sw_driver_interface_info *info);
+
+void sw_handle_per_cpu_msg(void *info);
+void sw_handle_per_cpu_msg_no_sched(void *info);
+void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info);
+
+void sw_set_collector_ops(const struct sw_hw_ops *hw_ops);
+
+/**
+ * Process all messages for the given time.
+ * @param[in]   when    The time period e.g. 'BEGIN' or 'END'
+ *
+ * @returns     0   on success, non-zero on error
+ */
+extern int sw_process_snapshot(enum sw_when_type when);
+extern int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu);
+#endif /* __SW_COLLECTOR_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_cta.h b/drivers/platform/x86/socwatch/inc/sw_cta.h
new file mode 100644
index 000000000000..48676c3d5e47
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_cta.h
@@ -0,0 +1,69 @@
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_CTA_H__
+#define __SW_CTA_H__
+
+
+bool sw_cta_register(void);
+bool sw_cta_unregister(void);
+
+void sw_read_cta_info(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+bool sw_cta_available(void);
+
+struct _sw_aggregator_msg *sw_cta_aggregators(void);
+
+#endif // __SW_CTA_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_defines.h b/drivers/platform/x86/socwatch/inc/sw_defines.h
index a670904e4e39..9b3fe5a3cbad 100644
--- a/drivers/platform/x86/socwatch/inc/sw_defines.h
+++ b/drivers/platform/x86/socwatch/inc/sw_defines.h
@@ -1,159 +1,157 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _PW_DEFINES_H_
-#define _PW_DEFINES_H_ 1
-
-#include "sw_version.h"
-
-/* ***************************************************
- * Common to kernel and userspace.
- * ***************************************************
- */
-#define PW_SUCCESS              0
-#define PW_ERROR                1
-#define PW_SUCCESS_NO_COLLECT   2
-
-/*
- * Helper macro to convert 'u64' to 'unsigned long long' to avoid gcc warnings.
- */
-#define TO_ULL(x) (unsigned long long)(x)
-/*
-* Convert an arg to 'long long'
-*/
-#define TO_LL(x) (long long)(x)
-/*
- * Convert an arg to 'unsigned long'
- */
-#define TO_UL(x) (unsigned long)(x)
-/*
- * Helper macro for string representation of a boolean value.
- */
-#define GET_BOOL_STRING(b) ((b) ? "TRUE" : "FALSE")
-
-/*
- * Circularly increment 'i' MODULO 'l'.
- * ONLY WORKS IF 'l' is (power of 2 - 1) ie.
- * l == (2 ^ x) - 1
- */
-#define CIRCULAR_INC(index, mask) (((index) + 1) & (mask))
-#define CIRCULAR_ADD(index, val, mask) (((index) + (val)) & (mask))
-/*
- * Circularly decrement 'i'.
- */
-#define CIRCULAR_DEC(i, m) ({			\
-	int __tmp1 = (i);			\
-	if (--__tmp1 < 0)			\
-		__tmp1 = (m); __tmp1; })
-/*
- * Retrieve size of an array.
- */
-#define SW_ARRAY_SIZE(array) (sizeof(array) / sizeof((array)[0]))
-/*
- * Should the driver count number of dropped samples?
- */
-#define DO_COUNT_DROPPED_SAMPLES 1
-/*
- * Extract F/W major, minor versions.
- * Assumes version numbers are 8b unsigned ints.
- */
-#define SW_GET_SCU_FW_VERSION_MAJOR(ver) (((ver) >> 8) & 0xff)
-#define SW_GET_SCU_FW_VERSION_MINOR(ver) ((ver) & 0xff)
-/*
- * Max size of process name retrieved from kernel.
- */
-#define SW_MAX_PROC_NAME_SIZE 16
-
-/*
- * Number of SOCPERF counters.
- * Needed by both Ring-0 and Ring-3
- */
-#define SW_NUM_SOCPERF_COUNTERS 9
-
-/*
- * Max size of process name retrieved from kernel space.
- */
-#define SW_MAX_PROC_NAME_SIZE 16
-/*
- * Max size of kernel wakelock name.
- */
-#define SW_MAX_KERNEL_WAKELOCK_NAME_SIZE 100
-
-/* Data value read when a telemetry data read fails. */
-#define SW_TELEM_READ_FAIL_VALUE 0xF00DF00DF00DF00DUL
-
-#ifdef SWW_MERGE
-typedef enum {
-	SW_STOP_EVENT = 0,
-	SW_CS_EXIT_EVENT,
-	SW_COUNTER_RESET_EVENT,
-	SW_COUNTER_HOTKEY_EVENT,
-	SW_MAX_COLLECTION_EVENT
-} collector_stop_event_t;
-#endif /* SWW_MERGE */
-
-#define MAX_UNSIGNED_16_BIT_VALUE 0xFFFF
-#define MAX_UNSIGNED_24_BIT_VALUE 0xFFFFFF
-#define MAX_UNSIGNED_32_BIT_VALUE 0xFFFFFFFF
-#define MAX_UNSIGNED_64_BIT_VALUE 0xFFFFFFFFFFFFFFFF
-/*
- * TELEM BAR CONFIG
- */
-#define MAX_TELEM_BAR_CFG	3
-#define TELEM_MCHBAR_CFG	0
-#define TELEM_IPC1BAR_CFG	1
-#define TELEM_SSRAMBAR_CFG	2
-
-#endif /* _PW_DEFINES_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _PW_DEFINES_H_
+#define _PW_DEFINES_H_ 1
+
+/* ***************************************************
+ * Common to kernel and userspace.
+ * ***************************************************
+ */
+#define PW_SUCCESS              0
+#define PW_ERROR                1
+#define PW_SUCCESS_NO_COLLECT   2
+
+/*
+ * Helper macro to convert 'u64' to 'unsigned long long' to avoid gcc warnings.
+ */
+#define TO_ULL(x) (unsigned long long)(x)
+/*
+* Convert an arg to 'long long'
+*/
+#define TO_LL(x) (long long)(x)
+/*
+ * Convert an arg to 'unsigned long'
+ */
+#define TO_UL(x) (unsigned long)(x)
+/*
+ * Helper macro for string representation of a boolean value.
+ */
+#define GET_BOOL_STRING(b) ((b) ? "TRUE" : "FALSE")
+
+/*
+ * Circularly increment 'i' MODULO 'l'.
+ * ONLY WORKS IF 'l' is (power of 2 - 1) ie.
+ * l == (2 ^ x) - 1
+ */
+#define CIRCULAR_INC(index, mask) (((index) + 1) & (mask))
+#define CIRCULAR_ADD(index, val, mask) (((index) + (val)) & (mask))
+/*
+ * Circularly decrement 'i'.
+ */
+#define CIRCULAR_DEC(i, m) ({			\
+	int __tmp1 = (i);			\
+	if (--__tmp1 < 0)			\
+		__tmp1 = (m); __tmp1; })
+/*
+ * Retrieve size of an array.
+ */
+#define SW_ARRAY_SIZE(array) (sizeof(array) / sizeof((array)[0]))
+/*
+ * Should the driver count number of dropped samples?
+ */
+#define DO_COUNT_DROPPED_SAMPLES 1
+/*
+ * Extract F/W major, minor versions.
+ * Assumes version numbers are 8b unsigned ints.
+ */
+#define SW_GET_SCU_FW_VERSION_MAJOR(ver) (((ver) >> 8) & 0xff)
+#define SW_GET_SCU_FW_VERSION_MINOR(ver) ((ver) & 0xff)
+/*
+ * Max size of process name retrieved from kernel.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+
+/*
+ * Number of SOCPERF counters.
+ * Needed by both Ring-0 and Ring-3
+ */
+#define SW_NUM_SOCPERF_COUNTERS 9
+
+/*
+ * Max size of process name retrieved from kernel space.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+/*
+ * Max size of kernel wakelock name.
+ */
+#define SW_MAX_KERNEL_WAKELOCK_NAME_SIZE 100
+
+/* Data value read when a telemetry data read fails. */
+#define SW_TELEM_READ_FAIL_VALUE 0xF00DF00DF00DF00DUL
+
+#ifdef SWW_MERGE
+typedef enum {
+	SW_STOP_EVENT = 0,
+	SW_CS_EXIT_EVENT,
+	SW_COUNTER_RESET_EVENT,
+	SW_COUNTER_HOTKEY_EVENT,
+	SW_MAX_COLLECTION_EVENT
+} collector_stop_event_t;
+#endif /* SWW_MERGE */
+
+#define MAX_UNSIGNED_16_BIT_VALUE 0xFFFF
+#define MAX_UNSIGNED_24_BIT_VALUE 0xFFFFFF
+#define MAX_UNSIGNED_32_BIT_VALUE 0xFFFFFFFF
+#define MAX_UNSIGNED_64_BIT_VALUE 0xFFFFFFFFFFFFFFFF
+/*
+ * TELEM BAR CONFIG
+ */
+#define MAX_TELEM_BAR_CFG	3
+#define TELEM_MCHBAR_CFG	0
+#define TELEM_IPC1BAR_CFG	1
+#define TELEM_SSRAMBAR_CFG	2
+
+#endif /* _PW_DEFINES_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_file_ops.h b/drivers/platform/x86/socwatch/inc/sw_file_ops.h
index bba7e5ddbb87..a6ed219c1df0 100644
--- a/drivers/platform/x86/socwatch/inc/sw_file_ops.h
+++ b/drivers/platform/x86/socwatch/inc/sw_file_ops.h
@@ -1,69 +1,69 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#ifndef __SW_FILE_OPS_H__
-#define __SW_FILE_OPS_H__
-
-enum sw_driver_collection_cmd;
-struct sw_file_ops {
-	long (*ioctl_handler)(unsigned int ioctl_num, void *local_args);
-	int (*stop_handler)(void);
-	enum sw_driver_collection_cmd (*get_current_cmd)(void);
-	bool (*should_flush)(void);
-};
-
-int sw_register_dev(struct sw_file_ops *ops);
-void sw_unregister_dev(void);
-
-#endif /* __SW_FILE_OPS_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_FILE_OPS_H__
+#define __SW_FILE_OPS_H__
+
+enum sw_driver_collection_cmd;
+struct sw_file_ops {
+	long (*ioctl_handler)(unsigned int ioctl_num, void *local_args);
+	int (*stop_handler)(void);
+	enum sw_driver_collection_cmd (*get_current_cmd)(void);
+	bool (*should_flush)(void);
+};
+
+int sw_register_dev(struct sw_file_ops *ops);
+void sw_unregister_dev(void);
+
+#endif /* __SW_FILE_OPS_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_hardware_io.h b/drivers/platform/x86/socwatch/inc/sw_hardware_io.h
index 9e207f2a473a..3f71e52b3d2a 100644
--- a/drivers/platform/x86/socwatch/inc/sw_hardware_io.h
+++ b/drivers/platform/x86/socwatch/inc/sw_hardware_io.h
@@ -1,119 +1,124 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#ifndef __SW_HARDWARE_IO_H__
-#define __SW_HARDWARE_IO_H__
-
-#include "sw_structs.h"
-
-typedef int (*sw_io_desc_init_func_t)
-	(struct sw_driver_io_descriptor *descriptor);
-typedef void (*sw_hardware_op_func_t)
-	(char *dst_vals,
-	int cpu, const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes);
-typedef int (*sw_io_desc_print_func_t)
-	(const struct sw_driver_io_descriptor *descriptor);
-typedef int (*sw_io_desc_reset_func_t)
-	(const struct sw_driver_io_descriptor *descriptor);
-typedef bool (*sw_io_desc_available_func_t)(void);
-typedef bool (*sw_hw_op_post_config_func_t)(void);
-
-/**
- * struct sw_hw_ops - Operations for each of the HW collection mechanisms
- *                    in swkernelcollector.
- * @name:           A descriptive name used to identify this particular
- *                  operation.
- * @init:           Initialize a metric's collection.
- * @read:           Read a metric's data.
- * @write:          Write to the HW for the metric(?).
- * @print:          Print out the data.
- * @reset:          Opposite of init--called after we're done collecting.
- * @available:      Decide whether this H/W op is available on the current
- *                  platform.
- * @post_config:    Perform any post-configuration steps.
- */
-struct sw_hw_ops {
-	const char *name;
-	sw_io_desc_init_func_t       init;
-	sw_hardware_op_func_t        read;
-	sw_hardware_op_func_t        write;
-	sw_io_desc_print_func_t      print;
-	sw_io_desc_reset_func_t      reset;
-	sw_io_desc_available_func_t  available;
-	sw_hw_op_post_config_func_t  post_config;
-};
-
-bool sw_is_valid_hw_op_id(int id);
-int sw_get_hw_op_id(const struct sw_hw_ops *op);
-const struct sw_hw_ops *sw_get_hw_ops_for(int id);
-const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op);
-
-int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
-			void *priv, bool return_on_error);
-
-/**
- * Add an operation to the list of providers.
- */
-int sw_register_hw_op(const struct sw_hw_ops *ops);
-/**
- * Register all H/W operations.
- */
-int sw_register_hw_ops(void);
-/**
- * Unregister previously registered H/W operations.
- */
-void sw_free_hw_ops(void);
-
-#endif /* __SW_HARDWARE_IO_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_HARDWARE_IO_H__
+#define __SW_HARDWARE_IO_H__
+
+#include "sw_structs.h"
+
+typedef int (*sw_io_desc_init_func_t)
+	(struct sw_driver_io_descriptor *descriptor);
+typedef void (*sw_hardware_op_func_t)
+	(char *dst_vals,
+	int cpu, const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes);
+typedef int (*sw_io_desc_print_func_t)
+	(const struct sw_driver_io_descriptor *descriptor);
+typedef int (*sw_io_desc_reset_func_t)
+	(const struct sw_driver_io_descriptor *descriptor);
+typedef bool (*sw_io_desc_available_func_t)(void);
+typedef bool (*sw_hw_op_post_config_func_t)(void);
+typedef bool (*sw_hw_op_reg_unreg_func_t)(void);
+
+/**
+ * struct sw_hw_ops - Operations for each of the HW collection mechanisms
+ *                    in swkernelcollector.
+ * @name:           A descriptive name used to identify this particular
+ *                  operation.
+ * @register:       Called when the hardware ops is being registered
+ * @init:           Initialize a metric's collection.
+ * @read:           Read a metric's data.
+ * @write:          Write to the HW for the metric(?).
+ * @print:          Print out the data.
+ * @reset:          Opposite of init--called after we're done collecting.
+ * @available:      Decide whether this H/W op is available on the current
+ *                  platform.
+ * @post_config:    Perform any post-configuration steps.
+ * @unregister:     Called when the hardware op is being unregistered
+ */
+struct sw_hw_ops {
+	const char *name;
+	sw_hw_op_reg_unreg_func_t	reg;
+	sw_io_desc_init_func_t       	init;
+	sw_hardware_op_func_t        	read;
+	sw_hardware_op_func_t        	write;
+	sw_io_desc_print_func_t      	print;
+	sw_io_desc_reset_func_t      	reset;
+	sw_io_desc_available_func_t  	available;
+	sw_hw_op_post_config_func_t  	post_config;
+	sw_hw_op_reg_unreg_func_t	unreg;
+};
+
+bool sw_is_valid_hw_op_id(int id);
+int sw_get_hw_op_id(const struct sw_hw_ops *op);
+const struct sw_hw_ops *sw_get_hw_ops_for(int id);
+const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op);
+
+int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
+			void *priv, bool return_on_error);
+
+/**
+ * Add an operation to the list of providers.
+ */
+int sw_register_hw_op(const struct sw_hw_ops *ops);
+/**
+ * Register all H/W operations.
+ */
+int sw_register_hw_ops(void);
+/**
+ * Unregister previously registered H/W operations.
+ */
+void sw_free_hw_ops(void);
+
+#endif /* __SW_HARDWARE_IO_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_internal.h b/drivers/platform/x86/socwatch/inc/sw_internal.h
index 3e027e4f63fc..c06fbe0e99f1 100644
--- a/drivers/platform/x86/socwatch/inc/sw_internal.h
+++ b/drivers/platform/x86/socwatch/inc/sw_internal.h
@@ -1,146 +1,150 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_DATA_STRUCTS_H__
-#define __SW_DATA_STRUCTS_H__
-
-/*
- * Taken from 'sw_driver'
- * TODO: move to separate file?
- */
-#include <linux/module.h>
-#include <linux/moduleparam.h>
-#include <linux/kernel.h>
-#include <linux/slab.h>
-#include <linux/cpumask.h>
-#include <linux/hrtimer.h>
-#include <linux/fs.h>      /* inode */
-#include <linux/device.h>  /* class_create */
-#include <linux/cdev.h>    /* cdev_alloc */
-#include <linux/vmalloc.h> /* vmalloc */
-#include <linux/sched.h>   /* TASK_INTERRUPTIBLE */
-#include <linux/wait.h>    /* wait_event_interruptible */
-#include <linux/pci.h>     /* pci_get_bus_and_slot */
-#include <linux/version.h> /* LINUX_VERSION_CODE */
-#include <linux/sfi.h>     /* For SFI F/W version */
-#include <asm/hardirq.h>
-#include <linux/cpufreq.h>
-#include <asm/local.h>     /* local_t */
-#include <linux/hardirq.h> /* "in_atomic" */
-#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
-    #include <asm/uaccess.h>   /* copy_to_user */
-#else
-    #include <linux/uaccess.h>   /* copy_to_user */
-#endif /* LINUX_VERSION_CODE */
-
-#ifdef CONFIG_X86_WANT_INTEL_MID
-    #include <asm/intel-mid.h>
-#endif /* CONFIG_X86_WANT_INTEL_MID */
-/*
- * End taken from sw_driver
- */
-
-#include "sw_structs.h"
-#include "sw_ioctl.h"
-#include "sw_list.h"
-
-/* ******************************************
- * Compile time constants
- * ******************************************
- */
-#define GET_POLLED_CPU() (sw_max_num_cpus)
-#define CAS32(p, o, n) (cmpxchg((p), (o), (n)) == (o))
-
-/* ******************************************
- * Function declarations.
- * ******************************************
- */
-/*
- * Output to user.
- */
-unsigned long sw_copy_to_user(char __user *dst, char *src, size_t bytes_to_copy);
-bool sw_check_output_buffer_params(void __user *buffer, size_t bytes_to_read,
-	size_t buff_size);
-/*
- * smp call function.
- */
-void sw_schedule_work(const struct cpumask *mask, void (*work)(void *),
-	void *data);
-/*
- * Save IRQ flags and retrieve cpu number.
- */
-int sw_get_cpu(unsigned long *flags);
-/*
- * Restore IRQ flags.
- */
-void sw_put_cpu(unsigned long flags);
-/*
- * Set module scope for cpu frequencies.
- */
-int sw_set_module_scope_for_cpus(void);
-/*
- * reset module scope for cpu frequencies.
- */
-int sw_reset_module_scope_for_cpus(void);
-/*
- * Setup p-unit/pmc telemetry
- */
-int sw_setup_telem(u64 addrs[3]);
-/*
- * Tear down p-unit/pmc telemetry
- */
-void sw_destroy_telem(void);
-
-#endif /* __SW_DATA_STRUCTS_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_DATA_STRUCTS_H__
+#define __SW_DATA_STRUCTS_H__
+
+/*
+ * Taken from 'sw_driver'
+ * TODO: move to separate file?
+ */
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/cpumask.h>
+#include <linux/hrtimer.h>
+#include <linux/fs.h>      /* inode */
+#include <linux/device.h>  /* class_create */
+#include <linux/cdev.h>    /* cdev_alloc */
+#include <linux/vmalloc.h> /* vmalloc */
+#include <linux/sched.h>   /* TASK_INTERRUPTIBLE */
+#include <linux/wait.h>    /* wait_event_interruptible */
+#include <linux/pci.h>     /* pci_get_bus_and_slot */
+#include <linux/version.h> /* LINUX_VERSION_CODE */
+#include <linux/sfi.h>     /* For SFI F/W version */
+#include <asm/hardirq.h>
+#include <linux/cpufreq.h>
+#include <asm/local.h>     /* local_t */
+#include <linux/hardirq.h> /* "in_atomic" */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
+    #include <asm/uaccess.h>   /* copy_to_user */
+#else
+    #include <linux/uaccess.h>   /* copy_to_user */
+#endif /* LINUX_VERSION_CODE */
+
+#ifdef CONFIG_X86_WANT_INTEL_MID
+    #include <asm/intel-mid.h>
+#endif /* CONFIG_X86_WANT_INTEL_MID */
+/*
+ * End taken from sw_driver
+ */
+
+#include "sw_structs.h"
+#include "sw_ioctl.h"
+#include "sw_list.h"
+
+/* ******************************************
+ * Compile time constants
+ * ******************************************
+ */
+#define GET_POLLED_CPU() (sw_max_num_cpus)
+#define CAS32(p, o, n) (cmpxchg((p), (o), (n)) == (o))
+
+/* ******************************************
+ * Function declarations.
+ * ******************************************
+ */
+/*
+ * Output to user.
+ */
+unsigned long sw_copy_to_user(char __user *dst, char *src, size_t bytes_to_copy);
+bool sw_check_output_buffer_params(void __user *buffer, size_t bytes_to_read,
+	size_t buff_size);
+/*
+ * smp call function.
+ */
+void sw_schedule_work(const struct cpumask *mask, void (*work)(void *),
+	void *data);
+/*
+ * Save IRQ flags and retrieve cpu number.
+ */
+int sw_get_cpu(unsigned long *flags);
+/*
+ * Restore IRQ flags.
+ */
+void sw_put_cpu(unsigned long flags);
+/*
+ * Set module scope for cpu frequencies.
+ */
+int sw_set_module_scope_for_cpus(void);
+/*
+ * reset module scope for cpu frequencies.
+ */
+int sw_reset_module_scope_for_cpus(void);
+/*
+ * Setup p-unit/pmc telemetry
+ */
+int sw_setup_telem(u64 addrs[3]);
+/*
+ * Tear down p-unit/pmc telemetry
+ */
+void sw_destroy_telem(void);
+/*
+ * Retrieve CTA endpoint descriptors
+ */
+struct _sw_aggregator_msg const *sw_get_cta_aggregators(void);
+
+#endif /* __SW_DATA_STRUCTS_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_ioctl.h b/drivers/platform/x86/socwatch/inc/sw_ioctl.h
index 43a1f69af9ab..0d644020ce64 100644
--- a/drivers/platform/x86/socwatch/inc/sw_ioctl.h
+++ b/drivers/platform/x86/socwatch/inc/sw_ioctl.h
@@ -1,299 +1,315 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_IOCTL_H__
-#define __SW_IOCTL_H__ 1
-
-#if defined(__linux__) || defined(__QNX__)
-	#if __KERNEL__
-		#include <linux/ioctl.h>
-		#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
-			#include <asm/compat.h>
-			#include <linux/compat.h>
-		#endif /* COMPAT && x64 */
-	#else /* !__KERNEL__ */
-		#include <sys/ioctl.h>
-	#endif /* __KERNEL__ */
-#endif /* __linux__ */
-/*
- * Ensure we pull in definition of 'DO_COUNT_DROPPED_SAMPLES'!
- */
-#include "sw_defines.h"
-
-#ifdef ONECORE
-#ifndef __KERNEL__
-#include <winioctl.h>
-#endif /* __KERNEL__ */
-#endif /* ONECORE */
-
-/*
- * The APWR-specific IOCTL magic
- * number -- used to ensure IOCTLs
- * are delivered to the correct
- * driver.
- */
-/* #define APWR_IOCTL_MAGIC_NUM 0xdead */
-#define APWR_IOCTL_MAGIC_NUM 100
-
-/*
- * The name of the device file
- */
-/* #define DEVICE_FILE_NAME "/dev/pw_driver_char_dev" */
-#define PW_DEVICE_FILE_NAME "/dev/apwr_driver_char_dev"
-#define PW_DEVICE_NAME "apwr_driver_char_dev"
-
-enum sw_ioctl_cmd {
-	sw_ioctl_cmd_none = 0,
-	sw_ioctl_cmd_config,
-	sw_ioctl_cmd_cmd,
-	sw_ioctl_cmd_poll,
-	sw_ioctl_cmd_immediate_io,
-	sw_ioctl_cmd_scu_version,
-	sw_ioctl_cmd_read_immediate,
-	sw_ioctl_cmd_driver_version,
-	sw_ioctl_cmd_avail_trace,
-	sw_ioctl_cmd_avail_notify,
-	sw_ioctl_cmd_avail_collect,
-	sw_ioctl_cmd_topology_changes,
-	sw_ioctl_cmd_config_continuous,
-	sw_ioctl_cmd_read_continuous,
-	sw_ioctl_cmd_telem_bar,
-};
-/*
- * The actual IOCTL commands.
- *
- * From the kernel documentation:
- * "_IOR" ==> Read IOCTL
- * "_IOW" ==> Write IOCTL
- * "_IOWR" ==> Read/Write IOCTL
- *
- * Where "Read" and "Write" are from the user's perspective
- * (similar to the file "read" and "write" calls).
- */
-#ifdef SWW_MERGE /* Windows */
-	 /*
-	  * Device type		   -- in the "User Defined" range."
-	  */
-	#define POWER_I_CONF_TYPE 40000
-
-	/* List assigned tracepoint id */
-	#define CSIR_TRACEPOINT_ID_MASK			1
-	#define DEVICE_STATE_TRACEPOINT_ID_MASK		2
-	#define CSIR_SEPARATE_TRACEPOINT_ID_MASK	3
-	#define RESET_TRACEPOINT_ID_MASK		4
-	#define DISPLAY_ON_TRACEPOINT_ID_MASK		5
-
-	/*
-	 * The IOCTL function codes from 0x800 to 0xFFF are for customer use.
-	 */
-	#define PW_IOCTL_CONFIG \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x900, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_START_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x901, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_STOP_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x902, METHOD_BUFFERED, FILE_ANY_ACCESS)
-
-	/* TODO: pause, resume, cancel not supported yet */
-	#define PW_IOCTL_PAUSE_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x903, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_RESUME_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x904, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_CANCEL_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x905, METHOD_BUFFERED, FILE_ANY_ACCESS)
-
-	#define PW_IOCTL_GET_PROCESSOR_GROUP_TOPOLOGY \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x906, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_TOPOLOGY \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x907, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x908, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_IMMEDIATE_IO \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x909, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_DRV_CLEANUP \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90A, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_SET_COLLECTION_EVENT \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90B, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_TRY_STOP_EVENT \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90C, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_SET_PCH_ACTIVE_INTERVAL \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90D, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_SET_TELEM_BAR \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90E, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_METADATA \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x90F, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_SET_GBE_INTERVAL \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x910, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_ENABLE_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x911, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_DISABLE_COLLECTION \
-	CTL_CODE(POWER_I_CONF_TYPE, 0x912, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_DRIVER_BUILD_DATE \
-		CTL_CODE(POWER_I_CONF_TYPE, 0x913, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_CONFIG_CONTINUOUS \
-		CTL_CODE(POWER_I_CONF_TYPE, 0x914, METHOD_BUFFERED, FILE_ANY_ACCESS)
-	#define PW_IOCTL_READ_CONTINUOUS \
-		CTL_CODE(POWER_I_CONF_TYPE, 0x915, METHOD_BUFFERED, FILE_ANY_ACCESS)
-
-#elif !defined(__APPLE__)
-	#define PW_IOCTL_CONFIG	\
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, struct sw_driver_ioctl_arg *)
-	#if DO_COUNT_DROPPED_SAMPLES
-		#define PW_IOCTL_CMD \
-			_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg *)
-	#else
-		#define PW_IOCTL_CMD \
-			_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg *)
-	#endif /* DO_COUNT_DROPPED_SAMPLES */
-	#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
-	#define PW_IOCTL_IMMEDIATE_IO	\
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_SCU_FW_VERSION \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_READ_IMMEDIATE \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_DRIVER_VERSION \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_GET_TOPOLOGY_CHANGES \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_CONFIG_CONTINUOUS \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_READ_CONTINUOUS \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, struct sw_driver_ioctl_arg *)
-	#define PW_IOCTL_SET_TELEM_BAR \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, struct sw_driver_ioctl_arg *)
-#else /* __APPLE__ */
-	#define PW_IOCTL_CONFIG \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, struct sw_driver_ioctl_arg)
-	#if DO_COUNT_DROPPED_SAMPLES
-		#define PW_IOCTL_CMD \
-			_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
-	#else
-		#define PW_IOCTL_CMD \
-			_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
-	#endif /* DO_COUNT_DROPPED_SAMPLES */
-	#define PW_IOCTL_POLL \
-		_IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
-	#define PW_IOCTL_IMMEDIATE_IO \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_SCU_FW_VERSION \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_READ_IMMEDIATE \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_DRIVER_VERSION \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_GET_TOPOLOGY_CHANGES \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_CONFIG_CONTINUOUS \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_READ_CONTINUOUS \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, struct sw_driver_ioctl_arg)
-	#define PW_IOCTL_SET_TELEM_BAR \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, struct sw_driver_ioctl_arg *)
-#endif /* __APPLE__ */
-
-/*
- * 32b-compatible version of the above
- * IOCTL numbers. Required ONLY for
- * 32b compatibility on 64b systems,
- * and ONLY by the driver.
- */
-#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
-	#define PW_IOCTL_CONFIG32 \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, compat_uptr_t)
-#if DO_COUNT_DROPPED_SAMPLES
-	#define PW_IOCTL_CMD32 \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
-#else
-	#define PW_IOCTL_CMD32 \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
-#endif /* DO_COUNT_DROPPED_SAMPLES */
-	#define PW_IOCTL_POLL32 \
-		_IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
-	#define PW_IOCTL_IMMEDIATE_IO32 \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, compat_uptr_t)
-	#define PW_IOCTL_GET_SCU_FW_VERSION32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, compat_uptr_t)
-	#define PW_IOCTL_READ_IMMEDIATE32 \
-		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, compat_uptr_t)
-	#define PW_IOCTL_GET_DRIVER_VERSION32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, compat_uptr_t)
-	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, compat_uptr_t)
-	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, compat_uptr_t)
-	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, compat_uptr_t)
-	#define PW_IOCTL_GET_TOPOLOGY_CHANGES32 \
-		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, compat_uptr_t)
-	#define PW_IOCTL_CONFIG_CONTINUOUS32 \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, compat_uptr_t)
-	#define PW_IOCTL_READ_CONTINUOUS32 \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, compat_uptr_t)
-	#define PW_IOCTL_SET_TELEM_BAR32 \
-		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, compat_uptr_t)
-#endif /* defined(CONFIG_COMPAT) && defined(CONFIG_X86_64) */
-#endif /* __SW_IOCTL_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_IOCTL_H__
+#define __SW_IOCTL_H__ 1
+
+#if defined(__linux__) || defined(__QNX__)
+	#if __KERNEL__
+		#include <linux/ioctl.h>
+		#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+			#include <asm/compat.h>
+			#include <linux/compat.h>
+		#endif /* COMPAT && x64 */
+	#else /* !__KERNEL__ */
+		#include <sys/ioctl.h>
+	#endif /* __KERNEL__ */
+#endif /* __linux__ */
+/*
+ * Ensure we pull in definition of 'DO_COUNT_DROPPED_SAMPLES'!
+ */
+#include "sw_defines.h"
+
+#ifdef ONECORE
+#ifndef __KERNEL__
+#include <winioctl.h>
+#endif /* __KERNEL__ */
+#endif /* ONECORE */
+
+/*
+ * The APWR-specific IOCTL magic
+ * number -- used to ensure IOCTLs
+ * are delivered to the correct
+ * driver.
+ */
+/* #define APWR_IOCTL_MAGIC_NUM 0xdead */
+#define APWR_IOCTL_MAGIC_NUM 100
+
+/*
+ * The name of the device file
+ */
+/* #define DEVICE_FILE_NAME "/dev/pw_driver_char_dev" */
+#define PW_DEVICE_FILE_NAME "/dev/apwr_driver_char_dev"
+#define PW_DEVICE_NAME "apwr_driver_char_dev"
+
+enum sw_ioctl_cmd {
+	sw_ioctl_cmd_none = 0,
+	sw_ioctl_cmd_config,
+	sw_ioctl_cmd_cmd,
+	sw_ioctl_cmd_poll,
+	sw_ioctl_cmd_immediate_io,
+	sw_ioctl_cmd_scu_version,
+	sw_ioctl_cmd_read_immediate,
+	sw_ioctl_cmd_driver_version,
+	sw_ioctl_cmd_avail_trace,
+	sw_ioctl_cmd_avail_notify,
+	sw_ioctl_cmd_avail_collect,
+	sw_ioctl_cmd_topology_changes,
+	sw_ioctl_cmd_config_continuous,
+	sw_ioctl_cmd_read_continuous,
+	sw_ioctl_cmd_telem_bar,
+	sw_ioctl_cmd_avail_cta_aggregators,
+};
+/*
+ * The actual IOCTL commands.
+ *
+ * From the kernel documentation:
+ * "_IOR" ==> Read IOCTL
+ * "_IOW" ==> Write IOCTL
+ * "_IOWR" ==> Read/Write IOCTL
+ *
+ * Where "Read" and "Write" are from the user's perspective
+ * (similar to the file "read" and "write" calls).
+ */
+#ifdef SWW_MERGE /* Windows */
+	 /*
+	  * Device type		   -- in the "User Defined" range."
+	  */
+	#define POWER_I_CONF_TYPE 40000
+
+	/* List assigned tracepoint id */
+	#define CSIR_TRACEPOINT_ID_MASK			1
+	#define DEVICE_STATE_TRACEPOINT_ID_MASK		2
+	#define CSIR_SEPARATE_TRACEPOINT_ID_MASK	3
+	#define RESET_TRACEPOINT_ID_MASK		4
+	#define DISPLAY_ON_TRACEPOINT_ID_MASK		5
+
+	/*
+	 * The IOCTL function codes from 0x800 to 0xFFF are for customer use.
+	 */
+	#define PW_IOCTL_CONFIG \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x900, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_START_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x901, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_STOP_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x902, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+	/* TODO: pause, resume, cancel not supported yet */
+	#define PW_IOCTL_PAUSE_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x903, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_RESUME_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x904, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_CANCEL_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x905, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+	#define PW_IOCTL_GET_PROCESSOR_GROUP_TOPOLOGY \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x906, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_TOPOLOGY \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x907, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x908, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_IMMEDIATE_IO \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x909, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_DRV_CLEANUP \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90A, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_SET_COLLECTION_EVENT \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90B, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_TRY_STOP_EVENT \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90C, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_SET_PCH_ACTIVE_INTERVAL \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90D, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_SET_TELEM_BAR \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90E, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_METADATA \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90F, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_SET_GBE_INTERVAL \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x910, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_ENABLE_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x911, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_DISABLE_COLLECTION \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x912, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_DRIVER_BUILD_DATE \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x913, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_CONFIG_CONTINUOUS \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x914, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_READ_CONTINUOUS \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x915, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_GET_AGGREGATOR_BAR \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x916, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_SET_DPST_INTERVAL \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x917, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_GET_AGGREGATOR_INFO \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x918, METHOD_BUFFERED, FILE_ANY_ACCESS)
+	#define PW_IOCTL_GET_AGGREGATOR_SIZE \
+		CTL_CODE(POWER_I_CONF_TYPE, 0x919, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+#elif !defined(__APPLE__)
+	#define PW_IOCTL_CONFIG	\
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, struct sw_driver_ioctl_arg *)
+	#if DO_COUNT_DROPPED_SAMPLES
+		#define PW_IOCTL_CMD \
+			_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg *)
+	#else
+		#define PW_IOCTL_CMD \
+			_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg *)
+	#endif /* DO_COUNT_DROPPED_SAMPLES */
+	#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+	#define PW_IOCTL_IMMEDIATE_IO	\
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_SCU_FW_VERSION \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_READ_IMMEDIATE \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_DRIVER_VERSION \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_GET_TOPOLOGY_CHANGES \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_CONFIG_CONTINUOUS \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_READ_CONTINUOUS \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_SET_TELEM_BAR \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, struct sw_driver_ioctl_arg *)
+	#define PW_IOCTL_AVAIL_CTA_AGGREGATORS \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_cta_aggregators, struct sw_driver_ioctl_arg *)
+
+#else /* __APPLE__ */
+	#define PW_IOCTL_CONFIG \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, struct sw_driver_ioctl_arg)
+	#if DO_COUNT_DROPPED_SAMPLES
+		#define PW_IOCTL_CMD \
+			_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
+	#else
+		#define PW_IOCTL_CMD \
+			_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
+	#endif /* DO_COUNT_DROPPED_SAMPLES */
+	#define PW_IOCTL_POLL \
+		_IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+	#define PW_IOCTL_IMMEDIATE_IO \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_SCU_FW_VERSION \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_READ_IMMEDIATE \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_DRIVER_VERSION \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_GET_TOPOLOGY_CHANGES \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_CONFIG_CONTINUOUS \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_READ_CONTINUOUS \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_SET_TELEM_BAR \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, struct sw_driver_ioctl_arg)
+	#define PW_IOCTL_AVAIL_CTA_AGGREGATORS \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_cta_aggregators, struct sw_driver_ioctl_arg)
+#endif /* __APPLE__ */
+
+/*
+ * 32b-compatible version of the above
+ * IOCTL numbers. Required ONLY for
+ * 32b compatibility on 64b systems,
+ * and ONLY by the driver.
+ */
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+	#define PW_IOCTL_CONFIG32 \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, compat_uptr_t)
+#if DO_COUNT_DROPPED_SAMPLES
+	#define PW_IOCTL_CMD32 \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#else
+	#define PW_IOCTL_CMD32 \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#endif /* DO_COUNT_DROPPED_SAMPLES */
+	#define PW_IOCTL_POLL32 \
+		_IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+	#define PW_IOCTL_IMMEDIATE_IO32 \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, compat_uptr_t)
+	#define PW_IOCTL_GET_SCU_FW_VERSION32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, compat_uptr_t)
+	#define PW_IOCTL_READ_IMMEDIATE32 \
+		_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, compat_uptr_t)
+	#define PW_IOCTL_GET_DRIVER_VERSION32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, compat_uptr_t)
+	#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, compat_uptr_t)
+	#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, compat_uptr_t)
+	#define PW_IOCTL_GET_AVAILABLE_COLLECTORS32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, compat_uptr_t)
+	#define PW_IOCTL_GET_TOPOLOGY_CHANGES32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, compat_uptr_t)
+	#define PW_IOCTL_CONFIG_CONTINUOUS32 \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config_continuous, compat_uptr_t)
+	#define PW_IOCTL_READ_CONTINUOUS32 \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_continuous, compat_uptr_t)
+	#define PW_IOCTL_SET_TELEM_BAR32 \
+		_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_telem_bar, compat_uptr_t)
+	#define PW_IOCTL_AVAIL_CTA_AGGREGATORS32 \
+		_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_cta_aggregators, compat_uptr_t)
+#endif /* defined(CONFIG_COMPAT) && defined(CONFIG_X86_64) */
+#endif /* __SW_IOCTL_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h b/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
index eaa730491a6e..dd004a1f45a4 100644
--- a/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
+++ b/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
@@ -1,164 +1,164 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _SW_KERNEL_DEFINES_H_
-#define _SW_KERNEL_DEFINES_H_ 1
-
-#include "sw_defines.h"
-
-#if defined(__APPLE__)
-	#define likely(x)   (x)
-	#define unlikely(x) (x)
-#endif /* __APPLE__ */
-
-#if !defined(__APPLE__)
-	#define CPU() (raw_smp_processor_id())
-	#define RAW_CPU() (raw_smp_processor_id())
-#else
-	#define CPU() (cpu_number())
-	#define RAW_CPU() (cpu_number())
-#endif /* __APPLE__ */
-
-#define TID() (current->pid)
-#define PID() (current->tgid)
-#define NAME() (current->comm)
-#define PKG(c) (cpu_data(c).phys_proc_id)
-#define IT_REAL_INCR() (current->signal->it_real_incr.tv64)
-
-#define ATOMIC_CAS(ptr, old_val, new_val)							  \
-	(cmpxchg((ptr), (old_val), (new_val)) == (old_val))
-
-/*
- * Should we measure overheads?
- * '1' ==> YES
- * '0' ==> NO
- */
-#define DO_OVERHEAD_MEASUREMENTS 0
-/*
- * Should we track memory usage?
- * '1' ==> YES
- * '0' ==> NO
- */
-#define DO_TRACK_MEMORY_USAGE 0
-/*
- * Are we compiling with driver profiling support
- * turned ON? If YES then force 'DO_OVERHEAD_MEASUREMENTS'
- * and 'DO_TRACK_MEMORY_USAGE' to be TRUE.
- */
-#if DO_DRIVER_PROFILING
-	#if !DO_OVERHEAD_MEASUREMENTS
-		#undef DO_OVERHEAD_MEASUREMENTS
-		#define DO_OVERHEAD_MEASUREMENTS 1
-	#endif /* DO_OVERHEAD_MEASUREMENTS */
-	#if !DO_TRACK_MEMORY_USAGE
-		#undef DO_TRACK_MEMORY_USAGE
-		#define DO_TRACK_MEMORY_USAGE 1
-	#endif /* DO_TRACK_MEMORY_USAGE */
-#endif /* DO_DRIVER_PROFILING */
-/*
- * Should we allow debug output.
- * Set to: "1" ==> 'OUTPUT' is enabled.
- *		 "0" ==> 'OUTPUT' is disabled.
- */
-#define DO_DEBUG_OUTPUT 0
-/*
- * Control whether to output driver ERROR messages.
- * These are independent of the 'OUTPUT' macro
- * (which controls debug messages).
- * Set to '1' ==> Print driver error messages (to '/var/log/messages')
- *		'0' ==> Do NOT print driver error messages
- */
-#define DO_PRINT_DRIVER_ERROR_MESSAGES 1
-/*
- * Macros to control output printing.
- */
-#if !defined(__APPLE__)
-	#if DO_DEBUG_OUTPUT
-		#define pw_pr_debug(...) pr_info(__VA_ARGS__)
-		#define pw_pr_warn(...) pr_warn(__VA_ARGS__)
-	#else
-		#define pw_pr_debug(...)
-		#define pw_pr_warn(...)
-	#endif
-	#define pw_pr_force(...) pr_info(__VA_ARGS__)
-#else
-	#if DO_DEBUG_OUTPUT
-		#define pw_pr_debug(...) IOLog(__VA_ARGS__)
-		#define pw_pr_warn(...) IOLog(__VA_ARGS__)
-	#else
-		#define pw_pr_debug(...)
-		#define pw_pr_warn(...)
-	#endif
-	#define pw_pr_force(...) IOLog(__VA_ARGS__)
-#endif /* __APPLE__ */
-
-/*
- * Macro for driver error messages.
- */
-#if !defined(__APPLE__)
-	#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
-		#define pw_pr_error(...) pr_err(__VA_ARGS__)
-	#else
-		#define pw_pr_error(...)
-	#endif
-#else
-	#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
-		#define pw_pr_error(...) IOLog(__VA_ARGS__)
-	#else
-		#define pw_pr_error(...)
-	#endif
-#endif /* __APPLE__ */
-
-#endif /* _SW_KERNEL_DEFINES_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SW_KERNEL_DEFINES_H_
+#define _SW_KERNEL_DEFINES_H_ 1
+
+#include "sw_defines.h"
+
+#if defined(__APPLE__)
+	#define likely(x)   (x)
+	#define unlikely(x) (x)
+#endif /* __APPLE__ */
+
+#if !defined(__APPLE__)
+	#define CPU() (raw_smp_processor_id())
+	#define RAW_CPU() (raw_smp_processor_id())
+#else
+	#define CPU() (cpu_number())
+	#define RAW_CPU() (cpu_number())
+#endif /* __APPLE__ */
+
+#define TID() (current->pid)
+#define PID() (current->tgid)
+#define NAME() (current->comm)
+#define PKG(c) (cpu_data(c).phys_proc_id)
+#define IT_REAL_INCR() (current->signal->it_real_incr.tv64)
+
+#define ATOMIC_CAS(ptr, old_val, new_val)							  \
+	(cmpxchg((ptr), (old_val), (new_val)) == (old_val))
+
+/*
+ * Should we measure overheads?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_OVERHEAD_MEASUREMENTS 0
+/*
+ * Should we track memory usage?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_TRACK_MEMORY_USAGE 0
+/*
+ * Are we compiling with driver profiling support
+ * turned ON? If YES then force 'DO_OVERHEAD_MEASUREMENTS'
+ * and 'DO_TRACK_MEMORY_USAGE' to be TRUE.
+ */
+#if DO_DRIVER_PROFILING
+	#if !DO_OVERHEAD_MEASUREMENTS
+		#undef DO_OVERHEAD_MEASUREMENTS
+		#define DO_OVERHEAD_MEASUREMENTS 1
+	#endif /* DO_OVERHEAD_MEASUREMENTS */
+	#if !DO_TRACK_MEMORY_USAGE
+		#undef DO_TRACK_MEMORY_USAGE
+		#define DO_TRACK_MEMORY_USAGE 1
+	#endif /* DO_TRACK_MEMORY_USAGE */
+#endif /* DO_DRIVER_PROFILING */
+/*
+ * Should we allow debug output.
+ * Set to: "1" ==> 'OUTPUT' is enabled.
+ *		 "0" ==> 'OUTPUT' is disabled.
+ */
+#define DO_DEBUG_OUTPUT 0
+/*
+ * Control whether to output driver ERROR messages.
+ * These are independent of the 'OUTPUT' macro
+ * (which controls debug messages).
+ * Set to '1' ==> Print driver error messages (to '/var/log/messages')
+ *		'0' ==> Do NOT print driver error messages
+ */
+#define DO_PRINT_DRIVER_ERROR_MESSAGES 1
+/*
+ * Macros to control output printing.
+ */
+#if !defined(__APPLE__)
+	#if DO_DEBUG_OUTPUT
+		#define pw_pr_debug(...) pr_info(__VA_ARGS__)
+		#define pw_pr_warn(...) pr_warn(__VA_ARGS__)
+	#else
+		#define pw_pr_debug(...)
+		#define pw_pr_warn(...)
+	#endif
+	#define pw_pr_force(...) pr_info(__VA_ARGS__)
+#else
+	#if DO_DEBUG_OUTPUT
+		#define pw_pr_debug(...) IOLog(__VA_ARGS__)
+		#define pw_pr_warn(...) IOLog(__VA_ARGS__)
+	#else
+		#define pw_pr_debug(...)
+		#define pw_pr_warn(...)
+	#endif
+	#define pw_pr_force(...) IOLog(__VA_ARGS__)
+#endif /* __APPLE__ */
+
+/*
+ * Macro for driver error messages.
+ */
+#if !defined(__APPLE__)
+	#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+		#define pw_pr_error(...) pr_err(__VA_ARGS__)
+	#else
+		#define pw_pr_error(...)
+	#endif
+#else
+	#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+		#define pw_pr_error(...) IOLog(__VA_ARGS__)
+	#else
+		#define pw_pr_error(...)
+	#endif
+#endif /* __APPLE__ */
+
+#endif /* _SW_KERNEL_DEFINES_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_list.h b/drivers/platform/x86/socwatch/inc/sw_list.h
index 9b632beefa84..e29522bf389b 100644
--- a/drivers/platform/x86/socwatch/inc/sw_list.h
+++ b/drivers/platform/x86/socwatch/inc/sw_list.h
@@ -1,76 +1,76 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#ifndef __SW_LIST_H__
-#define __SW_LIST_H__
-
-#include <linux/list.h>
-
-#define SW_DEFINE_LIST_HEAD(name, dummy)            struct list_head name
-#define SW_DECLARE_LIST_HEAD(name, dummy)           extern struct list_head name
-#define SW_LIST_ENTRY(name, dummy)                  struct list_head name
-#define SW_LIST_HEAD_VAR(dummy)                     struct list_head
-#define SW_LIST_HEAD_INIT(head)                     INIT_LIST_HEAD(head)
-#define SW_LIST_ENTRY_INIT(node, field)             INIT_LIST_HEAD(&node->field)
-#define SW_LIST_ADD(head, node, field)              \
-	list_add_tail(&node->field, head)
-#define SW_LIST_GET_HEAD_ENTRY(head, type, field)   \
-	list_first_entry(head, struct type, field)
-#define SW_LIST_UNLINK(node, field)                 list_del(&node->field)
-#define SW_LIST_FOR_EACH_ENTRY(node, head, field)   \
-	list_for_each_entry(node, head, field)
-#define SW_LIST_EMPTY(head)                         list_empty(head)
-#define SW_LIST_HEAD_INITIALIZER(head)              LIST_HEAD_INIT(head)
-
-#endif /* __SW_LIST_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_LIST_H__
+#define __SW_LIST_H__
+
+#include <linux/list.h>
+
+#define SW_DEFINE_LIST_HEAD(name, dummy)            struct list_head name
+#define SW_DECLARE_LIST_HEAD(name, dummy)           extern struct list_head name
+#define SW_LIST_ENTRY(name, dummy)                  struct list_head name
+#define SW_LIST_HEAD_VAR(dummy)                     struct list_head
+#define SW_LIST_HEAD_INIT(head)                     INIT_LIST_HEAD(head)
+#define SW_LIST_ENTRY_INIT(node, field)             INIT_LIST_HEAD(&node->field)
+#define SW_LIST_ADD(head, node, field)              \
+	list_add_tail(&node->field, head)
+#define SW_LIST_GET_HEAD_ENTRY(head, type, field)   \
+	list_first_entry(head, struct type, field)
+#define SW_LIST_UNLINK(node, field)                 list_del(&node->field)
+#define SW_LIST_FOR_EACH_ENTRY(node, head, field)   \
+	list_for_each_entry(node, head, field)
+#define SW_LIST_EMPTY(head)                         list_empty(head)
+#define SW_LIST_HEAD_INITIALIZER(head)              LIST_HEAD_INIT(head)
+
+#endif /* __SW_LIST_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_lock_defs.h b/drivers/platform/x86/socwatch/inc/sw_lock_defs.h
index 42914f8998f7..d2d441aace07 100644
--- a/drivers/platform/x86/socwatch/inc/sw_lock_defs.h
+++ b/drivers/platform/x86/socwatch/inc/sw_lock_defs.h
@@ -1,104 +1,104 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Description: file containing locking routines
- * used by the power driver.
- */
-
-#ifndef __SW_LOCK_DEFS_H__
-#define __SW_LOCK_DEFS_H__
-
-/*
- * Spinlocks
- */
-#define SW_DEFINE_SPINLOCK(s)   DEFINE_SPINLOCK(s)
-#define SW_DECLARE_SPINLOCK(s)  static spinlock_t s
-
-#define SW_INIT_SPINLOCK(s)     spin_lock_init(&s)
-#define SW_DESTROY_SPINLOCK(s)  /* NOP */
-
-#define LOCK(l) { \
-	unsigned long l##_flags; \
-	spin_lock_irqsave(&(l), l##_flags);
-
-#define UNLOCK(l) \
-	spin_unlock_irqrestore(&(l), l##_flags); \
-}
-
-/*
- * R/W locks
- */
-#define SW_DECLARE_RWLOCK(l) static rwlock_t l
-#define SW_INIT_RWLOCK(l) rwlock_init(&(l))
-#define SW_DESTROY_RWLOCK(l) /* NOP */
-
-#define READ_LOCK(l) { \
-	unsigned long l##_flags; \
-	read_lock_irqsave(&(l), l##_flags);
-
-#define READ_UNLOCK(l) \
-	read_unlock_irqrestore(&(l), l##_flags); \
-}
-
-#define WRITE_LOCK(l) { \
-	unsigned long l##_flags; \
-	write_lock_irqsave(&(l), l##_flags);
-
-#define WRITE_UNLOCK(l) \
-	write_unlock_irqrestore(&(l), l##_flags); \
-}
-
-#endif /* __SW_LOCK_DEFS_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Description: file containing locking routines
+ * used by the power driver.
+ */
+
+#ifndef __SW_LOCK_DEFS_H__
+#define __SW_LOCK_DEFS_H__
+
+/*
+ * Spinlocks
+ */
+#define SW_DEFINE_SPINLOCK(s)   DEFINE_SPINLOCK(s)
+#define SW_DECLARE_SPINLOCK(s)  static spinlock_t s
+
+#define SW_INIT_SPINLOCK(s)     spin_lock_init(&s)
+#define SW_DESTROY_SPINLOCK(s)  /* NOP */
+
+#define LOCK(l) { \
+	unsigned long l##_flags; \
+	spin_lock_irqsave(&(l), l##_flags);
+
+#define UNLOCK(l) \
+	spin_unlock_irqrestore(&(l), l##_flags); \
+}
+
+/*
+ * R/W locks
+ */
+#define SW_DECLARE_RWLOCK(l) static rwlock_t l
+#define SW_INIT_RWLOCK(l) rwlock_init(&(l))
+#define SW_DESTROY_RWLOCK(l) /* NOP */
+
+#define READ_LOCK(l) { \
+	unsigned long l##_flags; \
+	read_lock_irqsave(&(l), l##_flags);
+
+#define READ_UNLOCK(l) \
+	read_unlock_irqrestore(&(l), l##_flags); \
+}
+
+#define WRITE_LOCK(l) { \
+	unsigned long l##_flags; \
+	write_lock_irqsave(&(l), l##_flags);
+
+#define WRITE_UNLOCK(l) \
+	write_unlock_irqrestore(&(l), l##_flags); \
+}
+
+#endif /* __SW_LOCK_DEFS_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_mem.h b/drivers/platform/x86/socwatch/inc/sw_mem.h
index b8797fd1dab1..0944d80947d1 100644
--- a/drivers/platform/x86/socwatch/inc/sw_mem.h
+++ b/drivers/platform/x86/socwatch/inc/sw_mem.h
@@ -1,81 +1,81 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Description: file containing memory management routines
- * used by the power driver.
- */
-
-#ifndef _SW_MEM_H_
-#define _SW_MEM_H_ 1
-
-#include "sw_types.h"
-
-void *sw_kmalloc(size_t size, unsigned int flags);
-void sw_kfree(const void *obj);
-/*
- * Allocate free pages.
- */
-unsigned long sw_allocate_pages(unsigned int flags,
-	unsigned int alloc_size_in_bytes);
-/*
- * Free up previously allocated pages.
- */
-void sw_release_pages(unsigned long addr, unsigned int alloc_size_in_bytes);
-
-u64 sw_get_total_bytes_alloced(void);
-u64 sw_get_max_bytes_alloced(void);
-u64 sw_get_curr_bytes_alloced(void);
-#endif /* _SW_MEM_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Description: file containing memory management routines
+ * used by the power driver.
+ */
+
+#ifndef _SW_MEM_H_
+#define _SW_MEM_H_ 1
+
+#include "sw_types.h"
+
+void *sw_kmalloc(size_t size, unsigned int flags);
+void sw_kfree(const void *obj);
+/*
+ * Allocate free pages.
+ */
+unsigned long sw_allocate_pages(unsigned int flags,
+	unsigned int alloc_size_in_bytes);
+/*
+ * Free up previously allocated pages.
+ */
+void sw_release_pages(unsigned long addr, unsigned int alloc_size_in_bytes);
+
+u64 sw_get_total_bytes_alloced(void);
+u64 sw_get_max_bytes_alloced(void);
+u64 sw_get_curr_bytes_alloced(void);
+#endif /* _SW_MEM_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_ops_provider.h b/drivers/platform/x86/socwatch/inc/sw_ops_provider.h
index 69b1b70a3fdc..30e6937f943f 100644
--- a/drivers/platform/x86/socwatch/inc/sw_ops_provider.h
+++ b/drivers/platform/x86/socwatch/inc/sw_ops_provider.h
@@ -1,61 +1,61 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#ifndef __SW_OPS_PROVIDER_H__
-#define __SW_OPS_PROVIDER_H__
-
-int sw_register_ops_providers(void);
-void sw_free_ops_providers(void);
-
-#endif /* __SW_OPS_PROVIDER_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_OPS_PROVIDER_H__
+#define __SW_OPS_PROVIDER_H__
+
+int sw_register_ops_providers(void);
+void sw_free_ops_providers(void);
+
+#endif /* __SW_OPS_PROVIDER_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_output_buffer.h b/drivers/platform/x86/socwatch/inc/sw_output_buffer.h
index d7138a7aa866..e8efba2ff4ba 100644
--- a/drivers/platform/x86/socwatch/inc/sw_output_buffer.h
+++ b/drivers/platform/x86/socwatch/inc/sw_output_buffer.h
@@ -1,142 +1,142 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _SW_OUTPUT_BUFFER_H_
-#define _SW_OUTPUT_BUFFER_H_ 1
-/*
- * Special mask for the case where all buffers have been flushed.
- */
-/* #define sw_ALL_WRITES_DONE_MASK 0xffffffff */
-#define SW_ALL_WRITES_DONE_MASK ((u32)-1)
-/*
- * Special mask for the case where no data is available to be read.
- */
-#define SW_NO_DATA_AVAIL_MASK ((u32)-2)
-
-/*
- * Forward declarations.
- */
-struct sw_driver_msg;
-
-/*
- * Data structures.
- */
-enum sw_wakeup_action {
-	SW_WAKEUP_ACTION_DIRECT,
-	SW_WAKEUP_ACTION_TIMER,
-	SW_WAKEUP_ACTION_NONE,
-};
-
-/*
- * Variable declarations.
- */
-extern u64 sw_num_samples_produced, sw_num_samples_dropped;
-extern unsigned long sw_buffer_alloc_size;
-extern int sw_max_num_cpus;
-extern wait_queue_head_t sw_reader_queue;
-
-/*
- * Public API.
- */
-int sw_init_per_cpu_buffers(void);
-void sw_destroy_per_cpu_buffers(void);
-void sw_reset_per_cpu_buffers(void);
-
-void sw_count_samples_produced_dropped(void);
-
-int sw_produce_generic_msg(struct sw_driver_msg *, enum sw_wakeup_action);
-
-bool sw_any_seg_full(u32 *val, bool is_flush_mode);
-ssize_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read);
-
-unsigned int sw_get_output_buffer_size(void);
-
-void sw_wait_once(void);
-void sw_wakeup(void);
-
-void sw_print_output_buffer_overheads(void);
-
-/*
- * Init reader queue.
- */
-int sw_init_reader_queue(void);
-/*
- * Destroy reader queue.
- */
-void sw_destroy_reader_queue(void);
-/*
- * Wakeup client waiting for a full buffer.
- */
-void sw_wakeup_reader(enum sw_wakeup_action);
-/*
- * Wakeup client waiting for a full buffer, and
- * cancel any timers initialized by the reader
- * subsys.
- */
-void sw_cancel_reader(void);
-/*
- * Print some stats about the reader subsys.
- */
-void sw_print_reader_stats(void);
-
-/* *************************************************
- * For circular buffer (continuous profiling)
- * *************************************************
- */
-long initialize_circular_buffer(size_t size);
-void reset_circular_buffer(void);
-void destroy_circular_buffer(void);
-
-#endif /* _SW_OUTPUT_BUFFER_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SW_OUTPUT_BUFFER_H_
+#define _SW_OUTPUT_BUFFER_H_ 1
+/*
+ * Special mask for the case where all buffers have been flushed.
+ */
+/* #define sw_ALL_WRITES_DONE_MASK 0xffffffff */
+#define SW_ALL_WRITES_DONE_MASK ((u32)-1)
+/*
+ * Special mask for the case where no data is available to be read.
+ */
+#define SW_NO_DATA_AVAIL_MASK ((u32)-2)
+
+/*
+ * Forward declarations.
+ */
+struct sw_driver_msg;
+
+/*
+ * Data structures.
+ */
+enum sw_wakeup_action {
+	SW_WAKEUP_ACTION_DIRECT,
+	SW_WAKEUP_ACTION_TIMER,
+	SW_WAKEUP_ACTION_NONE,
+};
+
+/*
+ * Variable declarations.
+ */
+extern u64 sw_num_samples_produced, sw_num_samples_dropped;
+extern unsigned long sw_buffer_alloc_size;
+extern int sw_max_num_cpus;
+extern wait_queue_head_t sw_reader_queue;
+
+/*
+ * Public API.
+ */
+int sw_init_per_cpu_buffers(void);
+void sw_destroy_per_cpu_buffers(void);
+void sw_reset_per_cpu_buffers(void);
+
+void sw_count_samples_produced_dropped(void);
+
+int sw_produce_generic_msg(struct sw_driver_msg *, enum sw_wakeup_action);
+
+bool sw_any_seg_full(u32 *val, bool is_flush_mode);
+ssize_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read);
+
+unsigned int sw_get_output_buffer_size(void);
+
+void sw_wait_once(void);
+void sw_wakeup(void);
+
+void sw_print_output_buffer_overheads(void);
+
+/*
+ * Init reader queue.
+ */
+int sw_init_reader_queue(void);
+/*
+ * Destroy reader queue.
+ */
+void sw_destroy_reader_queue(void);
+/*
+ * Wakeup client waiting for a full buffer.
+ */
+void sw_wakeup_reader(enum sw_wakeup_action);
+/*
+ * Wakeup client waiting for a full buffer, and
+ * cancel any timers initialized by the reader
+ * subsys.
+ */
+void sw_cancel_reader(void);
+/*
+ * Print some stats about the reader subsys.
+ */
+void sw_print_reader_stats(void);
+
+/* *************************************************
+ * For circular buffer (continuous profiling)
+ * *************************************************
+ */
+long initialize_circular_buffer(size_t size);
+void reset_circular_buffer(void);
+void destroy_circular_buffer(void);
+
+#endif /* _SW_OUTPUT_BUFFER_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h b/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
index b665f5438a3f..25148def4dc7 100644
--- a/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
+++ b/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
@@ -1,184 +1,184 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-/*
- * Description: file containing overhead measurement
- * routines used by the power driver.
- */
-
-#ifndef _PW_OVERHEAD_MEASUREMENTS_H_
-#define _PW_OVERHEAD_MEASUREMENTS_H_
-
-/*
- * Helper macro to declare variables required
- * for conducting overhead measurements.
- */
-/*
- * For each function that you want to profile,
- * do the following (e.g. function 'foo'):
- * **************************************************
- * DECLARE_OVERHEAD_VARS(foo);
- * **************************************************
- * This will declare the two variables required
- * to keep track of overheads incurred in
- * calling/servicing 'foo'. Note that the name
- * that you declare here *MUST* match the function name!
- */
-
-#if DO_OVERHEAD_MEASUREMENTS
-
-#ifndef __get_cpu_var
-	/*
-	 * Kernels >= 3.19 don't include a definition
-	 * of '__get_cpu_var'. Create one now.
-	 */
-	#define __get_cpu_var(var) (*this_cpu_ptr(&var))
-#endif /* __get_cpu_var */
-#ifndef __raw_get_cpu_var
-	/*
-	 * Kernels >= 3.19 don't include a definition
-	 * of '__raw_get_cpu_var'. Create one now.
-	 */
-	#define __raw_get_cpu_var(var) (*raw_cpu_ptr(&var))
-#endif /* __get_cpu_var */
-
-extern u64 sw_timestamp(void);
-
-#define DECLARE_OVERHEAD_VARS(name)					\
-	static DEFINE_PER_CPU(u64, name##_elapsed_time);		\
-	static DEFINE_PER_CPU(local_t, name##_num_iters) =		\
-							 LOCAL_INIT(0);	\
-	static inline u64 get_my_cumulative_elapsed_time_##name(void)	\
-	{								\
-		return *(&__get_cpu_var(name##_elapsed_time));		\
-	}								\
-	static inline int get_my_cumulative_num_iters_##name(void)	\
-	{								\
-		return local_read(&__get_cpu_var(name##_num_iters));	\
-	}								\
-	static inline u64 name##_get_cumulative_elapsed_time_for(	\
-							int cpu)	\
-	{								\
-		return *(&per_cpu(name##_elapsed_time, cpu));		\
-	}								\
-	static inline int name##_get_cumulative_num_iters_for(int cpu)	\
-	{								\
-		return local_read(&per_cpu(name##_num_iters, cpu));	\
-	}								\
-	static inline void name##_get_cumulative_overhead_params(	\
-							u64 *time,	\
-							int *iters)	\
-	{								\
-		int cpu = 0;						\
-		*time = 0; *iters = 0;					\
-		for_each_online_cpu(cpu) {				\
-			*iters += name##_get_cumulative_num_iters_for(	\
-								cpu);	\
-			*time += name##_get_cumulative_elapsed_time_for(\
-								cpu);	\
-		}							\
-		return;							\
-	}								\
-	static inline void name##_print_cumulative_overhead_params(	\
-							const char *str)\
-	{								\
-		int num = 0;						\
-		u64 time = 0;						\
-		name##_get_cumulative_overhead_params(&time, &num);	\
-		pw_pr_error("%s: %d iters took %llu nano seconds!\n",	\
-			str, num, time);				\
-	}
-
-#define DO_PER_CPU_OVERHEAD_FUNC(func, ...) do {			\
-	u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);		\
-	u64 tmp_1 = 0, tmp_2 = 0;					\
-	local_inc(&__raw_get_cpu_var(func##_num_iters));		\
-	tmp_1 = sw_timestamp();						\
-	{								\
-		func(__VA_ARGS__);						\
-	}								\
-	tmp_2 = sw_timestamp();						\
-	*(__v) += (tmp_2 - tmp_1);					\
-} while (0)
-
-#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...) ({		\
-	type __ret;							\
-	u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);		\
-	u64 tmp_1 = 0, tmp_2 = 0;					\
-	local_inc(&__raw_get_cpu_var(func##_num_iters));		\
-	tmp_1 = sw_timestamp();						\
-	{								\
-		__ret = func(__VA_ARGS__);					\
-	}								\
-	tmp_2 = sw_timestamp();						\
-	*(__v) += (tmp_2 - tmp_1);					\
-	__ret;								\
-})
-
-#else /* !DO_OVERHEAD_MEASUREMENTS */
-#define DECLARE_OVERHEAD_VARS(name)					\
-	static inline void name##_print_cumulative_overhead_params(	\
-							const char *str)\
-		{ /* NOP */ }
-
-#define DO_PER_CPU_OVERHEAD_FUNC(func, ...) func(__VA_ARGS__)
-#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...) func(__VA_ARGS__)
-
-#endif /* DO_OVERHEAD_MEASUREMENTS */
-
-#define PRINT_CUMULATIVE_OVERHEAD_PARAMS(name, str)			\
-	name##_print_cumulative_overhead_params(str)
-
-#endif /* _PW_OVERHEAD_MEASUREMENTS_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * Description: file containing overhead measurement
+ * routines used by the power driver.
+ */
+
+#ifndef _PW_OVERHEAD_MEASUREMENTS_H_
+#define _PW_OVERHEAD_MEASUREMENTS_H_
+
+/*
+ * Helper macro to declare variables required
+ * for conducting overhead measurements.
+ */
+/*
+ * For each function that you want to profile,
+ * do the following (e.g. function 'foo'):
+ * **************************************************
+ * DECLARE_OVERHEAD_VARS(foo);
+ * **************************************************
+ * This will declare the two variables required
+ * to keep track of overheads incurred in
+ * calling/servicing 'foo'. Note that the name
+ * that you declare here *MUST* match the function name!
+ */
+
+#if DO_OVERHEAD_MEASUREMENTS
+
+#ifndef __get_cpu_var
+	/*
+	 * Kernels >= 3.19 don't include a definition
+	 * of '__get_cpu_var'. Create one now.
+	 */
+	#define __get_cpu_var(var) (*this_cpu_ptr(&var))
+#endif /* __get_cpu_var */
+#ifndef __raw_get_cpu_var
+	/*
+	 * Kernels >= 3.19 don't include a definition
+	 * of '__raw_get_cpu_var'. Create one now.
+	 */
+	#define __raw_get_cpu_var(var) (*raw_cpu_ptr(&var))
+#endif /* __get_cpu_var */
+
+extern u64 sw_timestamp(void);
+
+#define DECLARE_OVERHEAD_VARS(name)					\
+	static DEFINE_PER_CPU(u64, name##_elapsed_time);		\
+	static DEFINE_PER_CPU(local_t, name##_num_iters) =		\
+							 LOCAL_INIT(0);	\
+	static inline u64 get_my_cumulative_elapsed_time_##name(void)	\
+	{								\
+		return *(&__get_cpu_var(name##_elapsed_time));		\
+	}								\
+	static inline int get_my_cumulative_num_iters_##name(void)	\
+	{								\
+		return local_read(&__get_cpu_var(name##_num_iters));	\
+	}								\
+	static inline u64 name##_get_cumulative_elapsed_time_for(	\
+							int cpu)	\
+	{								\
+		return *(&per_cpu(name##_elapsed_time, cpu));		\
+	}								\
+	static inline int name##_get_cumulative_num_iters_for(int cpu)	\
+	{								\
+		return local_read(&per_cpu(name##_num_iters, cpu));	\
+	}								\
+	static inline void name##_get_cumulative_overhead_params(	\
+							u64 *time,	\
+							int *iters)	\
+	{								\
+		int cpu = 0;						\
+		*time = 0; *iters = 0;					\
+		for_each_online_cpu(cpu) {				\
+			*iters += name##_get_cumulative_num_iters_for(	\
+								cpu);	\
+			*time += name##_get_cumulative_elapsed_time_for(\
+								cpu);	\
+		}							\
+		return;							\
+	}								\
+	static inline void name##_print_cumulative_overhead_params(	\
+							const char *str)\
+	{								\
+		int num = 0;						\
+		u64 time = 0;						\
+		name##_get_cumulative_overhead_params(&time, &num);	\
+		pw_pr_error("%s: %d iters took %llu nano seconds!\n",	\
+			str, num, time);				\
+	}
+
+#define DO_PER_CPU_OVERHEAD_FUNC(func, ...) do {			\
+	u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);		\
+	u64 tmp_1 = 0, tmp_2 = 0;					\
+	local_inc(&__raw_get_cpu_var(func##_num_iters));		\
+	tmp_1 = sw_timestamp();						\
+	{								\
+		func(__VA_ARGS__);						\
+	}								\
+	tmp_2 = sw_timestamp();						\
+	*(__v) += (tmp_2 - tmp_1);					\
+} while (0)
+
+#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...) ({		\
+	type __ret;							\
+	u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);		\
+	u64 tmp_1 = 0, tmp_2 = 0;					\
+	local_inc(&__raw_get_cpu_var(func##_num_iters));		\
+	tmp_1 = sw_timestamp();						\
+	{								\
+		__ret = func(__VA_ARGS__);					\
+	}								\
+	tmp_2 = sw_timestamp();						\
+	*(__v) += (tmp_2 - tmp_1);					\
+	__ret;								\
+})
+
+#else /* !DO_OVERHEAD_MEASUREMENTS */
+#define DECLARE_OVERHEAD_VARS(name)					\
+	static inline void name##_print_cumulative_overhead_params(	\
+							const char *str)\
+		{ /* NOP */ }
+
+#define DO_PER_CPU_OVERHEAD_FUNC(func, ...) func(__VA_ARGS__)
+#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...) func(__VA_ARGS__)
+
+#endif /* DO_OVERHEAD_MEASUREMENTS */
+
+#define PRINT_CUMULATIVE_OVERHEAD_PARAMS(name, str)			\
+	name##_print_cumulative_overhead_params(str)
+
+#endif /* _PW_OVERHEAD_MEASUREMENTS_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_structs.h b/drivers/platform/x86/socwatch/inc/sw_structs.h
index 3c1bcc46be2f..b7df32aef84a 100644
--- a/drivers/platform/x86/socwatch/inc/sw_structs.h
+++ b/drivers/platform/x86/socwatch/inc/sw_structs.h
@@ -1,544 +1,585 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_STRUCTS_H__
-#define __SW_STRUCTS_H__ 1
-
-#include "sw_types.h"
-
-/*
- * An enumeration of MSR types.
- * Required if we want to differentiate
- * between different types of MSRs.
- */
-enum sw_msr_type {
-	SW_MSR_TYPE_THREAD,
-	SW_MSR_TYPE_CORE,
-	SW_MSR_TYPE_MODULE,
-	SW_MSR_TYPE_PACKAGE,
-	SW_MSR_TYPE_SOC,
-	SW_MSR_TYPE_MAX,
-};
-
-/*
- * Convenience for a 'string' data type.
- * Not strictly required.
- */
-#pragma pack(push, 1)
-typedef struct sw_string_type {
-	pw_u16_t len;
-	char data[1];
-} sw_string_type_t;
-#pragma pack(pop)
-#define SW_STRING_TYPE_HEADER_SIZE()			\
-	(sizeof(struct sw_string_type) - sizeof(char[1]))
-
-#pragma pack(push, 1)
-struct sw_key_value_payload {
-	pw_u16_t m_numKeyValuePairs;
-	char data[1];
-};
-#pragma pack(pop)
-#define SW_KEY_VALUE_PAYLOAD_HEADER_SIZE()		\
-	(sizeof(struct sw_key_value_payload) - sizeof(char[1]))
-
-typedef enum sw_kernel_wakelock_type {
-	/* A kernel wakelock was acquired */
-	SW_WAKE_LOCK = 0,
-	/* A kernel wakelock was released */
-	SW_WAKE_UNLOCK = 1,
-	/* A kernel wakelock was acquired with a timeout */
-	SW_WAKE_LOCK_TIMEOUT = 2,
-	/* A kernel wakelock was acquired before the collection started*/
-	SW_WAKE_LOCK_INITIAL = 3,
-	/* All previously held kernel wakelocks were
-	 * released -- used in ACPI S3 notifications
-	 */
-	SW_WAKE_UNLOCK_ALL = 4,
-} sw_kernel_wakelock_type_t;
-
-typedef enum sw_when_type {
-	SW_WHEN_TYPE_BEGIN = 0, /* Start snapshot */
-	SW_WHEN_TYPE_POLL,
-	SW_WHEN_TYPE_NOTIFIER,
-	SW_WHEN_TYPE_TRACEPOINT,
-	SW_WHEN_TYPE_END, /* Stop snapshot */
-	SW_WHEN_TYPE_NONE
-} sw_when_type_t;
-
-/**
- * trigger_bits is defined to use type pw_u8_t that makes only
- * upto 8 types possible
- */
-#define SW_TRIGGER_BEGIN_MASK()		(1U << SW_WHEN_TYPE_BEGIN)
-#define SW_TRIGGER_END_MASK()		(1U << SW_WHEN_TYPE_END)
-#define SW_TRIGGER_POLL_MASK()		(1U << SW_WHEN_TYPE_POLL)
-#define SW_TRIGGER_TRACEPOINT_MASK()	(1U << SW_WHEN_TYPE_TRACEPOINT)
-#define SW_TRIGGER_NOTIFIER_MASK()	(1U << SW_WHEN_TYPE_NOTIFIER)
-#define SW_GET_TRIGGER_MASK_VALUE(m)	(1U << (m))
-#define SW_TRIGGER_MASK_ALL()		(0xFF)
-
-enum sw_io_cmd {
-	SW_IO_CMD_READ = 0,
-	SW_IO_CMD_WRITE,
-	SW_IO_CMD_MAX
-};
-
-
-#pragma pack(push, 1)
-struct sw_driver_msr_io_descriptor {
-	pw_u64_t address;
-	enum sw_msr_type type;
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-struct sw_driver_ipc_mmio_io_descriptor {
-	union {
-#ifdef SWW_MERGE
-#pragma warning(push)
-/* disable C4201: nonstandard extension used: nameless struct/union */
-#pragma warning(disable:4201)
-#endif
-		struct {
-			pw_u16_t command;
-			pw_u16_t sub_command;
-		};
-#ifdef SWW_MERGE
-#pragma warning(pop) /* enable C4201 */
-#endif
-		union {
-			/* (sub_command << 12) | (command) */
-			pw_u32_t ipc_command;
-			pw_u8_t is_gbe; /* Used only for GBE MMIO */
-		};
-	};
-	/* TODO: add a section for 'ctrl_address' and
-	 * 'ctrl_remapped_address'
-	 */
-	union {
-		pw_u64_t data_address; /* Will be "io_remapped" */
-		pw_u64_t data_remapped_address;
-	};
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-struct sw_driver_pci_io_descriptor {
-	pw_u32_t bus;
-	pw_u32_t device;
-	pw_u32_t function;
-#ifdef __QNX__
-	union {
-		pw_u32_t offset;
-		pw_u32_t index;
-	};
-#else /* __QNX__ */
-	pw_u32_t offset;
-#endif /* __QNX__ */
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-struct sw_driver_configdb_io_descriptor {
-	/* pw_u32_t port; */
-	/* pw_u32_t offset; */
-	pw_u32_t address;
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-struct sw_driver_trace_args_io_descriptor {
-	/* Number of valid entries in the 'args' array, below;
-	 * 1 <= num_args <= 7
-	 */
-	pw_u8_t num_args;
-	/* Max of 7 args can be recorded */
-	pw_u8_t args[7];
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-/**
- * struct - sw_driver_telem_io_descriptor - Telemetry Metric descriptor
- *
- * @id:	(Client & Driver) Telemetry ID of the counter to read.
- * @idx:   (Driver only) index into telem array to read, or the row
- *		of the telem_indirect table to lookup the telem array index.
- * @unit:  Unit from which to collect:  0 = PMC, 1 = PUNIT
- *		Values come from the telemetry_unit enum.
- * @scale_op:  When there are multiple instances of a telem value (e.g.
- *		module C-states) the operation to use when scaling the CPU ID
- *		and adding it to the telemetry data ID.
- * @scale_val: Amount to scale an ID (when scaling one.)
- *
- * Like all hardware mechanism descriptors, the client uses this to pass
- * metric hardware properties (unit and ID) to the driver.  The driver
- * uses it to program the telemetry unit.
- *
- * Users can specify that IDs should be scaled based on the CPU id, using
- * the equation: ID = ID_value + (cpuid <scaling_op> <scaling_val>)
- * where <scaling_op> is one of +, *, /, or %, and scaling_val is an integer
- * value.  This gives you:
- *	Operation			scale_op	 	scale_val
- *	Single instance of an ID	*			0
- *	sequentially increasing
- *	CPU-specific values		*			1
- *	Per module cpu-specific
- *	values (2 cores/module)		/			2
- *	Round Robin assignment		%			cpu_count
- *
- * Note that scaling_value of 0 implies that no scaling should be
- * applied.  While (*, 1) is equivalent to (+, 0), the scaling value of 0
- * is reserved/defined to mean "no scaling", and is disallowed.
- *
- * If you're really tight on space, you could always fold unit and
- * scale_op into a single byte without a lot of pain or even effort.
- */
-struct sw_driver_telem_io_descriptor {
-	union {
-		pw_u16_t id;
-		pw_s8_t  idx;
-	};
-	pw_u8_t   unit;
-	pw_u8_t   scale_op;
-	pw_u16_t  scale_val;
-};
-#pragma pack(pop)
-enum telemetry_unit { TELEM_PUNIT = 0, TELEM_PMC, TELEM_UNIT_NONE };
-#define TELEM_MAX_ID	0xFFFF  /* Maximum value of a Telemtry event ID. */
-#define TELEM_MAX_SCALE 0xFFFF  /* Maximum ID scaling value. */
-#define TELEM_OP_ADD	'+'	 /* Addition operator */
-#define TELEM_OP_MULT	'*'	 /* Multiplication operator */
-#define TELEM_OP_DIV	'/'	 /* Division operator */
-#define TELEM_OP_MOD	'%'	 /* Modulus operator */
-#define TELEM_OP_NONE	'X'	 /* No operator--Not a scaled ID */
-
-#pragma pack(push, 1)
-struct sw_driver_mailbox_io_descriptor {
-	union {
-		/*
-		 * Will be "io_remapped"
-		 */
-		pw_u64_t interface_address;
-		pw_u64_t interface_remapped_address;
-	};
-	union {
-		/*
-		 * Will be "io_remapped"
-		 */
-		pw_u64_t data_address;
-		pw_u64_t data_remapped_address;
-	};
-	pw_u64_t command;
-	pw_u64_t command_mask;
-	pw_u16_t run_busy_bit;
-	pw_u16_t is_msr_type;
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-struct sw_driver_pch_mailbox_io_descriptor {
-	union {
-		/*
-		 * Will be "io_remapped"
-		 */
-		pw_u64_t mtpmc_address;
-		pw_u64_t mtpmc_remapped_address;
-	};
-	union {
-		/*
-	 	* Will be "io_remapped"
-		*/
-		pw_u64_t msg_full_sts_address;
-		pw_u64_t msg_full_sts_remapped_address;
-	};
-	union {
-		/*
-		 * Will be "io_remapped"
-		 */
-		pw_u64_t mfpmc_address;
-		pw_u64_t mfpmc_remapped_address;
-	};
-	pw_u32_t data_address;
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-typedef struct sw_driver_io_descriptor {
-	pw_u16_t collection_type;
-	/* TODO: specify READ/WRITE */
-	pw_s16_t collection_command; /* One of 'enum sw_io_cmd' */
-	pw_u16_t counter_size_in_bytes; /* The number of bytes to READ or WRITE */
-	union {
-		struct sw_driver_msr_io_descriptor 		msr_descriptor;
-		struct sw_driver_ipc_mmio_io_descriptor 	ipc_descriptor;
-		struct sw_driver_ipc_mmio_io_descriptor 	mmio_descriptor;
-		struct sw_driver_pci_io_descriptor		pci_descriptor;
-		struct sw_driver_configdb_io_descriptor		configdb_descriptor;
-		struct sw_driver_trace_args_io_descriptor 	trace_args_descriptor;
-		struct sw_driver_telem_io_descriptor		telem_descriptor;
-		struct sw_driver_pch_mailbox_io_descriptor 	pch_mailbox_descriptor;
-		struct sw_driver_mailbox_io_descriptor		mailbox_descriptor;
-	};
-	pw_u64_t write_value; /* The value to WRITE */
-} sw_driver_io_descriptor_t;
-#pragma pack(pop)
-
-/**
- * sw_driver_interface_info is used to map data collected by kernel-level
- * collectors to metrics.  The client passes one of these structs to the
- * driver for each metric the driver should collect.  The driver tags the
- * collected data (messages) using info from this struct. When processing
- * data from the driver, the client uses its copy of this data to
- * identify the plugin, metric, and message IDs of each message.
- */
-#pragma pack(push, 1)
-struct sw_driver_interface_info {
-	pw_u64_t tracepoint_id_mask;
-	pw_u64_t notifier_id_mask;
-	pw_s16_t cpu_mask;  /* On which CPU(s) should the driver read the data? */
-			/* Currently:  -2 ==> read on ALL CPUs, */
-			/* -1 ==> read on ANY CPU, */
-			/* >= 0 ==> the specific CPU to read on */
-	pw_s16_t plugin_id; /* Metric Plugin SID */
-	pw_s16_t metric_id; /* Domain-specific ID assigned by each Metric Plugin */
-	pw_s16_t msg_id; /* Msg ID retrieved from the SoC Watch config file */
-	pw_u16_t num_io_descriptors; /* Number of descriptors in the array, below. */
-	pw_u8_t  trigger_bits;  /* Mask of 'when bits' to fire this collector. */
-	pw_u16_t sampling_interval_msec; /* Sampling interval, in msecs */
-	pw_u8_t  descriptors[1];	 /* Array of sw_driver_io_descriptor structs. */
-};
-#pragma pack(pop)
-
-#define SW_DRIVER_INTERFACE_INFO_HEADER_SIZE()	\
-	(sizeof(struct sw_driver_interface_info) - sizeof(pw_u8_t[1]))
-
-#pragma pack(push, 1)
-struct sw_driver_interface_msg {
-	/* Number of 'sw_driver_interface_info' structs contained within
-	 * the 'infos' variable, below
-	 */
-	pw_u16_t num_infos;
-	/* Min time to wait before polling; used exclusively
-	 * with the low overhead, context-switch based
-	 * polling mode
-	 */
-	pw_u16_t min_polling_interval_msecs;
-	pw_u8_t infos[1];
-};
-#pragma pack(pop)
-#define SW_DRIVER_INTERFACE_MSG_HEADER_SIZE()	\
-	(sizeof(struct sw_driver_interface_msg) - sizeof(pw_u8_t[1]))
-
-typedef enum sw_name_id_type {
-	SW_NAME_TYPE_TRACEPOINT,
-	SW_NAME_TYPE_NOTIFIER,
-	SW_NAME_TYPE_COLLECTOR,
-	SW_NAME_TYPE_MAX,
-} sw_name_id_type_t;
-
-#pragma pack(push, 1)
-struct sw_name_id_pair {
-	pw_u16_t id;
-	pw_u16_t type; /* One of 'sw_name_id_type' */
-	struct sw_string_type name;
-};
-#pragma pack(pop)
-#define SW_NAME_ID_HEADER_SIZE() 		\
-	(sizeof(struct sw_name_id_pair) - sizeof(struct sw_string_type))
-
-#pragma pack(push, 1)
-struct sw_name_info_msg {
-	pw_u16_t num_name_id_pairs;
-	pw_u16_t payload_len;
-	pw_u8_t pairs[1];
-};
-#pragma pack(pop)
-
-/**
- * This is the basic data structure for passing data collected by the
- * kernel-level collectors up to the client.  In addition to the data
- * (payload), it contains the minimum metadata required for the client
- * to identify the source of that data.
- */
-#pragma pack(push, 1)
-typedef struct sw_driver_msg {
-	pw_u64_t tsc;
-	pw_u16_t cpuidx;
-	/* Cannot have more than 256 plugins */
-	pw_u8_t  plugin_id;
-	/* Each plugin cannot handle more than 256 metrics */
-	pw_u8_t  metric_id;
-	/* Each metric cannot have more than 256 components */
-	pw_u8_t  msg_id;
-	pw_u16_t payload_len;
-	/* pw_u64_t p_payload;  // Ptr to payload */
-	union {
-		/* Ensure size of struct is consistent on x86, x64 */
-		pw_u64_t __dummy;
-		/* Ptr to payload (collected data values). */
-		char	*p_payload;
-	};
-} sw_driver_msg_t;
-#pragma pack(pop)
-#define SW_DRIVER_MSG_HEADER_SIZE() 		\
-	(sizeof(struct sw_driver_msg) - sizeof(pw_u64_t))
-
-typedef enum sw_driver_collection_cmd {
-	SW_DRIVER_START_COLLECTION = 1,
-	SW_DRIVER_STOP_COLLECTION = 2,
-	SW_DRIVER_PAUSE_COLLECTION = 3,
-	SW_DRIVER_RESUME_COLLECTION = 4,
-	SW_DRIVER_CANCEL_COLLECTION = 5,
-} sw_driver_collection_cmd_t;
-
-#pragma pack(push, 1)
-struct sw_driver_version_info {
-	pw_u16_t major;
-	pw_u16_t minor;
-	pw_u16_t other;
-};
-#pragma pack(pop)
-
-enum cpu_action {
-	SW_CPU_ACTION_NONE,
-	SW_CPU_ACTION_OFFLINE,
-	SW_CPU_ACTION_ONLINE_PREPARE,
-	SW_CPU_ACTION_ONLINE,
-	SW_CPU_ACTION_MAX,
-};
-#pragma pack(push, 1)
-struct sw_driver_topology_change {
-	pw_u64_t timestamp; /* timestamp */
-	enum cpu_action type; /* One of 'enum cpu_action' */
-	pw_u16_t cpu; /* logical cpu */
-	pw_u16_t core; /* core id */
-	pw_u16_t pkg; /* pkg/physical id */
-};
-struct sw_driver_topology_msg {
-	pw_u16_t num_entries;
-	pw_u8_t topology_entries[1];
-};
-#pragma pack(pop)
-
-/**
- * An enumeration of possible pm states that
- * SoC Watch is interested in
- */
-enum sw_pm_action {
-	SW_PM_ACTION_NONE,
-	SW_PM_ACTION_SUSPEND_ENTER,
-	SW_PM_ACTION_SUSPEND_EXIT,
-	SW_PM_ACTION_HIBERNATE_ENTER,
-	SW_PM_ACTION_HIBERNATE_EXIT,
-	SW_PM_ACTION_MAX,
-};
-
-/**
- * An enumeration of possible actions that trigger
- * the power notifier
- */
-enum sw_pm_mode {
-	SW_PM_MODE_FIRMWARE,
-	SW_PM_MODE_NONE,
-};
-
-#define SW_PM_VALUE(mode, action) ((mode) << 16 | (action))
-
-#pragma pack(push, 1)
-/*
- * Structure for continuous collection
- */
-struct sw_driver_continuous_collect {
-	/* Size of data that needs to be collected every second */
-	pw_u32_t collection_size;
-	/* struct sw_driver_interface_msg for this collection */
-	pw_u8_t payload[1];
-};
-#define SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE() 		\
-		(sizeof(struct sw_driver_continuous_collect) -	\
-			sizeof(pw_u8_t[1]))
-#pragma pack(pop)
-
-/*
- * Wrapper for ioctl arguments.
- * EVERY ioctl MUST use this struct!
- */
-#pragma pack(push, 1)
-struct sw_driver_ioctl_arg {
-	pw_s32_t in_len;
-	pw_s32_t out_len;
-	/* pw_u64_t p_in_arg; // Pointer to input arg */
-	/* pw_u64_t p_out_arg; // Pointer to output arg */
-	char *in_arg;
-	char *out_arg;
-};
-#pragma pack(pop)
-
-#pragma pack(push, 1)
-typedef struct sw_driver_msg_interval {
-	/* Cannot have more than 256 plugins */
-	pw_u8_t  plugin_id;
-	/* Each plugin cannot handle more than 256 metrics */
-	pw_u8_t  metric_id;
-	/* Each metric cannot have more than 256 components */
-	pw_u8_t  msg_id;
-	/* collection interval */
-	pw_u16_t interval;
-} sw_driver_msg_interval_t;
-#pragma pack(pop)
-
-#endif /* __SW_STRUCTS_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_STRUCTS_H__
+#define __SW_STRUCTS_H__ 1
+
+#include "sw_types.h"
+
+
+/*
+ * An enumeration of MSR types.
+ * Required if we want to differentiate
+ * between different types of MSRs.
+ */
+enum sw_msr_type {
+	SW_MSR_TYPE_THREAD,
+	SW_MSR_TYPE_CORE,
+	SW_MSR_TYPE_MODULE,
+	SW_MSR_TYPE_PACKAGE,
+	SW_MSR_TYPE_SOC,
+	SW_MSR_TYPE_MAX,
+};
+
+/*
+ * Convenience for a 'string' data type.
+ * Not strictly required.
+ */
+#pragma pack(push, 1)
+typedef struct sw_string_type {
+	pw_u16_t len;
+	char data[1];
+} sw_string_type_t;
+#pragma pack(pop)
+#define SW_STRING_TYPE_HEADER_SIZE()			\
+	(sizeof(struct sw_string_type) - sizeof(char[1]))
+
+#pragma pack(push, 1)
+struct sw_key_value_payload {
+	pw_u16_t m_numKeyValuePairs;
+	char data[1];
+};
+#pragma pack(pop)
+#define SW_KEY_VALUE_PAYLOAD_HEADER_SIZE()		\
+	(sizeof(struct sw_key_value_payload) - sizeof(char[1]))
+
+typedef enum sw_kernel_wakelock_type {
+	/* A kernel wakelock was acquired */
+	SW_WAKE_LOCK = 0,
+	/* A kernel wakelock was released */
+	SW_WAKE_UNLOCK = 1,
+	/* A kernel wakelock was acquired with a timeout */
+	SW_WAKE_LOCK_TIMEOUT = 2,
+	/* A kernel wakelock was acquired before the collection started*/
+	SW_WAKE_LOCK_INITIAL = 3,
+	/* All previously held kernel wakelocks were
+	 * released -- used in ACPI S3 notifications
+	 */
+	SW_WAKE_UNLOCK_ALL = 4,
+} sw_kernel_wakelock_type_t;
+
+typedef enum sw_when_type {
+	SW_WHEN_TYPE_BEGIN = 0, /* Start snapshot */
+	SW_WHEN_TYPE_POLL,
+	SW_WHEN_TYPE_NOTIFIER,
+	SW_WHEN_TYPE_TRACEPOINT,
+	SW_WHEN_TYPE_END, /* Stop snapshot */
+	SW_WHEN_TYPE_NONE
+} sw_when_type_t;
+
+/**
+ * trigger_bits is defined to use type pw_u8_t that makes only
+ * upto 8 types possible
+ */
+#define SW_TRIGGER_BEGIN_MASK()		(1U << SW_WHEN_TYPE_BEGIN)
+#define SW_TRIGGER_END_MASK()		(1U << SW_WHEN_TYPE_END)
+#define SW_TRIGGER_POLL_MASK()		(1U << SW_WHEN_TYPE_POLL)
+#define SW_TRIGGER_TRACEPOINT_MASK()	(1U << SW_WHEN_TYPE_TRACEPOINT)
+#define SW_TRIGGER_NOTIFIER_MASK()	(1U << SW_WHEN_TYPE_NOTIFIER)
+#define SW_GET_TRIGGER_MASK_VALUE(m)	(1U << (m))
+#define SW_TRIGGER_MASK_ALL()		(0xFF)
+
+/*
+ * AGGREGATOR TELEMETRY
+ */
+#define MAX_TELEM_AGGR_DEVICES 10
+
+
+enum sw_io_cmd {
+	SW_IO_CMD_READ = 0,
+	SW_IO_CMD_WRITE,
+	SW_IO_CMD_MAX
+};
+
+
+#pragma pack(push, 1)
+struct sw_driver_msr_io_descriptor {
+	pw_u64_t address;
+	enum sw_msr_type type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_ipc_mmio_io_descriptor {
+	union {
+#ifdef SWW_MERGE
+#pragma warning(push)
+/* disable C4201: nonstandard extension used: nameless struct/union */
+#pragma warning(disable:4201)
+#endif
+		struct {
+			pw_u16_t command;
+			pw_u16_t sub_command;
+		};
+#ifdef SWW_MERGE
+#pragma warning(pop) /* enable C4201 */
+#endif
+		union {
+			/* (sub_command << 12) | (command) */
+			pw_u32_t ipc_command;
+			pw_u8_t is_gbe; /* Used only for GBE MMIO */
+		};
+	};
+	/* TODO: add a section for 'ctrl_address' and
+	 * 'ctrl_remapped_address'
+	 */
+	union {
+		pw_u64_t data_address; /* Will be "io_remapped" */
+		pw_u64_t data_remapped_address;
+	};
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pci_io_descriptor {
+	pw_u32_t bus;
+	pw_u32_t device;
+	pw_u32_t function;
+#ifdef __QNX__
+	union {
+		pw_u32_t offset;
+		pw_u32_t index;
+	};
+#else /* __QNX__ */
+	pw_u32_t offset;
+#endif /* __QNX__ */
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_configdb_io_descriptor {
+	/* pw_u32_t port; */
+	/* pw_u32_t offset; */
+	pw_u32_t address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_trace_args_io_descriptor {
+	/* Number of valid entries in the 'args' array, below;
+	 * 1 <= num_args <= 7
+	 */
+	pw_u8_t num_args;
+	/* Max of 7 args can be recorded */
+	pw_u8_t args[7];
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+/**
+ * struct - sw_driver_telem_io_descriptor - Telemetry Metric descriptor
+ *
+ * @id:	(Client & Driver) Telemetry ID of the counter to read.
+ * @idx:   (Driver only) index into telem array to read, or the row
+ *		of the telem_indirect table to lookup the telem array index.
+ * @unit:  Unit from which to collect:  0 = PMC, 1 = PUNIT
+ *		Values come from the telemetry_unit enum.
+ * @scale_op:  When there are multiple instances of a telem value (e.g.
+ *		module C-states) the operation to use when scaling the CPU ID
+ *		and adding it to the telemetry data ID.
+ * @scale_val: Amount to scale an ID (when scaling one.)
+ *
+ * Like all hardware mechanism descriptors, the client uses this to pass
+ * metric hardware properties (unit and ID) to the driver.  The driver
+ * uses it to program the telemetry unit.
+ *
+ * Users can specify that IDs should be scaled based on the CPU id, using
+ * the equation: ID = ID_value + (cpuid <scaling_op> <scaling_val>)
+ * where <scaling_op> is one of +, *, /, or %, and scaling_val is an integer
+ * value.  This gives you:
+ *	Operation			scale_op	 	scale_val
+ *	Single instance of an ID	*			0
+ *	sequentially increasing
+ *	CPU-specific values		*			1
+ *	Per module cpu-specific
+ *	values (2 cores/module)		/			2
+ *	Round Robin assignment		%			cpu_count
+ *
+ * Note that scaling_value of 0 implies that no scaling should be
+ * applied.  While (*, 1) is equivalent to (+, 0), the scaling value of 0
+ * is reserved/defined to mean "no scaling", and is disallowed.
+ *
+ * If you're really tight on space, you could always fold unit and
+ * scale_op into a single byte without a lot of pain or even effort.
+ */
+struct sw_driver_telem_io_descriptor {
+	union {
+		pw_u16_t id;
+		pw_s8_t  idx;
+	};
+	pw_u8_t   unit;
+	pw_u8_t   scale_op;
+	pw_u16_t  scale_val;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+/**
+ * struct - sw_driver_aggr_telem_io_descriptor - Aggregate Telemetry Metric descriptor
+ * This descriptor is used to interact with TA and CTA driver to get aggregate telemetry data
+ * @num_entries: number of entries we want to read from aggregate telemetry SRAM.
+ * Note: These entries should be contigous then only TA and CTA driver can read them together
+ * @offset First offset which we want to read from aggregate telemetry data
+ * All the offsets are specified in the XML file
+ */
+struct sw_driver_aggr_telem_io_descriptor {
+    pw_u64_t  offset;
+    pw_u64_t  data_remapped_address;
+    pw_u32_t  num_entries;
+};
+#pragma pack(pop)
+
+enum telemetry_unit { TELEM_PUNIT = 0, TELEM_PMC, TELEM_UNIT_NONE };
+#define TELEM_MAX_ID	0xFFFF  /* Maximum value of a Telemtry event ID. */
+#define TELEM_MAX_SCALE 0xFFFF  /* Maximum ID scaling value. */
+#define TELEM_OP_ADD	'+'	 /* Addition operator */
+#define TELEM_OP_MULT	'*'	 /* Multiplication operator */
+#define TELEM_OP_DIV	'/'	 /* Division operator */
+#define TELEM_OP_MOD	'%'	 /* Modulus operator */
+#define TELEM_OP_NONE	'X'	 /* No operator--Not a scaled ID */
+
+#pragma pack(push, 1)
+struct sw_driver_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t interface_address;
+		pw_u64_t interface_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t data_address;
+		pw_u64_t data_remapped_address;
+	};
+	pw_u64_t command;
+	pw_u64_t command_mask;
+	pw_u16_t run_busy_bit;
+	pw_u16_t is_msr_type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pch_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mtpmc_address;
+		pw_u64_t mtpmc_remapped_address;
+	};
+	union {
+		/*
+	 	* Will be "io_remapped"
+		*/
+		pw_u64_t msg_full_sts_address;
+		pw_u64_t msg_full_sts_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mfpmc_address;
+		pw_u64_t mfpmc_remapped_address;
+	};
+	pw_u32_t data_address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_io_descriptor {
+	pw_u16_t collection_type;
+	/* TODO: specify READ/WRITE */
+	pw_s16_t collection_command; /* One of 'enum sw_io_cmd' */
+	pw_u16_t counter_size_in_bytes; /* The number of bytes to READ or WRITE */
+	union {
+			struct sw_driver_msr_io_descriptor          msr_descriptor;
+			struct sw_driver_ipc_mmio_io_descriptor     ipc_descriptor;
+			struct sw_driver_ipc_mmio_io_descriptor     mmio_descriptor;
+			struct sw_driver_pci_io_descriptor          pci_descriptor;
+			struct sw_driver_configdb_io_descriptor     configdb_descriptor;
+			struct sw_driver_trace_args_io_descriptor   trace_args_descriptor;
+			struct sw_driver_telem_io_descriptor        telem_descriptor;
+			struct sw_driver_pch_mailbox_io_descriptor  pch_mailbox_descriptor;
+			struct sw_driver_mailbox_io_descriptor      mailbox_descriptor;
+			struct sw_driver_aggr_telem_io_descriptor   aggr_telem_descriptor;
+	};
+	pw_u64_t write_value; /* The value to WRITE */
+} sw_driver_io_descriptor_t;
+#pragma pack(pop)
+
+/**
+ * sw_driver_interface_info is used to map data collected by kernel-level
+ * collectors to metrics.  The client passes one of these structs to the
+ * driver for each metric the driver should collect.  The driver tags the
+ * collected data (messages) using info from this struct. When processing
+ * data from the driver, the client uses its copy of this data to
+ * identify the plugin, metric, and message IDs of each message.
+ */
+#pragma pack(push, 1)
+struct sw_driver_interface_info {
+	pw_u64_t tracepoint_id_mask;
+	pw_u64_t notifier_id_mask;
+	pw_s16_t cpu_mask;  /* On which CPU(s) should the driver read the data? */
+			/* Currently:  -2 ==> read on ALL CPUs, */
+			/* -1 ==> read on ANY CPU, */
+			/* >= 0 ==> the specific CPU to read on */
+	pw_s16_t plugin_id; /* Metric Plugin SID */
+	pw_s16_t metric_id; /* Domain-specific ID assigned by each Metric Plugin */
+	pw_s16_t msg_id; /* Msg ID retrieved from the SoC Watch config file */
+	pw_u16_t num_io_descriptors; /* Number of descriptors in the array, below. */
+	pw_u8_t  trigger_bits;  /* Mask of 'when bits' to fire this collector. */
+	pw_u16_t sampling_interval_msec; /* Sampling interval, in msecs */
+	pw_u8_t  descriptors[1];	 /* Array of sw_driver_io_descriptor structs. */
+};
+#pragma pack(pop)
+
+#define SW_DRIVER_INTERFACE_INFO_HEADER_SIZE()	\
+	(sizeof(struct sw_driver_interface_info) - sizeof(pw_u8_t[1]))
+
+#pragma pack(push, 1)
+struct sw_driver_interface_msg {
+	/* Number of 'sw_driver_interface_info' structs contained within
+	 * the 'infos' variable, below
+	 */
+	pw_u16_t num_infos;
+	/* Min time to wait before polling; used exclusively
+	 * with the low overhead, context-switch based
+	 * polling mode
+	 */
+	pw_u16_t min_polling_interval_msecs;
+	pw_u8_t infos[1];
+};
+#pragma pack(pop)
+#define SW_DRIVER_INTERFACE_MSG_HEADER_SIZE()	\
+	(sizeof(struct sw_driver_interface_msg) - sizeof(pw_u8_t[1]))
+
+typedef enum sw_name_id_type {
+	SW_NAME_TYPE_TRACEPOINT,
+	SW_NAME_TYPE_NOTIFIER,
+	SW_NAME_TYPE_COLLECTOR,
+	SW_NAME_TYPE_MAX,
+} sw_name_id_type_t;
+
+#pragma pack(push, 1)
+struct sw_name_id_pair {
+	pw_u16_t id;
+	pw_u16_t type; /* One of 'sw_name_id_type' */
+	struct sw_string_type name;
+};
+#pragma pack(pop)
+#define SW_NAME_ID_HEADER_SIZE() 		\
+	(sizeof(struct sw_name_id_pair) - sizeof(struct sw_string_type))
+
+#pragma pack(push, 1)
+struct sw_name_info_msg {
+	pw_u16_t num_name_id_pairs;
+	pw_u16_t payload_len;
+	pw_u8_t pairs[1];
+};
+#pragma pack(pop)
+
+/**
+ * This is the basic data structure for passing data collected by the
+ * kernel-level collectors up to the client.  In addition to the data
+ * (payload), it contains the minimum metadata required for the client
+ * to identify the source of that data.
+ */
+#pragma pack(push, 1)
+typedef struct sw_driver_msg {
+	pw_u64_t tsc;
+	pw_u16_t cpuidx;
+	/* Cannot have more than 256 plugins */
+	pw_u8_t  plugin_id;
+	/* Each plugin cannot handle more than 256 metrics */
+	pw_u8_t  metric_id;
+	/* Each metric cannot have more than 256 components */
+	pw_u8_t  msg_id;
+	pw_u16_t payload_len;
+	/* pw_u64_t p_payload;  // Ptr to payload */
+	union {
+		/* Ensure size of struct is consistent on x86, x64 */
+		pw_u64_t __dummy;
+		/* Ptr to payload (collected data values). */
+		char	*p_payload;
+	};
+} sw_driver_msg_t;
+#pragma pack(pop)
+#define SW_DRIVER_MSG_HEADER_SIZE() 		\
+	(sizeof(struct sw_driver_msg) - sizeof(pw_u64_t))
+
+typedef enum sw_driver_collection_cmd {
+	SW_DRIVER_START_COLLECTION = 1,
+	SW_DRIVER_STOP_COLLECTION = 2,
+	SW_DRIVER_PAUSE_COLLECTION = 3,
+	SW_DRIVER_RESUME_COLLECTION = 4,
+	SW_DRIVER_CANCEL_COLLECTION = 5,
+} sw_driver_collection_cmd_t;
+
+#pragma pack(push, 1)
+struct sw_driver_version_info {
+	pw_u16_t major;
+	pw_u16_t minor;
+	pw_u16_t other;
+};
+#pragma pack(pop)
+
+enum cpu_action {
+	SW_CPU_ACTION_NONE,
+	SW_CPU_ACTION_OFFLINE,
+	SW_CPU_ACTION_ONLINE_PREPARE,
+	SW_CPU_ACTION_ONLINE,
+	SW_CPU_ACTION_MAX,
+};
+#pragma pack(push, 1)
+struct sw_driver_topology_change {
+	pw_u64_t timestamp; /* timestamp */
+	enum cpu_action type; /* One of 'enum cpu_action' */
+	pw_u16_t cpu; /* logical cpu */
+	pw_u16_t core; /* core id */
+	pw_u16_t pkg; /* pkg/physical id */
+};
+struct sw_driver_topology_msg {
+	pw_u16_t num_entries;
+	pw_u8_t topology_entries[1];
+};
+#pragma pack(pop)
+
+/**
+ * An enumeration of possible pm states that
+ * SoC Watch is interested in
+ */
+enum sw_pm_action {
+	SW_PM_ACTION_NONE,
+	SW_PM_ACTION_SUSPEND_ENTER,
+	SW_PM_ACTION_SUSPEND_EXIT,
+	SW_PM_ACTION_HIBERNATE_ENTER,
+	SW_PM_ACTION_HIBERNATE_EXIT,
+	SW_PM_ACTION_MAX,
+};
+
+/**
+ * An enumeration of possible actions that trigger
+ * the power notifier
+ */
+enum sw_pm_mode {
+	SW_PM_MODE_FIRMWARE,
+	SW_PM_MODE_NONE,
+};
+
+#define SW_PM_VALUE(mode, action) ((mode) << 16 | (action))
+
+#pragma pack(push, 1)
+/*
+ * Structure for continuous collection
+ */
+struct sw_driver_continuous_collect {
+	/* Size of data that needs to be collected every second */
+	pw_u32_t collection_size;
+	/* struct sw_driver_interface_msg for this collection */
+	pw_u8_t payload[1];
+};
+#define SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE() 		\
+		(sizeof(struct sw_driver_continuous_collect) -	\
+			sizeof(pw_u8_t[1]))
+#pragma pack(pop)
+
+/*
+ * Wrapper for ioctl arguments.
+ * EVERY ioctl MUST use this struct!
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	/* pw_u64_t p_in_arg; // Pointer to input arg */
+	/* pw_u64_t p_out_arg; // Pointer to output arg */
+	char *in_arg;
+	char *out_arg;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_msg_interval {
+	/* Cannot have more than 256 plugins */
+	pw_u8_t  plugin_id;
+	/* Each plugin cannot handle more than 256 metrics */
+	pw_u8_t  metric_id;
+	/* Each metric cannot have more than 256 components */
+	pw_u8_t  msg_id;
+	/* collection interval */
+	pw_u16_t interval;
+} sw_driver_msg_interval_t;
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct _sw_aggregator_info {
+	pw_u64_t startAddress;
+	pw_u32_t globalUniqueID;
+	pw_u32_t size;
+} sw_aggregator_info;
+
+typedef struct _sw_aggregator_msg {
+	pw_u32_t num_entries;
+	sw_aggregator_info info[MAX_TELEM_AGGR_DEVICES]; /* Array of sw_aggregator_info structs. */
+} sw_aggregator_msg;
+
+#define AGGREGATOR_BUFFER_SIZE(entries) (sizeof(sw_aggregator_info) * entries + sizeof(pw_u32_t))
+
+#pragma pack(pop)
+
+#endif /* __SW_STRUCTS_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_telem.h b/drivers/platform/x86/socwatch/inc/sw_telem.h
index 3cfbb4ec515d..d9f7d3d9a222 100644
--- a/drivers/platform/x86/socwatch/inc/sw_telem.h
+++ b/drivers/platform/x86/socwatch/inc/sw_telem.h
@@ -1,76 +1,76 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _SW_TELEM_H_
-#define _SW_TELEM_H_ 1
-
-#include "sw_structs.h"      /* sw_driver_io_descriptor */
-#include "sw_types.h"        /* u8 and other types */
-
-int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor);
-void sw_read_telem_info(char *dst_vals, int cpu,
-			const struct sw_driver_io_descriptor *descriptor,
-			u16 counter_size_in_bytes);
-void sw_write_telem_info(char *dst_vals, int cpu,
-			const struct sw_driver_io_descriptor *descriptor,
-			u16 counter_size_in_bytes);
-int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor);
-bool sw_telem_available(void);
-bool sw_telem_post_config(void);
-
-int setup_telem(u64 addrs[3]);
-void destroy_telem(void);
-
-#endif /* SW_TELEM_H */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _SW_TELEM_H_
+#define _SW_TELEM_H_ 1
+
+#include "sw_structs.h"      /* sw_driver_io_descriptor */
+#include "sw_types.h"        /* u8 and other types */
+
+int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor);
+void sw_read_telem_info(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+void sw_write_telem_info(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor);
+bool sw_telem_available(void);
+bool sw_telem_post_config(void);
+
+int setup_telem(u64 addrs[3]);
+void destroy_telem(void);
+
+#endif /* SW_TELEM_H */
diff --git a/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h b/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
index 470f962858a8..61e10efca418 100644
--- a/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
+++ b/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
@@ -1,81 +1,81 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#ifndef __SW_TRACE_NOTIFIER_PROVIDER_H__
-#define __SW_TRACE_NOTIFIER_PROVIDER_H__
-
-u64 sw_timestamp(void);
-/*
- * Some architectures and OS versions require a "discovery"
- * phase for tracepoints and/or notifiers. Allow for that here.
- */
-int sw_extract_trace_notifier_providers(void);
-/*
- * Reset trace/notifier providers at the end
- * of a collection.
- */
-void sw_reset_trace_notifier_providers(void);
-/*
- * Print statistics on trace/notifier provider overheads.
- */
-void sw_print_trace_notifier_provider_overheads(void);
-/*
- * Add all trace/notifier providers.
- */
-int sw_add_trace_notifier_providers(void);
-/*
- * Remove previously added providers.
- */
-void sw_remove_trace_notifier_providers(void);
-#endif /* __SW_TRACE_NOTIFIER_PROVIDER_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#ifndef __SW_TRACE_NOTIFIER_PROVIDER_H__
+#define __SW_TRACE_NOTIFIER_PROVIDER_H__
+
+u64 sw_timestamp(void);
+/*
+ * Some architectures and OS versions require a "discovery"
+ * phase for tracepoints and/or notifiers. Allow for that here.
+ */
+int sw_extract_trace_notifier_providers(void);
+/*
+ * Reset trace/notifier providers at the end
+ * of a collection.
+ */
+void sw_reset_trace_notifier_providers(void);
+/*
+ * Print statistics on trace/notifier provider overheads.
+ */
+void sw_print_trace_notifier_provider_overheads(void);
+/*
+ * Add all trace/notifier providers.
+ */
+int sw_add_trace_notifier_providers(void);
+/*
+ * Remove previously added providers.
+ */
+void sw_remove_trace_notifier_providers(void);
+#endif /* __SW_TRACE_NOTIFIER_PROVIDER_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h b/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
index 70e5b83a72ca..45caa550804b 100644
--- a/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
+++ b/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
@@ -1,155 +1,155 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_TRACEPOINT_HANDLERS_H__
-#define __SW_TRACEPOINT_HANDLERS_H__
-
-#include "sw_internal.h"
-
-extern pw_u16_t sw_min_polling_interval_msecs;
-
-enum sw_trace_data_type {
-	SW_TRACE_COLLECTOR_TRACEPOINT,
-	SW_TRACE_COLLECTOR_NOTIFIER
-};
-
-struct sw_trace_notifier_name {
-	const char *kernel_name;   /* The tracepoint name; used by the kernel
-				    * to identify tracepoints
-				    */
-	const char *abstract_name; /* An abstract name used by plugins
-				    * tospecify tracepoints-of-interest;
-				    * shared with Ring-3
-				    */
-};
-
-typedef struct sw_trace_notifier_data sw_trace_notifier_data_t;
-
-typedef int (*sw_trace_notifier_register_func)(
-			struct sw_trace_notifier_data *node);
-typedef int (*sw_trace_notifier_unregister_func)(
-			struct sw_trace_notifier_data *node);
-
-struct sw_trace_notifier_data {
-	/* Tracepoint or Notifier */
-	enum sw_trace_data_type type;
-	/* Tracepoint name(s) */
-	const struct sw_trace_notifier_name *name;
-	/* probe register function */
-	sw_trace_notifier_register_func probe_register;
-	/* probe unregister function */
-	sw_trace_notifier_unregister_func probe_unregister;
-	struct tracepoint *tp;
-	bool always_register;	/* Set to TRUE if this tracepoint/notifier must
-				 * ALWAYS be registered, regardless of whether
-				 * the user has specified anything to collect
-				 */
-	bool was_registered;
-	/* List of 'sw_collector_data' instances for this
-	 * tracepoint or notifier
-	 */
-	SW_DEFINE_LIST_HEAD(list, sw_collector_data);
-};
-
-struct sw_topology_node {
-	struct sw_driver_topology_change change;
-
-	SW_LIST_ENTRY(list, sw_topology_node);
-};
-
-/* List of entries tracking changes in CPU topology */
-SW_DECLARE_LIST_HEAD(sw_topology_list, sw_topology_node);
-extern size_t sw_num_topology_entries; /* Size of the 'sw_topology_list' */
-
-int sw_extract_tracepoints(void);
-int sw_register_trace_notifiers(void);
-int sw_unregister_trace_notifiers(void);
-
-/*
- * Register a single TRACE/NOTIFY provider.
- */
-int sw_register_trace_notify_provider(struct sw_trace_notifier_data *tnode);
-/*
- * Add all TRACE/NOTIFY providers.
- */
-int sw_add_trace_notify(void);
-void sw_remove_trace_notify(void);
-
-void sw_reset_trace_notifier_lists(void);
-
-void sw_print_trace_notifier_overheads(void);
-
-int sw_for_each_tracepoint_node(
-	int (*func)(struct sw_trace_notifier_data *node, void *priv),
-	void *priv,
-	bool return_on_error);
-int sw_for_each_notifier_node(
-	int (*func)(struct sw_trace_notifier_data *node, void *priv),
-	void *priv,
-	bool return_on_error);
-
-int sw_get_trace_notifier_id(struct sw_trace_notifier_data *node);
-
-const char *sw_get_trace_notifier_kernel_name(
-	struct sw_trace_notifier_data *node);
-const char *sw_get_trace_notifier_abstract_name(
-	struct sw_trace_notifier_data *node);
-
-/*
- * Clear out the topology list.
- */
-void sw_clear_topology_list(void);
-
-#endif /* __SW_TRACEPOINT_HANDLERS_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_TRACEPOINT_HANDLERS_H__
+#define __SW_TRACEPOINT_HANDLERS_H__
+
+#include "sw_internal.h"
+
+extern pw_u16_t sw_min_polling_interval_msecs;
+
+enum sw_trace_data_type {
+	SW_TRACE_COLLECTOR_TRACEPOINT,
+	SW_TRACE_COLLECTOR_NOTIFIER
+};
+
+struct sw_trace_notifier_name {
+	const char *kernel_name;   /* The tracepoint name; used by the kernel
+				    * to identify tracepoints
+				    */
+	const char *abstract_name; /* An abstract name used by plugins
+				    * tospecify tracepoints-of-interest;
+				    * shared with Ring-3
+				    */
+};
+
+typedef struct sw_trace_notifier_data sw_trace_notifier_data_t;
+
+typedef int (*sw_trace_notifier_register_func)(
+			struct sw_trace_notifier_data *node);
+typedef int (*sw_trace_notifier_unregister_func)(
+			struct sw_trace_notifier_data *node);
+
+struct sw_trace_notifier_data {
+	/* Tracepoint or Notifier */
+	enum sw_trace_data_type type;
+	/* Tracepoint name(s) */
+	const struct sw_trace_notifier_name *name;
+	/* probe register function */
+	sw_trace_notifier_register_func probe_register;
+	/* probe unregister function */
+	sw_trace_notifier_unregister_func probe_unregister;
+	struct tracepoint *tp;
+	bool always_register;	/* Set to TRUE if this tracepoint/notifier must
+				 * ALWAYS be registered, regardless of whether
+				 * the user has specified anything to collect
+				 */
+	bool was_registered;
+	/* List of 'sw_collector_data' instances for this
+	 * tracepoint or notifier
+	 */
+	SW_DEFINE_LIST_HEAD(list, sw_collector_data);
+};
+
+struct sw_topology_node {
+	struct sw_driver_topology_change change;
+
+	SW_LIST_ENTRY(list, sw_topology_node);
+};
+
+/* List of entries tracking changes in CPU topology */
+SW_DECLARE_LIST_HEAD(sw_topology_list, sw_topology_node);
+extern size_t sw_num_topology_entries; /* Size of the 'sw_topology_list' */
+
+int sw_extract_tracepoints(void);
+int sw_register_trace_notifiers(void);
+int sw_unregister_trace_notifiers(void);
+
+/*
+ * Register a single TRACE/NOTIFY provider.
+ */
+int sw_register_trace_notify_provider(struct sw_trace_notifier_data *tnode);
+/*
+ * Add all TRACE/NOTIFY providers.
+ */
+int sw_add_trace_notify(void);
+void sw_remove_trace_notify(void);
+
+void sw_reset_trace_notifier_lists(void);
+
+void sw_print_trace_notifier_overheads(void);
+
+int sw_for_each_tracepoint_node(
+	int (*func)(struct sw_trace_notifier_data *node, void *priv),
+	void *priv,
+	bool return_on_error);
+int sw_for_each_notifier_node(
+	int (*func)(struct sw_trace_notifier_data *node, void *priv),
+	void *priv,
+	bool return_on_error);
+
+int sw_get_trace_notifier_id(struct sw_trace_notifier_data *node);
+
+const char *sw_get_trace_notifier_kernel_name(
+	struct sw_trace_notifier_data *node);
+const char *sw_get_trace_notifier_abstract_name(
+	struct sw_trace_notifier_data *node);
+
+/*
+ * Clear out the topology list.
+ */
+void sw_clear_topology_list(void);
+
+#endif /* __SW_TRACEPOINT_HANDLERS_H__ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_types.h b/drivers/platform/x86/socwatch/inc/sw_types.h
index e9af829c31c8..b6be5483bd7c 100644
--- a/drivers/platform/x86/socwatch/inc/sw_types.h
+++ b/drivers/platform/x86/socwatch/inc/sw_types.h
@@ -1,151 +1,151 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef _PW_TYPES_H_
-#define _PW_TYPES_H_
-
-#if defined(__linux__) || defined(__APPLE__) || defined(__QNX__)
-
-#ifndef __KERNEL__
-/*
- * Called from Ring-3.
- */
-#include <stdint.h> /* Grab 'uint64_t' etc. */
-#include <unistd.h> /* Grab 'pid_t' */
-/*
- * UNSIGNED types...
- */
-typedef uint8_t  u8;
-typedef uint16_t u16;
-typedef uint32_t u32;
-typedef uint64_t u64;
-/*
- * SIGNED types...
- */
-typedef int8_t s8;
-typedef int16_t s16;
-typedef int32_t s32;
-typedef int64_t s64;
-
-#else /* __KERNEL__ */
-#if !defined(__APPLE__)
-#include <linux/types.h>
-#else /* __APPLE__ */
-#include <sys/types.h>
-#include <stdint.h> /* Grab 'uint64_t' etc. */
-
-typedef uint8_t  u8;
-typedef uint16_t u16;
-typedef uint32_t u32;
-typedef uint64_t u64;
-/*
-* SIGNED types...
-*/
-typedef int8_t s8;
-typedef int16_t s16;
-typedef int32_t s32;
-typedef int64_t s64;
-#endif /* __APPLE__ */
-#endif /* __KERNEL__ */
-
-#elif defined(_WIN32)
-typedef __int32 int32_t;
-typedef unsigned __int32 uint32_t;
-typedef __int64 int64_t;
-typedef unsigned __int64 uint64_t;
-
-/*
- * UNSIGNED types...
- */
-typedef unsigned char u8;
-typedef unsigned short u16;
-typedef unsigned int u32;
-typedef unsigned long long u64;
-
-/*
- * SIGNED types...
- */
-typedef signed char s8;
-typedef signed short s16;
-typedef signed int s32;
-typedef signed long long s64;
-typedef s32 pid_t;
-typedef s32 ssize_t;
-
-#endif /* _WIN32 */
-
-/* ************************************
- * Common to both operating systems.
- * ************************************
- */
-/*
- * UNSIGNED types...
- */
-typedef u8 pw_u8_t;
-typedef u16 pw_u16_t;
-typedef u32 pw_u32_t;
-typedef u64 pw_u64_t;
-
-/*
- * SIGNED types...
- */
-typedef s8 pw_s8_t;
-typedef s16 pw_s16_t;
-typedef s32 pw_s32_t;
-typedef s64 pw_s64_t;
-
-typedef pid_t pw_pid_t;
-
-#endif /* _PW_TYPES_H_ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _PW_TYPES_H_
+#define _PW_TYPES_H_
+
+#if defined(__linux__) || defined(__APPLE__) || defined(__QNX__)
+
+#ifndef __KERNEL__
+/*
+ * Called from Ring-3.
+ */
+#include <stdint.h> /* Grab 'uint64_t' etc. */
+#include <unistd.h> /* Grab 'pid_t' */
+/*
+ * UNSIGNED types...
+ */
+typedef uint8_t  u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+ * SIGNED types...
+ */
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#else /* __KERNEL__ */
+#if !defined(__APPLE__)
+#include <linux/types.h>
+#else /* __APPLE__ */
+#include <sys/types.h>
+#include <stdint.h> /* Grab 'uint64_t' etc. */
+
+typedef uint8_t  u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+* SIGNED types...
+*/
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+#endif /* __APPLE__ */
+#endif /* __KERNEL__ */
+
+#elif defined(_WIN32)
+typedef __int32 int32_t;
+typedef unsigned __int32 uint32_t;
+typedef __int64 int64_t;
+typedef unsigned __int64 uint64_t;
+
+/*
+ * UNSIGNED types...
+ */
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long long u64;
+
+/*
+ * SIGNED types...
+ */
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long long s64;
+typedef s32 pid_t;
+typedef s32 ssize_t;
+
+#endif /* _WIN32 */
+
+/* ************************************
+ * Common to both operating systems.
+ * ************************************
+ */
+/*
+ * UNSIGNED types...
+ */
+typedef u8 pw_u8_t;
+typedef u16 pw_u16_t;
+typedef u32 pw_u32_t;
+typedef u64 pw_u64_t;
+
+/*
+ * SIGNED types...
+ */
+typedef s8 pw_s8_t;
+typedef s16 pw_s16_t;
+typedef s32 pw_s32_t;
+typedef s64 pw_s64_t;
+
+typedef pid_t pw_pid_t;
+
+#endif /* _PW_TYPES_H_ */
diff --git a/drivers/platform/x86/socwatch/inc/sw_version.h b/drivers/platform/x86/socwatch/inc/sw_version.h
index 2b3330a57f93..c5bdedaf1489 100644
--- a/drivers/platform/x86/socwatch/inc/sw_version.h
+++ b/drivers/platform/x86/socwatch/inc/sw_version.h
@@ -1,73 +1,66 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#ifndef __SW_VERSION_H__
-#define __SW_VERSION_H__ 1
-
-/*
- * SOCWatch driver version
- */
-#define SW_DRIVER_VERSION_MAJOR 2
-#define SW_DRIVER_VERSION_MINOR 10
-#define SW_DRIVER_VERSION_OTHER 1
-
-/*
- * Every SOC Watch userspace component shares the same version number.
- */
-#define SOCWATCH_VERSION_MAJOR 2
-#define SOCWATCH_VERSION_MINOR 10
-#define SOCWATCH_VERSION_OTHER 1
-
-#endif /* __SW_VERSION_H__ */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef __SW_VERSION_H__
+#define __SW_VERSION_H__ 1
+
+/*
+ * SOCWatch driver version
+ */
+#define SW_DRIVER_VERSION_MAJOR 2
+#define SW_DRIVER_VERSION_MINOR 11
+#define SW_DRIVER_VERSION_OTHER 0
+
+#endif /* __SW_VERSION_H__ */
diff --git a/drivers/platform/x86/socwatch/sw_collector.c b/drivers/platform/x86/socwatch/sw_collector.c
index 652d5af6113b..c1be7b9d8b1e 100644
--- a/drivers/platform/x86/socwatch/sw_collector.c
+++ b/drivers/platform/x86/socwatch/sw_collector.c
@@ -1,684 +1,684 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "sw_collector.h"
-#include "sw_hardware_io.h"
-#include "sw_internal.h"
-#include "sw_kernel_defines.h"
-#include "sw_mem.h"
-#include "sw_output_buffer.h"
-#include "sw_structs.h"
-#include "sw_types.h"
-
-/* -------------------------------------------------
- * Variables.
- * -------------------------------------------------
- */
-const static struct sw_hw_ops *s_hw_ops;
-/* -------------------------------------------------
- * Function definitions.
- * -------------------------------------------------
- */
-/*
- * Driver interface info functions.
- */
-
-/**
- * sw_copy_driver_interface_info_i - Allocate and copy the passed-in "info".
- *
- * @info: Information about the metric and collection properties
- *
- * Returns: a pointer to the newly allocated sw_driver_interface_info,
- *          which is a copy of the version passed in via the info pointer.
- */
-struct sw_driver_interface_info *
-sw_copy_driver_interface_info_i(const struct sw_driver_interface_info *info)
-{
-	size_t size;
-	struct sw_driver_interface_info *node = NULL;
-
-	if (!info) {
-		pw_pr_error("ERROR: NULL sw_driver_interface_info in alloc!\n");
-		return node;
-	}
-
-	size = SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
-	       (info->num_io_descriptors *
-		sizeof(struct sw_driver_io_descriptor));
-	node = (struct sw_driver_interface_info *)sw_kmalloc(size, GFP_KERNEL);
-	if (!node) {
-		pw_pr_error("ERROR allocating driver interface info!\n");
-		return node;
-	}
-	memcpy((char *)node, (const char *)info, size);
-
-	/*
-	 * Do debug dump.
-	 */
-	pw_pr_debug(
-		"DRIVER info has plugin_ID = %d, metric_ID = %d, msg_ID = %d\n",
-		node->plugin_id, node->metric_id, node->msg_id);
-
-	return node;
-}
-
-int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info)
-{
-	/*
-	 * Do any initialization here.
-	 * For now, only IPC/MMIO descriptors need to be initialized.
-	 */
-	int i = 0;
-	struct sw_driver_io_descriptor *descriptor = NULL;
-
-	if (!info) {
-		pw_pr_error("ERROR: no info!\n");
-		return -PW_ERROR;
-	}
-	for (i = 0,
-	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
-	     i < info->num_io_descriptors; ++i, ++descriptor) {
-		if (sw_init_driver_io_descriptor(descriptor))
-			return -PW_ERROR;
-	}
-	return PW_SUCCESS;
-}
-
-int sw_init_ops_i(const struct sw_hw_ops **ops,
-		const struct sw_driver_interface_info *info)
-{
-	int i = 0;
-	struct sw_driver_io_descriptor *descriptor = NULL;
-
-	if (!ops || !info)
-		return -PW_ERROR;
-
-	for (i = 0,
-	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
-	i < info->num_io_descriptors; ++i, ++descriptor) {
-		ops[i] = sw_get_hw_ops_for(descriptor->collection_type);
-		if (ops[i] == NULL)
-			return -PW_ERROR;
-	}
-	return PW_SUCCESS;
-}
-
-size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info)
-{
-	size_t size = 0;
-	int i = 0;
-
-	if (info) {
-		for (i = 0;
-			i < info->num_io_descriptors;
-			size += ((struct sw_driver_io_descriptor *)
-				info->descriptors)[i].counter_size_in_bytes,
-			++i
-		)
-			;
-	}
-	return size;
-}
-
-sw_driver_msg_t *
-sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info,
-			 size_t per_msg_payload_size)
-{
-	size_t per_msg_size = 0, total_size = 0;
-	sw_driver_msg_t *msg = NULL;
-
-	if (!info)
-		return NULL;
-
-	per_msg_size = sizeof(struct sw_driver_msg) + per_msg_payload_size;
-	total_size = per_msg_size * num_possible_cpus();
-	msg = (sw_driver_msg_t *)sw_kmalloc(total_size, GFP_KERNEL);
-	if (msg) {
-		int cpu = -1;
-
-		memset(msg, 0, total_size);
-		for_each_possible_cpu(cpu) {
-			sw_driver_msg_t *__msg = GET_MSG_SLOT_FOR_CPU(
-				msg, cpu, per_msg_payload_size);
-			char *__payload =
-				(char *)__msg + sizeof(struct sw_driver_msg);
-
-			__msg->cpuidx = (pw_u16_t)cpu;
-			__msg->plugin_id = (pw_u8_t)info->plugin_id;
-			__msg->metric_id = (pw_u8_t)info->metric_id;
-			__msg->msg_id = (pw_u8_t)info->msg_id;
-			__msg->payload_len = per_msg_payload_size;
-			__msg->p_payload = __payload;
-			pw_pr_debug(
-				"[%d]: per_msg_payload_size = %zx, msg = %p, payload = %p\n",
-				cpu, per_msg_payload_size, __msg, __payload);
-		}
-	}
-	return msg;
-}
-
-const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors)
-{
-	size_t size = num_io_descriptors * sizeof(struct sw_hw_ops *);
-	const struct sw_hw_ops **ops = sw_kmalloc(size, GFP_KERNEL);
-
-	if (ops)
-		memset(ops, 0, size);
-
-	return ops;
-}
-
-/**
- * sw_add_driver_info() - Add a collector node to the list called at this
- *                      "when type".
- * @head:   The collector node list to add the new node to.
- * @info:   Driver information to add to the list.
- *
- *  This function allocates and links in a "collector node" for each
- *  collector based on the collector info in the info parameter.
- *  The function allocates the new node, and links it to a local copy
- *  of the passed-in driver interface info.  If the collector has an
- *  init function among its operations, it iterates through the
- *  descriptors in info, passing each one to the init function.
- *
- *  Finally, it allocates and initializes the "collector message" which
- *  buffers a data sample that this collector gathers during the run.
- *
- * Returns:  -PW_ERROR on failure, PW_SUCCESS on success.
- */
-int sw_add_driver_info(void *list_head,
-		       const struct sw_driver_interface_info *info)
-{
-	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
-	struct sw_collector_data *node = sw_alloc_collector_node();
-
-	if (!node) {
-		pw_pr_error("ERROR allocating collector node!\n");
-		return -PW_ERROR;
-	}
-
-	node->info = sw_copy_driver_interface_info_i(info);
-	if (!node->info) {
-		pw_pr_error(
-			"ERROR allocating or copying driver_interface_info!\n");
-		sw_free_collector_node(node);
-		return -PW_ERROR;
-	}
-	/*
-	 * Initialize the collectors in the node's descriptors.
-	 */
-	if (sw_init_driver_interface_info_i(node->info)) {
-		pw_pr_error(
-			"ERROR initializing a driver_interface_info node!\n");
-		sw_free_collector_node(node);
-		return -PW_ERROR;
-	}
-	/*
-	 * Allocate the ops array. We do this one time as an optimization
-	 * (we could always just repeatedly call 'sw_get_hw_ops_for()'
-	 * during the collection but we want to avoid that overhead)
-	 */
-	node->ops = sw_alloc_ops_i(info->num_io_descriptors);
-	if (!node->ops || sw_init_ops_i(node->ops, info)) {
-		pw_pr_error("ERROR initializing the ops array!\n");
-		sw_free_collector_node(node);
-		return -PW_ERROR;
-	}
-	/*
-	 * Allocate and initialize the "collector message".
-	 */
-	node->per_msg_payload_size = sw_get_payload_size_i(info);
-	pw_pr_debug("Debug: Per msg payload size = %u\n",
-		    (unsigned int)node->per_msg_payload_size);
-	node->msg = sw_alloc_collector_msg_i(info, node->per_msg_payload_size);
-	if (!node->msg) {
-		pw_pr_error("ERROR allocating space for a collector msg!\n");
-		sw_free_collector_node(node);
-		return -PW_ERROR;
-	}
-	pw_pr_debug("NODE = %p, NODE->MSG = %p\n", node, node->msg);
-	cpumask_clear(&node->cpumask);
-	{
-		/*
-		 * For now, use following protocol:
-		 * cpu_mask == -2 ==> Collect on ALL CPUs
-		 * cpu_mask == -1 ==> Collect on ANY CPU
-		 * cpu_mask >= 0 ==> Collect on a specific CPU
-		 */
-		if (node->info->cpu_mask >= 0) {
-			/*
-			 * Collect data on 'node->info->cpu_mask'
-			 */
-			cpumask_set_cpu(node->info->cpu_mask, &node->cpumask);
-			pw_pr_debug("OK: set CPU = %d\n", node->info->cpu_mask);
-		} else if (node->info->cpu_mask == -1) {
-			/*
-			 * Collect data on ANY CPU.  Leave empty as a flag to
-			 * signify user wishes to collect data on 'ANY' cpu.
-			 */
-			pw_pr_debug("OK: set ANY CPU\n");
-		} else {
-			/*
-			 * Collect data on ALL cpus.
-			 */
-			cpumask_copy(&node->cpumask, cpu_present_mask);
-			pw_pr_debug("OK: set ALL CPUs\n");
-		}
-	}
-	SW_LIST_ADD(head, node, list);
-	return PW_SUCCESS;
-}
-
-void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info)
-{
-	if (info)
-		sw_kfree(info);
-}
-
-void sw_free_ops_i(const struct sw_hw_ops **ops)
-{
-	if (ops)
-		sw_kfree(ops);
-}
-
-int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info)
-{
-	/*
-	 * Do any finalization here.
-	 * For now, only IPC/MMIO descriptors need to be finalized.
-	 */
-	int i = 0;
-	struct sw_driver_io_descriptor *descriptor = NULL;
-
-	if (!info) {
-		pw_pr_error("ERROR: no info!\n");
-		return -PW_ERROR;
-	}
-	for (i = 0,
-	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
-	     i < info->num_io_descriptors; ++i, ++descriptor) {
-		if (sw_reset_driver_io_descriptor(descriptor))
-			return -PW_ERROR;
-	}
-	return PW_SUCCESS;
-}
-
-/* If this descriptor's collector has an init function, call it passing in */
-/* this descriptor.  That allows the collector to perform any initialization */
-/* or registration specific to this metric. */
-int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
-{
-	sw_io_desc_init_func_t init_func = NULL;
-	const struct sw_hw_ops *ops =
-		sw_get_hw_ops_for(descriptor->collection_type);
-
-	if (ops == NULL) {
-		pw_pr_error("NULL ops found in init_driver_io_desc: type %d\n",
-			    descriptor->collection_type);
-		return -PW_ERROR;
-	}
-	init_func = ops->init;
-
-	if (init_func) {
-		int retval = (*init_func)(descriptor);
-
-		if (retval)
-			pw_pr_error("(*init) return value for type %d: %d\n",
-				    descriptor->collection_type, retval);
-
-		return retval;
-	}
-	return PW_SUCCESS;
-}
-
-/*
- * If this descriptor's collector has a finalize function, call it passing in
- * this
- * descriptor. This allows the collector to perform any finalization specific to
- * this metric.
- */
-int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
-{
-	sw_io_desc_reset_func_t reset_func = NULL;
-	const struct sw_hw_ops *ops =
-		sw_get_hw_ops_for(descriptor->collection_type);
-
-	if (ops == NULL) {
-		pw_pr_error("NULL ops found in reset_driver_io_desc: type %d\n",
-			    descriptor->collection_type);
-		return -PW_ERROR;
-	}
-	pw_pr_debug("calling reset on descriptor of type %d\n",
-		    descriptor->collection_type);
-	reset_func = ops->reset;
-
-	if (reset_func) {
-		int retval = (*reset_func)(descriptor);
-
-		if (retval)
-			pw_pr_error("(*reset) return value for type %d: %d\n",
-				    descriptor->collection_type, retval);
-
-		return retval;
-	}
-	return PW_SUCCESS;
-}
-
-int sw_handle_driver_io_descriptor(
-	char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	const struct sw_hw_ops *hw_ops)
-{
-	typedef void (*sw_hardware_io_func_t)(
-		char *, int, const struct sw_driver_io_descriptor *, u16);
-	sw_hardware_io_func_t hardware_io_func = NULL;
-
-	if (descriptor->collection_command < SW_IO_CMD_READ ||
-	    descriptor->collection_command > SW_IO_CMD_WRITE) {
-		return -PW_ERROR;
-	}
-	switch (descriptor->collection_command) {
-	case SW_IO_CMD_READ:
-		hardware_io_func = hw_ops->read;
-		break;
-	case SW_IO_CMD_WRITE:
-		hardware_io_func = hw_ops->write;
-		break;
-	default:
-		break;
-	}
-	if (hardware_io_func) {
-		(*hardware_io_func)(dst_vals, cpu, descriptor,
-				    descriptor->counter_size_in_bytes);
-	} else {
-		pw_pr_debug(
-			"NO ops to satisfy %u operation for collection type %u!\n",
-			descriptor->collection_command,
-			descriptor->collection_type);
-	}
-	return PW_SUCCESS;
-}
-
-void sw_free_collector_msg_i(sw_driver_msg_t *msg)
-{
-	if (msg)
-		sw_kfree(msg);
-}
-
-void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action)
-{
-	/*
-	 * Basic algo:
-	 * For each descriptor in 'node->info->descriptors'; do:
-	 * 1. Perform H/W read; use 'descriptor->collection_type' to
-	 * determine type of read; use 'descriptor->counter_size_in_bytes'
-	 * for read size. Use msg->p_payload[dst_idx] as dst address
-	 * 2. Increment dst idx by 'descriptor->counter_size_in_bytes'
-	 */
-	struct sw_collector_data *node = (struct sw_collector_data *)info;
-	int cpu = RAW_CPU();
-	u16 num_descriptors = node->info->num_io_descriptors, i = 0;
-	struct sw_driver_io_descriptor *descriptors =
-		(struct sw_driver_io_descriptor *)node->info->descriptors;
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	char *dst_vals = msg->p_payload;
-	const struct sw_hw_ops **ops = node->ops;
-	bool wasAnyWrite = false;
-
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = cpu;
-
-	for (i = 0; i < num_descriptors; ++i,
-	    dst_vals += descriptors->counter_size_in_bytes, ++descriptors) {
-		if (unlikely(ops[i] == NULL)) {
-			pw_pr_debug("NULL OPS!\n");
-			continue;
-		}
-		if (descriptors->collection_command == SW_IO_CMD_WRITE)
-			wasAnyWrite = true;
-
-		if (sw_handle_driver_io_descriptor(dst_vals, cpu, descriptors,
-						   ops[i]))
-			pw_pr_error("ERROR reading descriptor with type %d\n",
-				    descriptors->collection_type);
-	}
-
-	/*
-	 * We produce messages only on READs. Note that SWA prohibits
-	 * messages that contain both READ and WRITE descriptors, so it
-	 * is enough to check if there was ANY WRITE descriptor in this
-	 * message.
-	 */
-	if (likely(wasAnyWrite == false)) {
-		if (sw_produce_generic_msg(msg, action))
-			pw_pr_warn("WARNING: could NOT produce message!\n");
-	}
-}
-
-/*
- * Collector list and node functions.
- */
-struct sw_collector_data *sw_alloc_collector_node(void)
-{
-	struct sw_collector_data *node = (struct sw_collector_data *)sw_kmalloc(
-		sizeof(struct sw_collector_data), GFP_KERNEL);
-
-	if (node) {
-		node->per_msg_payload_size = 0x0;
-		node->last_update_jiffies = 0x0;
-		node->info = NULL;
-		node->ops = NULL;
-		node->msg = NULL;
-		SW_LIST_ENTRY_INIT(node, list);
-	}
-	return node;
-}
-
-void sw_free_collector_node(struct sw_collector_data *node)
-{
-	if (node) {
-		if (node->info) {
-			sw_reset_driver_interface_info_i(node->info);
-			sw_free_driver_interface_info_i(node->info);
-			node->info = NULL;
-		}
-		if (node->ops) {
-			sw_free_ops_i(node->ops);
-			node->ops = NULL;
-		}
-		if (node->msg) {
-			sw_free_collector_msg_i(node->msg);
-			node->msg = NULL;
-		}
-		sw_kfree(node);
-	}
-}
-
-int sw_handle_collector_node(struct sw_collector_data *node)
-{
-	if (!node || !node->info || !node->ops || !node->msg)
-		return -PW_ERROR;
-
-	pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
-	sw_schedule_work(&node->cpumask, &sw_handle_per_cpu_msg, node);
-	return PW_SUCCESS;
-}
-
-int sw_handle_collector_node_on_cpu(struct sw_collector_data *node, int cpu)
-{
-	if (!node || !node->info || !node->ops || !node->msg)
-		return -PW_ERROR;
-
-	/*
-	 * Check if this node indicates it should be scheduled
-	 * on the given cpu. If so, clear all other CPUs from the
-	 * mask and schedule the node.
-	 */
-	if (cpumask_test_cpu(cpu, &node->cpumask)) {
-		struct cpumask tmp_mask;
-
-		cpumask_clear(&tmp_mask);
-		cpumask_set_cpu(cpu, &tmp_mask);
-		pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
-		sw_schedule_work(&tmp_mask, &sw_handle_per_cpu_msg, node);
-	}
-	return PW_SUCCESS;
-}
-
-void sw_init_collector_list(void *list_head)
-{
-	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
-	SW_LIST_HEAD_INIT(head);
-}
-
-void sw_destroy_collector_list(void *list_head)
-{
-	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
-	while (!SW_LIST_EMPTY(head)) {
-		struct sw_collector_data *curr =
-			SW_LIST_GET_HEAD_ENTRY(head,
-				sw_collector_data,
-				list);
-
-		BUG_ON(!curr->info);
-		SW_LIST_UNLINK(curr, list);
-		sw_free_collector_node(curr);
-	}
-}
-
-/**
- * sw_handle_collector_list - Iterate through the collector list, calling
- *                            func() upon each element.
- * @list_head:  The collector list head.
- * @func:  The function to call for each collector.
- *
- * This function is called when one of the "when types" fires, since the
- * passed-in collector node list is the list of collections to do at that time.
- *
- * Returns: PW_SUCCESS on success, -PW_ERROR on error.
- */
-int sw_handle_collector_list(void *list_head,
-			     int (*func)(struct sw_collector_data *data))
-{
-	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
-	int retVal = PW_SUCCESS;
-	struct sw_collector_data *curr = NULL;
-
-	if (!head || !func)
-		return -PW_ERROR;
-
-	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
-	{
-		pw_pr_debug("HANDLING\n");
-		if ((*func)(curr))
-			retVal = -PW_ERROR;
-
-	}
-	return retVal;
-}
-
-int sw_handle_collector_list_on_cpu(void *list_head,
-				    int (*func)(struct sw_collector_data *data,
-						int cpu),
-				    int cpu)
-{
-	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
-	int retVal = PW_SUCCESS;
-	struct sw_collector_data *curr = NULL;
-
-	if (!head || !func)
-		return -PW_ERROR;
-
-	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
-	{
-		pw_pr_debug("HANDLING\n");
-		if ((*func)(curr, cpu))
-			retVal = -PW_ERROR;
-
-	}
-	return retVal;
-}
-
-void sw_handle_per_cpu_msg(void *info)
-{
-	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_DIRECT);
-}
-
-void sw_handle_per_cpu_msg_no_sched(void *info)
-{
-	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_TIMER);
-}
-
-void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info)
-{
-	if (unlikely(cpu == RAW_CPU()))
-		sw_handle_per_cpu_msg_no_sched(info);
-	else {
-		pw_pr_debug("[%d] is handling for %d\n", RAW_CPU(), cpu);
-		/*
-		 * No need to disable preemption -- 'smp_call_function_single'
-		 * does that for us.
-		 */
-		smp_call_function_single(
-			cpu, &sw_handle_per_cpu_msg_no_sched, info,
-			false
-			/* false ==> do NOT wait for function completion */);
-	}
-}
-
-void sw_set_collector_ops(const struct sw_hw_ops *hw_ops)
-{
-	s_hw_ops = hw_ops;
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "sw_collector.h"
+#include "sw_hardware_io.h"
+#include "sw_internal.h"
+#include "sw_kernel_defines.h"
+#include "sw_mem.h"
+#include "sw_output_buffer.h"
+#include "sw_structs.h"
+#include "sw_types.h"
+
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+const static struct sw_hw_ops *s_hw_ops;
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * Driver interface info functions.
+ */
+
+/**
+ * sw_copy_driver_interface_info_i - Allocate and copy the passed-in "info".
+ *
+ * @info: Information about the metric and collection properties
+ *
+ * Returns: a pointer to the newly allocated sw_driver_interface_info,
+ *          which is a copy of the version passed in via the info pointer.
+ */
+struct sw_driver_interface_info *
+sw_copy_driver_interface_info_i(const struct sw_driver_interface_info *info)
+{
+	size_t size;
+	struct sw_driver_interface_info *node = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: NULL sw_driver_interface_info in alloc!\n");
+		return node;
+	}
+
+	size = SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+	       (info->num_io_descriptors *
+		sizeof(struct sw_driver_io_descriptor));
+	node = (struct sw_driver_interface_info *)sw_kmalloc(size, GFP_KERNEL);
+	if (!node) {
+		pw_pr_error("ERROR allocating driver interface info!\n");
+		return node;
+	}
+	memcpy((char *)node, (const char *)info, size);
+
+	/*
+	 * Do debug dump.
+	 */
+	pw_pr_debug(
+		"DRIVER info has plugin_ID = %d, metric_ID = %d, msg_ID = %d\n",
+		node->plugin_id, node->metric_id, node->msg_id);
+
+	return node;
+}
+
+int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	/*
+	 * Do any initialization here.
+	 * For now, only IPC/MMIO descriptors need to be initialized.
+	 */
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: no info!\n");
+		return -PW_ERROR;
+	}
+	for (i = 0,
+	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	     i < info->num_io_descriptors; ++i, ++descriptor) {
+		if (sw_init_driver_io_descriptor(descriptor))
+			return -PW_ERROR;
+	}
+	return PW_SUCCESS;
+}
+
+int sw_init_ops_i(const struct sw_hw_ops **ops,
+		const struct sw_driver_interface_info *info)
+{
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!ops || !info)
+		return -PW_ERROR;
+
+	for (i = 0,
+	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	i < info->num_io_descriptors; ++i, ++descriptor) {
+		ops[i] = sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops[i] == NULL)
+			return -PW_ERROR;
+	}
+	return PW_SUCCESS;
+}
+
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info)
+{
+	size_t size = 0;
+	int i = 0;
+
+	if (info) {
+		for (i = 0;
+			i < info->num_io_descriptors;
+			size += ((struct sw_driver_io_descriptor *)
+				info->descriptors)[i].counter_size_in_bytes,
+			++i
+		)
+			;
+	}
+	return size;
+}
+
+sw_driver_msg_t *
+sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info,
+			 size_t per_msg_payload_size)
+{
+	size_t per_msg_size = 0, total_size = 0;
+	sw_driver_msg_t *msg = NULL;
+
+	if (!info)
+		return NULL;
+
+	per_msg_size = sizeof(struct sw_driver_msg) + per_msg_payload_size;
+	total_size = per_msg_size * num_possible_cpus();
+	msg = (sw_driver_msg_t *)sw_kmalloc(total_size, GFP_KERNEL);
+	if (msg) {
+		int cpu = -1;
+
+		memset(msg, 0, total_size);
+		for_each_possible_cpu(cpu) {
+			sw_driver_msg_t *__msg = GET_MSG_SLOT_FOR_CPU(
+				msg, cpu, per_msg_payload_size);
+			char *__payload =
+				(char *)__msg + sizeof(struct sw_driver_msg);
+
+			__msg->cpuidx = (pw_u16_t)cpu;
+			__msg->plugin_id = (pw_u8_t)info->plugin_id;
+			__msg->metric_id = (pw_u8_t)info->metric_id;
+			__msg->msg_id = (pw_u8_t)info->msg_id;
+			__msg->payload_len = per_msg_payload_size;
+			__msg->p_payload = __payload;
+			pw_pr_debug(
+				"[%d]: per_msg_payload_size = %zx, msg = %p, payload = %p\n",
+				cpu, per_msg_payload_size, __msg, __payload);
+		}
+	}
+	return msg;
+}
+
+const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors)
+{
+	size_t size = num_io_descriptors * sizeof(struct sw_hw_ops *);
+	const struct sw_hw_ops **ops = sw_kmalloc(size, GFP_KERNEL);
+
+	if (ops)
+		memset(ops, 0, size);
+
+	return ops;
+}
+
+/**
+ * sw_add_driver_info() - Add a collector node to the list called at this
+ *                      "when type".
+ * @head:   The collector node list to add the new node to.
+ * @info:   Driver information to add to the list.
+ *
+ *  This function allocates and links in a "collector node" for each
+ *  collector based on the collector info in the info parameter.
+ *  The function allocates the new node, and links it to a local copy
+ *  of the passed-in driver interface info.  If the collector has an
+ *  init function among its operations, it iterates through the
+ *  descriptors in info, passing each one to the init function.
+ *
+ *  Finally, it allocates and initializes the "collector message" which
+ *  buffers a data sample that this collector gathers during the run.
+ *
+ * Returns:  -PW_ERROR on failure, PW_SUCCESS on success.
+ */
+int sw_add_driver_info(void *list_head,
+		       const struct sw_driver_interface_info *info)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	struct sw_collector_data *node = sw_alloc_collector_node();
+
+	if (!node) {
+		pw_pr_error("ERROR allocating collector node!\n");
+		return -PW_ERROR;
+	}
+
+	node->info = sw_copy_driver_interface_info_i(info);
+	if (!node->info) {
+		pw_pr_error(
+			"ERROR allocating or copying driver_interface_info!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Initialize the collectors in the node's descriptors.
+	 */
+	if (sw_init_driver_interface_info_i(node->info)) {
+		pw_pr_error(
+			"ERROR initializing a driver_interface_info node!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Allocate the ops array. We do this one time as an optimization
+	 * (we could always just repeatedly call 'sw_get_hw_ops_for()'
+	 * during the collection but we want to avoid that overhead)
+	 */
+	node->ops = sw_alloc_ops_i(info->num_io_descriptors);
+	if (!node->ops || sw_init_ops_i(node->ops, info)) {
+		pw_pr_error("ERROR initializing the ops array!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Allocate and initialize the "collector message".
+	 */
+	node->per_msg_payload_size = sw_get_payload_size_i(info);
+	pw_pr_debug("Debug: Per msg payload size = %u\n",
+		    (unsigned int)node->per_msg_payload_size);
+	node->msg = sw_alloc_collector_msg_i(info, node->per_msg_payload_size);
+	if (!node->msg) {
+		pw_pr_error("ERROR allocating space for a collector msg!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	pw_pr_debug("NODE = %p, NODE->MSG = %p\n", node, node->msg);
+	cpumask_clear(&node->cpumask);
+	{
+		/*
+		 * For now, use following protocol:
+		 * cpu_mask == -2 ==> Collect on ALL CPUs
+		 * cpu_mask == -1 ==> Collect on ANY CPU
+		 * cpu_mask >= 0 ==> Collect on a specific CPU
+		 */
+		if (node->info->cpu_mask >= 0) {
+			/*
+			 * Collect data on 'node->info->cpu_mask'
+			 */
+			cpumask_set_cpu(node->info->cpu_mask, &node->cpumask);
+			pw_pr_debug("OK: set CPU = %d\n", node->info->cpu_mask);
+		} else if (node->info->cpu_mask == -1) {
+			/*
+			 * Collect data on ANY CPU.  Leave empty as a flag to
+			 * signify user wishes to collect data on 'ANY' cpu.
+			 */
+			pw_pr_debug("OK: set ANY CPU\n");
+		} else {
+			/*
+			 * Collect data on ALL cpus.
+			 */
+			cpumask_copy(&node->cpumask, cpu_present_mask);
+			pw_pr_debug("OK: set ALL CPUs\n");
+		}
+	}
+	SW_LIST_ADD(head, node, list);
+	return PW_SUCCESS;
+}
+
+void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	if (info)
+		sw_kfree(info);
+}
+
+void sw_free_ops_i(const struct sw_hw_ops **ops)
+{
+	if (ops)
+		sw_kfree(ops);
+}
+
+int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	/*
+	 * Do any finalization here.
+	 * For now, only IPC/MMIO descriptors need to be finalized.
+	 */
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: no info!\n");
+		return -PW_ERROR;
+	}
+	for (i = 0,
+	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	     i < info->num_io_descriptors; ++i, ++descriptor) {
+		if (sw_reset_driver_io_descriptor(descriptor))
+			return -PW_ERROR;
+	}
+	return PW_SUCCESS;
+}
+
+/* If this descriptor's collector has an init function, call it passing in */
+/* this descriptor.  That allows the collector to perform any initialization */
+/* or registration specific to this metric. */
+int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+	sw_io_desc_init_func_t init_func = NULL;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+
+	if (ops == NULL) {
+		pw_pr_error("NULL ops found in init_driver_io_desc: type %d\n",
+			    descriptor->collection_type);
+		return -PW_ERROR;
+	}
+	init_func = ops->init;
+
+	if (init_func) {
+		int retval = (*init_func)(descriptor);
+
+		if (retval)
+			pw_pr_error("(*init) return value for type %d: %d\n",
+				    descriptor->collection_type, retval);
+
+		return retval;
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * If this descriptor's collector has a finalize function, call it passing in
+ * this
+ * descriptor. This allows the collector to perform any finalization specific to
+ * this metric.
+ */
+int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+	sw_io_desc_reset_func_t reset_func = NULL;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+
+	if (ops == NULL) {
+		pw_pr_error("NULL ops found in reset_driver_io_desc: type %d\n",
+			    descriptor->collection_type);
+		return -PW_ERROR;
+	}
+	pw_pr_debug("calling reset on descriptor of type %d\n",
+		    descriptor->collection_type);
+	reset_func = ops->reset;
+
+	if (reset_func) {
+		int retval = (*reset_func)(descriptor);
+
+		if (retval)
+			pw_pr_error("(*reset) return value for type %d: %d\n",
+				    descriptor->collection_type, retval);
+
+		return retval;
+	}
+	return PW_SUCCESS;
+}
+
+int sw_handle_driver_io_descriptor(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	const struct sw_hw_ops *hw_ops)
+{
+	typedef void (*sw_hardware_io_func_t)(
+		char *, int, const struct sw_driver_io_descriptor *, u16);
+	sw_hardware_io_func_t hardware_io_func = NULL;
+
+	if (descriptor->collection_command < SW_IO_CMD_READ ||
+	    descriptor->collection_command > SW_IO_CMD_WRITE) {
+		return -PW_ERROR;
+	}
+	switch (descriptor->collection_command) {
+	case SW_IO_CMD_READ:
+		hardware_io_func = hw_ops->read;
+		break;
+	case SW_IO_CMD_WRITE:
+		hardware_io_func = hw_ops->write;
+		break;
+	default:
+		break;
+	}
+	if (hardware_io_func) {
+		(*hardware_io_func)(dst_vals, cpu, descriptor,
+				    descriptor->counter_size_in_bytes);
+	} else {
+		pw_pr_debug(
+			"NO ops to satisfy %u operation for collection type %u!\n",
+			descriptor->collection_command,
+			descriptor->collection_type);
+	}
+	return PW_SUCCESS;
+}
+
+void sw_free_collector_msg_i(sw_driver_msg_t *msg)
+{
+	if (msg)
+		sw_kfree(msg);
+}
+
+void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action)
+{
+	/*
+	 * Basic algo:
+	 * For each descriptor in 'node->info->descriptors'; do:
+	 * 1. Perform H/W read; use 'descriptor->collection_type' to
+	 * determine type of read; use 'descriptor->counter_size_in_bytes'
+	 * for read size. Use msg->p_payload[dst_idx] as dst address
+	 * 2. Increment dst idx by 'descriptor->counter_size_in_bytes'
+	 */
+	struct sw_collector_data *node = (struct sw_collector_data *)info;
+	int cpu = RAW_CPU();
+	u16 num_descriptors = node->info->num_io_descriptors, i = 0;
+	struct sw_driver_io_descriptor *descriptors =
+		(struct sw_driver_io_descriptor *)node->info->descriptors;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+	const struct sw_hw_ops **ops = node->ops;
+	bool wasAnyWrite = false;
+
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = cpu;
+
+	for (i = 0; i < num_descriptors; ++i,
+	    dst_vals += descriptors->counter_size_in_bytes, ++descriptors) {
+		if (unlikely(ops[i] == NULL)) {
+			pw_pr_debug("NULL OPS!\n");
+			continue;
+		}
+		if (descriptors->collection_command == SW_IO_CMD_WRITE)
+			wasAnyWrite = true;
+
+		if (sw_handle_driver_io_descriptor(dst_vals, cpu, descriptors,
+						   ops[i]))
+			pw_pr_error("ERROR reading descriptor with type %d\n",
+				    descriptors->collection_type);
+	}
+
+	/*
+	 * We produce messages only on READs. Note that SWA prohibits
+	 * messages that contain both READ and WRITE descriptors, so it
+	 * is enough to check if there was ANY WRITE descriptor in this
+	 * message.
+	 */
+	if (likely(wasAnyWrite == false)) {
+		if (sw_produce_generic_msg(msg, action))
+			pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+}
+
+/*
+ * Collector list and node functions.
+ */
+struct sw_collector_data *sw_alloc_collector_node(void)
+{
+	struct sw_collector_data *node = (struct sw_collector_data *)sw_kmalloc(
+		sizeof(struct sw_collector_data), GFP_KERNEL);
+
+	if (node) {
+		node->per_msg_payload_size = 0x0;
+		node->last_update_jiffies = 0x0;
+		node->info = NULL;
+		node->ops = NULL;
+		node->msg = NULL;
+		SW_LIST_ENTRY_INIT(node, list);
+	}
+	return node;
+}
+
+void sw_free_collector_node(struct sw_collector_data *node)
+{
+	if (node) {
+		if (node->info) {
+			sw_reset_driver_interface_info_i(node->info);
+			sw_free_driver_interface_info_i(node->info);
+			node->info = NULL;
+		}
+		if (node->ops) {
+			sw_free_ops_i(node->ops);
+			node->ops = NULL;
+		}
+		if (node->msg) {
+			sw_free_collector_msg_i(node->msg);
+			node->msg = NULL;
+		}
+		sw_kfree(node);
+	}
+}
+
+int sw_handle_collector_node(struct sw_collector_data *node)
+{
+	if (!node || !node->info || !node->ops || !node->msg)
+		return -PW_ERROR;
+
+	pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+	sw_schedule_work(&node->cpumask, &sw_handle_per_cpu_msg, node);
+	return PW_SUCCESS;
+}
+
+int sw_handle_collector_node_on_cpu(struct sw_collector_data *node, int cpu)
+{
+	if (!node || !node->info || !node->ops || !node->msg)
+		return -PW_ERROR;
+
+	/*
+	 * Check if this node indicates it should be scheduled
+	 * on the given cpu. If so, clear all other CPUs from the
+	 * mask and schedule the node.
+	 */
+	if (cpumask_test_cpu(cpu, &node->cpumask)) {
+		struct cpumask tmp_mask;
+
+		cpumask_clear(&tmp_mask);
+		cpumask_set_cpu(cpu, &tmp_mask);
+		pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+		sw_schedule_work(&tmp_mask, &sw_handle_per_cpu_msg, node);
+	}
+	return PW_SUCCESS;
+}
+
+void sw_init_collector_list(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	SW_LIST_HEAD_INIT(head);
+}
+
+void sw_destroy_collector_list(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_collector_data *curr =
+			SW_LIST_GET_HEAD_ENTRY(head,
+				sw_collector_data,
+				list);
+
+		BUG_ON(!curr->info);
+		SW_LIST_UNLINK(curr, list);
+		sw_free_collector_node(curr);
+	}
+}
+
+/**
+ * sw_handle_collector_list - Iterate through the collector list, calling
+ *                            func() upon each element.
+ * @list_head:  The collector list head.
+ * @func:  The function to call for each collector.
+ *
+ * This function is called when one of the "when types" fires, since the
+ * passed-in collector node list is the list of collections to do at that time.
+ *
+ * Returns: PW_SUCCESS on success, -PW_ERROR on error.
+ */
+int sw_handle_collector_list(void *list_head,
+			     int (*func)(struct sw_collector_data *data))
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	int retVal = PW_SUCCESS;
+	struct sw_collector_data *curr = NULL;
+
+	if (!head || !func)
+		return -PW_ERROR;
+
+	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
+	{
+		pw_pr_debug("HANDLING\n");
+		if ((*func)(curr))
+			retVal = -PW_ERROR;
+
+	}
+	return retVal;
+}
+
+int sw_handle_collector_list_on_cpu(void *list_head,
+				    int (*func)(struct sw_collector_data *data,
+						int cpu),
+				    int cpu)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	int retVal = PW_SUCCESS;
+	struct sw_collector_data *curr = NULL;
+
+	if (!head || !func)
+		return -PW_ERROR;
+
+	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
+	{
+		pw_pr_debug("HANDLING\n");
+		if ((*func)(curr, cpu))
+			retVal = -PW_ERROR;
+
+	}
+	return retVal;
+}
+
+void sw_handle_per_cpu_msg(void *info)
+{
+	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_handle_per_cpu_msg_no_sched(void *info)
+{
+	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_TIMER);
+}
+
+void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info)
+{
+	if (unlikely(cpu == RAW_CPU()))
+		sw_handle_per_cpu_msg_no_sched(info);
+	else {
+		pw_pr_debug("[%d] is handling for %d\n", RAW_CPU(), cpu);
+		/*
+		 * No need to disable preemption -- 'smp_call_function_single'
+		 * does that for us.
+		 */
+		smp_call_function_single(
+			cpu, &sw_handle_per_cpu_msg_no_sched, info,
+			false
+			/* false ==> do NOT wait for function completion */);
+	}
+}
+
+void sw_set_collector_ops(const struct sw_hw_ops *hw_ops)
+{
+	s_hw_ops = hw_ops;
+}
diff --git a/drivers/platform/x86/socwatch/sw_cta.c b/drivers/platform/x86/socwatch/sw_cta.c
new file mode 100644
index 000000000000..ddf15e03fd4d
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_cta.c
@@ -0,0 +1,324 @@
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/compiler.h>     /* Definition of __weak */
+#include <linux/kref.h> /* struct kref */
+#include <linux/notifier.h> /* struct notifier_block */
+
+#include "sw_structs.h"      /* sw_driver_io_descriptor */
+#include "sw_cta.h"
+
+/* *********************************
+ * Begin CTA driver import
+ * *********************************
+ */
+
+/*
+ * Struct definitions taken from CTA driver.
+ */
+
+enum intel_cta_ep_type {
+        INTEL_CTA_EP_TYPE_ANY = 0,
+        INTEL_CTA_EP_TYPE_PUNIT,
+        INTEL_CTA_EP_TYPE_GPU,
+        NUM_INTEL_CTA_EP_TYPES
+};
+
+struct telem_header {
+        u8      access_type;
+        u8      telem_type;
+        u16     size;
+        u32     guid;
+        u32     base_offset;
+        u32     tele_id;
+};
+
+struct discovery_entry {
+        struct telem_header     header;
+        void __iomem            *disc_offset;
+        void __iomem            *counter_base;
+};
+
+/**
+ * struct telem_endpoint - telemetry endpoint description
+ * @pci_bus_no:      pci device bus number
+ * @pci_devfn:       encoded pci device and function number
+ * @num_entries:     Number of entries
+ * @telem_ep_type:   Device specific telemetry type
+ * @header_size:     Size in bytes of the discovery header
+ * @discovery_entry: Pointer to list of telemetry entries. Used by
+ *                   driver API to get direct access to the endpoint.
+ *                   Do not dereference directly. Use driver API which
+ *                   checks for device availability.
+ * @kref:            Driver reference count - do not modify directly. Use
+ *                   the register_endpoint and unregister_endpoint API's.
+ */
+struct telem_endpoint {
+        u16                             pci_bus_no;
+        u8                              pci_devfn;
+        int                             num_entries;
+        u32                             telem_ep_type;  //TODO: Implement
+        u32                             header_size;
+        struct discovery_entry          *entry;
+        struct kref                     kref;
+};
+
+struct telem_header_info {
+        u8      access_type;
+        u16     size;
+        u32     guid;
+};
+
+/*
+ * Weak linkage of functions from the CTA driver
+ */
+/**
+ * cta_telem_get_next_endpoint() - Get next id for a telemetry endpoint
+ * @start:  starting index to look from
+ * @type:   filter for type of endpoint
+ *
+ * Return:
+ * * index       - index of next present endpoint after start
+ * * 0           - when no more endpoints are present after start
+ */
+extern unsigned long __weak
+cta_telem_get_next_endpoint(unsigned long start,
+                            enum intel_cta_ep_type type);
+
+/**
+ * cta_telem_register_endpoint() - Register use of telemetry endpoint
+ * @handle: ID associated with the telemetry endpoint
+ *
+ * Return:
+ * * ep          - Succes
+ * * -ENXIO      - telemetry endpoint not found
+ */
+
+extern struct telem_endpoint * __weak
+cta_telem_register_endpoint(unsigned long handle);
+
+/**
+ * cta_telem_unregister_endpoint() - Unregister use of telemetry endpoint
+ * @ep:   ep structure to populate.
+ */
+extern void __weak
+cta_telem_unregister_endpoint(struct telem_endpoint *ep);
+
+
+/**
+ * cta_telem_get_header_info() - Get header info from a telem entry
+ * @handle: ID associated with the telemetry endpoint
+ * @idx:    Index of the entry
+ * @info: Allocated header info structure to fill
+ *
+ * Return:
+ * * 0           - Success
+ * * -ENXIO      - telemetry endpoint not found
+ * * -EINVAL     - bad argument (@idx out of range or null @info)
+ */
+extern int __weak
+cta_telem_get_header_info(unsigned long handle, u8 idx,
+                          struct telem_header_info *info);
+/**
+ * cta_telem_counter_read32() - Read dwords from telemetry sram into buffer
+ * @ep:     Pointer to telemetry endpoint to access
+ * @index:  Index of the entry
+ * @offset: Register offset in bytes
+ * @data:   Preallocated dword buffer to fill
+ * @count:  Number of dwords requested
+ *
+ * Callers must ensure reads are aligned, and that both the entry index and
+ * offset are within bounds. When the call returns -ENODEV, the device has
+ * been removed and callers should unregister the telemetry endpoint.
+ *
+ * Context: RCU used to guard reads from device removal and unmap. Do not
+ *          sleep.
+ *
+ * Return:
+ * * 0           - Success
+ * * -ENODEV     - The device had been removed.
+ */
+extern int __weak
+cta_telem_counter_read32(struct telem_endpoint *ep, u32 index, u32 offset,
+                         u32 *data, u32 count);
+/**
+ * cta_telem_counter_read64() - Read qwords from telemetry sram into buffer
+ * @ep:     Pointer to telemetry endpoint to access
+ * @index:  Index of the entry
+ * @offset: Register offset in bytes
+ * @data:   Preallocated buffer of qwords
+ * @count:  Number of qwords requested
+ *
+ * Callers must ensure reads are aligned, and that both the entry index and
+ * offset are within bounds. When the call returns -ENODEV, the device has
+ * been removed and callers should unregister the telemetry endpoint.
+ *
+ * Context: RCU used to guard reads from device removal and unmap. Do not
+ *          sleep.
+ *
+ * Return:
+ * * 0           - Success
+ * * -ENODEV     - The device had been removed.
+ */
+extern int __weak
+cta_telem_counter_read64(struct telem_endpoint *ep, u32 index, u32 offset,
+                         u64 *data, u32 count);
+
+extern int __weak cta_telem_register_notifier(struct notifier_block *nb);
+extern int __weak cta_telem_unregister_notifier(struct notifier_block *nb);
+
+/* *********************************
+ * End CTA driver import
+ * *********************************
+ */
+
+#define MAX_TELEM_ENDPOINTS MAX_TELEM_AGGR_DEVICES /* For now */
+static struct telem_endpoint *s_telem_endpoints[MAX_TELEM_ENDPOINTS]; /* TODO: make this a linked list instead */
+size_t s_endpoint_index = 0;
+
+static struct _sw_aggregator_msg s_telem_aggregators;
+
+void sw_read_cta_info(char *dst, int cpu,
+		const struct sw_driver_io_descriptor *descriptor,
+		u16 counter_size_in_bytes)
+{
+	u64 *data64 = (u64 *)dst;
+	u32 *data32 = (u32 *)dst;
+	int retval = 0;
+	const struct sw_driver_aggr_telem_io_descriptor *td =
+		&(descriptor->aggr_telem_descriptor);
+	u32 offset = (u32)td->offset;
+	struct telem_endpoint *ep = s_telem_endpoints[0];
+
+	/* We can only support one endpoint as of now */
+	if (!ep) {
+		return;
+	}
+	switch (descriptor->counter_size_in_bytes) {
+		case 4:
+			retval = cta_telem_counter_read32(ep, 0/*index*/, offset, data32, td->num_entries);
+			break;
+		case 8:
+			retval = cta_telem_counter_read64(ep, 0/*index*/, offset, data64, td->num_entries);
+			break;
+		default:
+			printk(KERN_ERR "Invalid CTA counter size %u\n", descriptor->counter_size_in_bytes);
+			return;
+	}
+	if (retval) {
+		printk(KERN_ERR "Error reading %u byte CTA value from offset 0x%x, val = %d\n", descriptor->counter_size_in_bytes, offset, retval);
+	}
+}
+
+bool sw_cta_available(void)
+{
+	/* 1: check if the CTA driver is loaded */
+	if (!cta_telem_get_next_endpoint) {
+		return false;
+	}
+	/* 2: TODO: other checks here */
+	/*
+	 * Note: registering telemetry endpoints done in 'register' since
+	 * those endpoints also need to be unregistered (Done in 'fini')
+	 */
+	return true;
+}
+
+bool sw_cta_register(void)
+{
+	unsigned long index = 0;
+	if (!sw_cta_available()) {
+		return false;
+	}
+        s_telem_aggregators.num_entries = 0;
+        s_endpoint_index = 0;
+	/*
+	 * Retrieve list of telemetry endpoints.
+	 * TODO: we can only support one endpoint as of now, so should we be
+	 * checking the GUID to retrieve onl tthe endpoints of interest?
+	 */
+	s_endpoint_index = 0;
+	while ((index = cta_telem_get_next_endpoint(index, INTEL_CTA_EP_TYPE_ANY)) && s_endpoint_index < (MAX_TELEM_ENDPOINTS-1)) {
+		struct telem_header_info telem_info;
+		u8 idx = 0;
+		if (cta_telem_get_header_info(index, idx, &telem_info)) {
+			printk(KERN_ERR "Could not retrieve telemetry header for CTA endpoint %lu\n", index);
+			continue;
+		}
+		s_telem_endpoints[s_endpoint_index] = cta_telem_register_endpoint(index);
+		s_telem_aggregators.info[s_telem_aggregators.num_entries++].globalUniqueID = telem_info.guid;
+		++s_endpoint_index;
+	}
+	return s_endpoint_index > 0;
+}
+
+bool sw_cta_unregister(void)
+{
+	size_t i=0;
+	if (!sw_cta_available()) {
+		return false;
+	}
+	for (i=0; i<s_endpoint_index; ++i) {
+		cta_telem_unregister_endpoint(s_telem_endpoints[i]);
+	}
+	s_endpoint_index = 0;
+	s_telem_aggregators.num_entries = 0;
+	return true;
+}
+
+struct _sw_aggregator_msg *sw_cta_aggregators(void)
+{
+	return &s_telem_aggregators;
+}
diff --git a/drivers/platform/x86/socwatch/sw_driver.c b/drivers/platform/x86/socwatch/sw_driver.c
index 0a8ad70b800b..c51bbcd29415 100644
--- a/drivers/platform/x86/socwatch/sw_driver.c
+++ b/drivers/platform/x86/socwatch/sw_driver.c
@@ -1,1551 +1,1567 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#define MOD_AUTHOR "Gautam Upadhyaya <gautam.upadhyaya@intel.com>"
-#define MOD_DESC "SoC Watch kernel module"
-
-#include "sw_internal.h"
-#include "sw_structs.h"
-#include "sw_kernel_defines.h"
-#include "sw_types.h"
-#include "sw_mem.h"
-#include "sw_ioctl.h"
-#include "sw_output_buffer.h"
-#include "sw_hardware_io.h"
-#include "sw_overhead_measurements.h"
-#include "sw_tracepoint_handlers.h"
-#include "sw_collector.h"
-#include "sw_file_ops.h"
-
-/* -------------------------------------------------
- * Compile time constants.
- * -------------------------------------------------
- */
-/*
- * Number of entries in the 'sw_collector_lists' array
- */
-#define NUM_COLLECTOR_MODES (SW_WHEN_TYPE_END - SW_WHEN_TYPE_BEGIN + 1)
-#define PW_OUTPUT_BUFFER_SIZE                                                  \
-	256 /* Number of output messages in each per-cpu buffer */
-/*
- * Check if tracepoint/notifier ID is in (user-supplied) mask
- */
-#define IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)                                 \
-	((id) >= 0 && (((mask) >> (id)) & 0x1))
-
-/* -------------------------------------------------
- *  Local function declarations.
- * -------------------------------------------------
- */
-int sw_load_driver_i(void);
-void sw_unload_driver_i(void);
-int sw_init_collector_lists_i(void);
-void sw_destroy_collector_lists_i(void);
-int sw_init_data_structures_i(void);
-void sw_destroy_data_structures_i(void);
-int sw_get_arch_details_i(void);
-void sw_iterate_driver_info_lists_i(void);
-void sw_handle_immediate_request_i(void *request);
-int sw_print_collector_node_i(struct sw_collector_data *data);
-int sw_collection_start_i(void);
-int sw_collection_stop_i(void);
-int sw_collection_poll_i(void);
-size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
-sw_driver_msg_t *sw_alloc_collector_msg_i(
-	const struct sw_driver_interface_info *info,
-	size_t per_msg_payload_size);
-static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
-                                       void *p_local_args);
-static long sw_set_driver_infos_i(
-	struct sw_driver_interface_msg __user *remote_msg, int local_len);
-static long sw_handle_cmd_i(
-	sw_driver_collection_cmd_t cmd, u64 __user* remote_out_args);
-static void sw_do_extract_scu_fw_version(void);
-static long sw_get_available_name_id_mappings_i(
-	enum sw_name_id_type type,
-	struct sw_name_info_msg __user* remote_info,
-	size_t local_len);
-static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void);
-static bool sw_should_flush_buffer_i(void);
-
-/* -------------------------------------------------
- * Data structures.
- * -------------------------------------------------
- */
-/*
- * Structure to hold current CMD state
- * of the device driver. Constantly evolving, but
- * that's OK -- this is internal to the driver
- * and is NOT exported.
- */
-struct swa_internal_state {
-	/*
-	 * Indicates which command was specified
-	 * last e.g. START, STOP etc.
-	 */
-	sw_driver_collection_cmd_t cmd;
-	/*
-	 * Should we write to our per-cpu output buffers?
-	 * YES if we're actively collecting.
-	 * NO if we're not.
-	 */
-	bool write_to_buffers;
-	/*
-	 * Should we "drain/flush" the per-cpu output buffers?
-	 * (See "device_read" for an explanation)
-	 */
-	bool drain_buffers;
-	/* Others... */
-};
-
-/* -------------------------------------------------
- * Variables.
- * -------------------------------------------------
- */
-static bool do_force_module_scope_for_cpu_frequencies;
-module_param(do_force_module_scope_for_cpu_frequencies, bool, 0400);
-MODULE_PARM_DESC(
-	do_force_module_scope_for_cpu_frequencies,
-	"Toggle module scope for cpu frequencies. Sets \"affected_cpus\" and \"related_cpus\" of cpufreq_policy.");
-
-static unsigned short sw_buffer_num_pages = 16;
-module_param(sw_buffer_num_pages, ushort, 0400);
-MODULE_PARM_DESC(
-	sw_buffer_num_pages,
-	"Specify number of 4kB pages to use for each per-cpu buffer. MUST be a power of 2! Default value = 16 (64 kB)");
-
-/* TODO: convert from 'list_head' to 'hlist_head' */
-/*
- * sw_collector_lists is an array of linked lists of "collector nodes"
- * (sw_collector_data structs).  It is indexed by the sw_when_type_t's.
- * Each list holds the collectors to "execute" at a specific time,
- * e.g. the beginning of the run, at a poll interval, tracepoint, etc.
- */
-static SW_DEFINE_LIST_HEAD(sw_collector_lists,
-			   sw_collector_data)[NUM_COLLECTOR_MODES];
-static __read_mostly u16 sw_scu_fw_major_minor;
-
-static struct swa_internal_state s_internal_state;
-static struct sw_file_ops s_ops = {
-	.ioctl_handler = &sw_unlocked_handle_ioctl_i,
-	.stop_handler = &sw_collection_stop_i,
-	.get_current_cmd = &sw_get_collection_cmd_i,
-	.should_flush = &sw_should_flush_buffer_i,
-};
-
-/*
- * For each function that you want to profile,
- * do the following (e.g. function 'foo'):
- * **************************************************
- * DECLARE_OVERHEAD_VARS(foo);
- * **************************************************
- * This will declare the two variables required
- * to keep track of overheads incurred in
- * calling/servicing 'foo'. Note that the name
- * that you declare here *MUST* match the function name!
- */
-
-DECLARE_OVERHEAD_VARS(sw_collection_poll_i); /* for POLL */
-DECLARE_OVERHEAD_VARS(sw_any_seg_full);
-
-/*
- * String representation of the various 'SW_WHEN_TYPE_XYZ' enum values.
- * Debugging ONLY!
- */
-#if DO_DEBUG_OUTPUT
-static const char * const s_when_type_names[] = { "BEGIN", "POLL", "NOTIFIER",
-					   "TRACEPOINT", "END" };
-#endif /* DO_DEBUG_OUTPUT */
-
-/* -------------------------------------------------
- * Function definitions.
- * -------------------------------------------------
- */
-/*
- * External functions.
- */
-int sw_process_snapshot(enum sw_when_type when)
-{
-	if (when > SW_WHEN_TYPE_END) {
-		pw_pr_error("invalid snapshot time %d specified!\n", when);
-		return -EINVAL;
-	}
-	if (sw_handle_collector_list(&sw_collector_lists[when],
-				     &sw_handle_collector_node)) {
-		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
-			    when);
-		return -EIO;
-	}
-	return 0;
-}
-
-int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu)
-{
-	if (when > SW_WHEN_TYPE_END) {
-		pw_pr_error("invalid snapshot time %d specified!\n", when);
-		return -EINVAL;
-	}
-	if (sw_handle_collector_list_on_cpu(&sw_collector_lists[when],
-					    &sw_handle_collector_node_on_cpu,
-					    cpu)) {
-		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
-			    when);
-		return -EIO;
-	}
-	return 0;
-}
-
-/*
- * Driver interface info and collector list functions.
- */
-int sw_print_collector_node_i(struct sw_collector_data *curr)
-{
-	pw_u16_t num_descriptors = 0;
-	sw_io_desc_print_func_t print_func = NULL;
-	struct sw_driver_io_descriptor *descriptor = NULL;
-	struct sw_driver_interface_info *info = NULL;
-
-	if (!curr)
-		return -PW_ERROR;
-
-	info = curr->info;
-	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
-	pw_pr_debug(
-		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
-		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
-	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
-	     --num_descriptors, ++descriptor) {
-		const struct sw_hw_ops *ops =
-			sw_get_hw_ops_for(descriptor->collection_type);
-		if (ops == NULL)
-			return -PW_ERROR;
-
-		print_func = ops->print;
-		if (print_func && (*print_func)(descriptor))
-			return -PW_ERROR;
-
-	}
-	return PW_SUCCESS;
-}
-
-/*
- * Driver interface info and collector list functions.
- */
-
-/**
- * sw_reset_collector_node_i - Call the reset op on all of the descriptors
- *                             in coll that have one.
- * @coll: The data structure containing an array of collector descriptors.
- *
- * Return: PW_SUCCESS if all of the resets succeeded, -PW_ERROR if any failed.
- */
-static int sw_reset_collector_node_i(struct sw_collector_data *coll)
-{
-	struct sw_driver_io_descriptor *descriptor = NULL;
-	struct sw_driver_interface_info *info = NULL;
-	int num_descriptors;
-	int retcode = PW_SUCCESS;
-
-	if (!coll)
-		return -PW_ERROR;
-
-	info = coll->info;
-
-	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
-	pw_pr_debug(
-		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
-		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
-	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
-	     --num_descriptors, ++descriptor) {
-		const struct sw_hw_ops *ops =
-			sw_get_hw_ops_for(descriptor->collection_type);
-		if (ops && ops->reset && (*ops->reset)(descriptor))
-			retcode = -PW_ERROR;
-
-	}
-	return retcode;
-}
-
-static int sw_iterate_trace_notifier_list_i(struct sw_trace_notifier_data *node,
-				     void *dummy)
-{
-	return sw_handle_collector_list(&node->list,
-					&sw_print_collector_node_i);
-}
-
-void sw_iterate_driver_info_lists_i(void)
-{
-	sw_when_type_t which;
-
-	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
-		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
-		/* Should NEVER happen! */
-		if (sw_handle_collector_list(
-			    &sw_collector_lists[which],
-			    &sw_print_collector_node_i))
-			pw_pr_error(
-				"WARNING: error occurred while printing values!\n");
-
-	}
-
-	if (sw_for_each_tracepoint_node(&sw_iterate_trace_notifier_list_i, NULL,
-					false /*return-on-error*/))
-		pw_pr_error(
-			"WARNING: error occurred while printing tracepoint values!\n");
-
-	if (sw_for_each_notifier_node(&sw_iterate_trace_notifier_list_i, NULL,
-				      false /*return-on-error*/))
-		pw_pr_error(
-			"WARNING: error occurred while printing notifier values!\n");
-
-}
-
-static void sw_reset_collectors_i(void)
-{
-	sw_when_type_t which;
-
-	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
-		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
-		if (sw_handle_collector_list(&sw_collector_lists[which],
-					     &sw_reset_collector_node_i))
-			pw_pr_error(
-				"WARNING: error occurred while resetting a collector!\n");
-
-	}
-}
-
-int sw_init_data_structures_i(void)
-{
-	/*
-	 * Find the # CPUs in this system.
-	 * Update: use 'num_possible' instead of 'num_present' in case
-	 * the cpus aren't numbered contiguously
-	 */
-	sw_max_num_cpus = num_possible_cpus();
-
-	/*
-	 * Initialize our trace subsys: MUST be called
-	 * BEFORE 'sw_init_collector_lists_i()!
-	 */
-	if (sw_add_trace_notify()) {
-		sw_destroy_data_structures_i();
-		return -PW_ERROR;
-	}
-	if (sw_init_collector_lists_i()) {
-		sw_destroy_data_structures_i();
-		return -PW_ERROR;
-	}
-	if (sw_init_per_cpu_buffers()) {
-		sw_destroy_data_structures_i();
-		return -PW_ERROR;
-	}
-	if (sw_register_hw_ops()) {
-		sw_destroy_data_structures_i();
-		return -PW_ERROR;
-	}
-	return PW_SUCCESS;
-}
-
-void sw_destroy_data_structures_i(void)
-{
-	sw_free_hw_ops();
-	sw_destroy_per_cpu_buffers();
-	sw_destroy_collector_lists_i();
-	sw_remove_trace_notify();
-
-	/* Should already have been called from 'collection_stop' */
-	sw_destroy_telem();
-}
-
-int sw_get_arch_details_i(void)
-{
-	/*
-	 * SCU F/W version (if applicable)
-	 */
-	sw_do_extract_scu_fw_version();
-	return PW_SUCCESS;
-}
-
-#define INIT_FLAG ((void *)0)
-#define DESTROY_FLAG ((void *)1)
-
-static int
-sw_init_destroy_trace_notifier_lists_i(struct sw_trace_notifier_data *node,
-				       void *is_init)
-{
-	if (is_init == INIT_FLAG)
-		sw_init_collector_list(&node->list);
-	else
-		sw_destroy_collector_list(&node->list);
-
-	node->was_registered = false;
-
-	return PW_SUCCESS;
-}
-
-int sw_init_collector_lists_i(void)
-{
-	int i = 0;
-
-	for (i = 0; i < NUM_COLLECTOR_MODES; ++i)
-		sw_init_collector_list(&sw_collector_lists[i]);
-
-	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
-				    INIT_FLAG, false /*return-on-error*/);
-	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
-				  INIT_FLAG, false /*return-on-error*/);
-
-	return PW_SUCCESS;
-}
-
-void sw_destroy_collector_lists_i(void)
-{
-	int i = 0;
-
-	for (i = 0; i < NUM_COLLECTOR_MODES; ++i)
-		sw_destroy_collector_list(&sw_collector_lists[i]);
-
-	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
-				    DESTROY_FLAG, false /*return-on-error*/);
-	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
-				  DESTROY_FLAG, false /*return-on-error*/);
-}
-
-/*
- * Used for {READ,WRITE}_IMMEDIATE requests.
- */
-struct sw_immediate_request_info {
-	struct sw_driver_io_descriptor *local_descriptor;
-	char *dst_vals;
-	int *retVal;
-};
-void sw_handle_immediate_request_i(void *request)
-{
-	struct sw_immediate_request_info *info =
-		(struct sw_immediate_request_info *)request;
-	struct sw_driver_io_descriptor *descriptor = info->local_descriptor;
-	char *dst_vals = info->dst_vals;
-	const struct sw_hw_ops *ops =
-		sw_get_hw_ops_for(descriptor->collection_type);
-	if (likely(ops != NULL))
-		*(info->retVal) = sw_handle_driver_io_descriptor(
-			dst_vals, RAW_CPU(), descriptor, ops);
-	else
-		pw_pr_error(
-			"No operations found to satisfy collection type %u!\n",
-			descriptor->collection_type);
-}
-
-static int num_times_polled;
-
-int sw_collection_start_i(void)
-{
-	/*
-	 * Reset the poll tick counter.
-	 */
-	num_times_polled = 0;
-	/*
-	 * Update the output buffers.
-	 */
-	sw_reset_per_cpu_buffers();
-	/*
-	 * Ensure clients don't think we're in 'flush' mode.
-	 */
-	s_internal_state.drain_buffers = false;
-	/*
-	 * Set the 'command'
-	 */
-	s_internal_state.cmd = SW_DRIVER_START_COLLECTION;
-	/*
-	 * Clear out the topology list
-	 */
-	sw_clear_topology_list();
-	/*
-	 * Handle 'START' snapshots, if any.
-	 */
-	{
-		if (sw_handle_collector_list(
-			    &sw_collector_lists[SW_WHEN_TYPE_BEGIN],
-			    &sw_handle_collector_node)) {
-			pw_pr_error(
-				"ERROR: could NOT handle START collector list!\n");
-			return -PW_ERROR;
-		}
-	}
-	/*
-	 * Register any required tracepoints and notifiers.
-	 */
-	{
-		if (sw_register_trace_notifiers()) {
-			pw_pr_error("ERROR registering trace_notifiers!\n");
-			sw_unregister_trace_notifiers();
-			return -PW_ERROR;
-		}
-	}
-	pw_pr_debug("OK, STARTED collection!\n");
-	return PW_SUCCESS;
-}
-
-int sw_collection_stop_i(void)
-{
-	/*
-	 * Unregister any registered tracepoints and notifiers.
-	 */
-	if (sw_unregister_trace_notifiers())
-		pw_pr_warn(
-			"Warning: some trace_notifier probe functions could NOT be unregistered!\n");
-
-	/*
-	 * Handle 'STOP' snapshots, if any.
-	 */
-	if (sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_END],
-				     &sw_handle_collector_node)) {
-		pw_pr_error("ERROR: could NOT handle STOP collector list!\n");
-		return -PW_ERROR;
-	}
-	/*
-	 * Set the 'command'
-	 */
-	s_internal_state.cmd = SW_DRIVER_STOP_COLLECTION;
-	/*
-	 * Tell consumers to 'flush' all buffers. We need to
-	 * defer this as long as possible because it needs to be
-	 * close to the 'wake_up_interruptible', below.
-	 */
-	s_internal_state.drain_buffers = true;
-	smp_mb(); /* order memory access */
-	/*
-	 * Wakeup any sleeping readers, and cleanup any
-	 * timers in the reader subsys.
-	 */
-	sw_cancel_reader();
-	/*
-	 * Collect stats on samples produced and dropped.
-	 * TODO: call from 'device_read()' instead?
-	 */
-	sw_count_samples_produced_dropped();
-#if DO_OVERHEAD_MEASUREMENTS
-	pw_pr_force("DEBUG: there were %llu samples produced and %llu samples \
-		dropped in buffer v5!\n", sw_num_samples_produced,
-		sw_num_samples_dropped);
-#endif // DO_OVERHEAD_MEASUREMENTS
-	/*
-	 * DEBUG: iterate over collection lists.
-	 */
-	sw_iterate_driver_info_lists_i();
-	/*
-	 * Shut down any collectors that need shutting down.
-	 */
-	sw_reset_collectors_i();
-	/*
-	 * Clear out the collector lists.
-	 */
-	sw_destroy_collector_lists_i();
-	/*
-	 * Free up circular buffer
-	 */
-	destroy_circular_buffer();
-	/*
-	 * Remove telemetry mappings
-	 */
-	sw_destroy_telem();
-	pw_pr_debug("OK, STOPPED collection!\n");
-#if DO_OVERHEAD_MEASUREMENTS
-	pw_pr_force("There were %d poll ticks!\n", num_times_polled);
-#endif /* DO_OVERHEAD_MEASUREMENTS */
-	return PW_SUCCESS;
-}
-
-int sw_collection_poll_i(void)
-{
-	/*
-	 * Handle 'POLL' timer expirations.
-	 */
-	if (SW_LIST_EMPTY(&sw_collector_lists[SW_WHEN_TYPE_POLL]))
-		pw_pr_debug("DEBUG: EMPTY POLL LIST\n");
-
-	++num_times_polled;
-	return sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_POLL],
-					&sw_handle_collector_node);
-}
-
-/*
- * Private data for the 'sw_add_trace_notifier_driver_info_i' function.
- */
-struct tn_data {
-	struct sw_driver_interface_info *info;
-	u64 mask;
-};
-
-static int
-sw_add_trace_notifier_driver_info_i(struct sw_trace_notifier_data *node,
-				    void *priv)
-{
-	struct tn_data *data = (struct tn_data *)priv;
-	struct sw_driver_interface_info *local_info = data->info;
-	u64 mask = data->mask;
-	int id = sw_get_trace_notifier_id(node);
-
-	if (IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)) {
-		pw_pr_debug("TRACEPOINT ID = %d is IN mask 0x%llx\n", id, mask);
-		if (sw_add_driver_info(&node->list, local_info)) {
-			pw_pr_error(
-				"WARNING: could NOT add driver info to list!\n");
-			return -PW_ERROR;
-		}
-	}
-	return PW_SUCCESS;
-}
-
-static int sw_post_config_i(const struct sw_hw_ops *op, void *priv)
-{
-	/* op not available */
-	if (!op->available || !(*op->available)())
-		return 0;
-
-	if (!op->post_config || (*op->post_config)())
-		return 0;
-
-	return -EIO;
-}
-
-/**
- * sw_set_driver_infos_i - Process the collection config data passed down
- *                         from the client.
- * @remote_msg: The user space address of our ioctl data.
- * @local_len:  The number of bytes of remote_msg we should copy.
- *
- * This function copies the ioctl data from user space to kernel
- * space.  That data is an array of sw_driver_interface_info structs,
- * which hold information about tracepoints, notifiers, and collector
- * configuration info for this collection run..  For each driver_info
- * struct, it calls the appropriate "add info" (registration/
- * configuration) function for each of the "when types" (begin, poll,
- * notifier, tracepoint, end) which should trigger a collection
- * operation for that collector.
- *
- * When this function is done, the data structures corresponding to
- * collection should be configured and initialized.
- *
- *
- * Returns: PW_SUCCESS on success, or a non-zero on an error.
- */
-static long
-sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg,
-		      int local_len)
-{
-	struct sw_driver_interface_info *local_info = NULL;
-	struct sw_driver_interface_msg *local_msg = vmalloc(local_len);
-	pw_u8_t read_triggers = 0x0;
-	pw_u16_t num_infos = 0;
-	sw_when_type_t i = SW_WHEN_TYPE_BEGIN;
-	char *__data = (char *)local_msg->infos;
-	size_t dst_idx = 0;
-
-	if (!local_msg) {
-		pw_pr_error("ERROR allocating space for local message!\n");
-		return -EFAULT;
-	}
-	if (copy_from_user(local_msg, (struct sw_driver_interface_msg __user *)
-			   remote_msg, local_len)) {
-		pw_pr_error("ERROR copying message from user space!\n");
-		vfree(local_msg);
-		return -EFAULT;
-	}
-	/*
-	 * We aren't allowed to config the driver multiple times between
-	 * collections. Clear out any previous config values.
-	 */
-	sw_destroy_collector_lists_i();
-
-	/*
-	 * Did the user specify a min polling interval?
-	 */
-	sw_min_polling_interval_msecs = local_msg->min_polling_interval_msecs;
-	pw_pr_debug("min_polling_interval_msecs = %u\n",
-		    sw_min_polling_interval_msecs);
-
-	num_infos = local_msg->num_infos;
-	pw_pr_debug("LOCAL NUM INFOS = %u\n", num_infos);
-	for (; num_infos > 0; --num_infos) {
-		local_info =
-			(struct sw_driver_interface_info *)&__data[dst_idx];
-		dst_idx += (SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
-			    local_info->num_io_descriptors *
-				    sizeof(struct sw_driver_io_descriptor));
-		read_triggers = local_info->trigger_bits;
-		pw_pr_debug(
-			"read_triggers = %u, # msrs = %u, new dst_idx = %u\n",
-			(unsigned int)read_triggers,
-			(unsigned int)local_info->num_io_descriptors,
-			(unsigned int)dst_idx);
-		for (i = SW_WHEN_TYPE_BEGIN; i <= SW_WHEN_TYPE_END;
-		     ++i, read_triggers >>= 1) {
-			if (read_triggers & 0x1) { /* Bit 'i' is set */
-				pw_pr_debug("BIT %d is SET!\n", i);
-				if (i == SW_WHEN_TYPE_TRACEPOINT) {
-					struct tn_data tn_data = {
-						local_info,
-						local_info->tracepoint_id_mask
-					};
-					pw_pr_debug(
-						"TRACEPOINT, MASK = 0x%llx\n",
-						local_info->tracepoint_id_mask);
-					sw_for_each_tracepoint_node(
-					   &sw_add_trace_notifier_driver_info_i,
-					   &tn_data,
-					   false /*return-on-error*/);
-				} else if (i == SW_WHEN_TYPE_NOTIFIER) {
-					struct tn_data tn_data = {
-						local_info,
-						local_info->notifier_id_mask
-					};
-					pw_pr_debug(
-						"NOTIFIER, MASK = 0x%llx\n",
-						local_info->notifier_id_mask);
-					sw_for_each_notifier_node(
-					   &sw_add_trace_notifier_driver_info_i,
-					   &tn_data,
-					   false /*return-on-error*/);
-				} else {
-					if (sw_add_driver_info(
-						    &sw_collector_lists[i],
-						    local_info))
-						pw_pr_error(
-							"WARNING: could NOT add driver info to list for 'when type' %d!\n",
-							i);
-				}
-			}
-		}
-	}
-	if (sw_for_each_hw_op(&sw_post_config_i, NULL,
-			      false /*return-on-error*/))
-		pw_pr_error("POST-CONFIG error!\n");
-
-	vfree(local_msg);
-	memset(&s_internal_state, 0, sizeof(s_internal_state));
-	/*
-	 * DEBUG: iterate over collection lists.
-	 */
-	sw_iterate_driver_info_lists_i();
-	return PW_SUCCESS;
-}
-
-static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd,
-			    u64 __user *remote_out_args)
-{
-	/*
-	 * First, handle the command.
-	 */
-	if (cmd < SW_DRIVER_START_COLLECTION ||
-	    cmd > SW_DRIVER_CANCEL_COLLECTION) {
-		pw_pr_error("ERROR: invalid cmd = %d\n", cmd);
-		return -PW_ERROR;
-	}
-	switch (cmd) {
-	case SW_DRIVER_START_COLLECTION:
-		if (sw_collection_start_i())
-			return -PW_ERROR;
-
-		break;
-	case SW_DRIVER_STOP_COLLECTION:
-		if (sw_collection_stop_i())
-			return -PW_ERROR;
-
-		break;
-	default:
-		pw_pr_error("WARNING: unsupported command %d\n", cmd);
-		break;
-	}
-	/*
-	 * Then retrieve sample stats.
-	 */
-#if DO_COUNT_DROPPED_SAMPLES
-	if (cmd == SW_DRIVER_STOP_COLLECTION) {
-		u64 local_args[2] = { sw_num_samples_produced,
-				      sw_num_samples_dropped };
-		if (copy_to_user(remote_out_args, local_args,
-				 sizeof(local_args))) {
-			pw_pr_error(
-				"couldn't copy collection stats to user space!\n");
-			return -PW_ERROR;
-		}
-	}
-#endif /* DO_COUNT_DROPPED_SAMPLES */
-	return PW_SUCCESS;
-}
-
-#ifdef SFI_SIG_OEMB
-static int sw_do_parse_sfi_oemb_table(struct sfi_table_header *header)
-{
-#ifdef CONFIG_X86_WANT_INTEL_MID
-	struct sfi_table_oemb *oemb = (struct sfi_table_oemb *)
-		header; /* 'struct sfi_table_oemb' defined in 'intel-mid.h' */
-	if (!oemb) {
-		pw_pr_error("ERROR: NULL sfi table header!\n");
-		return -PW_ERROR;
-	}
-	sw_scu_fw_major_minor = (oemb->scu_runtime_major_version << 8) |
-				(oemb->scu_runtime_minor_version);
-	pw_pr_debug("DEBUG: major = %u, minor = %u\n",
-		    oemb->scu_runtime_major_version,
-		    oemb->scu_runtime_minor_version);
-#endif /* CONFIG_X86_WANT_INTEL_MID */
-	return PW_SUCCESS;
-}
-#endif /* SFI_SIG_OEMB */
-
-static void sw_do_extract_scu_fw_version(void)
-{
-	sw_scu_fw_major_minor = 0x0;
-#ifdef SFI_SIG_OEMB
-	if (sfi_table_parse(SFI_SIG_OEMB, NULL, NULL,
-			    &sw_do_parse_sfi_oemb_table))
-		pw_pr_force("WARNING: NO SFI information!\n");
-
-#endif /* SFI_SIG_OEMB */
-}
-
-static int sw_gather_trace_notifier_i(struct sw_trace_notifier_data *node,
-				      struct sw_name_info_msg *msg,
-				      enum sw_name_id_type type)
-{
-	pw_u16_t *idx = &msg->payload_len;
-	char *buffer = (char *)&msg->pairs[*idx];
-	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
-	int id = sw_get_trace_notifier_id(node);
-	struct sw_string_type *str = &pair->name;
-	const char *abstract_name = sw_get_trace_notifier_abstract_name(node);
-
-	if (likely(abstract_name && id >= 0)) {
-		++msg->num_name_id_pairs;
-		pair->type = type;
-		pair->id = (u16)id;
-		/* "+1" for trailing '\0' */
-		str->len = strlen(abstract_name) + 1;
-		memcpy(&str->data[0], abstract_name, str->len);
-
-		pw_pr_debug("TP[%d] = %s (%u)\n",
-			    sw_get_trace_notifier_id(node), abstract_name,
-			    (unsigned int)strlen(abstract_name));
-
-		*idx += SW_NAME_ID_HEADER_SIZE() +
-			SW_STRING_TYPE_HEADER_SIZE() + str->len;
-	}
-
-	return PW_SUCCESS;
-}
-
-static int sw_gather_tracepoint_i(struct sw_trace_notifier_data *node,
-				  void *priv)
-{
-	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
-					  SW_NAME_TYPE_TRACEPOINT);
-}
-
-static int sw_gather_notifier_i(struct sw_trace_notifier_data *node, void *priv)
-{
-	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
-					  SW_NAME_TYPE_NOTIFIER);
-}
-
-static long
-sw_get_available_trace_notifiers_i(enum sw_name_id_type type,
-				   struct sw_name_info_msg *local_info)
-{
-	long retVal = PW_SUCCESS;
-
-	if (type == SW_NAME_TYPE_TRACEPOINT)
-		retVal = sw_for_each_tracepoint_node(&sw_gather_tracepoint_i,
-						     local_info,
-						     false /*return-on-error*/);
-	else
-		retVal = sw_for_each_notifier_node(&sw_gather_notifier_i,
-						   local_info,
-						   false /*return-on-error*/);
-
-	pw_pr_debug(
-		"There are %u extracted traces/notifiers for a total of %u bytes!\n",
-		local_info->num_name_id_pairs, local_info->payload_len);
-
-	return retVal;
-}
-
-static int sw_gather_hw_op_i(const struct sw_hw_ops *op, void *priv)
-{
-	struct sw_name_info_msg *msg = (struct sw_name_info_msg *)priv;
-	pw_u16_t *idx = &msg->payload_len;
-	char *buffer = (char *)&msg->pairs[*idx];
-	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
-	struct sw_string_type *str = &pair->name;
-	const char *abstract_name = sw_get_hw_op_abstract_name(op);
-	int id = sw_get_hw_op_id(op);
-
-	pw_pr_debug("Gather Collector[%d] = %s\n", id, abstract_name);
-	if (likely(abstract_name && id >= 0)) {
-		/*
-		 * Final check: is this operation available on the
-		 * target platform? If 'available' function doesn't
-		 * exist then YES. Else call 'available'
-		 * function to decide.
-		 */
-		pw_pr_debug("%s has available = %p\n", abstract_name,
-			    op->available);
-		if (!op->available || (*op->available)()) {
-			++msg->num_name_id_pairs;
-			pair->type = SW_NAME_TYPE_COLLECTOR;
-			pair->id = (u16)id;
-			str->len = strlen(abstract_name) +
-				   1; /* "+1" for trailing '\0' */
-			memcpy(&str->data[0], abstract_name, str->len);
-
-			*idx += SW_NAME_ID_HEADER_SIZE() +
-				SW_STRING_TYPE_HEADER_SIZE() + str->len;
-		}
-	}
-
-	return PW_SUCCESS;
-}
-
-static long sw_get_available_collectors_i(struct sw_name_info_msg *local_info)
-{
-	return sw_for_each_hw_op(&sw_gather_hw_op_i, local_info,
-				 false /*return-on-error*/);
-}
-
-static long
-sw_get_available_name_id_mappings_i(enum sw_name_id_type type,
-				    struct sw_name_info_msg __user *remote_info,
-				    size_t local_len)
-{
-	char *buffer = vmalloc(local_len);
-	struct sw_name_info_msg *local_info = NULL;
-	long retVal = PW_SUCCESS;
-
-	if (!buffer) {
-		pw_pr_error("ERROR: couldn't alloc temp buffer!\n");
-		return -PW_ERROR;
-	}
-	memset(buffer, 0, local_len);
-	local_info = (struct sw_name_info_msg *)buffer;
-
-	if (type == SW_NAME_TYPE_COLLECTOR)
-		retVal = sw_get_available_collectors_i(local_info);
-	else
-		retVal = sw_get_available_trace_notifiers_i(type, local_info);
-
-	if (retVal == PW_SUCCESS) {
-		retVal = copy_to_user(remote_info, local_info, local_len);
-		if (retVal)
-			pw_pr_error(
-				"ERROR: couldn't copy tracepoint info to user space!\n");
-
-	}
-	vfree(buffer);
-	return retVal;
-}
-
-static long
-sw_get_topology_changes_i(struct sw_driver_topology_msg __user *remote_msg,
-			  size_t local_len)
-{
-	char *buffer = NULL;
-	struct sw_driver_topology_msg *local_msg = NULL;
-	size_t buffer_len = sizeof(struct sw_driver_topology_msg) +
-			    sw_num_topology_entries *
-				    sizeof(struct sw_driver_topology_change);
-	long retVal = PW_SUCCESS;
-	struct sw_driver_topology_change *dst = NULL;
-	size_t dst_idx = 0;
-
-	SW_LIST_HEAD_VAR(sw_topology_node) * head = (void *)&sw_topology_list;
-	struct sw_topology_node *tnode = NULL;
-
-	if (local_len < buffer_len) {
-		pw_pr_error(
-			"ERROR: insufficient buffer space to encode topology changes! Requires %zu, output space = %zu\n",
-			buffer_len, local_len);
-		return -EIO;
-	}
-
-	buffer = vmalloc(buffer_len);
-	if (!buffer) {
-		pw_pr_error(
-			"ERROR: couldn't allocate buffer for topology transfer!\n");
-		return -EIO;
-	}
-	memset(buffer, 0, buffer_len);
-
-	local_msg = (struct sw_driver_topology_msg *)buffer;
-	local_msg->num_entries = sw_num_topology_entries;
-	dst = (struct sw_driver_topology_change *)&local_msg
-		      ->topology_entries[0];
-	SW_LIST_FOR_EACH_ENTRY(tnode, head, list)
-	{
-		struct sw_driver_topology_change *change = &tnode->change;
-
-		memcpy(&dst[dst_idx++], change, sizeof(*change));
-	}
-	retVal = copy_to_user(remote_msg, local_msg, buffer_len);
-	if (retVal)
-		pw_pr_error(
-			"ERROR: couldn't copy topology changes to user space!\n");
-
-	vfree(buffer);
-	return retVal;
-}
-
-static long sw_read_continuous_i(char *remote_buffer, size_t local_len)
-{
-	/* TODO: call 'consume_buffer' directly? */
-	ssize_t val = sw_consume_data(0 /*mask, dummy*/, remote_buffer,
-					local_len);
-	if (val <= 0)
-		return val;
-
-	return 0;
-}
-
-static long sw_set_telem_cfgs_i(char *remote_cfg, size_t local_len)
-{
-	u64 *local_cfg = vmalloc(local_len);
-	int retval = 0;
-
-	if (!local_cfg) {
-		pw_pr_error("ERROR allocating space for local telem cfgs!\n");
-		return -EFAULT;
-	}
-	if (copy_from_user(local_cfg, remote_cfg, local_len)) {
-		pw_pr_error("ERROR copying message from user space!\n");
-		retval = -EFAULT;
-		goto done_set_telem_cfgs;
-	}
-	if (sw_setup_telem(local_cfg)) {
-		pw_pr_error("Couldn't setup telemetry\n");
-		retval = -1;
-	}
-done_set_telem_cfgs:
-	vfree(local_cfg);
-	return retval;
-}
-
-static long sw_set_continuous_i(
-	struct sw_driver_continuous_collect __user *remote_msg,
-	int local_len)
-{
-    pw_u32_t buffer_size = 0;
-	long ret = get_user(buffer_size, &remote_msg->collection_size);
-	if (ret)
-		return ret;
-
-	if (buffer_size == 0) {
-		pw_pr_error("Cannot allocate a zero length buffer!\n");
-		return -EINVAL;
-	}
-	ret = initialize_circular_buffer(buffer_size);
-	if (ret)
-		return ret;
-
-	ret = sw_set_driver_infos_i((struct sw_driver_interface_msg __user *)
-		remote_msg->payload, local_len);
-	if (ret) {
-		destroy_circular_buffer();
-		return ret;
-	}
-	return 0;
-}
-
-#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
-#define MATCH_IOCTL(num, pred) ((num) == (pred) || (num) == (pred##32))
-#else
-#define MATCH_IOCTL(num, pred) ((num) == (pred))
-#endif
-
-static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
-					   void *p_local_args)
-{
-	struct sw_driver_ioctl_arg local_args;
-	int local_in_len, local_out_len;
-
-	if (!p_local_args) {
-		pw_pr_error("ERROR: NULL p_local_args value?!\n");
-		return -PW_ERROR;
-	}
-
-	/*
-	 * (1) Sanity check:
-	 * Before doing anything, double check to
-	 * make sure this IOCTL was really intended
-	 * for us!
-	 */
-	if (_IOC_TYPE(ioctl_num) != APWR_IOCTL_MAGIC_NUM) {
-		pw_pr_error(
-			"ERROR: requested IOCTL TYPE (%d) != \
-				APWR_IOCTL_MAGIC_NUM (%d)\n",
-			_IOC_TYPE(ioctl_num), APWR_IOCTL_MAGIC_NUM);
-		return -PW_ERROR;
-	}
-	/*
-	 * (2) Extract arg lengths.
-	 */
-	local_args = *((struct sw_driver_ioctl_arg *)p_local_args);
-
-	local_in_len = local_args.in_len;
-	local_out_len = local_args.out_len;
-	pw_pr_debug("GU: local_in_len = %d, local_out_len = %d\n", local_in_len,
-		local_out_len);
-	/*
-	 * (3) Service individual IOCTL requests.
-	 */
-	if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG)) {
-		pw_pr_debug("PW_IOCTL_CONFIG\n");
-		return sw_set_driver_infos_i(
-			(struct sw_driver_interface_msg __user *)
-				local_args.in_arg,
-			local_in_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG_CONTINUOUS)) {
-		pw_pr_debug("DEBUG: PW_IOCTL_CONFIG_CONTINUOUS\n");
-		return sw_set_continuous_i(
-			(struct sw_driver_continuous_collect __user *)
-				local_args.in_arg,
-			local_in_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CMD)) {
-		sw_driver_collection_cmd_t local_cmd;
-
-		pw_pr_debug("PW_IOCTL_CMD\n");
-	if (get_user(local_cmd,
-		(sw_driver_collection_cmd_t __user *)local_args.in_arg)) {
-		pw_pr_error("ERROR: could NOT extract cmd value!\n");
-		return -PW_ERROR;
-	}
-	return sw_handle_cmd_i(local_cmd, (u64 __user *)local_args.out_arg);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_POLL)) {
-		pw_pr_debug("PW_IOCTL_POLL\n");
-		return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_collection_poll_i);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_IMMEDIATE_IO)) {
-		struct sw_driver_interface_info *local_info;
-		struct sw_driver_io_descriptor *local_descriptor = NULL;
-		int retVal = PW_SUCCESS;
-		char *src_vals = NULL;
-		char *dst_vals = NULL;
-
-		pw_pr_debug("PW_IOCTL_IMMEDIATE_IO\n");
-		pw_pr_debug("local_in_len = %u\n", local_in_len);
-
-		src_vals = vmalloc(local_in_len);
-		if (!src_vals) {
-			pw_pr_error(
-				"ERROR allocating space for immediate IO\n");
-			return -PW_ERROR;
-		}
-		if (local_out_len) {
-			dst_vals = vmalloc(local_out_len);
-			if (!dst_vals) {
-				vfree(src_vals);
-				pw_pr_error(
-					"ERROR allocating space for immediate IO\n");
-				return -PW_ERROR;
-			}
-		}
-		if (copy_from_user(src_vals, (char __user *)local_args.in_arg,
-				   local_in_len)) {
-			pw_pr_error(
-				"ERROR copying in immediate IO descriptor\n");
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		local_info = (struct sw_driver_interface_info *)src_vals;
-		pw_pr_debug(
-			"OK, asked to perform immediate IO on cpu(s) %d, # descriptors = %d\n",
-			local_info->cpu_mask, local_info->num_io_descriptors);
-		/*
-		 * For now, require only a single descriptor.
-		 */
-		if (local_info->num_io_descriptors != 1) {
-			pw_pr_error(
-				"ERROR: told to perform immediate IO with %d descriptors -- MAX of 1 descriptor allowed!\n",
-				local_info->num_io_descriptors);
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		local_descriptor = ((struct sw_driver_io_descriptor *)
-						local_info->descriptors);
-		pw_pr_debug("Collection type after %d\n",
-				local_descriptor->collection_type);
-		/*
-		 * Check cpu mask for correctness here. For now, we do NOT allow
-		 * reading on ALL cpus.
-		 */
-		if ((int)local_info->cpu_mask < -1 ||
-			(int)local_info->cpu_mask >= (int)sw_max_num_cpus) {
-			pw_pr_error(
-				"ERROR: invalid cpu mask %d specified in immediate IO; valid values are: -1, [0 -- %d]!\n",
-				local_info->cpu_mask, sw_max_num_cpus - 1);
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		/*
-		 * Check collection type for correctness here
-		 */
-		pw_pr_debug(
-			"Asked to perform immediate IO with descriptor with type = %d, on cpu = %d\n",
-			local_descriptor->collection_type,
-			local_info->cpu_mask);
-		if (sw_is_valid_hw_op_id(local_descriptor->collection_type) ==
-			false) {
-			pw_pr_error(
-				"ERROR: invalid collection type %d specified for immediate IO\n",
-				(int)local_descriptor->collection_type);
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		/*
-		 * Check collection cmd for correctness here
-		 */
-		if (local_descriptor->collection_command < SW_IO_CMD_READ ||
-			local_descriptor->collection_command > SW_IO_CMD_WRITE) {
-			pw_pr_error(
-				"ERROR: invalid collection command %d specified for immediate IO\n",
-				local_descriptor->collection_command);
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		/*
-		 * Initialize the descriptor -- 'MMIO' and 'IPC' reads may need
-		 * an "ioremap_nocache"
-		 */
-		if (sw_init_driver_io_descriptor(local_descriptor)) {
-			pw_pr_error(
-				"ERROR initializing immediate IO descriptor\n");
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-		/*
-		 * OK, perform the actual IO.
-		 */
-		{
-			struct sw_immediate_request_info request_info = {
-				local_descriptor, dst_vals, &retVal
-			};
-			struct cpumask cpumask;
-
-			cpumask_clear(&cpumask);
-			switch (local_info->cpu_mask) {
-			case -1: /* IO on ANY CPU (assume current CPU) */
-				cpumask_set_cpu(RAW_CPU(), &cpumask);
-				pw_pr_debug("ANY CPU\n");
-				break;
-			default: /* IO on a particular CPU */
-				cpumask_set_cpu(local_info->cpu_mask, &cpumask);
-				pw_pr_debug("[%d] setting for %d\n", RAW_CPU(),
-						local_info->cpu_mask);
-				break;
-			}
-			sw_schedule_work(&cpumask,
-					 &sw_handle_immediate_request_i,
-					 &request_info);
-		}
-		if (retVal != PW_SUCCESS) {
-			pw_pr_error(
-				"ERROR performing immediate IO on one (or more) CPUs!\n");
-			goto ret_immediate_io_reset;
-		}
-		/*
-		 * OK, all done.
-		 */
-		if (local_descriptor->collection_command == SW_IO_CMD_READ) {
-			if (copy_to_user(local_args.out_arg, dst_vals,
-					 local_out_len)) {
-				pw_pr_error(
-					"ERROR copying %u bytes of value to userspace!\n",
-					local_out_len);
-				retVal = -PW_ERROR;
-				goto ret_immediate_io_reset;
-			}
-			pw_pr_debug(
-				"OK, copied %u bytes of value to userspace addr %p!\n",
-				local_out_len, local_args.out_arg);
-		}
-ret_immediate_io_reset:
-		/*
-		 * Reset the descriptor -- 'MMIO' and 'IPC' reads may have
-		 * performed an "ioremap_nocache" which now needs to be
-		 * unmapped.
-		 */
-		if (sw_reset_driver_io_descriptor(local_descriptor)) {
-			pw_pr_error(
-				"ERROR resetting immediate IO descriptor\n");
-			retVal = -PW_ERROR;
-			goto ret_immediate_io;
-		}
-ret_immediate_io:
-		vfree(src_vals);
-		if (dst_vals)
-			vfree(dst_vals);
-
-		return retVal;
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_SCU_FW_VERSION)) {
-		u32 local_data = (u32)sw_scu_fw_major_minor;
-
-		if (put_user(local_data, (u32 __user *)local_args.out_arg)) {
-			pw_pr_error(
-				"ERROR copying scu fw version to userspace!\n"
-				);
-			return -PW_ERROR;
-		}
-		return PW_SUCCESS;
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_DRIVER_VERSION)) {
-		pw_u64_t local_version =
-			(pw_u64_t)SW_DRIVER_VERSION_MAJOR << 32 |
-			(pw_u64_t)SW_DRIVER_VERSION_MINOR << 16 |
-			(pw_u64_t)SW_DRIVER_VERSION_OTHER;
-		if (put_user(local_version,
-			(u64 __user *)local_args.out_arg)) {
-			pw_pr_error(
-				"ERROR copying driver version to userspace!\n"
-				);
-			return -PW_ERROR;
-		}
-		return PW_SUCCESS;
-	} else if (MATCH_IOCTL(ioctl_num,
-			PW_IOCTL_GET_AVAILABLE_TRACEPOINTS)) {
-		pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n",
-			local_out_len);
-		return sw_get_available_name_id_mappings_i(
-			SW_NAME_TYPE_TRACEPOINT,
-			(struct sw_name_info_msg __user *)local_args.out_arg,
-			local_out_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_NOTIFIERS)) {
-		pw_pr_debug("DEBUG: AVAIL notifiers! local_out_len = %u\n",
-			local_out_len);
-		return sw_get_available_name_id_mappings_i(
-			SW_NAME_TYPE_NOTIFIER,
-			(struct sw_name_info_msg __user *)local_args.out_arg,
-			local_out_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_COLLECTORS)) {
-		pw_pr_debug("DEBUG: AVAIL collectors! local_out_len = %u\n",
-			local_out_len);
-		return sw_get_available_name_id_mappings_i(
-			SW_NAME_TYPE_COLLECTOR,
-			(struct sw_name_info_msg __user *)local_args.out_arg,
-			local_out_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_TOPOLOGY_CHANGES)) {
-		pw_pr_debug("DEBUG: TOPOLOGY changes! local_out_len = %u\n",
-			local_out_len);
-		return sw_get_topology_changes_i(
-			(struct sw_driver_topology_msg __user *)
-				local_args.out_arg, local_out_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_READ_CONTINUOUS)) {
-		pw_pr_debug("DEBUG: READ_CONTINUOUS!\n");
-		return sw_read_continuous_i(local_args.out_arg, local_out_len);
-	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_SET_TELEM_BAR)) {
-		pw_pr_debug("DEBUG: got a request to set telem bar!\n");
-		return sw_set_telem_cfgs_i(local_args.in_arg, local_in_len);
-	}
-
-	pw_pr_error("ERROR: invalid ioctl num: %u\n", _IOC_NR(ioctl_num));
-	return -PW_ERROR;
-}
-
-static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void)
-{
-	return s_internal_state.cmd;
-};
-
-static bool sw_should_flush_buffer_i(void)
-{
-	return s_internal_state.drain_buffers;
-};
-
-int sw_load_driver_i(void)
-{
-	/*
-	 * Set per-cpu buffer size.
-	 * First, Perform sanity checking of per-cpu buffer size.
-	 */
-	/*
-	 * 1. Num pages MUST be pow-of-2.
-	 */
-	{
-		if (sw_buffer_num_pages & (sw_buffer_num_pages - 1)) {
-			pw_pr_error(
-				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be a power of 2!\n",
-				sw_buffer_num_pages);
-			return -PW_ERROR;
-		}
-	}
-	/*
-	 * 2. Num pages MUST be <= 16 (i.e. per-cpu buffer size
-	 * MUST be <= 64 kB)
-	 */
-	{
-		if (sw_buffer_num_pages > 16) {
-			pw_pr_error(
-				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be <= 16!\n",
-				sw_buffer_num_pages);
-			return -PW_ERROR;
-		}
-	}
-	sw_buffer_alloc_size = sw_buffer_num_pages * PAGE_SIZE;
-	/*
-	 * Retrieve any arch details here.
-	 */
-	if (sw_get_arch_details_i()) {
-		pw_pr_error("ERROR retrieving arch details!\n");
-		return -PW_ERROR;
-	}
-	/*
-	 * Check to see if the user wants us to force
-	 * software coordination of CPU frequencies.
-	 */
-	if (do_force_module_scope_for_cpu_frequencies) {
-		pw_pr_force(
-			"DEBUG: FORCING MODULE SCOPE FOR CPU FREQUENCIES!\n");
-		if (sw_set_module_scope_for_cpus()) {
-			pw_pr_force("ERROR setting affected cpus\n");
-			return -PW_ERROR;
-		}
-		pw_pr_debug("OK, setting worked\n");
-	}
-	if (sw_init_data_structures_i()) {
-		pw_pr_error("ERROR initializing data structures!\n");
-		goto err_ret_init_data;
-	}
-	if (sw_register_dev(&s_ops))
-		goto err_ret_register_dev;
-	/*
-	 * Retrieve a list of tracepoint structs to use when
-	 * registering probe functions.
-	 */
-	{
-		if (sw_extract_tracepoints()) {
-			pw_pr_error(
-				"ERROR: could NOT retrieve a complete list of valid tracepoint structs!\n");
-			goto err_ret_tracepoint;
-		}
-	}
-	pw_pr_force("-----------------------------------------\n");
-	pw_pr_force("OK: LOADED SoC Watch Driver\n");
-#ifdef CONFIG_X86_WANT_INTEL_MID
-	pw_pr_force("SOC Identifier = %u, Stepping = %u\n",
-			intel_mid_identify_cpu(), intel_mid_soc_stepping());
-#endif /* CONFIG_X86_WANT_INTEL_MID */
-	pw_pr_force("-----------------------------------------\n");
-	return PW_SUCCESS;
-
-err_ret_tracepoint:
-	sw_unregister_dev();
-err_ret_register_dev:
-	sw_destroy_data_structures_i();
-err_ret_init_data:
-	if (do_force_module_scope_for_cpu_frequencies) {
-		if (sw_reset_module_scope_for_cpus())
-			pw_pr_force("ERROR resetting affected cpus\n");
-		else
-			pw_pr_debug("OK, resetting worked\n");
-	}
-	return -PW_ERROR;
-}
-
-void sw_unload_driver_i(void)
-{
-	sw_iterate_driver_info_lists_i();
-
-	sw_unregister_dev();
-
-	sw_destroy_data_structures_i();
-
-	if (do_force_module_scope_for_cpu_frequencies) {
-		if (sw_reset_module_scope_for_cpus())
-			pw_pr_force("ERROR resetting affected cpus\n");
-		else
-			pw_pr_debug("OK, resetting worked\n");
-	}
-
-	pw_pr_force("-----------------------------------------\n");
-	pw_pr_force("OK: UNLOADED SoC Watch Driver\n");
-
-	sw_print_trace_notifier_overheads();
-	sw_print_output_buffer_overheads();
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_collection_poll_i, "POLL");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_any_seg_full, "ANY_SEG_FULL");
-#if DO_TRACK_MEMORY_USAGE
-	/*
-	 * Dump memory stats.
-	 */
-	pw_pr_force(
-		"TOTAL # BYTES ALLOCED = %llu, CURR # BYTES ALLOCED = %llu, MAX # BYTES ALLOCED = %llu\n",
-		sw_get_total_bytes_alloced(),
-		sw_get_curr_bytes_alloced(),
-		sw_get_max_bytes_alloced());
-	if (unlikely(sw_get_curr_bytes_alloced())) {
-		pw_pr_force(
-			"***********************************************************************\n");
-		pw_pr_force(
-			"WARNING: possible memory leak: there are %llu bytes still allocated!\n",
-			sw_get_curr_bytes_alloced());
-		pw_pr_force(
-			"***********************************************************************\n");
-	}
-#endif /* DO_TRACK_MEMORY_USAGE */
-	pw_pr_force("-----------------------------------------\n");
-}
-
-module_init(sw_load_driver_i);
-module_exit(sw_unload_driver_i);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR(MOD_AUTHOR);
-MODULE_DESCRIPTION(MOD_DESC);
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#define MOD_AUTHOR "Gautam Upadhyaya <gautam.upadhyaya@intel.com>"
+#define MOD_DESC "SoC Watch kernel module"
+
+#include "sw_internal.h"
+#include "sw_structs.h"
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_mem.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+#include "sw_hardware_io.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_collector.h"
+#include "sw_file_ops.h"
+#include "sw_version.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Number of entries in the 'sw_collector_lists' array
+ */
+#define NUM_COLLECTOR_MODES (SW_WHEN_TYPE_END - SW_WHEN_TYPE_BEGIN + 1)
+#define PW_OUTPUT_BUFFER_SIZE                                                  \
+	256 /* Number of output messages in each per-cpu buffer */
+/*
+ * Check if tracepoint/notifier ID is in (user-supplied) mask
+ */
+#define IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)                                 \
+	((id) >= 0 && (((mask) >> (id)) & 0x1))
+
+/* -------------------------------------------------
+ *  Local function declarations.
+ * -------------------------------------------------
+ */
+int sw_load_driver_i(void);
+void sw_unload_driver_i(void);
+int sw_init_collector_lists_i(void);
+void sw_destroy_collector_lists_i(void);
+int sw_init_data_structures_i(void);
+void sw_destroy_data_structures_i(void);
+int sw_get_arch_details_i(void);
+void sw_iterate_driver_info_lists_i(void);
+void sw_handle_immediate_request_i(void *request);
+int sw_print_collector_node_i(struct sw_collector_data *data);
+int sw_collection_start_i(void);
+int sw_collection_stop_i(void);
+int sw_collection_poll_i(void);
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
+sw_driver_msg_t *sw_alloc_collector_msg_i(
+	const struct sw_driver_interface_info *info,
+	size_t per_msg_payload_size);
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+                                       void *p_local_args);
+static long sw_set_driver_infos_i(
+	struct sw_driver_interface_msg __user *remote_msg, int local_len);
+static long sw_handle_cmd_i(
+	sw_driver_collection_cmd_t cmd, u64 __user* remote_out_args);
+static void sw_do_extract_scu_fw_version(void);
+static long sw_get_available_name_id_mappings_i(
+	enum sw_name_id_type type,
+	struct sw_name_info_msg __user* remote_info,
+	size_t local_len);
+static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void);
+static bool sw_should_flush_buffer_i(void);
+
+/* -------------------------------------------------
+ * Data structures.
+ * -------------------------------------------------
+ */
+/*
+ * Structure to hold current CMD state
+ * of the device driver. Constantly evolving, but
+ * that's OK -- this is internal to the driver
+ * and is NOT exported.
+ */
+struct swa_internal_state {
+	/*
+	 * Indicates which command was specified
+	 * last e.g. START, STOP etc.
+	 */
+	sw_driver_collection_cmd_t cmd;
+	/*
+	 * Should we write to our per-cpu output buffers?
+	 * YES if we're actively collecting.
+	 * NO if we're not.
+	 */
+	bool write_to_buffers;
+	/*
+	 * Should we "drain/flush" the per-cpu output buffers?
+	 * (See "device_read" for an explanation)
+	 */
+	bool drain_buffers;
+	/* Others... */
+};
+
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+static bool do_force_module_scope_for_cpu_frequencies;
+module_param(do_force_module_scope_for_cpu_frequencies, bool, 0400);
+MODULE_PARM_DESC(
+	do_force_module_scope_for_cpu_frequencies,
+	"Toggle module scope for cpu frequencies. Sets \"affected_cpus\" and \"related_cpus\" of cpufreq_policy.");
+
+static unsigned short sw_buffer_num_pages = 16;
+module_param(sw_buffer_num_pages, ushort, 0400);
+MODULE_PARM_DESC(
+	sw_buffer_num_pages,
+	"Specify number of 4kB pages to use for each per-cpu buffer. MUST be a power of 2! Default value = 16 (64 kB)");
+
+/* TODO: convert from 'list_head' to 'hlist_head' */
+/*
+ * sw_collector_lists is an array of linked lists of "collector nodes"
+ * (sw_collector_data structs).  It is indexed by the sw_when_type_t's.
+ * Each list holds the collectors to "execute" at a specific time,
+ * e.g. the beginning of the run, at a poll interval, tracepoint, etc.
+ */
+static SW_DEFINE_LIST_HEAD(sw_collector_lists,
+			   sw_collector_data)[NUM_COLLECTOR_MODES];
+static __read_mostly u16 sw_scu_fw_major_minor;
+
+static struct swa_internal_state s_internal_state;
+static struct sw_file_ops s_ops = {
+	.ioctl_handler = &sw_unlocked_handle_ioctl_i,
+	.stop_handler = &sw_collection_stop_i,
+	.get_current_cmd = &sw_get_collection_cmd_i,
+	.should_flush = &sw_should_flush_buffer_i,
+};
+
+/*
+ * For each function that you want to profile,
+ * do the following (e.g. function 'foo'):
+ * **************************************************
+ * DECLARE_OVERHEAD_VARS(foo);
+ * **************************************************
+ * This will declare the two variables required
+ * to keep track of overheads incurred in
+ * calling/servicing 'foo'. Note that the name
+ * that you declare here *MUST* match the function name!
+ */
+
+DECLARE_OVERHEAD_VARS(sw_collection_poll_i); /* for POLL */
+DECLARE_OVERHEAD_VARS(sw_any_seg_full);
+
+/*
+ * String representation of the various 'SW_WHEN_TYPE_XYZ' enum values.
+ * Debugging ONLY!
+ */
+#if DO_DEBUG_OUTPUT
+static const char * const s_when_type_names[] = { "BEGIN", "POLL", "NOTIFIER",
+					   "TRACEPOINT", "END" };
+#endif /* DO_DEBUG_OUTPUT */
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * External functions.
+ */
+int sw_process_snapshot(enum sw_when_type when)
+{
+	if (when > SW_WHEN_TYPE_END) {
+		pw_pr_error("invalid snapshot time %d specified!\n", when);
+		return -EINVAL;
+	}
+	if (sw_handle_collector_list(&sw_collector_lists[when],
+				     &sw_handle_collector_node)) {
+		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
+			    when);
+		return -EIO;
+	}
+	return 0;
+}
+
+int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu)
+{
+	if (when > SW_WHEN_TYPE_END) {
+		pw_pr_error("invalid snapshot time %d specified!\n", when);
+		return -EINVAL;
+	}
+	if (sw_handle_collector_list_on_cpu(&sw_collector_lists[when],
+					    &sw_handle_collector_node_on_cpu,
+					    cpu)) {
+		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
+			    when);
+		return -EIO;
+	}
+	return 0;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+int sw_print_collector_node_i(struct sw_collector_data *curr)
+{
+	pw_u16_t num_descriptors = 0;
+	sw_io_desc_print_func_t print_func = NULL;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+	struct sw_driver_interface_info *info = NULL;
+
+	if (!curr)
+		return -PW_ERROR;
+
+	info = curr->info;
+	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	pw_pr_debug(
+		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
+	     --num_descriptors, ++descriptor) {
+		const struct sw_hw_ops *ops =
+			sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops == NULL)
+			return -PW_ERROR;
+
+		print_func = ops->print;
+		if (print_func && (*print_func)(descriptor))
+			return -PW_ERROR;
+
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+
+/**
+ * sw_reset_collector_node_i - Call the reset op on all of the descriptors
+ *                             in coll that have one.
+ * @coll: The data structure containing an array of collector descriptors.
+ *
+ * Return: PW_SUCCESS if all of the resets succeeded, -PW_ERROR if any failed.
+ */
+static int sw_reset_collector_node_i(struct sw_collector_data *coll)
+{
+	struct sw_driver_io_descriptor *descriptor = NULL;
+	struct sw_driver_interface_info *info = NULL;
+	int num_descriptors;
+	int retcode = PW_SUCCESS;
+
+	if (!coll)
+		return -PW_ERROR;
+
+	info = coll->info;
+
+	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	pw_pr_debug(
+		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
+	     --num_descriptors, ++descriptor) {
+		const struct sw_hw_ops *ops =
+			sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops && ops->reset && (*ops->reset)(descriptor))
+			retcode = -PW_ERROR;
+
+	}
+	return retcode;
+}
+
+static int sw_iterate_trace_notifier_list_i(struct sw_trace_notifier_data *node,
+				     void *dummy)
+{
+	return sw_handle_collector_list(&node->list,
+					&sw_print_collector_node_i);
+}
+
+void sw_iterate_driver_info_lists_i(void)
+{
+	sw_when_type_t which;
+
+	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+		/* Should NEVER happen! */
+		if (sw_handle_collector_list(
+			    &sw_collector_lists[which],
+			    &sw_print_collector_node_i))
+			pw_pr_error(
+				"WARNING: error occurred while printing values!\n");
+
+	}
+
+	if (sw_for_each_tracepoint_node(&sw_iterate_trace_notifier_list_i, NULL,
+					false /*return-on-error*/))
+		pw_pr_error(
+			"WARNING: error occurred while printing tracepoint values!\n");
+
+	if (sw_for_each_notifier_node(&sw_iterate_trace_notifier_list_i, NULL,
+				      false /*return-on-error*/))
+		pw_pr_error(
+			"WARNING: error occurred while printing notifier values!\n");
+
+}
+
+static void sw_reset_collectors_i(void)
+{
+	sw_when_type_t which;
+
+	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+		if (sw_handle_collector_list(&sw_collector_lists[which],
+					     &sw_reset_collector_node_i))
+			pw_pr_error(
+				"WARNING: error occurred while resetting a collector!\n");
+
+	}
+}
+
+int sw_init_data_structures_i(void)
+{
+	/*
+	 * Find the # CPUs in this system.
+	 * Update: use 'num_possible' instead of 'num_present' in case
+	 * the cpus aren't numbered contiguously
+	 */
+	sw_max_num_cpus = num_possible_cpus();
+
+	/*
+	 * Initialize our trace subsys: MUST be called
+	 * BEFORE 'sw_init_collector_lists_i()!
+	 */
+	if (sw_add_trace_notify()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_init_collector_lists_i()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_init_per_cpu_buffers()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_register_hw_ops()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	return PW_SUCCESS;
+}
+
+void sw_destroy_data_structures_i(void)
+{
+	sw_free_hw_ops();
+	sw_destroy_per_cpu_buffers();
+	sw_destroy_collector_lists_i();
+	sw_remove_trace_notify();
+
+	/* Should already have been called from 'collection_stop' */
+	sw_destroy_telem();
+}
+
+int sw_get_arch_details_i(void)
+{
+	/*
+	 * SCU F/W version (if applicable)
+	 */
+	sw_do_extract_scu_fw_version();
+	return PW_SUCCESS;
+}
+
+#define INIT_FLAG ((void *)0)
+#define DESTROY_FLAG ((void *)1)
+
+static int
+sw_init_destroy_trace_notifier_lists_i(struct sw_trace_notifier_data *node,
+				       void *is_init)
+{
+	if (is_init == INIT_FLAG)
+		sw_init_collector_list(&node->list);
+	else
+		sw_destroy_collector_list(&node->list);
+
+	node->was_registered = false;
+
+	return PW_SUCCESS;
+}
+
+int sw_init_collector_lists_i(void)
+{
+	int i = 0;
+
+	for (i = 0; i < NUM_COLLECTOR_MODES; ++i)
+		sw_init_collector_list(&sw_collector_lists[i]);
+
+	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
+				    INIT_FLAG, false /*return-on-error*/);
+	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
+				  INIT_FLAG, false /*return-on-error*/);
+
+	return PW_SUCCESS;
+}
+
+void sw_destroy_collector_lists_i(void)
+{
+	int i = 0;
+
+	for (i = 0; i < NUM_COLLECTOR_MODES; ++i)
+		sw_destroy_collector_list(&sw_collector_lists[i]);
+
+	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
+				    DESTROY_FLAG, false /*return-on-error*/);
+	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
+				  DESTROY_FLAG, false /*return-on-error*/);
+}
+
+/*
+ * Used for {READ,WRITE}_IMMEDIATE requests.
+ */
+struct sw_immediate_request_info {
+	struct sw_driver_io_descriptor *local_descriptor;
+	char *dst_vals;
+	int *retVal;
+};
+void sw_handle_immediate_request_i(void *request)
+{
+	struct sw_immediate_request_info *info =
+		(struct sw_immediate_request_info *)request;
+	struct sw_driver_io_descriptor *descriptor = info->local_descriptor;
+	char *dst_vals = info->dst_vals;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+	if (likely(ops != NULL))
+		*(info->retVal) = sw_handle_driver_io_descriptor(
+			dst_vals, RAW_CPU(), descriptor, ops);
+	else
+		pw_pr_error(
+			"No operations found to satisfy collection type %u!\n",
+			descriptor->collection_type);
+}
+
+static int num_times_polled;
+
+int sw_collection_start_i(void)
+{
+	/*
+	 * Reset the poll tick counter.
+	 */
+	num_times_polled = 0;
+	/*
+	 * Update the output buffers.
+	 */
+	sw_reset_per_cpu_buffers();
+	/*
+	 * Ensure clients don't think we're in 'flush' mode.
+	 */
+	s_internal_state.drain_buffers = false;
+	/*
+	 * Set the 'command'
+	 */
+	s_internal_state.cmd = SW_DRIVER_START_COLLECTION;
+	/*
+	 * Clear out the topology list
+	 */
+	sw_clear_topology_list();
+	/*
+	 * Handle 'START' snapshots, if any.
+	 */
+	{
+		if (sw_handle_collector_list(
+			    &sw_collector_lists[SW_WHEN_TYPE_BEGIN],
+			    &sw_handle_collector_node)) {
+			pw_pr_error(
+				"ERROR: could NOT handle START collector list!\n");
+			return -PW_ERROR;
+		}
+	}
+	/*
+	 * Register any required tracepoints and notifiers.
+	 */
+	{
+		if (sw_register_trace_notifiers()) {
+			pw_pr_error("ERROR registering trace_notifiers!\n");
+			sw_unregister_trace_notifiers();
+			return -PW_ERROR;
+		}
+	}
+	pw_pr_debug("OK, STARTED collection!\n");
+	return PW_SUCCESS;
+}
+
+int sw_collection_stop_i(void)
+{
+	/*
+	 * Unregister any registered tracepoints and notifiers.
+	 */
+	if (sw_unregister_trace_notifiers())
+		pw_pr_warn(
+			"Warning: some trace_notifier probe functions could NOT be unregistered!\n");
+
+	/*
+	 * Handle 'STOP' snapshots, if any.
+	 */
+	if (sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_END],
+				     &sw_handle_collector_node)) {
+		pw_pr_error("ERROR: could NOT handle STOP collector list!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Set the 'command'
+	 */
+	s_internal_state.cmd = SW_DRIVER_STOP_COLLECTION;
+	/*
+	 * Tell consumers to 'flush' all buffers. We need to
+	 * defer this as long as possible because it needs to be
+	 * close to the 'wake_up_interruptible', below.
+	 */
+	s_internal_state.drain_buffers = true;
+	smp_mb(); /* order memory access */
+	/*
+	 * Wakeup any sleeping readers, and cleanup any
+	 * timers in the reader subsys.
+	 */
+	sw_cancel_reader();
+	/*
+	 * Collect stats on samples produced and dropped.
+	 * TODO: call from 'device_read()' instead?
+	 */
+	sw_count_samples_produced_dropped();
+#if DO_OVERHEAD_MEASUREMENTS
+	pw_pr_force("DEBUG: there were %llu samples produced and %llu samples \
+		dropped in buffer v5!\n", sw_num_samples_produced,
+		sw_num_samples_dropped);
+#endif // DO_OVERHEAD_MEASUREMENTS
+	/*
+	 * DEBUG: iterate over collection lists.
+	 */
+	sw_iterate_driver_info_lists_i();
+	/*
+	 * Shut down any collectors that need shutting down.
+	 */
+	sw_reset_collectors_i();
+	/*
+	 * Clear out the collector lists.
+	 */
+	sw_destroy_collector_lists_i();
+	/*
+	 * Free up circular buffer
+	 */
+	destroy_circular_buffer();
+	/*
+	 * Remove telemetry mappings
+	 */
+	sw_destroy_telem();
+	pw_pr_debug("OK, STOPPED collection!\n");
+#if DO_OVERHEAD_MEASUREMENTS
+	pw_pr_force("There were %d poll ticks!\n", num_times_polled);
+#endif /* DO_OVERHEAD_MEASUREMENTS */
+	return PW_SUCCESS;
+}
+
+int sw_collection_poll_i(void)
+{
+	/*
+	 * Handle 'POLL' timer expirations.
+	 */
+	if (SW_LIST_EMPTY(&sw_collector_lists[SW_WHEN_TYPE_POLL]))
+		pw_pr_debug("DEBUG: EMPTY POLL LIST\n");
+
+	++num_times_polled;
+	return sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_POLL],
+					&sw_handle_collector_node);
+}
+
+/*
+ * Private data for the 'sw_add_trace_notifier_driver_info_i' function.
+ */
+struct tn_data {
+	struct sw_driver_interface_info *info;
+	u64 mask;
+};
+
+static int
+sw_add_trace_notifier_driver_info_i(struct sw_trace_notifier_data *node,
+				    void *priv)
+{
+	struct tn_data *data = (struct tn_data *)priv;
+	struct sw_driver_interface_info *local_info = data->info;
+	u64 mask = data->mask;
+	int id = sw_get_trace_notifier_id(node);
+
+	if (IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)) {
+		pw_pr_debug("TRACEPOINT ID = %d is IN mask 0x%llx\n", id, mask);
+		if (sw_add_driver_info(&node->list, local_info)) {
+			pw_pr_error(
+				"WARNING: could NOT add driver info to list!\n");
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+static int sw_post_config_i(const struct sw_hw_ops *op, void *priv)
+{
+	/* op not available */
+	if (!op->available || !(*op->available)())
+		return 0;
+
+	if (!op->post_config || (*op->post_config)())
+		return 0;
+
+	return -EIO;
+}
+
+/**
+ * sw_set_driver_infos_i - Process the collection config data passed down
+ *                         from the client.
+ * @remote_msg: The user space address of our ioctl data.
+ * @local_len:  The number of bytes of remote_msg we should copy.
+ *
+ * This function copies the ioctl data from user space to kernel
+ * space.  That data is an array of sw_driver_interface_info structs,
+ * which hold information about tracepoints, notifiers, and collector
+ * configuration info for this collection run..  For each driver_info
+ * struct, it calls the appropriate "add info" (registration/
+ * configuration) function for each of the "when types" (begin, poll,
+ * notifier, tracepoint, end) which should trigger a collection
+ * operation for that collector.
+ *
+ * When this function is done, the data structures corresponding to
+ * collection should be configured and initialized.
+ *
+ *
+ * Returns: PW_SUCCESS on success, or a non-zero on an error.
+ */
+static long
+sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg,
+		      int local_len)
+{
+	struct sw_driver_interface_info *local_info = NULL;
+	struct sw_driver_interface_msg *local_msg = vmalloc(local_len);
+	pw_u8_t read_triggers = 0x0;
+	pw_u16_t num_infos = 0;
+	sw_when_type_t i = SW_WHEN_TYPE_BEGIN;
+	char *__data = (char *)local_msg->infos;
+	size_t dst_idx = 0;
+
+	if (!local_msg) {
+		pw_pr_error("ERROR allocating space for local message!\n");
+		return -EFAULT;
+	}
+	if (copy_from_user(local_msg, (struct sw_driver_interface_msg __user *)
+			   remote_msg, local_len)) {
+		pw_pr_error("ERROR copying message from user space!\n");
+		vfree(local_msg);
+		return -EFAULT;
+	}
+	/*
+	 * We aren't allowed to config the driver multiple times between
+	 * collections. Clear out any previous config values.
+	 */
+	sw_destroy_collector_lists_i();
+
+	/*
+	 * Did the user specify a min polling interval?
+	 */
+	sw_min_polling_interval_msecs = local_msg->min_polling_interval_msecs;
+	pw_pr_debug("min_polling_interval_msecs = %u\n",
+		    sw_min_polling_interval_msecs);
+
+	num_infos = local_msg->num_infos;
+	pw_pr_debug("LOCAL NUM INFOS = %u\n", num_infos);
+	for (; num_infos > 0; --num_infos) {
+		local_info =
+			(struct sw_driver_interface_info *)&__data[dst_idx];
+		dst_idx += (SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+			    local_info->num_io_descriptors *
+				    sizeof(struct sw_driver_io_descriptor));
+		read_triggers = local_info->trigger_bits;
+		pw_pr_debug(
+			"read_triggers = %u, # msrs = %u, new dst_idx = %u\n",
+			(unsigned int)read_triggers,
+			(unsigned int)local_info->num_io_descriptors,
+			(unsigned int)dst_idx);
+		for (i = SW_WHEN_TYPE_BEGIN; i <= SW_WHEN_TYPE_END;
+		     ++i, read_triggers >>= 1) {
+			if (read_triggers & 0x1) { /* Bit 'i' is set */
+				pw_pr_debug("BIT %d is SET!\n", i);
+				if (i == SW_WHEN_TYPE_TRACEPOINT) {
+					struct tn_data tn_data = {
+						local_info,
+						local_info->tracepoint_id_mask
+					};
+					pw_pr_debug(
+						"TRACEPOINT, MASK = 0x%llx\n",
+						local_info->tracepoint_id_mask);
+					sw_for_each_tracepoint_node(
+					   &sw_add_trace_notifier_driver_info_i,
+					   &tn_data,
+					   false /*return-on-error*/);
+				} else if (i == SW_WHEN_TYPE_NOTIFIER) {
+					struct tn_data tn_data = {
+						local_info,
+						local_info->notifier_id_mask
+					};
+					pw_pr_debug(
+						"NOTIFIER, MASK = 0x%llx\n",
+						local_info->notifier_id_mask);
+					sw_for_each_notifier_node(
+					   &sw_add_trace_notifier_driver_info_i,
+					   &tn_data,
+					   false /*return-on-error*/);
+				} else {
+					if (sw_add_driver_info(
+						    &sw_collector_lists[i],
+						    local_info))
+						pw_pr_error(
+							"WARNING: could NOT add driver info to list for 'when type' %d!\n",
+							i);
+				}
+			}
+		}
+	}
+	if (sw_for_each_hw_op(&sw_post_config_i, NULL,
+			      false /*return-on-error*/))
+		pw_pr_error("POST-CONFIG error!\n");
+
+	vfree(local_msg);
+	memset(&s_internal_state, 0, sizeof(s_internal_state));
+	/*
+	 * DEBUG: iterate over collection lists.
+	 */
+	sw_iterate_driver_info_lists_i();
+	return PW_SUCCESS;
+}
+
+static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd,
+			    u64 __user *remote_out_args)
+{
+	/*
+	 * First, handle the command.
+	 */
+	if (cmd < SW_DRIVER_START_COLLECTION ||
+	    cmd > SW_DRIVER_CANCEL_COLLECTION) {
+		pw_pr_error("ERROR: invalid cmd = %d\n", cmd);
+		return -PW_ERROR;
+	}
+	switch (cmd) {
+	case SW_DRIVER_START_COLLECTION:
+		if (sw_collection_start_i())
+			return -PW_ERROR;
+
+		break;
+	case SW_DRIVER_STOP_COLLECTION:
+		if (sw_collection_stop_i())
+			return -PW_ERROR;
+
+		break;
+	default:
+		pw_pr_error("WARNING: unsupported command %d\n", cmd);
+		break;
+	}
+	/*
+	 * Then retrieve sample stats.
+	 */
+#if DO_COUNT_DROPPED_SAMPLES
+	if (cmd == SW_DRIVER_STOP_COLLECTION) {
+		u64 local_args[2] = { sw_num_samples_produced,
+				      sw_num_samples_dropped };
+		if (copy_to_user(remote_out_args, local_args,
+				 sizeof(local_args))) {
+			pw_pr_error(
+				"couldn't copy collection stats to user space!\n");
+			return -PW_ERROR;
+		}
+	}
+#endif /* DO_COUNT_DROPPED_SAMPLES */
+	return PW_SUCCESS;
+}
+
+#ifdef SFI_SIG_OEMB
+static int sw_do_parse_sfi_oemb_table(struct sfi_table_header *header)
+{
+#ifdef CONFIG_X86_WANT_INTEL_MID
+	struct sfi_table_oemb *oemb = (struct sfi_table_oemb *)
+		header; /* 'struct sfi_table_oemb' defined in 'intel-mid.h' */
+	if (!oemb) {
+		pw_pr_error("ERROR: NULL sfi table header!\n");
+		return -PW_ERROR;
+	}
+	sw_scu_fw_major_minor = (oemb->scu_runtime_major_version << 8) |
+				(oemb->scu_runtime_minor_version);
+	pw_pr_debug("DEBUG: major = %u, minor = %u\n",
+		    oemb->scu_runtime_major_version,
+		    oemb->scu_runtime_minor_version);
+#endif /* CONFIG_X86_WANT_INTEL_MID */
+	return PW_SUCCESS;
+}
+#endif /* SFI_SIG_OEMB */
+
+static void sw_do_extract_scu_fw_version(void)
+{
+	sw_scu_fw_major_minor = 0x0;
+#ifdef SFI_SIG_OEMB
+	if (sfi_table_parse(SFI_SIG_OEMB, NULL, NULL,
+			    &sw_do_parse_sfi_oemb_table))
+		pw_pr_force("WARNING: NO SFI information!\n");
+
+#endif /* SFI_SIG_OEMB */
+}
+
+static int sw_gather_trace_notifier_i(struct sw_trace_notifier_data *node,
+				      struct sw_name_info_msg *msg,
+				      enum sw_name_id_type type)
+{
+	pw_u16_t *idx = &msg->payload_len;
+	char *buffer = (char *)&msg->pairs[*idx];
+	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+	int id = sw_get_trace_notifier_id(node);
+	struct sw_string_type *str = &pair->name;
+	const char *abstract_name = sw_get_trace_notifier_abstract_name(node);
+
+	if (likely(abstract_name && id >= 0)) {
+		++msg->num_name_id_pairs;
+		pair->type = type;
+		pair->id = (u16)id;
+		/* "+1" for trailing '\0' */
+		str->len = strlen(abstract_name) + 1;
+		memcpy(&str->data[0], abstract_name, str->len);
+
+		pw_pr_debug("TP[%d] = %s (%u)\n",
+			    sw_get_trace_notifier_id(node), abstract_name,
+			    (unsigned int)strlen(abstract_name));
+
+		*idx += SW_NAME_ID_HEADER_SIZE() +
+			SW_STRING_TYPE_HEADER_SIZE() + str->len;
+	}
+
+	return PW_SUCCESS;
+}
+
+static int sw_gather_tracepoint_i(struct sw_trace_notifier_data *node,
+				  void *priv)
+{
+	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
+					  SW_NAME_TYPE_TRACEPOINT);
+}
+
+static int sw_gather_notifier_i(struct sw_trace_notifier_data *node, void *priv)
+{
+	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
+					  SW_NAME_TYPE_NOTIFIER);
+}
+
+static long
+sw_get_available_trace_notifiers_i(enum sw_name_id_type type,
+				   struct sw_name_info_msg *local_info)
+{
+	long retVal = PW_SUCCESS;
+
+	if (type == SW_NAME_TYPE_TRACEPOINT)
+		retVal = sw_for_each_tracepoint_node(&sw_gather_tracepoint_i,
+						     local_info,
+						     false /*return-on-error*/);
+	else
+		retVal = sw_for_each_notifier_node(&sw_gather_notifier_i,
+						   local_info,
+						   false /*return-on-error*/);
+
+	pw_pr_debug(
+		"There are %u extracted traces/notifiers for a total of %u bytes!\n",
+		local_info->num_name_id_pairs, local_info->payload_len);
+
+	return retVal;
+}
+
+static int sw_gather_hw_op_i(const struct sw_hw_ops *op, void *priv)
+{
+	struct sw_name_info_msg *msg = (struct sw_name_info_msg *)priv;
+	pw_u16_t *idx = &msg->payload_len;
+	char *buffer = (char *)&msg->pairs[*idx];
+	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+	struct sw_string_type *str = &pair->name;
+	const char *abstract_name = sw_get_hw_op_abstract_name(op);
+	int id = sw_get_hw_op_id(op);
+
+	pw_pr_debug("Gather Collector[%d] = %s\n", id, abstract_name);
+	if (likely(abstract_name && id >= 0)) {
+		/*
+		 * Final check: is this operation available on the
+		 * target platform? If 'available' function doesn't
+		 * exist then YES. Else call 'available'
+		 * function to decide.
+		 */
+		pw_pr_debug("%s has available = %p\n", abstract_name,
+			    op->available);
+		if (!op->available || (*op->available)()) {
+			++msg->num_name_id_pairs;
+			pair->type = SW_NAME_TYPE_COLLECTOR;
+			pair->id = (u16)id;
+			str->len = strlen(abstract_name) +
+				   1; /* "+1" for trailing '\0' */
+			memcpy(&str->data[0], abstract_name, str->len);
+
+			*idx += SW_NAME_ID_HEADER_SIZE() +
+				SW_STRING_TYPE_HEADER_SIZE() + str->len;
+		}
+	}
+
+	return PW_SUCCESS;
+}
+
+static long sw_get_available_collectors_i(struct sw_name_info_msg *local_info)
+{
+	return sw_for_each_hw_op(&sw_gather_hw_op_i, local_info,
+				 false /*return-on-error*/);
+}
+
+static long
+sw_get_available_name_id_mappings_i(enum sw_name_id_type type,
+				    struct sw_name_info_msg __user *remote_info,
+				    size_t local_len)
+{
+	char *buffer = vmalloc(local_len);
+	struct sw_name_info_msg *local_info = NULL;
+	long retVal = PW_SUCCESS;
+
+	if (!buffer) {
+		pw_pr_error("ERROR: couldn't alloc temp buffer!\n");
+		return -PW_ERROR;
+	}
+	memset(buffer, 0, local_len);
+	local_info = (struct sw_name_info_msg *)buffer;
+
+	if (type == SW_NAME_TYPE_COLLECTOR)
+		retVal = sw_get_available_collectors_i(local_info);
+	else
+		retVal = sw_get_available_trace_notifiers_i(type, local_info);
+
+	if (retVal == PW_SUCCESS) {
+		retVal = copy_to_user(remote_info, local_info, local_len);
+		if (retVal)
+			pw_pr_error(
+				"ERROR: couldn't copy tracepoint info to user space!\n");
+
+	}
+	vfree(buffer);
+	return retVal;
+}
+
+static long
+sw_get_topology_changes_i(struct sw_driver_topology_msg __user *remote_msg,
+			  size_t local_len)
+{
+	char *buffer = NULL;
+	struct sw_driver_topology_msg *local_msg = NULL;
+	size_t buffer_len = sizeof(struct sw_driver_topology_msg) +
+			    sw_num_topology_entries *
+				    sizeof(struct sw_driver_topology_change);
+	long retVal = PW_SUCCESS;
+	struct sw_driver_topology_change *dst = NULL;
+	size_t dst_idx = 0;
+
+	SW_LIST_HEAD_VAR(sw_topology_node) * head = (void *)&sw_topology_list;
+	struct sw_topology_node *tnode = NULL;
+
+	if (local_len < buffer_len) {
+		pw_pr_error(
+			"ERROR: insufficient buffer space to encode topology changes! Requires %zu, output space = %zu\n",
+			buffer_len, local_len);
+		return -EIO;
+	}
+
+	buffer = vmalloc(buffer_len);
+	if (!buffer) {
+		pw_pr_error(
+			"ERROR: couldn't allocate buffer for topology transfer!\n");
+		return -EIO;
+	}
+	memset(buffer, 0, buffer_len);
+
+	local_msg = (struct sw_driver_topology_msg *)buffer;
+	local_msg->num_entries = sw_num_topology_entries;
+	dst = (struct sw_driver_topology_change *)&local_msg
+		      ->topology_entries[0];
+	SW_LIST_FOR_EACH_ENTRY(tnode, head, list)
+	{
+		struct sw_driver_topology_change *change = &tnode->change;
+
+		memcpy(&dst[dst_idx++], change, sizeof(*change));
+	}
+	retVal = copy_to_user(remote_msg, local_msg, buffer_len);
+	if (retVal)
+		pw_pr_error(
+			"ERROR: couldn't copy topology changes to user space!\n");
+
+	vfree(buffer);
+	return retVal;
+}
+
+static long
+sw_get_cta_aggregators_i(struct _sw_aggregator_msg __user *remote_msg,
+			  size_t local_len)
+{
+	const struct _sw_aggregator_msg *_msg = sw_get_cta_aggregators();
+	long retval = copy_to_user(remote_msg, _msg, sizeof(*_msg));
+
+	return retval;
+}
+
+static long sw_read_continuous_i(char *remote_buffer, size_t local_len)
+{
+	/* TODO: call 'consume_buffer' directly? */
+	ssize_t val = sw_consume_data(0 /*mask, dummy*/, remote_buffer,
+					local_len);
+	if (val <= 0)
+		return val;
+
+	return 0;
+}
+
+static long sw_set_telem_cfgs_i(char *remote_cfg, size_t local_len)
+{
+	u64 *local_cfg = vmalloc(local_len);
+	int retval = 0;
+
+	if (!local_cfg) {
+		pw_pr_error("ERROR allocating space for local telem cfgs!\n");
+		return -EFAULT;
+	}
+	if (copy_from_user(local_cfg, remote_cfg, local_len)) {
+		pw_pr_error("ERROR copying message from user space!\n");
+		retval = -EFAULT;
+		goto done_set_telem_cfgs;
+	}
+	if (sw_setup_telem(local_cfg)) {
+		pw_pr_error("Couldn't setup telemetry\n");
+		retval = -1;
+	}
+done_set_telem_cfgs:
+	vfree(local_cfg);
+	return retval;
+}
+
+static long sw_set_continuous_i(
+	struct sw_driver_continuous_collect __user *remote_msg,
+	int local_len)
+{
+    pw_u32_t buffer_size = 0;
+	long ret = get_user(buffer_size, &remote_msg->collection_size);
+	if (ret)
+		return ret;
+
+	if (buffer_size == 0) {
+		pw_pr_error("Cannot allocate a zero length buffer!\n");
+		return -EINVAL;
+	}
+	ret = initialize_circular_buffer(buffer_size);
+	if (ret)
+		return ret;
+
+	ret = sw_set_driver_infos_i((struct sw_driver_interface_msg __user *)
+		remote_msg->payload, local_len);
+	if (ret) {
+		destroy_circular_buffer();
+		return ret;
+	}
+	return 0;
+}
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#define MATCH_IOCTL(num, pred) ((num) == (pred) || (num) == (pred##32))
+#else
+#define MATCH_IOCTL(num, pred) ((num) == (pred))
+#endif
+
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+					   void *p_local_args)
+{
+	struct sw_driver_ioctl_arg local_args;
+	int local_in_len, local_out_len;
+
+	if (!p_local_args) {
+		pw_pr_error("ERROR: NULL p_local_args value?!\n");
+		return -PW_ERROR;
+	}
+
+	/*
+	 * (1) Sanity check:
+	 * Before doing anything, double check to
+	 * make sure this IOCTL was really intended
+	 * for us!
+	 */
+	if (_IOC_TYPE(ioctl_num) != APWR_IOCTL_MAGIC_NUM) {
+		pw_pr_error(
+			"ERROR: requested IOCTL TYPE (%d) != \
+				APWR_IOCTL_MAGIC_NUM (%d)\n",
+			_IOC_TYPE(ioctl_num), APWR_IOCTL_MAGIC_NUM);
+		return -PW_ERROR;
+	}
+	/*
+	 * (2) Extract arg lengths.
+	 */
+	local_args = *((struct sw_driver_ioctl_arg *)p_local_args);
+
+	local_in_len = local_args.in_len;
+	local_out_len = local_args.out_len;
+	pw_pr_debug("GU: local_in_len = %d, local_out_len = %d\n", local_in_len,
+		local_out_len);
+	/*
+	 * (3) Service individual IOCTL requests.
+	 */
+	if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG)) {
+		pw_pr_debug("PW_IOCTL_CONFIG\n");
+		return sw_set_driver_infos_i(
+			(struct sw_driver_interface_msg __user *)
+				local_args.in_arg,
+			local_in_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG_CONTINUOUS)) {
+		pw_pr_debug("DEBUG: PW_IOCTL_CONFIG_CONTINUOUS\n");
+		return sw_set_continuous_i(
+			(struct sw_driver_continuous_collect __user *)
+				local_args.in_arg,
+			local_in_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CMD)) {
+		sw_driver_collection_cmd_t local_cmd;
+
+		pw_pr_debug("PW_IOCTL_CMD\n");
+	if (get_user(local_cmd,
+		(sw_driver_collection_cmd_t __user *)local_args.in_arg)) {
+		pw_pr_error("ERROR: could NOT extract cmd value!\n");
+		return -PW_ERROR;
+	}
+	return sw_handle_cmd_i(local_cmd, (u64 __user *)local_args.out_arg);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_POLL)) {
+		pw_pr_debug("PW_IOCTL_POLL\n");
+		return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_collection_poll_i);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_IMMEDIATE_IO)) {
+		struct sw_driver_interface_info *local_info;
+		struct sw_driver_io_descriptor *local_descriptor = NULL;
+		int retVal = PW_SUCCESS;
+		char *src_vals = NULL;
+		char *dst_vals = NULL;
+
+		pw_pr_debug("PW_IOCTL_IMMEDIATE_IO\n");
+		pw_pr_debug("local_in_len = %u\n", local_in_len);
+
+		src_vals = vmalloc(local_in_len);
+		if (!src_vals) {
+			pw_pr_error(
+				"ERROR allocating space for immediate IO\n");
+			return -PW_ERROR;
+		}
+		if (local_out_len) {
+			dst_vals = vmalloc(local_out_len);
+			if (!dst_vals) {
+				vfree(src_vals);
+				pw_pr_error(
+					"ERROR allocating space for immediate IO\n");
+				return -PW_ERROR;
+			}
+		}
+		if (copy_from_user(src_vals, (char __user *)local_args.in_arg,
+				   local_in_len)) {
+			pw_pr_error(
+				"ERROR copying in immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		local_info = (struct sw_driver_interface_info *)src_vals;
+		pw_pr_debug(
+			"OK, asked to perform immediate IO on cpu(s) %d, # descriptors = %d\n",
+			local_info->cpu_mask, local_info->num_io_descriptors);
+		/*
+		 * For now, require only a single descriptor.
+		 */
+		if (local_info->num_io_descriptors != 1) {
+			pw_pr_error(
+				"ERROR: told to perform immediate IO with %d descriptors -- MAX of 1 descriptor allowed!\n",
+				local_info->num_io_descriptors);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		local_descriptor = ((struct sw_driver_io_descriptor *)
+						local_info->descriptors);
+		pw_pr_debug("Collection type after %d\n",
+				local_descriptor->collection_type);
+		/*
+		 * Check cpu mask for correctness here. For now, we do NOT allow
+		 * reading on ALL cpus.
+		 */
+		if ((int)local_info->cpu_mask < -1 ||
+			(int)local_info->cpu_mask >= (int)sw_max_num_cpus) {
+			pw_pr_error(
+				"ERROR: invalid cpu mask %d specified in immediate IO; valid values are: -1, [0 -- %d]!\n",
+				local_info->cpu_mask, sw_max_num_cpus - 1);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Check collection type for correctness here
+		 */
+		pw_pr_debug(
+			"Asked to perform immediate IO with descriptor with type = %d, on cpu = %d\n",
+			local_descriptor->collection_type,
+			local_info->cpu_mask);
+		if (sw_is_valid_hw_op_id(local_descriptor->collection_type) ==
+			false) {
+			pw_pr_error(
+				"ERROR: invalid collection type %d specified for immediate IO\n",
+				(int)local_descriptor->collection_type);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Check collection cmd for correctness here
+		 */
+		if (local_descriptor->collection_command < SW_IO_CMD_READ ||
+			local_descriptor->collection_command > SW_IO_CMD_WRITE) {
+			pw_pr_error(
+				"ERROR: invalid collection command %d specified for immediate IO\n",
+				local_descriptor->collection_command);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Initialize the descriptor -- 'MMIO' and 'IPC' reads may need
+		 * an "ioremap_nocache"
+		 */
+		if (sw_init_driver_io_descriptor(local_descriptor)) {
+			pw_pr_error(
+				"ERROR initializing immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * OK, perform the actual IO.
+		 */
+		{
+			struct sw_immediate_request_info request_info = {
+				local_descriptor, dst_vals, &retVal
+			};
+			struct cpumask cpumask;
+
+			cpumask_clear(&cpumask);
+			switch (local_info->cpu_mask) {
+			case -1: /* IO on ANY CPU (assume current CPU) */
+				cpumask_set_cpu(RAW_CPU(), &cpumask);
+				pw_pr_debug("ANY CPU\n");
+				break;
+			default: /* IO on a particular CPU */
+				cpumask_set_cpu(local_info->cpu_mask, &cpumask);
+				pw_pr_debug("[%d] setting for %d\n", RAW_CPU(),
+						local_info->cpu_mask);
+				break;
+			}
+			sw_schedule_work(&cpumask,
+					 &sw_handle_immediate_request_i,
+					 &request_info);
+		}
+		if (retVal != PW_SUCCESS) {
+			pw_pr_error(
+				"ERROR performing immediate IO on one (or more) CPUs!\n");
+			goto ret_immediate_io_reset;
+		}
+		/*
+		 * OK, all done.
+		 */
+		if (local_descriptor->collection_command == SW_IO_CMD_READ) {
+			if (copy_to_user(local_args.out_arg, dst_vals,
+					 local_out_len)) {
+				pw_pr_error(
+					"ERROR copying %u bytes of value to userspace!\n",
+					local_out_len);
+				retVal = -PW_ERROR;
+				goto ret_immediate_io_reset;
+			}
+			pw_pr_debug(
+				"OK, copied %u bytes of value to userspace addr %p!\n",
+				local_out_len, local_args.out_arg);
+		}
+ret_immediate_io_reset:
+		/*
+		 * Reset the descriptor -- 'MMIO' and 'IPC' reads may have
+		 * performed an "ioremap_nocache" which now needs to be
+		 * unmapped.
+		 */
+		if (sw_reset_driver_io_descriptor(local_descriptor)) {
+			pw_pr_error(
+				"ERROR resetting immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+ret_immediate_io:
+		vfree(src_vals);
+		if (dst_vals)
+			vfree(dst_vals);
+
+		return retVal;
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_SCU_FW_VERSION)) {
+		u32 local_data = (u32)sw_scu_fw_major_minor;
+
+		if (put_user(local_data, (u32 __user *)local_args.out_arg)) {
+			pw_pr_error(
+				"ERROR copying scu fw version to userspace!\n"
+				);
+			return -PW_ERROR;
+		}
+		return PW_SUCCESS;
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_DRIVER_VERSION)) {
+		pw_u64_t local_version =
+			(pw_u64_t)SW_DRIVER_VERSION_MAJOR << 32 |
+			(pw_u64_t)SW_DRIVER_VERSION_MINOR << 16 |
+			(pw_u64_t)SW_DRIVER_VERSION_OTHER;
+		if (put_user(local_version,
+			(u64 __user *)local_args.out_arg)) {
+			pw_pr_error(
+				"ERROR copying driver version to userspace!\n"
+				);
+			return -PW_ERROR;
+		}
+		return PW_SUCCESS;
+	} else if (MATCH_IOCTL(ioctl_num,
+			PW_IOCTL_GET_AVAILABLE_TRACEPOINTS)) {
+		pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n",
+			local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_TRACEPOINT,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_NOTIFIERS)) {
+		pw_pr_debug("DEBUG: AVAIL notifiers! local_out_len = %u\n",
+			local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_NOTIFIER,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_COLLECTORS)) {
+		pw_pr_debug("DEBUG: AVAIL collectors! local_out_len = %u\n",
+			local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_COLLECTOR,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_TOPOLOGY_CHANGES)) {
+		pw_pr_debug("DEBUG: TOPOLOGY changes! local_out_len = %u\n",
+			local_out_len);
+		return sw_get_topology_changes_i(
+			(struct sw_driver_topology_msg __user *)
+				local_args.out_arg, local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_READ_CONTINUOUS)) {
+		pw_pr_debug("DEBUG: READ_CONTINUOUS!\n");
+		return sw_read_continuous_i(local_args.out_arg, local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_SET_TELEM_BAR)) {
+		pw_pr_debug("DEBUG: got a request to set telem bar!\n");
+		return sw_set_telem_cfgs_i(local_args.in_arg, local_in_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_AVAIL_CTA_AGGREGATORS)) {
+		pw_pr_debug("DEBUG: retrieve CTA aggregator list\n");
+		return sw_get_cta_aggregators_i(
+				(struct _sw_aggregator_msg __user *)local_args.out_arg,
+				local_out_len);
+	}
+
+	pw_pr_error("ERROR: invalid ioctl num: %u\n", _IOC_NR(ioctl_num));
+	return -PW_ERROR;
+}
+
+static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void)
+{
+	return s_internal_state.cmd;
+};
+
+static bool sw_should_flush_buffer_i(void)
+{
+	return s_internal_state.drain_buffers;
+};
+
+int sw_load_driver_i(void)
+{
+	/*
+	 * Set per-cpu buffer size.
+	 * First, Perform sanity checking of per-cpu buffer size.
+	 */
+	/*
+	 * 1. Num pages MUST be pow-of-2.
+	 */
+	{
+		if (sw_buffer_num_pages & (sw_buffer_num_pages - 1)) {
+			pw_pr_error(
+				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be a power of 2!\n",
+				sw_buffer_num_pages);
+			return -PW_ERROR;
+		}
+	}
+	/*
+	 * 2. Num pages MUST be <= 16 (i.e. per-cpu buffer size
+	 * MUST be <= 64 kB)
+	 */
+	{
+		if (sw_buffer_num_pages > 16) {
+			pw_pr_error(
+				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be <= 16!\n",
+				sw_buffer_num_pages);
+			return -PW_ERROR;
+		}
+	}
+	sw_buffer_alloc_size = sw_buffer_num_pages * PAGE_SIZE;
+	/*
+	 * Retrieve any arch details here.
+	 */
+	if (sw_get_arch_details_i()) {
+		pw_pr_error("ERROR retrieving arch details!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Check to see if the user wants us to force
+	 * software coordination of CPU frequencies.
+	 */
+	if (do_force_module_scope_for_cpu_frequencies) {
+		pw_pr_force(
+			"DEBUG: FORCING MODULE SCOPE FOR CPU FREQUENCIES!\n");
+		if (sw_set_module_scope_for_cpus()) {
+			pw_pr_force("ERROR setting affected cpus\n");
+			return -PW_ERROR;
+		}
+		pw_pr_debug("OK, setting worked\n");
+	}
+	if (sw_init_data_structures_i()) {
+		pw_pr_error("ERROR initializing data structures!\n");
+		goto err_ret_init_data;
+	}
+	if (sw_register_dev(&s_ops))
+		goto err_ret_register_dev;
+	/*
+	 * Retrieve a list of tracepoint structs to use when
+	 * registering probe functions.
+	 */
+	{
+		if (sw_extract_tracepoints()) {
+			pw_pr_error(
+				"ERROR: could NOT retrieve a complete list of valid tracepoint structs!\n");
+			goto err_ret_tracepoint;
+		}
+	}
+	pw_pr_force("-----------------------------------------\n");
+	pw_pr_force("OK: LOADED SoC Watch Driver\n");
+#ifdef CONFIG_X86_WANT_INTEL_MID
+	pw_pr_force("SOC Identifier = %u, Stepping = %u\n",
+			intel_mid_identify_cpu(), intel_mid_soc_stepping());
+#endif /* CONFIG_X86_WANT_INTEL_MID */
+	pw_pr_force("-----------------------------------------\n");
+	return PW_SUCCESS;
+
+err_ret_tracepoint:
+	sw_unregister_dev();
+err_ret_register_dev:
+	sw_destroy_data_structures_i();
+err_ret_init_data:
+	if (do_force_module_scope_for_cpu_frequencies) {
+		if (sw_reset_module_scope_for_cpus())
+			pw_pr_force("ERROR resetting affected cpus\n");
+		else
+			pw_pr_debug("OK, resetting worked\n");
+	}
+	return -PW_ERROR;
+}
+
+void sw_unload_driver_i(void)
+{
+	sw_iterate_driver_info_lists_i();
+
+	sw_unregister_dev();
+
+	sw_destroy_data_structures_i();
+
+	if (do_force_module_scope_for_cpu_frequencies) {
+		if (sw_reset_module_scope_for_cpus())
+			pw_pr_force("ERROR resetting affected cpus\n");
+		else
+			pw_pr_debug("OK, resetting worked\n");
+	}
+
+	pw_pr_force("-----------------------------------------\n");
+	pw_pr_force("OK: UNLOADED SoC Watch Driver\n");
+
+	sw_print_trace_notifier_overheads();
+	sw_print_output_buffer_overheads();
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_collection_poll_i, "POLL");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_any_seg_full, "ANY_SEG_FULL");
+#if DO_TRACK_MEMORY_USAGE
+	/*
+	 * Dump memory stats.
+	 */
+	pw_pr_force(
+		"TOTAL # BYTES ALLOCED = %llu, CURR # BYTES ALLOCED = %llu, MAX # BYTES ALLOCED = %llu\n",
+		sw_get_total_bytes_alloced(),
+		sw_get_curr_bytes_alloced(),
+		sw_get_max_bytes_alloced());
+	if (unlikely(sw_get_curr_bytes_alloced())) {
+		pw_pr_force(
+			"***********************************************************************\n");
+		pw_pr_force(
+			"WARNING: possible memory leak: there are %llu bytes still allocated!\n",
+			sw_get_curr_bytes_alloced());
+		pw_pr_force(
+			"***********************************************************************\n");
+	}
+#endif /* DO_TRACK_MEMORY_USAGE */
+	pw_pr_force("-----------------------------------------\n");
+}
+
+module_init(sw_load_driver_i);
+module_exit(sw_unload_driver_i);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR(MOD_AUTHOR);
+MODULE_DESCRIPTION(MOD_DESC);
diff --git a/drivers/platform/x86/socwatch/sw_file_ops.c b/drivers/platform/x86/socwatch/sw_file_ops.c
index 199ae560801e..504d851db539 100644
--- a/drivers/platform/x86/socwatch/sw_file_ops.c
+++ b/drivers/platform/x86/socwatch/sw_file_ops.c
@@ -1,337 +1,337 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/module.h>  /* try_module_get */
-#include <linux/fs.h>      /* inode */
-#include <linux/device.h>  /* class_create */
-#include <linux/cdev.h>    /* cdev_alloc */
-#include <linux/version.h> /* LINUX_VERSION_CODE */
-#if KERNEL_VERSION(4, 12, 0) > LINUX_VERSION_CODE
-    #include <asm/uaccess.h>   /* copy_to_user */
-#else
-    #include <linux/uaccess.h>   /* copy_to_user */
-#endif /* LINUX_VERSION_CODE */
-#include <linux/wait.h>    /* wait_event_interruptible */
-#include <linux/sched.h>   /* TASK_INTERRUPTIBLE */
-
-#include "sw_kernel_defines.h"
-#include "sw_types.h"
-#include "sw_structs.h"
-#include "sw_file_ops.h"
-#include "sw_ioctl.h"
-#include "sw_output_buffer.h"
-
-/* -------------------------------------------------
- * Compile time constants.
- * -------------------------------------------------
- */
-/*
- * Get current command.
- */
-#define GET_CMD() ((*s_file_ops->get_current_cmd)())
-/*
- * Check if we're currently collecting data.
- */
-#define IS_COLLECTING() ({					\
-	sw_driver_collection_cmd_t __cmd = GET_CMD();		\
-	bool __val = (__cmd == SW_DRIVER_START_COLLECTION ||	\
-			__cmd == SW_DRIVER_RESUME_COLLECTION);	\
-	__val; })
-
-/*
- * Check if we're currently paused.
- */
-#define IS_SLEEPING() ({					\
-	sw_driver_collection_cmd_t __cmd = GET_CMD();		\
-	bool __val = __cmd == SW_DRIVER_PAUSE_COLLECTION;	\
-	__val; })
-
-/*
- * Character device file MAJOR
- * number -- we're now obtaining
- * this dynamically.
- */
-static int apwr_dev_major_num = -1;
-/*
- * Variables to create the character device file
- */
-static dev_t apwr_dev;
-static struct cdev *apwr_cdev;
-static struct class *apwr_class;
-/*
- * Operations exported by the main driver.
- */
-static struct sw_file_ops *s_file_ops;
-/*
- * Is the device open right now? Used to prevent
- * concurent access into the same device.
- */
-#define DEV_IS_OPEN 0 /* see if device is in use */
-static volatile unsigned long dev_status;
-
-/*
- * File operations.
- */
-/*
- * Service an "open(...)" call from user-space.
- */
-static int sw_device_open_i(struct inode *inode, struct file *file)
-{
-	/*
-	 * We don't want to talk to two processes at the same time
-	 */
-	if (test_and_set_bit(DEV_IS_OPEN, &dev_status))
-		return -EBUSY; /* Device is busy */
-
-
-	if (!try_module_get(THIS_MODULE)) {
-		pw_pr_error("ERROR: Device not found!\n");
-		return -ENODEV;/* No such device */
-	}
-	pw_pr_debug("OK, allowed client open!\n");
-	return PW_SUCCESS;
-}
-
-/*
- * Service a "close(...)" call from user-space.
- */
-static int sw_device_release_i(struct inode *inode, struct file *file)
-{
-	/*
-	 * Did the client just try to zombie us?
-	 */
-	int retVal = PW_SUCCESS;
-
-	if (IS_COLLECTING()) {
-		pw_pr_error(
-			"ERROR: Detected ongoing collection on a device release!\n");
-		retVal = (*s_file_ops->stop_handler)();
-	}
-	module_put(THIS_MODULE);
-	/*
-	 * We're now ready for our next caller
-	 */
-	clear_bit(DEV_IS_OPEN, &dev_status);
-	return retVal;
-}
-
-static ssize_t sw_device_read_i(struct file *file, char __user *user_buffer,
-	size_t length, loff_t *offset)
-{
-	ssize_t bytes_read = 0;
-	u32 val = 0;
-
-	if (!user_buffer) {
-		pw_pr_error(
-			"ERROR: \"read\" called with an empty user_buffer?!\n");
-		return -PW_ERROR;
-	}
-	do {
-		val = SW_ALL_WRITES_DONE_MASK;
-		if (wait_event_interruptible(sw_reader_queue,
-			(sw_any_seg_full(&val, (*s_file_ops->should_flush)()) ||
-				 (!IS_COLLECTING() && !IS_SLEEPING())))) {
-			pw_pr_error("wait_event_interruptible error\n");
-			return -ERESTARTSYS;
-		}
-		pw_pr_debug("After wait: val = %u\n", val);
-	} while (val == SW_NO_DATA_AVAIL_MASK);
-	/*
-	 * Are we done producing/consuming?
-	 */
-	if (val == SW_ALL_WRITES_DONE_MASK)
-		return 0; /* "0" ==> EOF */
-
-	/*
-	 * Copy the buffer contents into userspace.
-	 */
-	/* 'read' returns # of bytes actually read */
-	bytes_read = sw_consume_data(val, user_buffer, length);
-	if (unlikely(bytes_read <= 0)) {
-		/* Cannot be EOF since that has already been checked above */
-		return -EIO;
-	}
-	return bytes_read;
-}
-
-/*
- * (1) Handle 32b IOCTLs in 32b kernel-space.
- * (2) Handle 64b IOCTLs in 64b kernel-space.
- */
-static long sw_device_unlocked_ioctl_i(
-	struct file *filp, unsigned int ioctl_num, unsigned long ioctl_param)
-{
-	struct sw_driver_ioctl_arg __user *remote_args =
-			(struct sw_driver_ioctl_arg __user *)ioctl_param;
-	struct sw_driver_ioctl_arg local_args;
-
-	if (copy_from_user(&local_args, remote_args, sizeof(local_args))) {
-		pw_pr_error("ERROR copying ioctl args from userspace\n");
-		return -PW_ERROR;
-	}
-	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
-};
-
-#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
-#include <linux/compat.h>
-/*
- * Helper struct for use in translating
- * IOCTLs from 32b user programs in 64b
- * kernels.
- */
-#pragma pack(push, 1)
-struct sw_driver_ioctl_arg32 {
-	pw_s32_t in_len;
-	pw_s32_t out_len;
-	compat_caddr_t in_arg;
-	compat_caddr_t out_arg;
-};
-#pragma pack(pop)
-
-/*
- * Handle 32b IOCTLs in 64b kernel-space.
- */
-static long sw_device_compat_ioctl_i(
-	struct file *file, unsigned int ioctl_num, unsigned long ioctl_param)
-{
-	struct sw_driver_ioctl_arg32 __user *remote_args32 =
-						compat_ptr(ioctl_param);
-	struct sw_driver_ioctl_arg local_args;
-	u32 data;
-
-	if (get_user(local_args.in_len, &remote_args32->in_len))
-		return -PW_ERROR;
-
-	if (get_user(local_args.out_len, &remote_args32->out_len))
-		return -PW_ERROR;
-
-	if (get_user(data, &remote_args32->in_arg))
-		return -PW_ERROR;
-
-	local_args.in_arg = (char *)(unsigned long)data;
-	if (get_user(data, &remote_args32->out_arg))
-		return -PW_ERROR;
-
-	local_args.out_arg = (char *)(unsigned long)data;
-	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
-}
-#endif
-
-/*
- * File operations exported by the driver.
- */
-static const struct file_operations s_fops = {
-	.open = &sw_device_open_i,
-	.read = &sw_device_read_i,
-	.unlocked_ioctl = &sw_device_unlocked_ioctl_i,
-#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
-	.compat_ioctl = &sw_device_compat_ioctl_i,
-#endif /* COMPAT && x64 */
-	.release = &sw_device_release_i,
-};
-
-
-/*
- * Device creation, deletion operations.
- */
-int sw_register_dev(struct sw_file_ops *ops)
-{
-	int ret;
-	/*
-	 * Ensure we have valid handlers!
-	 */
-	if (!ops) {
-		pw_pr_error("NULL file ops?!\n");
-		return -PW_ERROR;
-	}
-
-	/*
-	 * Create the character device
-	 */
-	ret = alloc_chrdev_region(&apwr_dev, 0, 1, PW_DEVICE_NAME);
-	apwr_dev_major_num = MAJOR(apwr_dev);
-	apwr_class = class_create(THIS_MODULE, "apwr");
-	if (IS_ERR(apwr_class))
-		pw_pr_error("Error registering apwr class\n");
-
-
-	device_create(apwr_class, NULL, apwr_dev, NULL, PW_DEVICE_NAME);
-	apwr_cdev = cdev_alloc();
-	if (apwr_cdev == NULL) {
-		pw_pr_error("Error allocating character device\n");
-		return ret;
-	}
-	apwr_cdev->owner = THIS_MODULE;
-	apwr_cdev->ops = &s_fops;
-	if (cdev_add(apwr_cdev, apwr_dev, 1) < 0)  {
-		pw_pr_error("Error registering device driver\n");
-		return ret;
-	}
-	s_file_ops = ops;
-
-	return ret;
-}
-
-void sw_unregister_dev(void)
-{
-	/*
-	 * Remove the device
-	 */
-	unregister_chrdev(apwr_dev_major_num, PW_DEVICE_NAME);
-	device_destroy(apwr_class, apwr_dev);
-	class_destroy(apwr_class);
-	unregister_chrdev_region(apwr_dev, 1);
-	cdev_del(apwr_cdev);
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/module.h>  /* try_module_get */
+#include <linux/fs.h>      /* inode */
+#include <linux/device.h>  /* class_create */
+#include <linux/cdev.h>    /* cdev_alloc */
+#include <linux/version.h> /* LINUX_VERSION_CODE */
+#if KERNEL_VERSION(4, 12, 0) > LINUX_VERSION_CODE
+    #include <asm/uaccess.h>   /* copy_to_user */
+#else
+    #include <linux/uaccess.h>   /* copy_to_user */
+#endif /* LINUX_VERSION_CODE */
+#include <linux/wait.h>    /* wait_event_interruptible */
+#include <linux/sched.h>   /* TASK_INTERRUPTIBLE */
+
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_structs.h"
+#include "sw_file_ops.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Get current command.
+ */
+#define GET_CMD() ((*s_file_ops->get_current_cmd)())
+/*
+ * Check if we're currently collecting data.
+ */
+#define IS_COLLECTING() ({					\
+	sw_driver_collection_cmd_t __cmd = GET_CMD();		\
+	bool __val = (__cmd == SW_DRIVER_START_COLLECTION ||	\
+			__cmd == SW_DRIVER_RESUME_COLLECTION);	\
+	__val; })
+
+/*
+ * Check if we're currently paused.
+ */
+#define IS_SLEEPING() ({					\
+	sw_driver_collection_cmd_t __cmd = GET_CMD();		\
+	bool __val = __cmd == SW_DRIVER_PAUSE_COLLECTION;	\
+	__val; })
+
+/*
+ * Character device file MAJOR
+ * number -- we're now obtaining
+ * this dynamically.
+ */
+static int apwr_dev_major_num = -1;
+/*
+ * Variables to create the character device file
+ */
+static dev_t apwr_dev;
+static struct cdev *apwr_cdev;
+static struct class *apwr_class;
+/*
+ * Operations exported by the main driver.
+ */
+static struct sw_file_ops *s_file_ops;
+/*
+ * Is the device open right now? Used to prevent
+ * concurent access into the same device.
+ */
+#define DEV_IS_OPEN 0 /* see if device is in use */
+static volatile unsigned long dev_status;
+
+/*
+ * File operations.
+ */
+/*
+ * Service an "open(...)" call from user-space.
+ */
+static int sw_device_open_i(struct inode *inode, struct file *file)
+{
+	/*
+	 * We don't want to talk to two processes at the same time
+	 */
+	if (test_and_set_bit(DEV_IS_OPEN, &dev_status))
+		return -EBUSY; /* Device is busy */
+
+
+	if (!try_module_get(THIS_MODULE)) {
+		pw_pr_error("ERROR: Device not found!\n");
+		return -ENODEV;/* No such device */
+	}
+	pw_pr_debug("OK, allowed client open!\n");
+	return PW_SUCCESS;
+}
+
+/*
+ * Service a "close(...)" call from user-space.
+ */
+static int sw_device_release_i(struct inode *inode, struct file *file)
+{
+	/*
+	 * Did the client just try to zombie us?
+	 */
+	int retVal = PW_SUCCESS;
+
+	if (IS_COLLECTING()) {
+		pw_pr_error(
+			"ERROR: Detected ongoing collection on a device release!\n");
+		retVal = (*s_file_ops->stop_handler)();
+	}
+	module_put(THIS_MODULE);
+	/*
+	 * We're now ready for our next caller
+	 */
+	clear_bit(DEV_IS_OPEN, &dev_status);
+	return retVal;
+}
+
+static ssize_t sw_device_read_i(struct file *file, char __user *user_buffer,
+	size_t length, loff_t *offset)
+{
+	ssize_t bytes_read = 0;
+	u32 val = 0;
+
+	if (!user_buffer) {
+		pw_pr_error(
+			"ERROR: \"read\" called with an empty user_buffer?!\n");
+		return -PW_ERROR;
+	}
+	do {
+		val = SW_ALL_WRITES_DONE_MASK;
+		if (wait_event_interruptible(sw_reader_queue,
+			(sw_any_seg_full(&val, (*s_file_ops->should_flush)()) ||
+				 (!IS_COLLECTING() && !IS_SLEEPING())))) {
+			pw_pr_error("wait_event_interruptible error\n");
+			return -ERESTARTSYS;
+		}
+		pw_pr_debug("After wait: val = %u\n", val);
+	} while (val == SW_NO_DATA_AVAIL_MASK);
+	/*
+	 * Are we done producing/consuming?
+	 */
+	if (val == SW_ALL_WRITES_DONE_MASK)
+		return 0; /* "0" ==> EOF */
+
+	/*
+	 * Copy the buffer contents into userspace.
+	 */
+	/* 'read' returns # of bytes actually read */
+	bytes_read = sw_consume_data(val, user_buffer, length);
+	if (unlikely(bytes_read <= 0)) {
+		/* Cannot be EOF since that has already been checked above */
+		return -EIO;
+	}
+	return bytes_read;
+}
+
+/*
+ * (1) Handle 32b IOCTLs in 32b kernel-space.
+ * (2) Handle 64b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_unlocked_ioctl_i(
+	struct file *filp, unsigned int ioctl_num, unsigned long ioctl_param)
+{
+	struct sw_driver_ioctl_arg __user *remote_args =
+			(struct sw_driver_ioctl_arg __user *)ioctl_param;
+	struct sw_driver_ioctl_arg local_args;
+
+	if (copy_from_user(&local_args, remote_args, sizeof(local_args))) {
+		pw_pr_error("ERROR copying ioctl args from userspace\n");
+		return -PW_ERROR;
+	}
+	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+};
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+/*
+ * Helper struct for use in translating
+ * IOCTLs from 32b user programs in 64b
+ * kernels.
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg32 {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	compat_caddr_t in_arg;
+	compat_caddr_t out_arg;
+};
+#pragma pack(pop)
+
+/*
+ * Handle 32b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_compat_ioctl_i(
+	struct file *file, unsigned int ioctl_num, unsigned long ioctl_param)
+{
+	struct sw_driver_ioctl_arg32 __user *remote_args32 =
+						compat_ptr(ioctl_param);
+	struct sw_driver_ioctl_arg local_args;
+	u32 data;
+
+	if (get_user(local_args.in_len, &remote_args32->in_len))
+		return -PW_ERROR;
+
+	if (get_user(local_args.out_len, &remote_args32->out_len))
+		return -PW_ERROR;
+
+	if (get_user(data, &remote_args32->in_arg))
+		return -PW_ERROR;
+
+	local_args.in_arg = (char *)(unsigned long)data;
+	if (get_user(data, &remote_args32->out_arg))
+		return -PW_ERROR;
+
+	local_args.out_arg = (char *)(unsigned long)data;
+	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+}
+#endif
+
+/*
+ * File operations exported by the driver.
+ */
+static const struct file_operations s_fops = {
+	.open = &sw_device_open_i,
+	.read = &sw_device_read_i,
+	.unlocked_ioctl = &sw_device_unlocked_ioctl_i,
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+	.compat_ioctl = &sw_device_compat_ioctl_i,
+#endif /* COMPAT && x64 */
+	.release = &sw_device_release_i,
+};
+
+
+/*
+ * Device creation, deletion operations.
+ */
+int sw_register_dev(struct sw_file_ops *ops)
+{
+	int ret;
+	/*
+	 * Ensure we have valid handlers!
+	 */
+	if (!ops) {
+		pw_pr_error("NULL file ops?!\n");
+		return -PW_ERROR;
+	}
+
+	/*
+	 * Create the character device
+	 */
+	ret = alloc_chrdev_region(&apwr_dev, 0, 1, PW_DEVICE_NAME);
+	apwr_dev_major_num = MAJOR(apwr_dev);
+	apwr_class = class_create(THIS_MODULE, "apwr");
+	if (IS_ERR(apwr_class))
+		pw_pr_error("Error registering apwr class\n");
+
+
+	device_create(apwr_class, NULL, apwr_dev, NULL, PW_DEVICE_NAME);
+	apwr_cdev = cdev_alloc();
+	if (apwr_cdev == NULL) {
+		pw_pr_error("Error allocating character device\n");
+		return ret;
+	}
+	apwr_cdev->owner = THIS_MODULE;
+	apwr_cdev->ops = &s_fops;
+	if (cdev_add(apwr_cdev, apwr_dev, 1) < 0)  {
+		pw_pr_error("Error registering device driver\n");
+		return ret;
+	}
+	s_file_ops = ops;
+
+	return ret;
+}
+
+void sw_unregister_dev(void)
+{
+	/*
+	 * Remove the device
+	 */
+	unregister_chrdev(apwr_dev_major_num, PW_DEVICE_NAME);
+	device_destroy(apwr_class, apwr_dev);
+	class_destroy(apwr_class);
+	unregister_chrdev_region(apwr_dev, 1);
+	cdev_del(apwr_cdev);
+}
diff --git a/drivers/platform/x86/socwatch/sw_hardware_io.c b/drivers/platform/x86/socwatch/sw_hardware_io.c
index ce5d05692165..44891333088d 100644
--- a/drivers/platform/x86/socwatch/sw_hardware_io.c
+++ b/drivers/platform/x86/socwatch/sw_hardware_io.c
@@ -1,182 +1,186 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "sw_types.h"
-#include "sw_kernel_defines.h"
-#include "sw_ops_provider.h"
-#include "sw_mem.h"
-#include "sw_internal.h"
-#include "sw_hardware_io.h"
-
-
-struct sw_ops_node {
-	const struct sw_hw_ops *op;
-	int id;
-
-	SW_LIST_ENTRY(list, sw_ops_node);
-};
-
-static SW_DEFINE_LIST_HEAD(s_ops, sw_ops_node) =
-			SW_LIST_HEAD_INITIALIZER(s_ops);
-
-static int s_op_idx = -1;
-
-/*
- * Function definitions.
- */
-int sw_get_hw_op_id(const struct sw_hw_ops *ops)
-{
-	if (ops && ops->name) {
-		struct sw_ops_node *node = NULL;
-
-		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
-			if (node->op->name &&
-				!strcmp(node->op->name, ops->name))
-				return node->id;
-		}
-	}
-	return -1;
-}
-
-const struct sw_hw_ops *sw_get_hw_ops_for(int id)
-{
-	struct sw_ops_node *node = NULL;
-
-	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
-		if (node->id == id)
-			return node->op;
-	}
-	return NULL;
-}
-
-bool sw_is_valid_hw_op_id(int id)
-{
-	struct sw_ops_node *node = NULL;
-
-	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
-		if (node->id == id)
-			return true;
-	}
-	return false;
-}
-
-const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op)
-{
-	if (op)
-		return op->name;
-
-	return NULL;
-}
-
-int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
-	void *priv, bool return_on_error)
-{
-	int retval = PW_SUCCESS;
-	struct sw_ops_node *node = NULL;
-
-	if (func) {
-		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
-			if ((*func)(node->op, priv)) {
-				retval = -EIO;
-				if (return_on_error)
-					break;
-			}
-		}
-	}
-	return retval;
-}
-
-int sw_register_hw_op(const struct sw_hw_ops *op)
-{
-	struct sw_ops_node *node = NULL;
-
-	if (!op) {
-		pw_pr_error("NULL input node in \"%s\"", __func__);
-		return -EIO;
-	}
-	node = sw_kmalloc(sizeof(struct sw_ops_node), GFP_KERNEL);
-	if (!node) {
-		pw_pr_error("sw_kmalloc error in \"%s\"", __func__);
-		return -ENOMEM;
-	}
-	node->op = op;
-	node->id = ++s_op_idx;
-	SW_LIST_ENTRY_INIT(node, list);
-	SW_LIST_ADD(&s_ops, node, list);
-	return PW_SUCCESS;
-}
-
-int sw_register_hw_ops(void)
-{
-	return sw_register_ops_providers();
-}
-
-void sw_free_hw_ops(void)
-{
-	/*
-	 * Free all nodes.
-	 */
-	while (!SW_LIST_EMPTY(&s_ops)) {
-		struct sw_ops_node *node =
-			SW_LIST_GET_HEAD_ENTRY(&s_ops, sw_ops_node, list);
-
-		SW_LIST_UNLINK(node, list);
-		sw_kfree(node);
-	}
-	/*
-	 * Call our providers to deallocate resources.
-	 */
-	sw_free_ops_providers();
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "sw_types.h"
+#include "sw_kernel_defines.h"
+#include "sw_ops_provider.h"
+#include "sw_mem.h"
+#include "sw_internal.h"
+#include "sw_hardware_io.h"
+
+
+struct sw_ops_node {
+	const struct sw_hw_ops *op;
+	int id;
+
+	SW_LIST_ENTRY(list, sw_ops_node);
+};
+
+static SW_DEFINE_LIST_HEAD(s_ops, sw_ops_node) =
+			SW_LIST_HEAD_INITIALIZER(s_ops);
+
+static int s_op_idx = -1;
+
+/*
+ * Function definitions.
+ */
+int sw_get_hw_op_id(const struct sw_hw_ops *ops)
+{
+	if (ops && ops->name) {
+		struct sw_ops_node *node = NULL;
+
+		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+			if (node->op->name &&
+				!strcmp(node->op->name, ops->name))
+				return node->id;
+		}
+	}
+	return -1;
+}
+
+const struct sw_hw_ops *sw_get_hw_ops_for(int id)
+{
+	struct sw_ops_node *node = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+		if (node->id == id)
+			return node->op;
+	}
+	return NULL;
+}
+
+bool sw_is_valid_hw_op_id(int id)
+{
+	struct sw_ops_node *node = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+		if (node->id == id)
+			return true;
+	}
+	return false;
+}
+
+const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op)
+{
+	if (op)
+		return op->name;
+
+	return NULL;
+}
+
+int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
+	void *priv, bool return_on_error)
+{
+	int retval = PW_SUCCESS;
+	struct sw_ops_node *node = NULL;
+
+	if (func) {
+		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list) {
+			if ((*func)(node->op, priv)) {
+				retval = -EIO;
+				if (return_on_error)
+					break;
+			}
+		}
+	}
+	return retval;
+}
+
+int sw_register_hw_op(const struct sw_hw_ops *op)
+{
+	struct sw_ops_node *node = NULL;
+
+	if (!op) {
+		pw_pr_error("NULL input node in \"%s\"", __func__);
+		return -EIO;
+	}
+	node = sw_kmalloc(sizeof(struct sw_ops_node), GFP_KERNEL);
+	if (!node) {
+		pw_pr_error("sw_kmalloc error in \"%s\"", __func__);
+		return -ENOMEM;
+	}
+	node->op = op;
+	node->id = ++s_op_idx;
+	SW_LIST_ENTRY_INIT(node, list);
+	SW_LIST_ADD(&s_ops, node, list);
+	/* Call this op's 'register' function */
+	if (op->reg) {
+		(*op->reg)(); /* return value is don't care */
+	}
+	return PW_SUCCESS;
+}
+
+int sw_register_hw_ops(void)
+{
+	return sw_register_ops_providers();
+}
+
+void sw_free_hw_ops(void)
+{
+	/*
+	 * Free all nodes.
+	 */
+	while (!SW_LIST_EMPTY(&s_ops)) {
+		struct sw_ops_node *node =
+			SW_LIST_GET_HEAD_ENTRY(&s_ops, sw_ops_node, list);
+
+		SW_LIST_UNLINK(node, list);
+		sw_kfree(node);
+	}
+	/*
+	 * Call our providers to deallocate resources.
+	 */
+	sw_free_ops_providers();
+}
diff --git a/drivers/platform/x86/socwatch/sw_internal.c b/drivers/platform/x86/socwatch/sw_internal.c
index 8ad36a989fd2..3334200e958d 100644
--- a/drivers/platform/x86/socwatch/sw_internal.c
+++ b/drivers/platform/x86/socwatch/sw_internal.c
@@ -1,247 +1,253 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "sw_hardware_io.h"
-#include "sw_mem.h"
-#include "sw_kernel_defines.h"
-#include "sw_telem.h"
-#include "sw_internal.h"
-
-bool sw_check_output_buffer_params(
-	void __user *buffer, size_t bytes_to_read, size_t buff_size)
-{
-	if (!buffer) {
-		pw_pr_error("ERROR: NULL ptr in sw_consume_data!\n");
-		return false;
-	}
-	if (bytes_to_read != buff_size) {
-		pw_pr_error("Error: bytes_to_read = %zu, required to be %zu\n",
-			bytes_to_read, buff_size);
-		return false;
-	}
-	return true;
-}
-
-unsigned long sw_copy_to_user(char __user *dst, char *src, size_t bytes_to_copy)
-{
-	return copy_to_user(dst, src, bytes_to_copy);
-}
-
-void sw_schedule_work(
-	const struct cpumask *mask, void (*work)(void *), void *data)
-{
-	/*
-	 * Did the user ask us to run on 'ANY' CPU?
-	 */
-	if (cpumask_empty(mask))
-		(*work)(data); /* Call on current CPU */
-	else {
-		preempt_disable();
-		{
-			/*
-			 * Did the user ask to run on this CPU?
-			 */
-			if (cpumask_test_cpu(RAW_CPU(), mask))
-				(*work)(data); /* Call on current CPU */
-
-			/*
-			 * OK, now check other CPUs.
-			 */
-			smp_call_function_many(mask, work, data, true);
-		}
-		preempt_enable();
-	}
-}
-
-int sw_get_cpu(unsigned long *flags)
-{
-	local_irq_save(*flags);
-	return get_cpu();
-}
-
-void sw_put_cpu(unsigned long flags)
-{
-	put_cpu();
-	local_irq_restore(flags);
-}
-
-#ifndef CONFIG_NR_CPUS_PER_MODULE
-	#define CONFIG_NR_CPUS_PER_MODULE 2
-#endif /* CONFIG_NR_CPUS_PER_MODULE */
-
-static void sw_get_cpu_sibling_mask(int cpu, struct cpumask *sibling_mask)
-{
-	unsigned int base =
-		(cpu/CONFIG_NR_CPUS_PER_MODULE) * CONFIG_NR_CPUS_PER_MODULE;
-	unsigned int i;
-
-	cpumask_clear(sibling_mask);
-	for (i = base; i < (base+CONFIG_NR_CPUS_PER_MODULE); ++i)
-		cpumask_set_cpu(i, sibling_mask);
-
-}
-
-struct pw_cpufreq_node {
-	int cpu;
-	struct cpumask cpus, related_cpus;
-	unsigned int shared_type;
-	struct list_head list;
-};
-static struct list_head pw_cpufreq_policy_lists;
-
-int sw_set_module_scope_for_cpus(void)
-{
-	/*
-	 * Warning: no support for cpu hotplugging!
-	 */
-	int cpu = 0;
-
-	INIT_LIST_HEAD(&pw_cpufreq_policy_lists);
-	for_each_online_cpu(cpu) {
-		struct cpumask sibling_mask;
-		struct pw_cpufreq_node *node = NULL;
-		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
-
-		if (!policy)
-			continue;
-
-		/*
-		 * Get siblings for this cpu.
-		 */
-		sw_get_cpu_sibling_mask(cpu, &sibling_mask);
-		/*
-		 * Check if affected_cpus already contains sibling_mask
-		 */
-		if (cpumask_subset(&sibling_mask, policy->cpus)) {
-			/*
-			 * 'sibling_mask' is already a subset of
-			 * affected_cpus -- nothing to do on this CPU.
-			 */
-			cpufreq_cpu_put(policy);
-			continue;
-		}
-
-		node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
-		if (node) {
-			cpumask_clear(&node->cpus);
-			cpumask_clear(&node->related_cpus);
-
-			node->cpu = cpu;
-			cpumask_copy(&node->cpus, policy->cpus);
-			cpumask_copy(&node->related_cpus, policy->related_cpus);
-			node->shared_type = policy->shared_type;
-		}
-
-		policy->shared_type = CPUFREQ_SHARED_TYPE_ALL;
-		/*
-		 * Set siblings. Don't worry about online/offline, that's
-		 * handled below.
-		 */
-		cpumask_copy(policy->cpus, &sibling_mask);
-		/*
-		 * Ensure 'related_cpus' is a superset of 'cpus'
-		 */
-		cpumask_or(policy->related_cpus,
-			policy->related_cpus, policy->cpus);
-		/*
-		 * Ensure 'cpus' only contains online cpus.
-		 */
-		cpumask_and(policy->cpus, policy->cpus, cpu_online_mask);
-
-		cpufreq_cpu_put(policy);
-
-		if (node) {
-			INIT_LIST_HEAD(&node->list);
-			list_add_tail(&node->list, &pw_cpufreq_policy_lists);
-		}
-	}
-	return PW_SUCCESS;
-}
-
-int sw_reset_module_scope_for_cpus(void)
-{
-	struct list_head *head = &pw_cpufreq_policy_lists;
-
-	while (!list_empty(head)) {
-		struct pw_cpufreq_node *node =
-			list_first_entry(head, struct pw_cpufreq_node, list);
-		int cpu = node->cpu;
-		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
-
-		if (!policy)
-			continue;
-
-		policy->shared_type = node->shared_type;
-		cpumask_copy(policy->related_cpus, &node->related_cpus);
-		cpumask_copy(policy->cpus, &node->cpus);
-
-		cpufreq_cpu_put(policy);
-
-		pw_pr_debug("OK, reset cpufreq_policy for cpu %d\n", cpu);
-		list_del(&node->list);
-		sw_kfree(node);
-	}
-	return PW_SUCCESS;
-}
-
-int sw_setup_telem(u64 addrs[3])
-{
-	return setup_telem(addrs);
-}
-
-void sw_destroy_telem(void)
-{
-	destroy_telem();
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "sw_hardware_io.h"
+#include "sw_mem.h"
+#include "sw_kernel_defines.h"
+#include "sw_telem.h"
+#include "sw_cta.h"
+#include "sw_internal.h"
+
+bool sw_check_output_buffer_params(
+	void __user *buffer, size_t bytes_to_read, size_t buff_size)
+{
+	if (!buffer) {
+		pw_pr_error("ERROR: NULL ptr in sw_consume_data!\n");
+		return false;
+	}
+	if (bytes_to_read != buff_size) {
+		pw_pr_error("Error: bytes_to_read = %zu, required to be %zu\n",
+			bytes_to_read, buff_size);
+		return false;
+	}
+	return true;
+}
+
+unsigned long sw_copy_to_user(char __user *dst, char *src, size_t bytes_to_copy)
+{
+	return copy_to_user(dst, src, bytes_to_copy);
+}
+
+void sw_schedule_work(
+	const struct cpumask *mask, void (*work)(void *), void *data)
+{
+	/*
+	 * Did the user ask us to run on 'ANY' CPU?
+	 */
+	if (cpumask_empty(mask))
+		(*work)(data); /* Call on current CPU */
+	else {
+		preempt_disable();
+		{
+			/*
+			 * Did the user ask to run on this CPU?
+			 */
+			if (cpumask_test_cpu(RAW_CPU(), mask))
+				(*work)(data); /* Call on current CPU */
+
+			/*
+			 * OK, now check other CPUs.
+			 */
+			smp_call_function_many(mask, work, data, true);
+		}
+		preempt_enable();
+	}
+}
+
+int sw_get_cpu(unsigned long *flags)
+{
+	local_irq_save(*flags);
+	return get_cpu();
+}
+
+void sw_put_cpu(unsigned long flags)
+{
+	put_cpu();
+	local_irq_restore(flags);
+}
+
+#ifndef CONFIG_NR_CPUS_PER_MODULE
+	#define CONFIG_NR_CPUS_PER_MODULE 2
+#endif /* CONFIG_NR_CPUS_PER_MODULE */
+
+static void sw_get_cpu_sibling_mask(int cpu, struct cpumask *sibling_mask)
+{
+	unsigned int base =
+		(cpu/CONFIG_NR_CPUS_PER_MODULE) * CONFIG_NR_CPUS_PER_MODULE;
+	unsigned int i;
+
+	cpumask_clear(sibling_mask);
+	for (i = base; i < (base+CONFIG_NR_CPUS_PER_MODULE); ++i)
+		cpumask_set_cpu(i, sibling_mask);
+
+}
+
+struct pw_cpufreq_node {
+	int cpu;
+	struct cpumask cpus, related_cpus;
+	unsigned int shared_type;
+	struct list_head list;
+};
+static struct list_head pw_cpufreq_policy_lists;
+
+int sw_set_module_scope_for_cpus(void)
+{
+	/*
+	 * Warning: no support for cpu hotplugging!
+	 */
+	int cpu = 0;
+
+	INIT_LIST_HEAD(&pw_cpufreq_policy_lists);
+	for_each_online_cpu(cpu) {
+		struct cpumask sibling_mask;
+		struct pw_cpufreq_node *node = NULL;
+		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+
+		if (!policy)
+			continue;
+
+		/*
+		 * Get siblings for this cpu.
+		 */
+		sw_get_cpu_sibling_mask(cpu, &sibling_mask);
+		/*
+		 * Check if affected_cpus already contains sibling_mask
+		 */
+		if (cpumask_subset(&sibling_mask, policy->cpus)) {
+			/*
+			 * 'sibling_mask' is already a subset of
+			 * affected_cpus -- nothing to do on this CPU.
+			 */
+			cpufreq_cpu_put(policy);
+			continue;
+		}
+
+		node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+		if (node) {
+			cpumask_clear(&node->cpus);
+			cpumask_clear(&node->related_cpus);
+
+			node->cpu = cpu;
+			cpumask_copy(&node->cpus, policy->cpus);
+			cpumask_copy(&node->related_cpus, policy->related_cpus);
+			node->shared_type = policy->shared_type;
+		}
+
+		policy->shared_type = CPUFREQ_SHARED_TYPE_ALL;
+		/*
+		 * Set siblings. Don't worry about online/offline, that's
+		 * handled below.
+		 */
+		cpumask_copy(policy->cpus, &sibling_mask);
+		/*
+		 * Ensure 'related_cpus' is a superset of 'cpus'
+		 */
+		cpumask_or(policy->related_cpus,
+			policy->related_cpus, policy->cpus);
+		/*
+		 * Ensure 'cpus' only contains online cpus.
+		 */
+		cpumask_and(policy->cpus, policy->cpus, cpu_online_mask);
+
+		cpufreq_cpu_put(policy);
+
+		if (node) {
+			INIT_LIST_HEAD(&node->list);
+			list_add_tail(&node->list, &pw_cpufreq_policy_lists);
+		}
+	}
+	return PW_SUCCESS;
+}
+
+int sw_reset_module_scope_for_cpus(void)
+{
+	struct list_head *head = &pw_cpufreq_policy_lists;
+
+	while (!list_empty(head)) {
+		struct pw_cpufreq_node *node =
+			list_first_entry(head, struct pw_cpufreq_node, list);
+		int cpu = node->cpu;
+		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+
+		if (!policy)
+			continue;
+
+		policy->shared_type = node->shared_type;
+		cpumask_copy(policy->related_cpus, &node->related_cpus);
+		cpumask_copy(policy->cpus, &node->cpus);
+
+		cpufreq_cpu_put(policy);
+
+		pw_pr_debug("OK, reset cpufreq_policy for cpu %d\n", cpu);
+		list_del(&node->list);
+		sw_kfree(node);
+	}
+	return PW_SUCCESS;
+}
+
+int sw_setup_telem(u64 addrs[3])
+{
+	return setup_telem(addrs);
+}
+
+void sw_destroy_telem(void)
+{
+	destroy_telem();
+}
+
+struct _sw_aggregator_msg const *sw_get_cta_aggregators(void)
+{
+	return sw_cta_aggregators();
+}
diff --git a/drivers/platform/x86/socwatch/sw_mem.c b/drivers/platform/x86/socwatch/sw_mem.c
index c1e22611ba67..a0a0c3485c6c 100644
--- a/drivers/platform/x86/socwatch/sw_mem.c
+++ b/drivers/platform/x86/socwatch/sw_mem.c
@@ -1,322 +1,322 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#include <linux/slab.h>
-
-#include "sw_kernel_defines.h"
-#include "sw_lock_defs.h"
-#include "sw_mem.h"
-
-/*
- * How do we behave if we ever
- * get an allocation error?
- * (a) Setting to '1' REFUSES ANY FURTHER
- * allocation requests.
- * (b) Setting to '0' treats each
- * allocation request as separate, and
- * handles them on an on-demand basis
- */
-#define DO_MEM_PANIC_ON_ALLOC_ERROR 0
-
-#if DO_MEM_PANIC_ON_ALLOC_ERROR
-/*
- * If we ever run into memory allocation errors then
- * stop (and drop) everything.
- */
-static atomic_t pw_mem_should_panic = ATOMIC_INIT(0);
-/*
- * Macro to check if PANIC is on.
- */
-#define MEM_PANIC() do {						\
-		atomic_set(&pw_mem_should_panic, 1);			\
-		smp_mb(); /* memory access ordering */			\
-	} while (0)
-
-#define SHOULD_TRACE() ({						\
-	bool __tmp = false;						\
-	smp_mb(); /* memory access ordering */				\
-	__tmp = (atomic_read(&pw_mem_should_panic) == 0);		\
-	__tmp; })
-
-#else /* if !DO_MEM_PANIC_ON_ALLOC_ERROR */
-
-#define MEM_PANIC()
-#define SHOULD_TRACE() (true)
-
-#endif
-
-/*
- * Variables to track memory usage.
- */
-/*
- * TOTAL num bytes allocated.
- */
-static u64 total_num_bytes_alloced;
-/*
- * Num of allocated bytes that have
- * not yet been freed.
- */
-static u64 curr_num_bytes_alloced;
-/*
- * Max # of allocated bytes that
- * have not been freed at any point
- * in time.
- */
-static u64 max_num_bytes_alloced;
-
-u64 sw_get_total_bytes_alloced(void)
-{
-	return total_num_bytes_alloced;
-};
-
-u64 sw_get_max_bytes_alloced(void)
-{
-	return max_num_bytes_alloced;
-};
-
-u64 sw_get_curr_bytes_alloced(void)
-{
-	return curr_num_bytes_alloced;
-};
-
-/*
- * Allocate free pages.
- * TODO: add memory tracker?
- */
-unsigned long sw_allocate_pages(
-	unsigned int flags, unsigned int alloc_size_in_bytes)
-{
-	return __get_free_pages(
-		(gfp_t)flags, get_order(alloc_size_in_bytes));
-}
-/*
- * Free up previously allocated pages.
- * TODO: add memory tracker?
- */
-void sw_release_pages(
-	unsigned long addr, unsigned int alloc_size_in_bytes)
-{
-	free_pages(addr, get_order(alloc_size_in_bytes));
-}
-
-#if DO_TRACK_MEMORY_USAGE
-
-/*
- * Lock to guard access to memory
- * debugging stats.
- */
-static SW_DEFINE_SPINLOCK(sw_kmalloc_lock);
-
-/*
- * Helper macros to print out
- * mem debugging stats.
- */
-#define TOTAL_NUM_BYTES_ALLOCED() total_num_bytes_alloced
-#define CURR_NUM_BYTES_ALLOCED() curr_num_bytes_alloced
-#define MAX_NUM_BYTES_ALLOCED() max_num_bytes_alloced
-
-/*
- * MAGIC number based memory tracker. Relies on
- * storing (a) a MAGIC marker and (b) the requested
- * size WITHIN the allocated block of memory. Standard
- * malloc-tracking stuff, really.
- *
- * Overview:
- * (1) ALLOCATION:
- * When asked to allocate a block of 'X' bytes, allocate
- * 'X' + 8 bytes. Then, in the FIRST 4 bytes, write the
- * requested size. In the NEXT 4 bytes, write a special
- * (i.e. MAGIC) number to let our deallocator know that
- * this block of memory was allocated using this technique.
- * Also, keep track of the number of bytes allocated.
- *
- * (2) DEALLOCATION:
- * When given an object to deallocate, we first check
- * the MAGIC number by decrementing the pointer by
- * 4 bytes and reading the (integer) stored there.
- * After ensuring the pointer was, in fact, allocated
- * by us, we then read the size of the allocated
- * block (again, by decrementing the pointer by 4
- * bytes and reading the integer size). We
- * use this size argument to decrement # of bytes
- * allocated.
- */
-#define PW_MEM_MAGIC 0xdeadbeef
-
-#define PW_ADD_MAGIC(x) ({					\
-	char *__tmp1 = (char *)(x);				\
-	*((int *)__tmp1) = PW_MEM_MAGIC;			\
-	__tmp1 += sizeof(int); __tmp1; })
-
-#define PW_ADD_SIZE(x, s) ({					\
-	char *__tmp1 = (char *)(x);				\
-	*((int *)__tmp1) = (s);					\
-	__tmp1 += sizeof(int); __tmp1; })
-
-#define PW_ADD_STAMP(x, s) PW_ADD_MAGIC(PW_ADD_SIZE((x), (s)))
-
-#define PW_IS_MAGIC(x) ({					\
-	int *__tmp1 = (int *)((char *)(x) - sizeof(int));	\
-	*__tmp1 == PW_MEM_MAGIC; })
-#define PW_REMOVE_STAMP(x) ({					\
-	char *__tmp1 = (char *)(x);				\
-	__tmp1 -= sizeof(int) * 2; __tmp1; })
-
-#define PW_GET_SIZE(x) (*((int *)(x)))
-
-void *sw_kmalloc(size_t size, unsigned int flags)
-{
-	size_t act_size = 0;
-	void *retVal = NULL;
-	/*
-	 * No point in allocating if
-	 * we were unable to allocate
-	 * previously!
-	 */
-	{
-		if (!SHOULD_TRACE())
-			return NULL;
-	}
-	/*
-	 * (1) Allocate requested block.
-	 */
-	act_size = size + sizeof(int) * 2;
-	retVal = kmalloc(act_size, (gfp_t)flags);
-	if (!retVal) {
-		/*
-		 * Panic if we couldn't allocate
-		 * requested memory.
-		 */
-		pw_pr_debug("ERROR: could NOT allocate memory!\n");
-		MEM_PANIC();
-		return NULL;
-	}
-	/*
-	 * (2) Update memory usage stats.
-	 */
-	LOCK(sw_kmalloc_lock);
-	{
-		total_num_bytes_alloced += size;
-		curr_num_bytes_alloced += size;
-		if (curr_num_bytes_alloced > max_num_bytes_alloced)
-			max_num_bytes_alloced = curr_num_bytes_alloced;
-	}
-	UNLOCK(sw_kmalloc_lock);
-	/*
-	 * (3) And finally, add the 'size'
-	 * and 'magic' stamps.
-	 */
-	return PW_ADD_STAMP(retVal, size);
-};
-
-void sw_kfree(const void *obj)
-{
-	void *tmp = NULL;
-	size_t size = 0;
-
-	/*
-	 * (1) Check if this block was allocated
-	 * by us.
-	 */
-	if (!PW_IS_MAGIC(obj)) {
-		pw_pr_debug("ERROR: %p is NOT a PW_MAGIC ptr!\n", obj);
-		return;
-	}
-	/*
-	 * (2) Strip the magic num...
-	 */
-	tmp = PW_REMOVE_STAMP(obj);
-	/*
-	 * ...and retrieve size of block.
-	 */
-	size = PW_GET_SIZE(tmp);
-	/*
-	 * (3) Update memory usage stats.
-	 */
-	LOCK(sw_kmalloc_lock);
-	{
-		curr_num_bytes_alloced -= size;
-	}
-	UNLOCK(sw_kmalloc_lock);
-	/*
-	 * And finally, free the block.
-	 */
-	kfree(tmp);
-};
-
-#else /* !DO_TRACK_MEMORY_USAGE */
-
-void *sw_kmalloc(size_t size, unsigned int flags)
-{
-	void *ret = NULL;
-
-	if (SHOULD_TRACE()) {
-		ret = kmalloc(size, (gfp_t)flags);
-		if (!ret) {
-			/*
-			 * Panic if we couldn't allocate
-			 * requested memory.
-			 */
-			MEM_PANIC();
-		}
-	}
-	return ret;
-};
-
-void sw_kfree(const void *mem)
-{
-	kfree(mem);
-};
-
-#endif /* DO_TRACK_MEMORY_USAGE */
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include <linux/slab.h>
+
+#include "sw_kernel_defines.h"
+#include "sw_lock_defs.h"
+#include "sw_mem.h"
+
+/*
+ * How do we behave if we ever
+ * get an allocation error?
+ * (a) Setting to '1' REFUSES ANY FURTHER
+ * allocation requests.
+ * (b) Setting to '0' treats each
+ * allocation request as separate, and
+ * handles them on an on-demand basis
+ */
+#define DO_MEM_PANIC_ON_ALLOC_ERROR 0
+
+#if DO_MEM_PANIC_ON_ALLOC_ERROR
+/*
+ * If we ever run into memory allocation errors then
+ * stop (and drop) everything.
+ */
+static atomic_t pw_mem_should_panic = ATOMIC_INIT(0);
+/*
+ * Macro to check if PANIC is on.
+ */
+#define MEM_PANIC() do {						\
+		atomic_set(&pw_mem_should_panic, 1);			\
+		smp_mb(); /* memory access ordering */			\
+	} while (0)
+
+#define SHOULD_TRACE() ({						\
+	bool __tmp = false;						\
+	smp_mb(); /* memory access ordering */				\
+	__tmp = (atomic_read(&pw_mem_should_panic) == 0);		\
+	__tmp; })
+
+#else /* if !DO_MEM_PANIC_ON_ALLOC_ERROR */
+
+#define MEM_PANIC()
+#define SHOULD_TRACE() (true)
+
+#endif
+
+/*
+ * Variables to track memory usage.
+ */
+/*
+ * TOTAL num bytes allocated.
+ */
+static u64 total_num_bytes_alloced;
+/*
+ * Num of allocated bytes that have
+ * not yet been freed.
+ */
+static u64 curr_num_bytes_alloced;
+/*
+ * Max # of allocated bytes that
+ * have not been freed at any point
+ * in time.
+ */
+static u64 max_num_bytes_alloced;
+
+u64 sw_get_total_bytes_alloced(void)
+{
+	return total_num_bytes_alloced;
+};
+
+u64 sw_get_max_bytes_alloced(void)
+{
+	return max_num_bytes_alloced;
+};
+
+u64 sw_get_curr_bytes_alloced(void)
+{
+	return curr_num_bytes_alloced;
+};
+
+/*
+ * Allocate free pages.
+ * TODO: add memory tracker?
+ */
+unsigned long sw_allocate_pages(
+	unsigned int flags, unsigned int alloc_size_in_bytes)
+{
+	return __get_free_pages(
+		(gfp_t)flags, get_order(alloc_size_in_bytes));
+}
+/*
+ * Free up previously allocated pages.
+ * TODO: add memory tracker?
+ */
+void sw_release_pages(
+	unsigned long addr, unsigned int alloc_size_in_bytes)
+{
+	free_pages(addr, get_order(alloc_size_in_bytes));
+}
+
+#if DO_TRACK_MEMORY_USAGE
+
+/*
+ * Lock to guard access to memory
+ * debugging stats.
+ */
+static SW_DEFINE_SPINLOCK(sw_kmalloc_lock);
+
+/*
+ * Helper macros to print out
+ * mem debugging stats.
+ */
+#define TOTAL_NUM_BYTES_ALLOCED() total_num_bytes_alloced
+#define CURR_NUM_BYTES_ALLOCED() curr_num_bytes_alloced
+#define MAX_NUM_BYTES_ALLOCED() max_num_bytes_alloced
+
+/*
+ * MAGIC number based memory tracker. Relies on
+ * storing (a) a MAGIC marker and (b) the requested
+ * size WITHIN the allocated block of memory. Standard
+ * malloc-tracking stuff, really.
+ *
+ * Overview:
+ * (1) ALLOCATION:
+ * When asked to allocate a block of 'X' bytes, allocate
+ * 'X' + 8 bytes. Then, in the FIRST 4 bytes, write the
+ * requested size. In the NEXT 4 bytes, write a special
+ * (i.e. MAGIC) number to let our deallocator know that
+ * this block of memory was allocated using this technique.
+ * Also, keep track of the number of bytes allocated.
+ *
+ * (2) DEALLOCATION:
+ * When given an object to deallocate, we first check
+ * the MAGIC number by decrementing the pointer by
+ * 4 bytes and reading the (integer) stored there.
+ * After ensuring the pointer was, in fact, allocated
+ * by us, we then read the size of the allocated
+ * block (again, by decrementing the pointer by 4
+ * bytes and reading the integer size). We
+ * use this size argument to decrement # of bytes
+ * allocated.
+ */
+#define PW_MEM_MAGIC 0xdeadbeef
+
+#define PW_ADD_MAGIC(x) ({					\
+	char *__tmp1 = (char *)(x);				\
+	*((int *)__tmp1) = PW_MEM_MAGIC;			\
+	__tmp1 += sizeof(int); __tmp1; })
+
+#define PW_ADD_SIZE(x, s) ({					\
+	char *__tmp1 = (char *)(x);				\
+	*((int *)__tmp1) = (s);					\
+	__tmp1 += sizeof(int); __tmp1; })
+
+#define PW_ADD_STAMP(x, s) PW_ADD_MAGIC(PW_ADD_SIZE((x), (s)))
+
+#define PW_IS_MAGIC(x) ({					\
+	int *__tmp1 = (int *)((char *)(x) - sizeof(int));	\
+	*__tmp1 == PW_MEM_MAGIC; })
+#define PW_REMOVE_STAMP(x) ({					\
+	char *__tmp1 = (char *)(x);				\
+	__tmp1 -= sizeof(int) * 2; __tmp1; })
+
+#define PW_GET_SIZE(x) (*((int *)(x)))
+
+void *sw_kmalloc(size_t size, unsigned int flags)
+{
+	size_t act_size = 0;
+	void *retVal = NULL;
+	/*
+	 * No point in allocating if
+	 * we were unable to allocate
+	 * previously!
+	 */
+	{
+		if (!SHOULD_TRACE())
+			return NULL;
+	}
+	/*
+	 * (1) Allocate requested block.
+	 */
+	act_size = size + sizeof(int) * 2;
+	retVal = kmalloc(act_size, (gfp_t)flags);
+	if (!retVal) {
+		/*
+		 * Panic if we couldn't allocate
+		 * requested memory.
+		 */
+		pw_pr_debug("ERROR: could NOT allocate memory!\n");
+		MEM_PANIC();
+		return NULL;
+	}
+	/*
+	 * (2) Update memory usage stats.
+	 */
+	LOCK(sw_kmalloc_lock);
+	{
+		total_num_bytes_alloced += size;
+		curr_num_bytes_alloced += size;
+		if (curr_num_bytes_alloced > max_num_bytes_alloced)
+			max_num_bytes_alloced = curr_num_bytes_alloced;
+	}
+	UNLOCK(sw_kmalloc_lock);
+	/*
+	 * (3) And finally, add the 'size'
+	 * and 'magic' stamps.
+	 */
+	return PW_ADD_STAMP(retVal, size);
+};
+
+void sw_kfree(const void *obj)
+{
+	void *tmp = NULL;
+	size_t size = 0;
+
+	/*
+	 * (1) Check if this block was allocated
+	 * by us.
+	 */
+	if (!PW_IS_MAGIC(obj)) {
+		pw_pr_debug("ERROR: %p is NOT a PW_MAGIC ptr!\n", obj);
+		return;
+	}
+	/*
+	 * (2) Strip the magic num...
+	 */
+	tmp = PW_REMOVE_STAMP(obj);
+	/*
+	 * ...and retrieve size of block.
+	 */
+	size = PW_GET_SIZE(tmp);
+	/*
+	 * (3) Update memory usage stats.
+	 */
+	LOCK(sw_kmalloc_lock);
+	{
+		curr_num_bytes_alloced -= size;
+	}
+	UNLOCK(sw_kmalloc_lock);
+	/*
+	 * And finally, free the block.
+	 */
+	kfree(tmp);
+};
+
+#else /* !DO_TRACK_MEMORY_USAGE */
+
+void *sw_kmalloc(size_t size, unsigned int flags)
+{
+	void *ret = NULL;
+
+	if (SHOULD_TRACE()) {
+		ret = kmalloc(size, (gfp_t)flags);
+		if (!ret) {
+			/*
+			 * Panic if we couldn't allocate
+			 * requested memory.
+			 */
+			MEM_PANIC();
+		}
+	}
+	return ret;
+};
+
+void sw_kfree(const void *mem)
+{
+	kfree(mem);
+};
+
+#endif /* DO_TRACK_MEMORY_USAGE */
diff --git a/drivers/platform/x86/socwatch/sw_ops_provider.c b/drivers/platform/x86/socwatch/sw_ops_provider.c
index 285bec0130fe..22d8bf24ac0d 100644
--- a/drivers/platform/x86/socwatch/sw_ops_provider.c
+++ b/drivers/platform/x86/socwatch/sw_ops_provider.c
@@ -1,1145 +1,1162 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/kernel.h>
-#include <linux/errno.h>
-#include <linux/pci.h> /* "pci_get_domain_bus_and_slot" */
-#include <linux/delay.h> /* "udelay" */
-#include <asm/msr.h>
-#ifdef CONFIG_RPMSG_IPC
-	#include <asm/intel_mid_rpmsg.h>
-#endif /* CONFIG_RPMSG_IPC */
-
-#include "sw_types.h"
-#include "sw_kernel_defines.h"
-#include "sw_hardware_io.h"
-#include "sw_telem.h"
-#include "sw_ops_provider.h"
-
-/*
- * Compile time constants.
- */
-/*
- * Should we be doing 'direct' PCI reads and writes?
- * '1' ==> YES, call "pci_{read,write}_config_dword()" directly
- * '0' ==> NO, Use the "intel_mid_msgbus_{read32,write32}_raw()" API
- *		(defined in 'intel_mid_pcihelpers.c')
- */
-#define DO_DIRECT_PCI_READ_WRITE 0
-#if !IS_ENABLED(CONFIG_ANDROID) || !defined(CONFIG_X86_WANT_INTEL_MID)
-    /*
-     * 'intel_mid_pcihelpers.h' is probably not present -- force
-     * direct PCI calls in this case.
-     */
-	#undef DO_DIRECT_PCI_READ_WRITE
-	#define DO_DIRECT_PCI_READ_WRITE  1
-#endif
-#if !DO_DIRECT_PCI_READ_WRITE
-	#include <asm/intel_mid_pcihelpers.h>
-#endif
-
-#define SW_PCI_MSG_CTRL_REG 0x000000D0
-#define SW_PCI_MSG_DATA_REG 0x000000D4
-
-/*
- *  NUM_RETRY & USEC_DELAY are used in PCH Mailbox (sw_read_pch_mailbox_info_i).
- *  Tested on KBL + SPT-LP. May need to revisit.
- */
-#define NUM_RETRY  100
-#define USEC_DELAY 100
-
-#define EXTCNF_CTRL 0xF00 /* offset for hw semaphore. */
-#define FWSM_CTRL 0x5B54 /* offset for fw semaphore */
-#define GBE_CTRL_OFFSET 0x34 /* GBE LPM offset */
-
-#define IS_HW_SEMAPHORE_SET(data) (data & (pw_u64_t)(0x1 << 6))
-#define IS_FW_SEMAPHORE_SET(data) (data & (pw_u64_t)0x1)
-/*
- * Number of retries for mailbox configuration
- */
-#define MAX_MAILBOX_ITERS 100
-
-/*
- * Local data structures.
- */
-/*
- * TODO: separate into H/W and S/W IO?
- */
-enum sw_io_type {
-	SW_IO_MSR		= 0,
-	SW_IO_IPC		= 1,
-	SW_IO_MMIO		= 2,
-	SW_IO_PCI		= 3,
-	SW_IO_CONFIGDB		= 4,
-	SW_IO_TRACE_ARGS	= 5,
-	SW_IO_WAKEUP		= 6,
-	SW_IO_SOCPERF		= 7,
-	SW_IO_PROC_NAME		= 8,
-	SW_IO_IRQ_NAME		= 9,
-	SW_IO_WAKELOCK		= 10,
-	SW_IO_TELEM		= 11,
-	SW_IO_PCH_MAILBOX	= 12,
-	SW_IO_MAILBOX		= 13,
-	SW_IO_MAX		= 14,
-};
-
-/*
- * "io_remapped" values for HW and FW semaphores
- */
-static struct {
-	volatile void __iomem *hw_semaphore;
-	volatile void __iomem *fw_semaphore;
-} s_gbe_semaphore = {NULL, NULL};
-
-/*
- * Function declarations.
- */
-/*
- * Exported by the SOCPERF driver.
- */
-extern void __weak SOCPERF_Read_Data3(void *data_buffer);
-
-/*
- * Init functions.
- */
-int sw_ipc_mmio_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
-int sw_pch_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
-int sw_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
-
-/*
- * Read functions.
- */
-void sw_read_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_read_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-
-/*
- * Write functions.
- */
-void sw_write_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_trace_args_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_wakeup_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-void sw_write_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
-
-/*
- * Print functions.
- */
-int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor);
-
-/*
- * Reset functions -- equal but opposite of init.
- */
-int sw_ipc_mmio_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
-int sw_pch_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
-int sw_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
-
-/*
- * Available functions.
- */
-bool sw_socperf_available_i(void);
-
-/*
- * Helper functions.
- */
-u32 sw_platform_configdb_read32(u32 address);
-u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
-u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
-bool sw_platform_pci_write32(u32 bus, u32 device, u32 function, u32 write_offset, u32 data_value);
-
-/*
- * Table of collector operations.
- */
-static const struct sw_hw_ops s_hw_ops[] = {
-	[SW_IO_MSR] = {
-		.name = "MSR",
-		.init = NULL,
-		.read = &sw_read_msr_info_i,
-		.write = &sw_write_msr_info_i,
-		.print = &sw_print_msr_io_descriptor,
-		.reset = NULL,
-		.available = NULL
-	},
-	[SW_IO_IPC] = {
-		.name = "IPC",
-		.init = &sw_ipc_mmio_descriptor_init_func_i,
-		.read = &sw_read_ipc_info_i,
-		.reset = &sw_ipc_mmio_descriptor_reset_func_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_MMIO] = {
-		.name = "MMIO",
-		.init = &sw_ipc_mmio_descriptor_init_func_i,
-		.read = &sw_read_mmio_info_i,
-		.write = &sw_write_mmio_info_i,
-		.reset = &sw_ipc_mmio_descriptor_reset_func_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_PCI] = {
-		.name = "PCI",
-		.read = &sw_read_pci_info_i,
-		.write = &sw_write_pci_info_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_CONFIGDB] = {
-		.name = "CONFIGDB",
-		.read = &sw_read_configdb_info_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_WAKEUP] = {
-		.name = "WAKEUP",
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_SOCPERF] = {
-		.name = "SOCPERF",
-		.read = &sw_read_socperf_info_i,
-		.available = &sw_socperf_available_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_PROC_NAME] = {
-		.name = "PROC-NAME",
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_IRQ_NAME] = {
-		.name = "IRQ-NAME",
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_WAKELOCK] = {
-		.name = "WAKELOCK",
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_TELEM] = {
-		.name = "TELEM",
-		.init = &sw_telem_init_func,
-		.read = &sw_read_telem_info,
-		.reset = &sw_reset_telem,
-		.available = &sw_telem_available,
-		.post_config = &sw_telem_post_config,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_PCH_MAILBOX] = {
-		.name = "PCH-MAILBOX",
-		.init = &sw_pch_mailbox_descriptor_init_func_i,
-		.read = &sw_read_pch_mailbox_info_i,
-		.reset = &sw_pch_mailbox_descriptor_reset_func_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_MAILBOX] = {
-		.name = "MAILBOX",
-		.init = &sw_mailbox_descriptor_init_func_i,
-		.read = &sw_read_mailbox_info_i,
-		.write = &sw_write_mailbox_info_i,
-		.reset = &sw_mailbox_descriptor_reset_func_i,
-		/* Other fields are don't care (will be set to NULL) */
-	},
-	[SW_IO_MAX] = {
-		.name = NULL,
-		/* Other fields are don't care (will be set to NULL) */
-	}
-};
-
-/*
- * Function definitions.
- */
-int sw_ipc_mmio_descriptor_init_func_i(
-	struct sw_driver_io_descriptor *descriptor)
-{
-	/* Perform any required 'io_remap' calls here */
-	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
-	u64 data_address = 0;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	if (descriptor->collection_type == SW_IO_IPC)
-		__ipc_mmio = &descriptor->ipc_descriptor;
-	else
-		__ipc_mmio = &descriptor->mmio_descriptor;
-
-	pw_pr_debug("cmd = %u, sub-cmd = %u, data_addr = 0x%llx\n"
-		__ipc_mmio->command, __ipc_mmio->sub_command,
-		__ipc_mmio->data_address);
-	data_address = __ipc_mmio->data_address;
-
-	if (!data_address)
-		return PW_SUCCESS;
-
-	__ipc_mmio->data_remapped_address =
-		(pw_u64_t)(unsigned long)ioremap_nocache(
-			(unsigned long)data_address,
-			descriptor->counter_size_in_bytes);
-	if ((void *)(unsigned long)__ipc_mmio->data_remapped_address == NULL)
-		return -EIO;
-
-	pw_pr_debug("mapped addr 0x%llx\n", __ipc_mmio->data_remapped_address);
-	if ((__ipc_mmio->is_gbe) &&
-		(!s_gbe_semaphore.hw_semaphore ||
-			!s_gbe_semaphore.fw_semaphore) &&
-				(data_address >= GBE_CTRL_OFFSET)) {
-
-		u64 hw_addr = (data_address - GBE_CTRL_OFFSET) + EXTCNF_CTRL;
-		u64 fw_addr = (data_address - GBE_CTRL_OFFSET) + FWSM_CTRL;
-		pw_pr_debug("Initializing GBE semaphore\n");
-
-		s_gbe_semaphore.hw_semaphore =
-			ioremap_nocache(
-				(unsigned long)hw_addr,
-				descriptor->counter_size_in_bytes);
-		s_gbe_semaphore.fw_semaphore =
-			ioremap_nocache(
-				(unsigned long)fw_addr,
-				descriptor->counter_size_in_bytes);
-		if (s_gbe_semaphore.hw_semaphore == NULL ||
-			s_gbe_semaphore.fw_semaphore == NULL) {
-			pw_pr_error(
-				"couldn't mmap hw/fw semaphores for GBE MMIO op!\n");
-			return -EIO;
-		}
-		pw_pr_debug(
-			"GBE has hw_sem = 0x%llx, fw_sem = 0x%llx, size = %u\n",
-			(unsigned long long)s_gbe_semaphore.hw_semaphore,
-			(unsigned long long)s_gbe_semaphore.fw_semaphore,
-			descriptor->counter_size_in_bytes);
-	}
-
-	return PW_SUCCESS;
-}
-
-int sw_pch_mailbox_descriptor_init_func_i(
-	struct sw_driver_io_descriptor *descriptor)
-{
-	/* Perform any required 'io_remap' calls here */
-	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	__pch_mailbox = &descriptor->pch_mailbox_descriptor;
-	pw_pr_debug("pch_mailbox data_addr = 0x%llx\n",
-		(unsigned long long)__pch_mailbox->data_address);
-	if (__pch_mailbox->mtpmc_address) {
-		__pch_mailbox->mtpmc_remapped_address =
-			(pw_u64_t)(unsigned long)ioremap_nocache(
-				(unsigned long)__pch_mailbox->mtpmc_address,
-				descriptor->counter_size_in_bytes);
-		if ((void *)(unsigned long)
-			__pch_mailbox->mtpmc_remapped_address == NULL)
-			return -PW_ERROR;
-
-		pw_pr_debug("mtpmc_mapped addr 0x%llx\n",
-			__pch_mailbox->mtpmc_remapped_address);
-	}
-	if (__pch_mailbox->msg_full_sts_address) {
-		__pch_mailbox->msg_full_sts_remapped_address =
-			(pw_u64_t)(unsigned long)ioremap_nocache(
-				(unsigned long)
-					__pch_mailbox->msg_full_sts_address,
-				descriptor->counter_size_in_bytes);
-		if ((void *)(unsigned long)
-			__pch_mailbox->msg_full_sts_remapped_address == NULL)
-			return -PW_ERROR;
-
-		pw_pr_debug("msg_full_sts_mapped addr 0x%llx\n",
-			__pch_mailbox->msg_full_sts_address);
-	}
-	if (__pch_mailbox->mfpmc_address) {
-		__pch_mailbox->mfpmc_remapped_address =
-			(pw_u64_t)(unsigned long)ioremap_nocache(
-				(unsigned long)__pch_mailbox->mfpmc_address,
-				descriptor->counter_size_in_bytes);
-		if ((void *)(unsigned long)
-			__pch_mailbox->mfpmc_remapped_address == NULL)
-			return -PW_ERROR;
-
-		pw_pr_debug("mfpmc_mapped addr 0x%llx\n",
-			__pch_mailbox->mfpmc_remapped_address);
-	}
-	return PW_SUCCESS;
-}
-
-int sw_mailbox_descriptor_init_func_i(
-	struct sw_driver_io_descriptor *descriptor)
-{
-	/* Perform any required 'io_remap' calls here */
-	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	__mailbox = &descriptor->mailbox_descriptor;
-
-	pw_pr_debug(
-		"type = %u, interface_address = 0x%llx, data_address = 0x%llx\n",
-		__mailbox->is_msr_type, __mailbox->interface_address,
-		__mailbox->data_address);
-
-	if (!__mailbox->is_msr_type) {
-		if (__mailbox->interface_address) {
-			__mailbox->interface_remapped_address =
-				(pw_u64_t)(unsigned long)ioremap_nocache(
-					(unsigned long)__mailbox->interface_address,
-					descriptor->counter_size_in_bytes);
-			if ((void *)(unsigned long)
-				__mailbox->interface_remapped_address == NULL) {
-				pw_pr_error(
-					"Couldn't iomap interface_address = 0x%llx\n",
-					__mailbox->interface_address);
-				return -PW_ERROR;
-			}
-		}
-		if (__mailbox->data_address) {
-			__mailbox->data_remapped_address =
-				(pw_u64_t)(unsigned long)ioremap_nocache(
-					(unsigned long)__mailbox->data_address,
-					descriptor->counter_size_in_bytes);
-			if ((void *)(unsigned long)
-				__mailbox->data_remapped_address == NULL) {
-				pw_pr_error(
-					"Couldn't iomap data_address = 0x%llx\n",
-					__mailbox->data_address);
-				return -PW_ERROR;
-			}
-		}
-		pw_pr_debug("OK, mapped addr 0x%llx, 0x%llx\n",
-			__mailbox->interface_remapped_address,
-			__mailbox->data_remapped_address);
-	}
-	return PW_SUCCESS;
-}
-
-void sw_read_msr_info_i(
-	char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	u64 address = descriptors->msr_descriptor.address;
-	u32 l = 0, h = 0;
-
-	if (likely(cpu == RAW_CPU()))
-		rdmsr_safe((unsigned long)address, &l, &h);
-	else {
-		if (rdmsr_safe_on_cpu(
-			cpu, (unsigned long)address, &l, &h)) {
-			pw_pr_warn(
-				"Failed to read MSR address = 0x%llx\n",
-				address);
-				l = 0; h = 0;
-		}
-	}
-	switch (counter_size_in_bytes) {
-	case 4:
-		*((u32 *)dst_vals) = l;
-		break;
-	case 8:
-		*((u64 *)dst_vals) = ((u64)h << 32) | l;
-		pw_pr_debug(
-			"read MSR value = %llu\n", *((u64 *)dst_vals));
-		break;
-	default:
-		break;
-	}
-}
-
-#ifdef CONFIG_RPMSG_IPC
-	#define SW_DO_IPC(cmd, sub_cmd) rpmsg_send_generic_simple_command(cmd, sub_cmd)
-#else
-	#define SW_DO_IPC(cmd, sub_cmd) (-ENODEV)
-#endif // CONFIG_RPMSG_IPC
-
-void sw_read_ipc_info_i(
-	char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	u16 cmd = descriptors->ipc_descriptor.command;
-	u16 sub_cmd = descriptors->ipc_descriptor.sub_command;
-	unsigned long remapped_address = (unsigned long)
-		descriptors->ipc_descriptor.data_remapped_address;
-
-	if (cmd || sub_cmd) {
-		pw_pr_debug("EXECUTING IPC Cmd = %u, %u\n", cmd, sub_cmd);
-		if (SW_DO_IPC(cmd, sub_cmd)) {
-			pw_pr_error("ERROR running IPC command(s)\n");
-			return;
-		}
-	}
-
-	if (remapped_address) {
-		pw_pr_debug("COPYING MMIO size %u\n", counter_size_in_bytes);
-		memcpy(dst_vals, (void *)remapped_address,
-				counter_size_in_bytes);
-	}
-	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
-}
-
-static void sw_read_gbe_mmio_info_i(
-	char *dst_vals,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	u32 hw_val = 0, fw_val = 0;
-	unsigned long remapped_address = (unsigned long)
-		descriptors->mmio_descriptor.data_remapped_address;
-	u64 write_value = descriptors->write_value;
-
-	memset(dst_vals, 0, counter_size_in_bytes);
-
-	pw_pr_debug(
-		"hw_sem = 0x%llx, fw_sem = 0x%llx, addr = 0x%lx, dst_vals = 0x%lx, size = %u\n",
-		(unsigned long long)s_gbe_semaphore.hw_semaphore,
-		(unsigned long long)s_gbe_semaphore.fw_semaphore,
-		remapped_address,
-		(unsigned long)dst_vals,
-		counter_size_in_bytes);
-	if (!s_gbe_semaphore.hw_semaphore || !s_gbe_semaphore.fw_semaphore ||
-		!remapped_address)
-		return;
-
-	memcpy_fromio(&hw_val, s_gbe_semaphore.hw_semaphore, sizeof(hw_val));
-	memcpy_fromio(&fw_val, s_gbe_semaphore.fw_semaphore, sizeof(fw_val));
-	pw_pr_debug("HW_VAL = 0x%lx, FW_VAL = 0x%lx\n",
-		(unsigned long)hw_val, (unsigned long)fw_val);
-	if (!IS_HW_SEMAPHORE_SET(hw_val) && !IS_FW_SEMAPHORE_SET(fw_val)) {
-		memcpy_toio((volatile void __iomem *)remapped_address,
-				&write_value, 4 /* counter_size_in_bytes*/);
-		memcpy_fromio(dst_vals, (volatile void __iomem *)remapped_address,
-				counter_size_in_bytes);
-	}
-}
-void sw_read_mmio_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	unsigned long remapped_address =
-	(unsigned long)descriptors->mmio_descriptor.data_remapped_address;
-
-	/* MMIO for GBE requires a mailbox-like operation */
-	if (descriptors->mmio_descriptor.is_gbe)
-		sw_read_gbe_mmio_info_i(dst_vals, descriptors, counter_size_in_bytes);
-	else {
-		if (remapped_address)
-			memcpy_fromio(dst_vals, (volatile void __iomem *)remapped_address,
-				counter_size_in_bytes);
-	}
-	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
-}
-
-void sw_read_pch_mailbox_info_i(
-	char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes)
-{
-	/*
-	 * TODO: spinlock?
-	 */
-	const struct sw_driver_pch_mailbox_io_descriptor *pch_mailbox =
-					 &descriptor->pch_mailbox_descriptor;
-	u32 address = pch_mailbox->data_address;
-	u64 mtpmc_remapped_address = pch_mailbox->mtpmc_remapped_address;
-	u64 msg_full_sts_remapped_address =
-		pch_mailbox->msg_full_sts_remapped_address;
-	u64 mfpmc_remapped_address = pch_mailbox->mfpmc_remapped_address;
-
-	/*
-	 * write address of desired device counter to request from PMC
-	 * (shift and add 2 to format device offset)
-	 */
-	if (mtpmc_remapped_address) {
-		int iter = 0;
-		u32 written_val = 0;
-		/* shift and add 2 to format device offset */
-		u32 write_value = (address << 16) + 2;
-
-		memcpy_toio(
-			(volatile void __iomem *)(unsigned long)mtpmc_remapped_address,
-			 &write_value, 4 /*counter_size_in_bytes*/);
-		/*
-		 * Check if address has been written using a while loop in
-		 * order to wait for the PMC to consume that address and to
-		 * introduce sufficient delay so that the message full
-		 * status bit has time to flip. This should ensure all is
-		 * ready when begin the wait loop for it to turn 0, which
-		 * indicates the value is available to be read.
-		 * (This fixes problem where values being read were huge.)
-		 */
-		do {
-			memcpy_fromio(&written_val,
-				(volatile void __iomem *)(unsigned long)mtpmc_remapped_address,
-				4 /*counter_size_in_bytes*/);
-			pw_pr_debug(
-				"DEBUG: written_val = 0x%x, address = 0x%x\n",
-				written_val, address);
-			udelay(USEC_DELAY);
-		} while ((written_val >> 16) != address && ++iter < NUM_RETRY);
-	}
-
-
-	/*
-	 * wait for PMC to set status indicating that device
-	 * counter is available for read.
-	 */
-	if (msg_full_sts_remapped_address) {
-		u32 status_wait = 0;
-		int iter = 0;
-
-		do {
-			memcpy_fromio(&status_wait,
-				(volatile void __iomem*)(unsigned long)
-					msg_full_sts_remapped_address,
-				4 /*counter_size_in_bytes*/);
-			pw_pr_debug("DEBUG: status_wait = 0x%x\n",
-				status_wait);
-			udelay(USEC_DELAY);
-		} while ((status_wait & 0x01000000) >> 24 &&
-			++iter < NUM_RETRY);
-	}
-
-	/*
-	 * read device counter
-	 */
-	if (mfpmc_remapped_address) {
-		memcpy_fromio(dst_vals,
-			(volatile void __iomem*)(unsigned long)mfpmc_remapped_address,
-			4 /*counter_size_in_bytes*/);
-		pw_pr_debug("DEBUG: read value = 0x%x\n",
-			*((pw_u32_t *)dst_vals));
-	}
-}
-
-void sw_read_mailbox_info_i(char *dst_vals, int cpu,
-		const struct sw_driver_io_descriptor *descriptor,
-		u16 counter_size_in_bytes)
-{
-	/*
-	 * TODO: spinlock?
-	 */
-	const struct sw_driver_mailbox_io_descriptor *mailbox =
-		&descriptor->mailbox_descriptor;
-	unsigned long interface_address = mailbox->interface_address;
-	unsigned long interface_remapped_address = mailbox->interface_remapped_address;
-	unsigned long data_address = mailbox->data_address;
-	size_t iter = 0;
-
-	if (mailbox->is_msr_type) {
-		u64 command = 0;
-
-		rdmsrl_safe(interface_address, &command);
-		command &= mailbox->command_mask;
-		command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
-		wrmsrl_safe(interface_address, command);
-		do {
-			udelay(1);
-			rdmsrl_safe(interface_address, &command);
-		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
-				++iter < MAX_MAILBOX_ITERS);
-		if (iter >= MAX_MAILBOX_ITERS) {
-			pw_pr_error("Couldn't write to BIOS mailbox\n");
-			command = MAX_UNSIGNED_64_BIT_VALUE;
-		} else
-			rdmsrl_safe(data_address, &command);
-		switch (counter_size_in_bytes) {
-		case 4:
-			*((u32 *)dst_vals) = (u32)command;
-			break;
-		case 8:
-			*((u64 *)dst_vals) = command;
-			break;
-		default:
-			pw_pr_error("Invalid counter size %u, assuming 4 bytes!\n", counter_size_in_bytes);
-			*((u32 *)dst_vals) = (u32)command;
-			break;
-		}
-	}  else {
-		u32 command = 0;
-		/* Always use 4 bytes, regardless of 'counter_size_in_bytes' */
-		const size_t counter_size = 4;
-
-		memcpy_fromio(&command,
-			(volatile void __iomem *)(unsigned long)interface_remapped_address,
-			sizeof(command));
-		command &= mailbox->command_mask;
-		command |= (u32)mailbox->command |
-				(u32)0x1 << mailbox->run_busy_bit;
-		memcpy_toio((volatile void __iomem *)(unsigned long)interface_remapped_address,
-			&command, sizeof(command));
-		do {
-			udelay(1);
-			memcpy_fromio(&command,
-				(volatile void __iomem *)(unsigned long)interface_remapped_address,
-				sizeof(command));
-		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
-				++iter < MAX_MAILBOX_ITERS);
-		if (iter >= MAX_MAILBOX_ITERS) {
-			pw_pr_error("Couldn't write to BIOS mailbox\n");
-			command = MAX_UNSIGNED_32_BIT_VALUE;
-		} else
-			memcpy_fromio(&command,
-				(volatile void __iomem *)(unsigned long)mailbox->data_remapped_address,
-				counter_size);
-
-		*((u32 *)dst_vals) = command;
-	}
-}
-
-void sw_read_pci_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	u32 bus = descriptors->pci_descriptor.bus;
-	u32 device = descriptors->pci_descriptor.device;
-	u32 function = descriptors->pci_descriptor.function;
-	u32 offset = descriptors->pci_descriptor.offset;
-	u32 data32 = 0;
-	u64 data64 = 0;
-
-	switch (counter_size_in_bytes) {
-	case 4:
-		data32 = sw_platform_pci_read32(bus, device, function,
-			0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */,
-			offset /* DATA-OFFSET */);
-		*((u32 *)dst_vals) = data32;
-		break;
-	case 8:
-		data64 = sw_platform_pci_read64(bus, device, function,
-			0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */,
-			offset /* DATA-OFFSET */);
-		*((u64 *)dst_vals) = data64;
-		break;
-	default:
-		pw_pr_error("ERROR: invalid read size = %u\n",
-				counter_size_in_bytes);
-	}
-}
-void sw_read_configdb_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	pw_u32_t address = descriptors->configdb_descriptor.address;
-	u32 data = sw_platform_configdb_read32(address);
-
-	pw_pr_debug(
-		"ADDRESS = 0x%x, CPU = %d, dst_vals = %p, counter size = %u, data = %u\n",
-		address, cpu, dst_vals, counter_size_in_bytes, data);
-	/*
-	 * 'counter_size_in_bytes' is ignored, for now.
-	 */
-	*((u32 *)dst_vals) = data;
-}
-void sw_read_socperf_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptors,
-	u16 counter_size_in_bytes)
-{
-	u64 *socperf_buffer = (u64 *)dst_vals;
-
-	memset(socperf_buffer, 0, counter_size_in_bytes);
-	SOCPERF_Read_Data3(socperf_buffer);
-
-}
-
-/**
- * Decide if the socperf interface is available for use
- * @returns	 true if available
- */
-bool sw_socperf_available_i(void)
-{
-	bool retVal = false;
-
-	/* The symbol below is weak.  We return 1 if we have a definition
-	 * for this socperf-driver-supplied symbol, or 0 if only the
-	 * weak definition exists. This test will suffice to detect if
-	 * the socperf driver is loaded.
-	 */
-	if (SOCPERF_Read_Data3) {
-		pw_pr_debug("INFO: SoCPerf support in ON!\n");
-		retVal = true;
-	} else
-		pw_pr_debug("INFO: SoCPerf support is OFF!\n");
-
-	return retVal;
-}
-
-
-/**
- * sw_platform_configdb_read32 - for reading PCI space through config registers
- *							   of the platform.
- * @address: An address in the PCI space
- *
- * Returns: the value read from address.
- */
-u32 sw_platform_configdb_read32(u32 address)
-{
-	u32 read_value = 0;
-#if DO_DIRECT_PCI_READ_WRITE
-	read_value = sw_platform_pci_read32(
-		0/*bus*/, 0/*device*/, 0/*function*/,
-		SW_PCI_MSG_CTRL_REG/*ctrl-offset*/, address/*ctrl-value*/,
-		SW_PCI_MSG_DATA_REG/*data-offset*/);
-#else /* !DO_DIRECT_PCI_READ_WRITE */
-	read_value = intel_mid_msgbus_read32_raw(address);
-#endif /* if DO_DIRECT_PCI_READ_WRITE */
-	pw_pr_debug("address = %u, value = %u\n", address, read_value);
-	return read_value;
-}
-
-u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function,
-		u32 write_offset, u32 write_value, u32 read_offset)
-{
-	u32 read_value = 0;
-	struct pci_dev *pci_root =
-		pci_get_domain_bus_and_slot(0, bus,
-			/* 0, PCI_DEVFN(0, 0)); */
-			PCI_DEVFN(device, function));
-
-	if (!pci_root)
-		return 0; /* Application will verify the data */
-
-	if (write_offset)
-		pci_write_config_dword(pci_root,
-			/* SW_PCI_MSG_CTRL_REG, address); */
-			write_offset, write_value);
-
-	pci_read_config_dword(pci_root,
-		/* SW_PCI_MSG_DATA_REG, &read_value); */
-		read_offset, &read_value);
-	return read_value;
-}
-
-u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 write_offset,
-	u32 write_value, u32 read_offset)
-{
-	u32 lo = sw_platform_pci_read32(
-		bus, device, function, 0 /* CTRL-OFFSET */,
-		0 /* CTRL-DATA, don't care */,
-		read_offset /* DATA-OFFSET */);
-	u32 hi = sw_platform_pci_read32(
-		bus, device, function, 0 /* CTRL-OFFSET */,
-		0 /* CTRL-DATA, don't care */,
-		read_offset + 4 /* DATA-OFFSET */);
-
-	return ((u64)hi << 32) | lo;
-}
-
-void sw_write_msr_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes)
-{
-	u64 write_value = descriptor->write_value;
-	u64 address = descriptor->msr_descriptor.address;
-
-	pw_pr_debug(
-		"ADDRESS = 0x%llx, CPU = %d, counter size = %u, value = %llu\n",
-		address, cpu, counter_size_in_bytes, write_value);
-	if (likely(cpu == RAW_CPU()))
-		wrmsrl_safe((unsigned long)address, write_value);
-	else {
-		u32 l = write_value & 0xffffffff;
-		u32 h = (write_value >> 32) & 0xffffffff;
-
-		wrmsr_safe_on_cpu(cpu, (u32)address, l, h);
-	}
-};
-
-void sw_write_mmio_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes)
-{
-	unsigned long remapped_address = (unsigned long)
-		descriptor->mmio_descriptor.data_remapped_address;
-	u64 write_value = descriptor->write_value;
-
-	if (remapped_address)
-		memcpy_toio((volatile void __iomem *)remapped_address, &write_value,
-			counter_size_in_bytes);
-
-	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
-};
-
-void sw_write_mailbox_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes)
-{
-	/*
-	 * TODO: spinlock?
-	 */
-	const struct sw_driver_mailbox_io_descriptor *mailbox =
-					&descriptor->mailbox_descriptor;
-	unsigned long interface_address = mailbox->interface_address;
-	unsigned long interface_remapped_address =
-					mailbox->interface_remapped_address;
-	unsigned long data_address = mailbox->data_address;
-	u64 data = descriptor->write_value;
-	size_t iter = 0;
-
-	if (mailbox->is_msr_type) {
-		u64 command = 0;
-
-		rdmsrl_safe(interface_address, &command);
-		command &= mailbox->command_mask;
-		command |= mailbox->command |
-				(u64)0x1 << mailbox->run_busy_bit;
-		wrmsrl_safe(data_address, data);
-		wrmsrl_safe(interface_address, command);
-		do {
-			rdmsrl_safe(interface_address, &command);
-		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
-				++iter < MAX_MAILBOX_ITERS);
-	} else {
-		u32 command = 0;
-
-		memcpy_fromio(&command,
-			(volatile void __iomem *)(unsigned long)interface_remapped_address,
-			sizeof(command));
-		command &= mailbox->command_mask;
-		command |= (u32)mailbox->command |
-			(u32)0x1 << mailbox->run_busy_bit;
-		memcpy_toio((volatile void __iomem *)(unsigned long)
-			mailbox->data_remapped_address,
-			&data, sizeof(data));
-		memcpy_toio((volatile void __iomem *)(unsigned long)interface_remapped_address,
-			&command, sizeof(command));
-		do {
-			memcpy_fromio(&command, (volatile void __iomem *)(unsigned long)
-				interface_remapped_address, sizeof(command));
-		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
-				++iter < MAX_MAILBOX_ITERS);
-	}
-}
-
-void sw_write_pci_info_i(char *dst_vals, int cpu,
-	const struct sw_driver_io_descriptor *descriptor,
-	u16 counter_size_in_bytes)
-{
-	u32 bus = descriptor->pci_descriptor.bus;
-	u32 device = descriptor->pci_descriptor.device;
-	u32 function = descriptor->pci_descriptor.function;
-	u32 offset = descriptor->pci_descriptor.offset;
-	u32 write_value = (u32)descriptor->write_value;
-	/*
-	 * 'counter_size_in_bytes' is ignored for now.
-	 */
-	if (!sw_platform_pci_write32(bus, device, function, offset,
-			write_value))
-		pw_pr_error("ERROR writing to PCI B/D/F/O %u/%u/%u/%u\n",
-			bus, device, function, offset);
-	else
-		pw_pr_debug("OK, successfully wrote to PCI B/D/F/O %u/%u/%u/%u\n",
-			bus, device, function, offset);
-
-};
-
-/*
- * Write to PCI space via config registers.
- */
-bool sw_platform_pci_write32(u32 bus, u32 device, u32 function,
-	u32 write_offset, u32 data_value)
-{
-	struct pci_dev *pci_root =
-		pci_get_domain_bus_and_slot(0, bus,
-			PCI_DEVFN(device, function));/* 0, PCI_DEVFN(0, 0)); */
-
-	if (!pci_root)
-		return false;
-
-
-	pci_write_config_dword(pci_root, write_offset, data_value);
-
-	return true;
-};
-
-int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor)
-{
-	if (!descriptor)
-		return -PW_ERROR;
-
-	pw_pr_debug("MSR address = 0x%llx\n",
-		descriptor->msr_descriptor.address);
-	return PW_SUCCESS;
-}
-
-int sw_ipc_mmio_descriptor_reset_func_i(
-	const struct sw_driver_io_descriptor *descriptor)
-{
-	/* Unmap previously mapped memory here */
-	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	if (descriptor->collection_type == SW_IO_IPC)
-		__ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)
-			&descriptor->ipc_descriptor;
-	else
-		__ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)
-			&descriptor->mmio_descriptor;
-
-	if (__ipc_mmio->data_remapped_address) {
-		pw_pr_debug("unmapping addr 0x%llx\n",
-			__ipc_mmio->data_remapped_address);
-		iounmap((volatile void __iomem *)(unsigned long)
-			__ipc_mmio->data_remapped_address);
-		__ipc_mmio->data_remapped_address = 0;
-	}
-	/* Uninitialize the GBE, if it wasn't already done */
-	if (s_gbe_semaphore.hw_semaphore ||
-			s_gbe_semaphore.fw_semaphore) {
-		pw_pr_debug("Uninitializing gbe!\n");
-		if (s_gbe_semaphore.hw_semaphore)
-			iounmap(s_gbe_semaphore.hw_semaphore);
-
-		if (s_gbe_semaphore.fw_semaphore)
-			iounmap(s_gbe_semaphore.fw_semaphore);
-
-		memset(&s_gbe_semaphore, 0, sizeof(s_gbe_semaphore));
-	}
-	return PW_SUCCESS;
-}
-
-int sw_pch_mailbox_descriptor_reset_func_i(
-	const struct sw_driver_io_descriptor *descriptor)
-{
-	/* Unmap previously mapped memory here */
-	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	__pch_mailbox = (struct sw_driver_pch_mailbox_io_descriptor *)
-			&descriptor->pch_mailbox_descriptor;
-	if (__pch_mailbox->mtpmc_remapped_address) {
-		pw_pr_debug("unmapping addr 0x%llx\n",
-			__pch_mailbox->mtpmc_remapped_address);
-		iounmap((volatile void __iomem *)(unsigned long)
-			__pch_mailbox->mtpmc_remapped_address);
-		__pch_mailbox->mtpmc_remapped_address = 0;
-	}
-	if (__pch_mailbox->msg_full_sts_remapped_address) {
-		pw_pr_debug("unmapping addr 0x%llx\n",
-			__pch_mailbox->msg_full_sts_remapped_address);
-		iounmap((volatile void __iomem *)(unsigned long)
-			__pch_mailbox->msg_full_sts_remapped_address);
-		__pch_mailbox->msg_full_sts_remapped_address = 0;
-	}
-	if (__pch_mailbox->mfpmc_remapped_address) {
-		pw_pr_debug("unmapping addr 0x%llx\n",
-			__pch_mailbox->mfpmc_remapped_address);
-		iounmap((volatile void __iomem *)(unsigned long)
-			__pch_mailbox->mfpmc_remapped_address);
-		__pch_mailbox->mfpmc_remapped_address = 0;
-	}
-	return PW_SUCCESS;
-}
-
-int sw_mailbox_descriptor_reset_func_i(
-	const struct sw_driver_io_descriptor *descriptor)
-{
-	/* Unmap previously mapped memory here */
-	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
-
-	if (!descriptor) /* Should NEVER happen */
-		return -PW_ERROR;
-
-	__mailbox = (struct sw_driver_mailbox_io_descriptor *)
-			&descriptor->mailbox_descriptor;
-	if (!__mailbox->is_msr_type) {
-		if (__mailbox->interface_remapped_address) {
-			pw_pr_debug("unmapping addr 0x%llx\n",
-				__mailbox->interface_remapped_address);
-			iounmap((volatile void __iomem *)(unsigned long)
-				__mailbox->interface_remapped_address);
-			__mailbox->interface_remapped_address = 0;
-		}
-		if (__mailbox->data_remapped_address) {
-			pw_pr_debug("unmapping addr 0x%llx\n",
-				__mailbox->data_remapped_address);
-			iounmap((volatile void __iomem *)(unsigned long)
-				__mailbox->data_remapped_address);
-			__mailbox->data_remapped_address = 0;
-		}
-	}
-	return PW_SUCCESS;
-}
-
-#define NUM_HW_OPS SW_ARRAY_SIZE(s_hw_ops)
-#define FOR_EACH_HW_OP(idx, op)					\
-	for (idx = 0; idx < NUM_HW_OPS && (op =  &s_hw_ops[idx]); ++idx)
-
-int sw_register_ops_providers(void)
-{
-	size_t idx = 0;
-	const struct sw_hw_ops *op = NULL;
-
-	FOR_EACH_HW_OP(idx, op) {
-		if (op->name && sw_register_hw_op(op)) {
-			pw_pr_error(
-				"ERROR registering provider %s\n", op->name);
-			return -EIO;
-		}
-	}
-	return PW_SUCCESS;
-}
-
-void sw_free_ops_providers(void)
-{
-	/* NOP */
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/pci.h> /* "pci_get_domain_bus_and_slot" */
+#include <linux/delay.h> /* "udelay" */
+#include <asm/msr.h>
+#ifdef CONFIG_RPMSG_IPC
+	#include <asm/intel_mid_rpmsg.h>
+#endif /* CONFIG_RPMSG_IPC */
+
+#include "sw_types.h"
+#include "sw_kernel_defines.h"
+#include "sw_hardware_io.h"
+#include "sw_telem.h"
+#include "sw_cta.h"
+#include "sw_ops_provider.h"
+
+/*
+ * Compile time constants.
+ */
+/*
+ * Should we be doing 'direct' PCI reads and writes?
+ * '1' ==> YES, call "pci_{read,write}_config_dword()" directly
+ * '0' ==> NO, Use the "intel_mid_msgbus_{read32,write32}_raw()" API
+ *		(defined in 'intel_mid_pcihelpers.c')
+ */
+#define DO_DIRECT_PCI_READ_WRITE 0
+#if !IS_ENABLED(CONFIG_ANDROID) || !defined(CONFIG_X86_WANT_INTEL_MID)
+    /*
+     * 'intel_mid_pcihelpers.h' is probably not present -- force
+     * direct PCI calls in this case.
+     */
+	#undef DO_DIRECT_PCI_READ_WRITE
+	#define DO_DIRECT_PCI_READ_WRITE  1
+#endif
+#if !DO_DIRECT_PCI_READ_WRITE
+	#include <asm/intel_mid_pcihelpers.h>
+#endif
+
+#define SW_PCI_MSG_CTRL_REG 0x000000D0
+#define SW_PCI_MSG_DATA_REG 0x000000D4
+
+/*
+ *  NUM_RETRY & USEC_DELAY are used in PCH Mailbox (sw_read_pch_mailbox_info_i).
+ *  Tested on KBL + SPT-LP. May need to revisit.
+ */
+#define NUM_RETRY  100
+#define USEC_DELAY 100
+
+#define EXTCNF_CTRL 0xF00 /* offset for hw semaphore. */
+#define FWSM_CTRL 0x5B54 /* offset for fw semaphore */
+#define GBE_CTRL_OFFSET 0x34 /* GBE LPM offset */
+
+#define IS_HW_SEMAPHORE_SET(data) (data & (pw_u64_t)(0x1 << 6))
+#define IS_FW_SEMAPHORE_SET(data) (data & (pw_u64_t)0x1)
+/*
+ * Number of retries for mailbox configuration
+ */
+#define MAX_MAILBOX_ITERS 100
+
+/*
+ * Local data structures.
+ */
+/*
+ * TODO: separate into H/W and S/W IO?
+ */
+enum sw_io_type {
+	SW_IO_MSR		= 0,
+	SW_IO_IPC		= 1,
+	SW_IO_MMIO		= 2,
+	SW_IO_PCI		= 3,
+	SW_IO_CONFIGDB		= 4,
+	SW_IO_TRACE_ARGS	= 5,
+	SW_IO_WAKEUP		= 6,
+	SW_IO_SOCPERF		= 7,
+	SW_IO_PROC_NAME		= 8,
+	SW_IO_IRQ_NAME		= 9,
+	SW_IO_WAKELOCK		= 10,
+	SW_IO_TELEM		= 11,
+	SW_IO_PCH_MAILBOX	= 12,
+	SW_IO_MAILBOX		= 13,
+	SW_IO_CTA		= 14,
+	SW_IO_MAX		= 15,
+};
+
+/*
+ * "io_remapped" values for HW and FW semaphores
+ */
+static struct {
+	volatile void __iomem *hw_semaphore;
+	volatile void __iomem *fw_semaphore;
+} s_gbe_semaphore = {NULL, NULL};
+
+/*
+ * Function declarations.
+ */
+/*
+ * Exported by the SOCPERF driver.
+ */
+extern void __weak SOCPERF_Read_Data3(void *data_buffer);
+
+/*
+ * Init functions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Read functions.
+ */
+void sw_read_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_read_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+
+/*
+ * Write functions.
+ */
+void sw_write_msr_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_ipc_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_mmio_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_mailbox_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_pci_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_configdb_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_trace_args_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_wakeup_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+void sw_write_socperf_info_i(char *dst_vals, int cpu, const struct sw_driver_io_descriptor *descriptor, u16 counter_size_in_bytes);
+
+/*
+ * Print functions.
+ */
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Reset functions -- equal but opposite of init.
+ */
+int sw_ipc_mmio_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_reset_func_i(const struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Available functions.
+ */
+bool sw_socperf_available_i(void);
+
+/*
+ * Helper functions.
+ */
+u32 sw_platform_configdb_read32(u32 address);
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 ctrl_offset, u32 ctrl_value, u32 data_offset);
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function, u32 write_offset, u32 data_value);
+
+/*
+ * Table of collector operations.
+ */
+static const struct sw_hw_ops s_hw_ops[] = {
+	[SW_IO_MSR] = {
+		.name = "MSR",
+		.init = NULL,
+		.read = &sw_read_msr_info_i,
+		.write = &sw_write_msr_info_i,
+		.print = &sw_print_msr_io_descriptor,
+		.reset = NULL,
+		.available = NULL
+	},
+	[SW_IO_IPC] = {
+		.name = "IPC",
+		.init = &sw_ipc_mmio_descriptor_init_func_i,
+		.read = &sw_read_ipc_info_i,
+		.reset = &sw_ipc_mmio_descriptor_reset_func_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_MMIO] = {
+		.name = "MMIO",
+		.init = &sw_ipc_mmio_descriptor_init_func_i,
+		.read = &sw_read_mmio_info_i,
+		.write = &sw_write_mmio_info_i,
+		.reset = &sw_ipc_mmio_descriptor_reset_func_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_PCI] = {
+		.name = "PCI",
+		.read = &sw_read_pci_info_i,
+		.write = &sw_write_pci_info_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_CONFIGDB] = {
+		.name = "CONFIGDB",
+		.read = &sw_read_configdb_info_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_WAKEUP] = {
+		.name = "WAKEUP",
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_SOCPERF] = {
+		.name = "SOCPERF",
+		.read = &sw_read_socperf_info_i,
+		.available = &sw_socperf_available_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_PROC_NAME] = {
+		.name = "PROC-NAME",
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_IRQ_NAME] = {
+		.name = "IRQ-NAME",
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_WAKELOCK] = {
+		.name = "WAKELOCK",
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_TELEM] = {
+		.name = "TELEM",
+		.init = &sw_telem_init_func,
+		.read = &sw_read_telem_info,
+		.reset = &sw_reset_telem,
+		.available = &sw_telem_available,
+		.post_config = &sw_telem_post_config,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_PCH_MAILBOX] = {
+		.name = "PCH-MAILBOX",
+		.init = &sw_pch_mailbox_descriptor_init_func_i,
+		.read = &sw_read_pch_mailbox_info_i,
+		.reset = &sw_pch_mailbox_descriptor_reset_func_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+	[SW_IO_MAILBOX] = {
+		.name = "MAILBOX",
+		.init = &sw_mailbox_descriptor_init_func_i,
+		.read = &sw_read_mailbox_info_i,
+		.write = &sw_write_mailbox_info_i,
+		.reset = &sw_mailbox_descriptor_reset_func_i,
+		/* Other fields are don't care (will be set to NULL) */
+	},
+        [SW_IO_CTA] = {
+		.name = "CTA",
+		.reg = &sw_cta_register,
+		.read = &sw_read_cta_info,
+		.available = &sw_cta_available,
+		.unreg = &sw_cta_unregister,
+		/* Other fields are don't care (will be set to NULL) */
+        },
+	[SW_IO_MAX] = {
+		.name = NULL,
+		/* Other fields are don't care (will be set to NULL) */
+	}
+};
+
+/*
+ * Function definitions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor)
+{
+	/* Perform any required 'io_remap' calls here */
+	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+	u64 data_address = 0;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	if (descriptor->collection_type == SW_IO_IPC)
+		__ipc_mmio = &descriptor->ipc_descriptor;
+	else
+		__ipc_mmio = &descriptor->mmio_descriptor;
+
+	pw_pr_debug("cmd = %u, sub-cmd = %u, data_addr = 0x%llx\n"
+		__ipc_mmio->command, __ipc_mmio->sub_command,
+		__ipc_mmio->data_address);
+	data_address = __ipc_mmio->data_address;
+
+	if (!data_address)
+		return PW_SUCCESS;
+
+	__ipc_mmio->data_remapped_address =
+		(pw_u64_t)(unsigned long)ioremap_nocache(
+			(unsigned long)data_address,
+			descriptor->counter_size_in_bytes);
+	if ((void *)(unsigned long)__ipc_mmio->data_remapped_address == NULL)
+		return -EIO;
+
+	pw_pr_debug("mapped addr 0x%llx\n", __ipc_mmio->data_remapped_address);
+	if ((__ipc_mmio->is_gbe) &&
+		(!s_gbe_semaphore.hw_semaphore ||
+			!s_gbe_semaphore.fw_semaphore) &&
+				(data_address >= GBE_CTRL_OFFSET)) {
+
+		u64 hw_addr = (data_address - GBE_CTRL_OFFSET) + EXTCNF_CTRL;
+		u64 fw_addr = (data_address - GBE_CTRL_OFFSET) + FWSM_CTRL;
+		pw_pr_debug("Initializing GBE semaphore\n");
+
+		s_gbe_semaphore.hw_semaphore =
+			ioremap_nocache(
+				(unsigned long)hw_addr,
+				descriptor->counter_size_in_bytes);
+		s_gbe_semaphore.fw_semaphore =
+			ioremap_nocache(
+				(unsigned long)fw_addr,
+				descriptor->counter_size_in_bytes);
+		if (s_gbe_semaphore.hw_semaphore == NULL ||
+			s_gbe_semaphore.fw_semaphore == NULL) {
+			pw_pr_error(
+				"couldn't mmap hw/fw semaphores for GBE MMIO op!\n");
+			return -EIO;
+		}
+		pw_pr_debug(
+			"GBE has hw_sem = 0x%llx, fw_sem = 0x%llx, size = %u\n",
+			(unsigned long long)s_gbe_semaphore.hw_semaphore,
+			(unsigned long long)s_gbe_semaphore.fw_semaphore,
+			descriptor->counter_size_in_bytes);
+	}
+
+	return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor)
+{
+	/* Perform any required 'io_remap' calls here */
+	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	__pch_mailbox = &descriptor->pch_mailbox_descriptor;
+	pw_pr_debug("pch_mailbox data_addr = 0x%llx\n",
+		(unsigned long long)__pch_mailbox->data_address);
+	if (__pch_mailbox->mtpmc_address) {
+		__pch_mailbox->mtpmc_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)__pch_mailbox->mtpmc_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)
+			__pch_mailbox->mtpmc_remapped_address == NULL)
+			return -PW_ERROR;
+
+		pw_pr_debug("mtpmc_mapped addr 0x%llx\n",
+			__pch_mailbox->mtpmc_remapped_address);
+	}
+	if (__pch_mailbox->msg_full_sts_address) {
+		__pch_mailbox->msg_full_sts_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)
+					__pch_mailbox->msg_full_sts_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)
+			__pch_mailbox->msg_full_sts_remapped_address == NULL)
+			return -PW_ERROR;
+
+		pw_pr_debug("msg_full_sts_mapped addr 0x%llx\n",
+			__pch_mailbox->msg_full_sts_address);
+	}
+	if (__pch_mailbox->mfpmc_address) {
+		__pch_mailbox->mfpmc_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)__pch_mailbox->mfpmc_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)
+			__pch_mailbox->mfpmc_remapped_address == NULL)
+			return -PW_ERROR;
+
+		pw_pr_debug("mfpmc_mapped addr 0x%llx\n",
+			__pch_mailbox->mfpmc_remapped_address);
+	}
+	return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor)
+{
+	/* Perform any required 'io_remap' calls here */
+	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	__mailbox = &descriptor->mailbox_descriptor;
+
+	pw_pr_debug(
+		"type = %u, interface_address = 0x%llx, data_address = 0x%llx\n",
+		__mailbox->is_msr_type, __mailbox->interface_address,
+		__mailbox->data_address);
+
+	if (!__mailbox->is_msr_type) {
+		if (__mailbox->interface_address) {
+			__mailbox->interface_remapped_address =
+				(pw_u64_t)(unsigned long)ioremap_nocache(
+					(unsigned long)__mailbox->interface_address,
+					descriptor->counter_size_in_bytes);
+			if ((void *)(unsigned long)
+				__mailbox->interface_remapped_address == NULL) {
+				pw_pr_error(
+					"Couldn't iomap interface_address = 0x%llx\n",
+					__mailbox->interface_address);
+				return -PW_ERROR;
+			}
+		}
+		if (__mailbox->data_address) {
+			__mailbox->data_remapped_address =
+				(pw_u64_t)(unsigned long)ioremap_nocache(
+					(unsigned long)__mailbox->data_address,
+					descriptor->counter_size_in_bytes);
+			if ((void *)(unsigned long)
+				__mailbox->data_remapped_address == NULL) {
+				pw_pr_error(
+					"Couldn't iomap data_address = 0x%llx\n",
+					__mailbox->data_address);
+				return -PW_ERROR;
+			}
+		}
+		pw_pr_debug("OK, mapped addr 0x%llx, 0x%llx\n",
+			__mailbox->interface_remapped_address,
+			__mailbox->data_remapped_address);
+	}
+	return PW_SUCCESS;
+}
+
+void sw_read_msr_info_i(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	u64 address = descriptors->msr_descriptor.address;
+	u32 l = 0, h = 0;
+
+	if (likely(cpu == RAW_CPU()))
+		rdmsr_safe((unsigned long)address, &l, &h);
+	else {
+		if (rdmsr_safe_on_cpu(
+			cpu, (unsigned long)address, &l, &h)) {
+			pw_pr_warn(
+				"Failed to read MSR address = 0x%llx\n",
+				address);
+				l = 0; h = 0;
+		}
+	}
+	switch (counter_size_in_bytes) {
+	case 4:
+		*((u32 *)dst_vals) = l;
+		break;
+	case 8:
+		*((u64 *)dst_vals) = ((u64)h << 32) | l;
+		pw_pr_debug(
+			"read MSR value = %llu\n", *((u64 *)dst_vals));
+		break;
+	default:
+		break;
+	}
+}
+
+#ifdef CONFIG_RPMSG_IPC
+	#define SW_DO_IPC(cmd, sub_cmd) rpmsg_send_generic_simple_command(cmd, sub_cmd)
+#else
+	#define SW_DO_IPC(cmd, sub_cmd) (-ENODEV)
+#endif // CONFIG_RPMSG_IPC
+
+void sw_read_ipc_info_i(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	u16 cmd = descriptors->ipc_descriptor.command;
+	u16 sub_cmd = descriptors->ipc_descriptor.sub_command;
+	unsigned long remapped_address = (unsigned long)
+		descriptors->ipc_descriptor.data_remapped_address;
+
+	if (cmd || sub_cmd) {
+		pw_pr_debug("EXECUTING IPC Cmd = %u, %u\n", cmd, sub_cmd);
+		if (SW_DO_IPC(cmd, sub_cmd)) {
+			pw_pr_error("ERROR running IPC command(s)\n");
+			return;
+		}
+	}
+
+	if (remapped_address) {
+		pw_pr_debug("COPYING MMIO size %u\n", counter_size_in_bytes);
+		memcpy(dst_vals, (void *)remapped_address,
+				counter_size_in_bytes);
+	}
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+static void sw_read_gbe_mmio_info_i(
+	char *dst_vals,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	u32 hw_val = 0, fw_val = 0;
+	unsigned long remapped_address = (unsigned long)
+		descriptors->mmio_descriptor.data_remapped_address;
+	u64 write_value = descriptors->write_value;
+
+	memset(dst_vals, 0, counter_size_in_bytes);
+
+	pw_pr_debug(
+		"hw_sem = 0x%llx, fw_sem = 0x%llx, addr = 0x%lx, dst_vals = 0x%lx, size = %u\n",
+		(unsigned long long)s_gbe_semaphore.hw_semaphore,
+		(unsigned long long)s_gbe_semaphore.fw_semaphore,
+		remapped_address,
+		(unsigned long)dst_vals,
+		counter_size_in_bytes);
+	if (!s_gbe_semaphore.hw_semaphore || !s_gbe_semaphore.fw_semaphore ||
+		!remapped_address)
+		return;
+
+	memcpy_fromio(&hw_val, s_gbe_semaphore.hw_semaphore, sizeof(hw_val));
+	memcpy_fromio(&fw_val, s_gbe_semaphore.fw_semaphore, sizeof(fw_val));
+	pw_pr_debug("HW_VAL = 0x%lx, FW_VAL = 0x%lx\n",
+		(unsigned long)hw_val, (unsigned long)fw_val);
+	if (!IS_HW_SEMAPHORE_SET(hw_val) && !IS_FW_SEMAPHORE_SET(fw_val)) {
+		memcpy_toio((volatile void __iomem *)remapped_address,
+				&write_value, 4 /* counter_size_in_bytes*/);
+		memcpy_fromio(dst_vals, (volatile void __iomem *)remapped_address,
+				counter_size_in_bytes);
+	}
+}
+void sw_read_mmio_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	unsigned long remapped_address =
+	(unsigned long)descriptors->mmio_descriptor.data_remapped_address;
+
+	/* MMIO for GBE requires a mailbox-like operation */
+	if (descriptors->mmio_descriptor.is_gbe)
+		sw_read_gbe_mmio_info_i(dst_vals, descriptors, counter_size_in_bytes);
+	else {
+		if (remapped_address)
+			memcpy_fromio(dst_vals, (volatile void __iomem *)remapped_address,
+				counter_size_in_bytes);
+	}
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+void sw_read_pch_mailbox_info_i(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_pch_mailbox_io_descriptor *pch_mailbox =
+					 &descriptor->pch_mailbox_descriptor;
+	u32 address = pch_mailbox->data_address;
+	u64 mtpmc_remapped_address = pch_mailbox->mtpmc_remapped_address;
+	u64 msg_full_sts_remapped_address =
+		pch_mailbox->msg_full_sts_remapped_address;
+	u64 mfpmc_remapped_address = pch_mailbox->mfpmc_remapped_address;
+
+	/*
+	 * write address of desired device counter to request from PMC
+	 * (shift and add 2 to format device offset)
+	 */
+	if (mtpmc_remapped_address) {
+		int iter = 0;
+		u32 written_val = 0;
+		/* shift and add 2 to format device offset */
+		u32 write_value = (address << 16) + 2;
+
+		memcpy_toio(
+			(volatile void __iomem *)(unsigned long)mtpmc_remapped_address,
+			 &write_value, 4 /*counter_size_in_bytes*/);
+		/*
+		 * Check if address has been written using a while loop in
+		 * order to wait for the PMC to consume that address and to
+		 * introduce sufficient delay so that the message full
+		 * status bit has time to flip. This should ensure all is
+		 * ready when begin the wait loop for it to turn 0, which
+		 * indicates the value is available to be read.
+		 * (This fixes problem where values being read were huge.)
+		 */
+		do {
+			memcpy_fromio(&written_val,
+				(volatile void __iomem *)(unsigned long)mtpmc_remapped_address,
+				4 /*counter_size_in_bytes*/);
+			pw_pr_debug(
+				"DEBUG: written_val = 0x%x, address = 0x%x\n",
+				written_val, address);
+			udelay(USEC_DELAY);
+		} while ((written_val >> 16) != address && ++iter < NUM_RETRY);
+	}
+
+
+	/*
+	 * wait for PMC to set status indicating that device
+	 * counter is available for read.
+	 */
+	if (msg_full_sts_remapped_address) {
+		u32 status_wait = 0;
+		int iter = 0;
+
+		do {
+			memcpy_fromio(&status_wait,
+				(volatile void __iomem*)(unsigned long)
+					msg_full_sts_remapped_address,
+				4 /*counter_size_in_bytes*/);
+			pw_pr_debug("DEBUG: status_wait = 0x%x\n",
+				status_wait);
+			udelay(USEC_DELAY);
+		} while ((status_wait & 0x01000000) >> 24 &&
+			++iter < NUM_RETRY);
+	}
+
+	/*
+	 * read device counter
+	 */
+	if (mfpmc_remapped_address) {
+		memcpy_fromio(dst_vals,
+			(volatile void __iomem*)(unsigned long)mfpmc_remapped_address,
+			4 /*counter_size_in_bytes*/);
+		pw_pr_debug("DEBUG: read value = 0x%x\n",
+			*((pw_u32_t *)dst_vals));
+	}
+}
+
+void sw_read_mailbox_info_i(char *dst_vals, int cpu,
+		const struct sw_driver_io_descriptor *descriptor,
+		u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_mailbox_io_descriptor *mailbox =
+		&descriptor->mailbox_descriptor;
+	unsigned long interface_address = mailbox->interface_address;
+	unsigned long interface_remapped_address = mailbox->interface_remapped_address;
+	unsigned long data_address = mailbox->data_address;
+	size_t iter = 0;
+
+	if (mailbox->is_msr_type) {
+		u64 command = 0;
+
+		rdmsrl_safe(interface_address, &command);
+		command &= mailbox->command_mask;
+		command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
+		wrmsrl_safe(interface_address, command);
+		do {
+			udelay(1);
+			rdmsrl_safe(interface_address, &command);
+		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
+				++iter < MAX_MAILBOX_ITERS);
+		if (iter >= MAX_MAILBOX_ITERS) {
+			pw_pr_error("Couldn't write to BIOS mailbox\n");
+			command = MAX_UNSIGNED_64_BIT_VALUE;
+		} else
+			rdmsrl_safe(data_address, &command);
+		switch (counter_size_in_bytes) {
+		case 4:
+			*((u32 *)dst_vals) = (u32)command;
+			break;
+		case 8:
+			*((u64 *)dst_vals) = command;
+			break;
+		default:
+			pw_pr_error("Invalid counter size %u, assuming 4 bytes!\n", counter_size_in_bytes);
+			*((u32 *)dst_vals) = (u32)command;
+			break;
+		}
+	}  else {
+		u32 command = 0;
+		/* Always use 4 bytes, regardless of 'counter_size_in_bytes' */
+		const size_t counter_size = 4;
+
+		memcpy_fromio(&command,
+			(volatile void __iomem *)(unsigned long)interface_remapped_address,
+			sizeof(command));
+		command &= mailbox->command_mask;
+		command |= (u32)mailbox->command |
+				(u32)0x1 << mailbox->run_busy_bit;
+		memcpy_toio((volatile void __iomem *)(unsigned long)interface_remapped_address,
+			&command, sizeof(command));
+		do {
+			udelay(1);
+			memcpy_fromio(&command,
+				(volatile void __iomem *)(unsigned long)interface_remapped_address,
+				sizeof(command));
+		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
+				++iter < MAX_MAILBOX_ITERS);
+		if (iter >= MAX_MAILBOX_ITERS) {
+			pw_pr_error("Couldn't write to BIOS mailbox\n");
+			command = MAX_UNSIGNED_32_BIT_VALUE;
+		} else
+			memcpy_fromio(&command,
+				(volatile void __iomem *)(unsigned long)mailbox->data_remapped_address,
+				counter_size);
+
+		*((u32 *)dst_vals) = command;
+	}
+}
+
+void sw_read_pci_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	u32 bus = descriptors->pci_descriptor.bus;
+	u32 device = descriptors->pci_descriptor.device;
+	u32 function = descriptors->pci_descriptor.function;
+	u32 offset = descriptors->pci_descriptor.offset;
+	u32 data32 = 0;
+	u64 data64 = 0;
+
+	switch (counter_size_in_bytes) {
+	case 4:
+		data32 = sw_platform_pci_read32(bus, device, function,
+			0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */,
+			offset /* DATA-OFFSET */);
+		*((u32 *)dst_vals) = data32;
+		break;
+	case 8:
+		data64 = sw_platform_pci_read64(bus, device, function,
+			0 /* CTRL-OFFSET */, 0 /* CTRL-DATA, don't care */,
+			offset /* DATA-OFFSET */);
+		*((u64 *)dst_vals) = data64;
+		break;
+	default:
+		pw_pr_error("ERROR: invalid read size = %u\n",
+				counter_size_in_bytes);
+	}
+}
+void sw_read_configdb_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	pw_u32_t address = descriptors->configdb_descriptor.address;
+	u32 data = sw_platform_configdb_read32(address);
+
+	pw_pr_debug(
+		"ADDRESS = 0x%x, CPU = %d, dst_vals = %p, counter size = %u, data = %u\n",
+		address, cpu, dst_vals, counter_size_in_bytes, data);
+	/*
+	 * 'counter_size_in_bytes' is ignored, for now.
+	 */
+	*((u32 *)dst_vals) = data;
+}
+void sw_read_socperf_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptors,
+	u16 counter_size_in_bytes)
+{
+	u64 *socperf_buffer = (u64 *)dst_vals;
+
+	memset(socperf_buffer, 0, counter_size_in_bytes);
+	SOCPERF_Read_Data3(socperf_buffer);
+
+}
+
+/**
+ * Decide if the socperf interface is available for use
+ * @returns	 true if available
+ */
+bool sw_socperf_available_i(void)
+{
+	bool retVal = false;
+
+	/* The symbol below is weak.  We return 1 if we have a definition
+	 * for this socperf-driver-supplied symbol, or 0 if only the
+	 * weak definition exists. This test will suffice to detect if
+	 * the socperf driver is loaded.
+	 */
+	if (SOCPERF_Read_Data3) {
+		pw_pr_debug("INFO: SoCPerf support in ON!\n");
+		retVal = true;
+	} else
+		pw_pr_debug("INFO: SoCPerf support is OFF!\n");
+
+	return retVal;
+}
+
+
+/**
+ * sw_platform_configdb_read32 - for reading PCI space through config registers
+ *							   of the platform.
+ * @address: An address in the PCI space
+ *
+ * Returns: the value read from address.
+ */
+u32 sw_platform_configdb_read32(u32 address)
+{
+	u32 read_value = 0;
+#if DO_DIRECT_PCI_READ_WRITE
+	read_value = sw_platform_pci_read32(
+		0/*bus*/, 0/*device*/, 0/*function*/,
+		SW_PCI_MSG_CTRL_REG/*ctrl-offset*/, address/*ctrl-value*/,
+		SW_PCI_MSG_DATA_REG/*data-offset*/);
+#else /* !DO_DIRECT_PCI_READ_WRITE */
+	read_value = intel_mid_msgbus_read32_raw(address);
+#endif /* if DO_DIRECT_PCI_READ_WRITE */
+	pw_pr_debug("address = %u, value = %u\n", address, read_value);
+	return read_value;
+}
+
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function,
+		u32 write_offset, u32 write_value, u32 read_offset)
+{
+	u32 read_value = 0;
+	struct pci_dev *pci_root =
+		pci_get_domain_bus_and_slot(0, bus,
+			/* 0, PCI_DEVFN(0, 0)); */
+			PCI_DEVFN(device, function));
+
+	if (!pci_root)
+		return 0; /* Application will verify the data */
+
+	if (write_offset)
+		pci_write_config_dword(pci_root,
+			/* SW_PCI_MSG_CTRL_REG, address); */
+			write_offset, write_value);
+
+	pci_read_config_dword(pci_root,
+		/* SW_PCI_MSG_DATA_REG, &read_value); */
+		read_offset, &read_value);
+	return read_value;
+}
+
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 write_offset,
+	u32 write_value, u32 read_offset)
+{
+	u32 lo = sw_platform_pci_read32(
+		bus, device, function, 0 /* CTRL-OFFSET */,
+		0 /* CTRL-DATA, don't care */,
+		read_offset /* DATA-OFFSET */);
+	u32 hi = sw_platform_pci_read32(
+		bus, device, function, 0 /* CTRL-OFFSET */,
+		0 /* CTRL-DATA, don't care */,
+		read_offset + 4 /* DATA-OFFSET */);
+
+	return ((u64)hi << 32) | lo;
+}
+
+void sw_write_msr_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes)
+{
+	u64 write_value = descriptor->write_value;
+	u64 address = descriptor->msr_descriptor.address;
+
+	pw_pr_debug(
+		"ADDRESS = 0x%llx, CPU = %d, counter size = %u, value = %llu\n",
+		address, cpu, counter_size_in_bytes, write_value);
+	if (likely(cpu == RAW_CPU()))
+		wrmsrl_safe((unsigned long)address, write_value);
+	else {
+		u32 l = write_value & 0xffffffff;
+		u32 h = (write_value >> 32) & 0xffffffff;
+
+		wrmsr_safe_on_cpu(cpu, (u32)address, l, h);
+	}
+};
+
+void sw_write_mmio_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes)
+{
+	unsigned long remapped_address = (unsigned long)
+		descriptor->mmio_descriptor.data_remapped_address;
+	u64 write_value = descriptor->write_value;
+
+	if (remapped_address)
+		memcpy_toio((volatile void __iomem *)remapped_address, &write_value,
+			counter_size_in_bytes);
+
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+};
+
+void sw_write_mailbox_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_mailbox_io_descriptor *mailbox =
+					&descriptor->mailbox_descriptor;
+	unsigned long interface_address = mailbox->interface_address;
+	unsigned long interface_remapped_address =
+					mailbox->interface_remapped_address;
+	unsigned long data_address = mailbox->data_address;
+	u64 data = descriptor->write_value;
+	size_t iter = 0;
+
+	if (mailbox->is_msr_type) {
+		u64 command = 0;
+
+		rdmsrl_safe(interface_address, &command);
+		command &= mailbox->command_mask;
+		command |= mailbox->command |
+				(u64)0x1 << mailbox->run_busy_bit;
+		wrmsrl_safe(data_address, data);
+		wrmsrl_safe(interface_address, command);
+		do {
+			rdmsrl_safe(interface_address, &command);
+		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
+				++iter < MAX_MAILBOX_ITERS);
+	} else {
+		u32 command = 0;
+
+		memcpy_fromio(&command,
+			(volatile void __iomem *)(unsigned long)interface_remapped_address,
+			sizeof(command));
+		command &= mailbox->command_mask;
+		command |= (u32)mailbox->command |
+			(u32)0x1 << mailbox->run_busy_bit;
+		memcpy_toio((volatile void __iomem *)(unsigned long)
+			mailbox->data_remapped_address,
+			&data, sizeof(data));
+		memcpy_toio((volatile void __iomem *)(unsigned long)interface_remapped_address,
+			&command, sizeof(command));
+		do {
+			memcpy_fromio(&command, (volatile void __iomem *)(unsigned long)
+				interface_remapped_address, sizeof(command));
+		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
+				++iter < MAX_MAILBOX_ITERS);
+	}
+}
+
+void sw_write_pci_info_i(char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes)
+{
+	u32 bus = descriptor->pci_descriptor.bus;
+	u32 device = descriptor->pci_descriptor.device;
+	u32 function = descriptor->pci_descriptor.function;
+	u32 offset = descriptor->pci_descriptor.offset;
+	u32 write_value = (u32)descriptor->write_value;
+	/*
+	 * 'counter_size_in_bytes' is ignored for now.
+	 */
+	if (!sw_platform_pci_write32(bus, device, function, offset,
+			write_value))
+		pw_pr_error("ERROR writing to PCI B/D/F/O %u/%u/%u/%u\n",
+			bus, device, function, offset);
+	else
+		pw_pr_debug("OK, successfully wrote to PCI B/D/F/O %u/%u/%u/%u\n",
+			bus, device, function, offset);
+
+};
+
+/*
+ * Write to PCI space via config registers.
+ */
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function,
+	u32 write_offset, u32 data_value)
+{
+	struct pci_dev *pci_root =
+		pci_get_domain_bus_and_slot(0, bus,
+			PCI_DEVFN(device, function));/* 0, PCI_DEVFN(0, 0)); */
+
+	if (!pci_root)
+		return false;
+
+
+	pci_write_config_dword(pci_root, write_offset, data_value);
+
+	return true;
+};
+
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor)
+{
+	if (!descriptor)
+		return -PW_ERROR;
+
+	pw_pr_debug("MSR address = 0x%llx\n",
+		descriptor->msr_descriptor.address);
+	return PW_SUCCESS;
+}
+
+int sw_ipc_mmio_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	if (descriptor->collection_type == SW_IO_IPC)
+		__ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)
+			&descriptor->ipc_descriptor;
+	else
+		__ipc_mmio = (struct sw_driver_ipc_mmio_io_descriptor *)
+			&descriptor->mmio_descriptor;
+
+	if (__ipc_mmio->data_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			__ipc_mmio->data_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+			__ipc_mmio->data_remapped_address);
+		__ipc_mmio->data_remapped_address = 0;
+	}
+	/* Uninitialize the GBE, if it wasn't already done */
+	if (s_gbe_semaphore.hw_semaphore ||
+			s_gbe_semaphore.fw_semaphore) {
+		pw_pr_debug("Uninitializing gbe!\n");
+		if (s_gbe_semaphore.hw_semaphore)
+			iounmap(s_gbe_semaphore.hw_semaphore);
+
+		if (s_gbe_semaphore.fw_semaphore)
+			iounmap(s_gbe_semaphore.fw_semaphore);
+
+		memset(&s_gbe_semaphore, 0, sizeof(s_gbe_semaphore));
+	}
+	return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	__pch_mailbox = (struct sw_driver_pch_mailbox_io_descriptor *)
+			&descriptor->pch_mailbox_descriptor;
+	if (__pch_mailbox->mtpmc_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			__pch_mailbox->mtpmc_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+			__pch_mailbox->mtpmc_remapped_address);
+		__pch_mailbox->mtpmc_remapped_address = 0;
+	}
+	if (__pch_mailbox->msg_full_sts_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			__pch_mailbox->msg_full_sts_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+			__pch_mailbox->msg_full_sts_remapped_address);
+		__pch_mailbox->msg_full_sts_remapped_address = 0;
+	}
+	if (__pch_mailbox->mfpmc_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			__pch_mailbox->mfpmc_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+			__pch_mailbox->mfpmc_remapped_address);
+		__pch_mailbox->mfpmc_remapped_address = 0;
+	}
+	return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+
+	if (!descriptor) /* Should NEVER happen */
+		return -PW_ERROR;
+
+	__mailbox = (struct sw_driver_mailbox_io_descriptor *)
+			&descriptor->mailbox_descriptor;
+	if (!__mailbox->is_msr_type) {
+		if (__mailbox->interface_remapped_address) {
+			pw_pr_debug("unmapping addr 0x%llx\n",
+				__mailbox->interface_remapped_address);
+			iounmap((volatile void __iomem *)(unsigned long)
+				__mailbox->interface_remapped_address);
+			__mailbox->interface_remapped_address = 0;
+		}
+		if (__mailbox->data_remapped_address) {
+			pw_pr_debug("unmapping addr 0x%llx\n",
+				__mailbox->data_remapped_address);
+			iounmap((volatile void __iomem *)(unsigned long)
+				__mailbox->data_remapped_address);
+			__mailbox->data_remapped_address = 0;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+#define NUM_HW_OPS SW_ARRAY_SIZE(s_hw_ops)
+#define FOR_EACH_HW_OP(idx, op)					\
+	for (idx = 0; idx < NUM_HW_OPS && (op =  &s_hw_ops[idx]); ++idx)
+
+int sw_register_ops_providers(void)
+{
+	size_t idx = 0;
+	const struct sw_hw_ops *op = NULL;
+
+	FOR_EACH_HW_OP(idx, op) {
+		if (op->name && sw_register_hw_op(op)) {
+			pw_pr_error(
+				"ERROR registering provider %s\n", op->name);
+			return -EIO;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+void sw_free_ops_providers(void)
+{
+	size_t idx = 0;
+	const struct sw_hw_ops *op = NULL;
+
+	FOR_EACH_HW_OP(idx, op) {
+		if (op->unreg) {
+			(*op->unreg)(); /* Return value is don't care */
+		}
+	}
+}
diff --git a/drivers/platform/x86/socwatch/sw_output_buffer.c b/drivers/platform/x86/socwatch/sw_output_buffer.c
index eaccc29f18ea..11ef7fcb2b69 100644
--- a/drivers/platform/x86/socwatch/sw_output_buffer.c
+++ b/drivers/platform/x86/socwatch/sw_output_buffer.c
@@ -1,838 +1,838 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include "sw_internal.h"
-#include "sw_output_buffer.h"
-#include "sw_kernel_defines.h"
-#include "sw_mem.h"
-#include "sw_lock_defs.h"
-#include "sw_overhead_measurements.h"
-
-/* -------------------------------------------------
- * Compile time constants and macros.
- * -------------------------------------------------
- */
-#define NUM_SEGS_PER_BUFFER 2 /* MUST be pow 2! */
-#define NUM_SEGS_PER_BUFFER_MASK (NUM_SEGS_PER_BUFFER - 1)
-/*
- * The size of the 'buffer' data array in each segment.
- */
-#define SW_SEG_DATA_SIZE (sw_buffer_alloc_size)
-/*
- * Min size of per-cpu output buffers.
- */
-#define SW_MIN_SEG_SIZE_BYTES (1 << 10) /* 1kB */
-#define SW_MIN_OUTPUT_BUFFER_SIZE (SW_MIN_SEG_SIZE_BYTES * NUM_SEGS_PER_BUFFER)
-/*
- * A symbolic constant for an empty buffer index.
- */
-#define EMPTY_SEG (-1)
-/*
- * How much space is available in a given segment?
- */
-#define EMPTY_TSC ((u64)-1)
-#define SEG_IS_FULL(seg) ({bool __full = false; \
-	smp_mb(); /* memory access ordering */\
-	__full = ((seg)->is_full != EMPTY_TSC); \
-	__full; })
-#define SEG_SET_FULL(seg, tsc) do { \
-	(seg)->is_full = (tsc); \
-	smp_mb(); /* memory access ordering */\
-} while (0)
-#define SEG_SET_EMPTY(seg) do { \
-	barrier(); \
-	(seg)->bytes_written = 0; \
-	SEG_SET_FULL(seg, EMPTY_TSC); \
-} while (0)
-#define SPACE_AVAIL(seg) (SW_SEG_DATA_SIZE - (seg)->bytes_written)
-#define SEG_IS_EMPTY(seg) (SPACE_AVAIL(seg) == SW_SEG_DATA_SIZE)
-
-#define GET_OUTPUT_BUFFER(cpu) (&per_cpu_output_buffers[(cpu)])
-/*
- * Convenience macro: iterate over each segment in a per-cpu output buffer.
- */
-#define for_each_segment(i) for (i = 0; i < NUM_SEGS_PER_BUFFER; ++i)
-#define for_each_seg(buffer, seg)					 \
-	for (int i = 0;							 \
-		i < NUM_SEGS_PER_BUFFER && (seg = (buffer)->segments[i]);\
-		++i)
-/*
- * How many buffers are we using?
- */
-#define GET_NUM_OUTPUT_BUFFERS() (sw_max_num_cpus + 1)
-/*
- * Convenience macro: iterate over each per-cpu output buffer.
- */
-#define for_each_output_buffer(i) for (i = 0; i < GET_NUM_OUTPUT_BUFFERS(); ++i)
-
-/* -------------------------------------------------
- * Local data structures.
- * -------------------------------------------------
- */
-struct sw_data_buffer {
-	u64 is_full;
-	u32 bytes_written;
-	char *buffer;
-} __packed;
-#define SW_SEG_HEADER_SIZE() (sizeof(struct sw_data_buffer) - sizeof(char *))
-
-struct sw_output_buffer {
-	struct sw_data_buffer buffers[NUM_SEGS_PER_BUFFER];
-	int buff_index;
-	u32 produced_samples;
-	u32 dropped_samples;
-	int last_seg_read;
-	unsigned int mem_alloc_size;
-	unsigned long free_pages;
-} ____cacheline_aligned_in_smp;
-
-/* *************************************************
- * For circular buffer (continuous profiling)
- * *************************************************
- */
-static char *output_buffer;
-
-struct buffer {
-	union {
-		char *data;
-		unsigned long free_pages;
-	};
-	size_t read_index, write_index;
-	unsigned long size;
-};
-SW_DECLARE_RWLOCK(sw_continuous_lock);
-
-static struct buffer buffer; /* TODO: rename */
-
-/* -------------------------------------------------
- * Function declarations.
- * -------------------------------------------------
- */
-extern u64 sw_timestamp(void);
-
-/* -------------------------------------------------
- * Variable definitions.
- * -------------------------------------------------
- */
-u64 sw_num_samples_produced = 0, sw_num_samples_dropped = 0;
-int sw_max_num_cpus = -1;
-
-DECLARE_OVERHEAD_VARS(sw_produce_generic_msg_i);
-/*
- * Per-cpu output buffers.
- */
-static struct sw_output_buffer *per_cpu_output_buffers;
-/*
- * Variables for book keeping.
- */
-static volatile int sw_last_cpu_read = -1;
-static volatile s32 sw_last_mask = -1;
-/*
- * Lock for the polled buffer.
- */
-SW_DECLARE_SPINLOCK(sw_polled_lock);
-/*
- * Buffer allocation size.
- */
-unsigned long sw_buffer_alloc_size = (1 << 16); /* 64 KB */
-
-/* -------------------------------------------------
- * Function definitions.
- * -------------------------------------------------
- */
-
-/* *************************************************
- * For circular buffer (continuous profiling)
- * *************************************************
- */
-#define MIN(x, y) ((x) <= (y) ? (x) : (y))
-
-#define IS_BUFFER_EMPTY(buffer)					\
-	((buffer).write_index == (buffer).read_index)
-#define IS_BUFFER_FULL(buffer)					\
-	((buffer).write_index ==				\
-		((buffer).read_index + 1) & (buffer.size - 1))
-
-static inline size_t get_space_available(struct buffer *buffer)
-{
-	size_t read = 0, write = 0;
-
-	smp_mb(); /* memory access ordering */
-	read = buffer->read_index;
-	write = buffer->write_index;
-	if (write < read)
-		return read - write;
-
-	return (buffer->size - write) + read;
-}
-
-static inline size_t get_data_available(struct buffer *buffer)
-{
-	size_t read = 0, write = 0;
-
-	smp_mb(); /* memory access ordering */
-	read = buffer->read_index;
-	write = buffer->write_index;
-	if (read <= write)
-		return write - read;
-
-	return (buffer->size - read) + write;
-}
-
-static void copy_wraparound(const char *src, size_t src_size, size_t *index)
-{
-	size_t buff_size_left = buffer.size - *index;
-	size_t to_write = MIN(buff_size_left, src_size);
-	size_t _index = *index;
-
-	if (src_size < buff_size_left) {
-		memcpy(&buffer.data[_index], src, src_size);
-		_index += src_size;
-	} else {
-		memcpy(&buffer.data[_index], src, to_write);
-		_index = 0;
-		src += to_write;
-		to_write = src_size - to_write;
-		memcpy(&buffer.data[_index], src, to_write);
-		_index += to_write;
-		pw_pr_debug("DEBUG: wrap memcpy\n");
-	}
-	*index = (*index + src_size) & (buffer.size - 1);
-}
-
-static int enqueue_data(struct sw_driver_msg *msg, enum sw_wakeup_action action)
-{
-	size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
-	bool wrapped = false;
-
-	msg->tsc = 0;
-
-	READ_LOCK(sw_continuous_lock);
-	while (true) {
-		size_t old_write_index = buffer.write_index;
-		size_t new_write_index = (old_write_index + size) &
-						(buffer.size - 1);
-
-		if (get_space_available(&buffer) < size)
-			break;
-
-		if (CAS32(&buffer.write_index, old_write_index,
-				new_write_index)) {
-			msg->tsc = sw_timestamp();
-			wrapped = new_write_index <= old_write_index;
-			/* First copy header */
-			copy_wraparound((const char *)msg,
-				SW_DRIVER_MSG_HEADER_SIZE(), &old_write_index);
-			/* Then copy payload */
-			copy_wraparound((const char *)msg->p_payload,
-				msg->payload_len, &old_write_index);
-			break;
-		}
-	}
-	READ_UNLOCK(sw_continuous_lock);
-	if (!msg->tsc)
-		pw_pr_error("ERROR: couldn't enqueue data\n");
-	if (wrapped)
-		pw_pr_debug("DEBUG: wrapped!\n");
-
-	return msg->tsc ? 0 : -1;
-}
-
-/*
- * Returns # of bytes successfully consumed on success
- * 0 on EOF (no error condition)
- */
-static size_t consume_buffer(void *dest, size_t bytes_to_read)
-{
-	size_t read_index = 0, write_index = 0, dst_index = 0;
-	size_t to_read = 0;
-	bool wrapped = false;
-	size_t read_size = bytes_to_read;
-	unsigned long bytes_not_copied = 0;
-	struct sw_driver_continuous_collect data = {0};
-
-	WRITE_LOCK(sw_continuous_lock);
-	smp_mb(); /* memory access ordering */
-	read_index = buffer.read_index;
-	write_index = buffer.write_index;
-	/* EXE sends size as header + payload; we only want payload */
-	read_size -= SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE();
-	data.collection_size = to_read =
-		MIN(read_size, get_data_available(&buffer));
-	pw_pr_debug(
-		"DEBUG: read = %zu, write = %zu, avail = %zu, to_read = %zu\n",
-		read_index, write_index, get_data_available(&buffer), to_read);
-	while (to_read) {
-		size_t curr_read = to_read;
-
-		if (read_index + to_read > buffer.size) {
-			curr_read = buffer.size - read_index;
-			wrapped = true;
-			pw_pr_debug(
-				"DEBUG: read = %zu, to_read = %zu, curr_read = %zu, buffer.size = %lu, WRAPPED!\n",
-				read_index, to_read, curr_read, buffer.size);
-		}
-		memcpy(&output_buffer[dst_index],
-			&buffer.data[read_index], curr_read);
-		read_index = (read_index + curr_read) & (buffer.size - 1);
-		to_read -= curr_read;
-		dst_index += curr_read;
-	}
-	buffer.read_index = read_index;
-	smp_mb(); /* memory access ordering */
-	pw_pr_debug("DEBUG: read at end of while = %zu\n", buffer.read_index);
-	WRITE_UNLOCK(sw_continuous_lock);
-
-	/*
-	 * Call 'copy_to_user' instead of 'sw_copy_to_user' since
-	 * sw_copy_to_user expects to see a 'struct uio' while this
-	 * is called from an IOCTL which does NOT have a 'struct uio'
-	 */
-	bytes_not_copied =
-	copy_to_user(dest, (char *)&data,
-		SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE());
-	if (bytes_not_copied)
-		return 0;
-
-	pw_pr_debug("DEBUG: collection size = %u\n", data.collection_size);
-	if (data.collection_size) {
-		bytes_not_copied =
-			copy_to_user(dest +
-				SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE(),
-				output_buffer, data.collection_size);
-		if (bytes_not_copied)
-			return 0;
-
-	}
-	return data.collection_size;
-}
-
-long initialize_circular_buffer(size_t size)
-{
-	size_t alloc_size = size, read_size = size;
-	/*
-	 * We require a power of two size
-	 */
-	pw_pr_debug("DEBUG: old alloc size = %zu\n", alloc_size);
-	if ((alloc_size & (alloc_size - 1)) != 0)
-		alloc_size = 1 << fls(alloc_size);
-
-	pw_pr_debug("DEBUG: new alloc size = %zu\n", alloc_size);
-	/* Create double-sized buffer */
-	alloc_size <<= 1;
-	pw_pr_debug("DEBUG: double alloc size = %zu\n", alloc_size);
-	memset(&buffer, 0, sizeof(buffer));
-	buffer.free_pages =
-		sw_allocate_pages(GFP_KERNEL | __GFP_ZERO, alloc_size);
-	if (!buffer.free_pages) {
-		pw_pr_error("Couldn't allocate space for buffer!\n");
-		return -ENOMEM;
-	}
-	buffer.read_index = buffer.write_index = 0;
-	buffer.size = alloc_size;
-	SW_INIT_RWLOCK(sw_continuous_lock);
-	/*
-	 * Create temp output buffer
-	 */
-	output_buffer = vmalloc(read_size);
-	if (!output_buffer) {
-		pw_pr_error(
-			"Couldn't create temporary buffer for data output!\n");
-		return -ENOMEM;
-	}
-	return 0;
-}
-
-void reset_output_buffers(void)
-{
-	buffer.read_index = buffer.write_index = 0;
-}
-
-
-void destroy_circular_buffer(void)
-{
-	if (buffer.free_pages) {
-		sw_release_pages(buffer.free_pages, buffer.size);
-		buffer.free_pages = 0;
-	}
-	if (output_buffer) {
-		vfree(output_buffer);
-		output_buffer = NULL;
-	}
-	SW_DESTROY_RWLOCK(sw_continuous_lock);
-	pw_pr_debug("DEBUG: read = %zu, write = %zu\n", buffer.read_index,
-	buffer.write_index);
-}
-
-/* *************************************************
- * For per-cpu buffers (non circular)
- * *************************************************
- */
-
-static char *reserve_seg_space_i(size_t size, int cpu, bool *should_wakeup,
-	u64 *reservation_tsc)
-{
-	struct sw_output_buffer *buffer = GET_OUTPUT_BUFFER(cpu);
-	int i = 0;
-	int buff_index = buffer->buff_index;
-	char *dst = NULL;
-
-	if (buff_index < 0 || buff_index >= NUM_SEGS_PER_BUFFER)
-		goto prod_seg_done;
-
-	for_each_segment(i) {
-		struct sw_data_buffer *seg = &buffer->buffers[buff_index];
-
-		if (SEG_IS_FULL(seg) == false) {
-			if (SPACE_AVAIL(seg) >= size) {
-				*reservation_tsc = sw_timestamp();
-				dst = &seg->buffer[seg->bytes_written];
-				seg->bytes_written += size;
-				smp_mb(); /* memory access ordering */
-				buffer->buff_index = buff_index;
-				buffer->produced_samples++;
-				goto prod_seg_done;
-			}
-			SEG_SET_FULL(seg, sw_timestamp());
-		}
-		buff_index = CIRCULAR_INC(buff_index, NUM_SEGS_PER_BUFFER_MASK);
-		*should_wakeup = true;
-	}
-prod_seg_done:
-	if (!dst)
-		buffer->dropped_samples++;
-
-	return dst;
-};
-
-#ifdef CONFIG_PREEMPT_COUNT
-static int produce_polled_msg(struct sw_driver_msg *msg,
-	enum sw_wakeup_action action)
-{
-	int cpu = GET_POLLED_CPU();
-	bool should_wakeup = false;
-	int retVal = PW_SUCCESS;
-
-	if (!msg)
-		return -PW_ERROR;
-
-	pw_pr_debug("POLLED! cpu = %d\n", cpu);
-	LOCK(sw_polled_lock);
-	{
-		size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
-		char *dst = reserve_seg_space_i(size, cpu,
-						&should_wakeup, &msg->tsc);
-
-		if (dst) {
-			/*
-			 * Assign a special CPU number to this CPU.
-			 * This is OK, because messages enqueued in this buffer
-			 * are always CPU agnostic (otherwise they would
-			 * be invoked from within a preempt_disable()d context
-			 * in 'sw_handle_collector_node_i()', which ensures
-			 * they will be enqueued within the
-			 * 'sw_produce_generic_msg_on_cpu()' function).
-			 */
-			msg->cpuidx = cpu;
-			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
-			dst += SW_DRIVER_MSG_HEADER_SIZE();
-			memcpy(dst, msg->p_payload, msg->payload_len);
-		} else {
-			pw_pr_debug("NO space in polled msg!\n");
-			retVal = -PW_ERROR;
-		}
-	}
-	UNLOCK(sw_polled_lock);
-	if (unlikely(should_wakeup))
-		sw_wakeup_reader(action);
-
-	return retVal;
-};
-#endif /* CONFIG_PREEMPT_COUNT */
-
-static int sw_produce_generic_msg_i(struct sw_driver_msg *msg,
-	enum sw_wakeup_action action)
-{
-	int retval = PW_SUCCESS;
-	bool should_wakeup = false;
-	int cpu = -1;
-	unsigned long flags = 0;
-
-	if (!msg) {
-		pw_pr_error("ERROR: CANNOT produce a NULL msg!\n");
-		return -PW_ERROR;
-	}
-
-	/* Check if we need to use circular buffer */
-	if (output_buffer)
-		return enqueue_data(msg, action);
-
-#ifdef CONFIG_PREEMPT_COUNT
-	if (!in_atomic())
-		return produce_polled_msg(msg, action);
-#endif
-
-	cpu = sw_get_cpu(&flags);
-	{
-		size_t size = msg->payload_len +
-				SW_DRIVER_MSG_HEADER_SIZE();
-		char *dst = reserve_seg_space_i(size, cpu, &should_wakeup,
-						&msg->tsc);
-
-		if (likely(dst)) {
-			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
-			dst += SW_DRIVER_MSG_HEADER_SIZE();
-			memcpy(dst, msg->p_payload, msg->payload_len);
-		} else
-			retval = -PW_ERROR;
-	}
-	sw_put_cpu(flags);
-
-	if (unlikely(should_wakeup))
-		sw_wakeup_reader(action);
-
-	return retval;
-};
-
-int sw_produce_generic_msg(struct sw_driver_msg *msg,
-	enum sw_wakeup_action action)
-{
-	return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_produce_generic_msg_i,
-		msg, action);
-};
-
-static int sw_init_per_cpu_buffers_i(unsigned long per_cpu_mem_size)
-{
-	int cpu = -1;
-
-	per_cpu_output_buffers =
-	(struct sw_output_buffer *)sw_kmalloc(sizeof(struct sw_output_buffer) *
-	GET_NUM_OUTPUT_BUFFERS(), GFP_KERNEL | __GFP_ZERO);
-	if (per_cpu_output_buffers == NULL) {
-		pw_pr_error(
-			"ERROR allocating space for per-cpu output buffers!\n");
-		sw_destroy_per_cpu_buffers();
-		return -PW_ERROR;
-	}
-	for_each_output_buffer(cpu) {
-		struct sw_output_buffer *buffer = &per_cpu_output_buffers[cpu];
-		char *buff = NULL;
-		int i = 0;
-
-		buffer->mem_alloc_size = per_cpu_mem_size;
-		buffer->free_pages = sw_allocate_pages(GFP_KERNEL | __GFP_ZERO,
-			(unsigned int)per_cpu_mem_size);
-		if (buffer->free_pages == 0) {
-			pw_pr_error("ERROR allocating pages for buffer [%d]!\n",
-				cpu);
-			sw_destroy_per_cpu_buffers();
-			return -PW_ERROR;
-		}
-		buff = (char *)buffer->free_pages;
-		for_each_segment(i) {
-			buffer->buffers[i].buffer = (char *)buff;
-			buff += SW_SEG_DATA_SIZE;
-		}
-	}
-	pw_pr_debug("PER_CPU_MEM_SIZE = %lu, order = %u\n",
-	(unsigned long)per_cpu_mem_size, get_order(per_cpu_mem_size));
-	return PW_SUCCESS;
-};
-
-int sw_init_per_cpu_buffers(void)
-{
-	unsigned int per_cpu_mem_size = sw_get_output_buffer_size();
-
-	pw_pr_debug("Buffer alloc size = %ld\n", sw_buffer_alloc_size);
-
-	if (GET_NUM_OUTPUT_BUFFERS() <= 0) {
-		pw_pr_error("ERROR: max # output buffers= %d\n",
-			GET_NUM_OUTPUT_BUFFERS());
-		return -PW_ERROR;
-	}
-
-	pw_pr_debug("DEBUG: sw_max_num_cpus = %d, num output buffers = %d\n",
-	sw_max_num_cpus, GET_NUM_OUTPUT_BUFFERS());
-
-	/*
-	 * Try to allocate per-cpu buffers. If allocation fails, decrease
-	 * buffer size and retry. Stop trying if size drops below 2KB
-	 * (which means 1KB for each buffer).
-	 */
-	while (per_cpu_mem_size >= SW_MIN_OUTPUT_BUFFER_SIZE &&
-		sw_init_per_cpu_buffers_i(per_cpu_mem_size)) {
-		pw_pr_debug("WARNING: couldn't allocate per-cpu buffers with size %u -- trying smaller size!\n",
-			per_cpu_mem_size);
-		sw_buffer_alloc_size >>= 1;
-		per_cpu_mem_size = sw_get_output_buffer_size();
-	}
-
-	if (unlikely(per_cpu_output_buffers == NULL)) {
-		pw_pr_error("ERROR: couldn't allocate space for per-cpu output buffers!\n");
-		return -PW_ERROR;
-	}
-	/*
-	 * Initialize our locks.
-	 */
-	SW_INIT_SPINLOCK(sw_polled_lock);
-
-	pw_pr_debug("OK, allocated per-cpu buffers with size = %lu\n",
-		(unsigned long)per_cpu_mem_size);
-
-	if (sw_init_reader_queue()) {
-		pw_pr_error("ERROR initializing reader subsys\n");
-		return -PW_ERROR;
-	}
-
-	return PW_SUCCESS;
-};
-
-void sw_destroy_per_cpu_buffers(void)
-{
-	int cpu = -1;
-
-	/*
-	 * Perform lock finalization.
-	 */
-	SW_DESTROY_SPINLOCK(sw_polled_lock);
-
-	if (per_cpu_output_buffers != NULL) {
-		for_each_output_buffer(cpu) {
-			struct sw_output_buffer *buffer =
-					&per_cpu_output_buffers[cpu];
-
-			if (buffer->free_pages != 0) {
-				sw_release_pages(buffer->free_pages,
-					buffer->mem_alloc_size);
-				buffer->free_pages = 0;
-			}
-		}
-		sw_kfree(per_cpu_output_buffers);
-		per_cpu_output_buffers = NULL;
-	}
-};
-
-void sw_reset_per_cpu_buffers(void)
-{
-	int cpu = 0, i = 0;
-
-	for_each_output_buffer(cpu) {
-		struct sw_output_buffer *buffer = GET_OUTPUT_BUFFER(cpu);
-
-		buffer->buff_index = buffer->dropped_samples =
-			buffer->produced_samples = 0;
-		buffer->last_seg_read = -1;
-
-		for_each_segment(i) {
-			struct sw_data_buffer *seg = &buffer->buffers[i];
-
-			memset(seg->buffer, 0, SW_SEG_DATA_SIZE);
-			SEG_SET_EMPTY(seg);
-		}
-	}
-	sw_last_cpu_read = -1;
-	sw_last_mask = -1;
-	pw_pr_debug("OK, reset per-cpu output buffers!\n");
-	/*
-	 * Reset circular buffer if it has been allocated
-	 */
-	if (output_buffer)
-		buffer.read_index = buffer.write_index = 0;
-
-};
-
-bool sw_any_seg_full(u32 *val, bool is_flush_mode)
-{
-	int num_visited = 0, i = 0;
-
-	if (!val) {
-		pw_pr_error("ERROR: NULL ptrs in %s!\n", __func__);
-		return false;
-	}
-
-	*val = SW_NO_DATA_AVAIL_MASK;
-	pw_pr_debug("Checking for full seg: val = %u, flush = %s\n",
-		 *val, GET_BOOL_STRING(is_flush_mode));
-	for_each_output_buffer(num_visited) {
-		int min_seg = EMPTY_SEG, non_empty_seg = EMPTY_SEG;
-		u64 min_tsc = EMPTY_TSC;
-		struct sw_output_buffer *buffer = NULL;
-
-		if (++sw_last_cpu_read >= GET_NUM_OUTPUT_BUFFERS())
-			sw_last_cpu_read = 0;
-
-		buffer = GET_OUTPUT_BUFFER(sw_last_cpu_read);
-		for_each_segment(i) {
-			struct sw_data_buffer *seg = &buffer->buffers[i];
-			u64 seg_tsc = seg->is_full;
-
-			if (SEG_IS_EMPTY(seg))
-				continue;
-
-			non_empty_seg = i;
-			if (seg_tsc < min_tsc) {
-				/*
-				 * Can only happen if seg was full, provided
-				 * 'EMPTY_TSC' is set to "(u64)-1"
-				 */
-				min_tsc = seg_tsc;
-				min_seg = i;
-			}
-		}
-		if (min_seg != EMPTY_SEG) {
-			*val = (sw_last_cpu_read & 0xffff) << 16 |
-				(min_seg & 0xffff);
-			return true;
-		} else if (is_flush_mode && non_empty_seg != EMPTY_SEG) {
-			*val = (sw_last_cpu_read & 0xffff) << 16 |
-				(non_empty_seg & 0xffff);
-			return true;
-		}
-	}
-	/*
-	 * Reaches here only if there's no data to be read.
-	 */
-	if (is_flush_mode) {
-		/*
-		 * We've drained all buffers and need to tell the userspace
-		 * application there isn't any data. Unfortunately, we can't
-		 * just return a 'zero' value for the mask (because that could
-		 * also indicate that segment # 0 of cpu #0 has data).
-		 */
-		*val = SW_ALL_WRITES_DONE_MASK;
-		return true;
-	}
-	return false;
-};
-
-/*
- * Returns: number of bytes consumed on SUCCESS, 0 on EOF, negative
- * error code on FAILURE
- */
-ssize_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read)
-{
-	int which_cpu = -1, which_seg = -1;
-	unsigned long bytes_not_copied = 0;
-	struct sw_output_buffer *buff = NULL;
-	struct sw_data_buffer *seg = NULL;
-	size_t bytes_read = 0;
-
-	/* Check if we need to use circular buffer */
-	if (output_buffer)
-		return (ssize_t)consume_buffer(buffer, bytes_to_read);
-
-	if (!sw_check_output_buffer_params(buffer, bytes_to_read,
-			SW_SEG_DATA_SIZE)) {
-		pw_pr_error("ERROR: invalid params to \"%s\"!\n", __func__);
-		return -EIO;
-	}
-
-	which_cpu = mask >> 16; which_seg = mask & 0xffff;
-	pw_pr_debug("CONSUME: cpu = %d, seg = %d\n", which_cpu, which_seg);
-	if (which_seg >= NUM_SEGS_PER_BUFFER) {
-		pw_pr_error(
-			"Error: which_seg (%d) >= NUM_SEGS_PER_BUFFER (%d)\n",
-			which_seg, NUM_SEGS_PER_BUFFER);
-		return -EIO;
-	}
-	/*
-	 * OK to access unlocked; either the segment is FULL, or no collection
-	 * is ongoing. In either case, we're GUARANTEED no producer is touching
-	 * this segment.
-	 */
-	buff = GET_OUTPUT_BUFFER(which_cpu);
-	seg = &buff->buffers[which_seg];
-
-	bytes_not_copied = sw_copy_to_user(buffer,
-		seg->buffer, seg->bytes_written); /* dst, src */
-
-	if (likely(bytes_not_copied == 0))
-		bytes_read = seg->bytes_written;
-	else {
-		pw_pr_error("Warning: couldn't copy %lu bytes\n",
-			bytes_not_copied);
-		bytes_read = 0;
-	}
-	SEG_SET_EMPTY(seg);
-	return bytes_read;
-}
-
-unsigned int sw_get_output_buffer_size(void)
-{
-	return (sw_buffer_alloc_size * NUM_SEGS_PER_BUFFER);
-};
-
-void sw_count_samples_produced_dropped(void)
-{
-	int cpu = 0;
-
-	sw_num_samples_produced = sw_num_samples_dropped = 0;
-	if (per_cpu_output_buffers == NULL)
-		return;
-
-	for_each_output_buffer(cpu) {
-		struct sw_output_buffer *buff = GET_OUTPUT_BUFFER(cpu);
-
-		sw_num_samples_dropped += buff->dropped_samples;
-		sw_num_samples_produced += buff->produced_samples;
-	}
-};
-
-void sw_print_output_buffer_overheads(void)
-{
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_produce_generic_msg_i,
-		"PRODUCE_GENERIC_MSG");
-	sw_print_reader_stats();
-};
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_kernel_defines.h"
+#include "sw_mem.h"
+#include "sw_lock_defs.h"
+#include "sw_overhead_measurements.h"
+
+/* -------------------------------------------------
+ * Compile time constants and macros.
+ * -------------------------------------------------
+ */
+#define NUM_SEGS_PER_BUFFER 2 /* MUST be pow 2! */
+#define NUM_SEGS_PER_BUFFER_MASK (NUM_SEGS_PER_BUFFER - 1)
+/*
+ * The size of the 'buffer' data array in each segment.
+ */
+#define SW_SEG_DATA_SIZE (sw_buffer_alloc_size)
+/*
+ * Min size of per-cpu output buffers.
+ */
+#define SW_MIN_SEG_SIZE_BYTES (1 << 10) /* 1kB */
+#define SW_MIN_OUTPUT_BUFFER_SIZE (SW_MIN_SEG_SIZE_BYTES * NUM_SEGS_PER_BUFFER)
+/*
+ * A symbolic constant for an empty buffer index.
+ */
+#define EMPTY_SEG (-1)
+/*
+ * How much space is available in a given segment?
+ */
+#define EMPTY_TSC ((u64)-1)
+#define SEG_IS_FULL(seg) ({bool __full = false; \
+	smp_mb(); /* memory access ordering */\
+	__full = ((seg)->is_full != EMPTY_TSC); \
+	__full; })
+#define SEG_SET_FULL(seg, tsc) do { \
+	(seg)->is_full = (tsc); \
+	smp_mb(); /* memory access ordering */\
+} while (0)
+#define SEG_SET_EMPTY(seg) do { \
+	barrier(); \
+	(seg)->bytes_written = 0; \
+	SEG_SET_FULL(seg, EMPTY_TSC); \
+} while (0)
+#define SPACE_AVAIL(seg) (SW_SEG_DATA_SIZE - (seg)->bytes_written)
+#define SEG_IS_EMPTY(seg) (SPACE_AVAIL(seg) == SW_SEG_DATA_SIZE)
+
+#define GET_OUTPUT_BUFFER(cpu) (&per_cpu_output_buffers[(cpu)])
+/*
+ * Convenience macro: iterate over each segment in a per-cpu output buffer.
+ */
+#define for_each_segment(i) for (i = 0; i < NUM_SEGS_PER_BUFFER; ++i)
+#define for_each_seg(buffer, seg)					 \
+	for (int i = 0;							 \
+		i < NUM_SEGS_PER_BUFFER && (seg = (buffer)->segments[i]);\
+		++i)
+/*
+ * How many buffers are we using?
+ */
+#define GET_NUM_OUTPUT_BUFFERS() (sw_max_num_cpus + 1)
+/*
+ * Convenience macro: iterate over each per-cpu output buffer.
+ */
+#define for_each_output_buffer(i) for (i = 0; i < GET_NUM_OUTPUT_BUFFERS(); ++i)
+
+/* -------------------------------------------------
+ * Local data structures.
+ * -------------------------------------------------
+ */
+struct sw_data_buffer {
+	u64 is_full;
+	u32 bytes_written;
+	char *buffer;
+} __packed;
+#define SW_SEG_HEADER_SIZE() (sizeof(struct sw_data_buffer) - sizeof(char *))
+
+struct sw_output_buffer {
+	struct sw_data_buffer buffers[NUM_SEGS_PER_BUFFER];
+	int buff_index;
+	u32 produced_samples;
+	u32 dropped_samples;
+	int last_seg_read;
+	unsigned int mem_alloc_size;
+	unsigned long free_pages;
+} ____cacheline_aligned_in_smp;
+
+/* *************************************************
+ * For circular buffer (continuous profiling)
+ * *************************************************
+ */
+static char *output_buffer;
+
+struct buffer {
+	union {
+		char *data;
+		unsigned long free_pages;
+	};
+	size_t read_index, write_index;
+	unsigned long size;
+};
+SW_DECLARE_RWLOCK(sw_continuous_lock);
+
+static struct buffer buffer; /* TODO: rename */
+
+/* -------------------------------------------------
+ * Function declarations.
+ * -------------------------------------------------
+ */
+extern u64 sw_timestamp(void);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+u64 sw_num_samples_produced = 0, sw_num_samples_dropped = 0;
+int sw_max_num_cpus = -1;
+
+DECLARE_OVERHEAD_VARS(sw_produce_generic_msg_i);
+/*
+ * Per-cpu output buffers.
+ */
+static struct sw_output_buffer *per_cpu_output_buffers;
+/*
+ * Variables for book keeping.
+ */
+static volatile int sw_last_cpu_read = -1;
+static volatile s32 sw_last_mask = -1;
+/*
+ * Lock for the polled buffer.
+ */
+SW_DECLARE_SPINLOCK(sw_polled_lock);
+/*
+ * Buffer allocation size.
+ */
+unsigned long sw_buffer_alloc_size = (1 << 16); /* 64 KB */
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+
+/* *************************************************
+ * For circular buffer (continuous profiling)
+ * *************************************************
+ */
+#define MIN(x, y) ((x) <= (y) ? (x) : (y))
+
+#define IS_BUFFER_EMPTY(buffer)					\
+	((buffer).write_index == (buffer).read_index)
+#define IS_BUFFER_FULL(buffer)					\
+	((buffer).write_index ==				\
+		((buffer).read_index + 1) & (buffer.size - 1))
+
+static inline size_t get_space_available(struct buffer *buffer)
+{
+	size_t read = 0, write = 0;
+
+	smp_mb(); /* memory access ordering */
+	read = buffer->read_index;
+	write = buffer->write_index;
+	if (write < read)
+		return read - write;
+
+	return (buffer->size - write) + read;
+}
+
+static inline size_t get_data_available(struct buffer *buffer)
+{
+	size_t read = 0, write = 0;
+
+	smp_mb(); /* memory access ordering */
+	read = buffer->read_index;
+	write = buffer->write_index;
+	if (read <= write)
+		return write - read;
+
+	return (buffer->size - read) + write;
+}
+
+static void copy_wraparound(const char *src, size_t src_size, size_t *index)
+{
+	size_t buff_size_left = buffer.size - *index;
+	size_t to_write = MIN(buff_size_left, src_size);
+	size_t _index = *index;
+
+	if (src_size < buff_size_left) {
+		memcpy(&buffer.data[_index], src, src_size);
+		_index += src_size;
+	} else {
+		memcpy(&buffer.data[_index], src, to_write);
+		_index = 0;
+		src += to_write;
+		to_write = src_size - to_write;
+		memcpy(&buffer.data[_index], src, to_write);
+		_index += to_write;
+		pw_pr_debug("DEBUG: wrap memcpy\n");
+	}
+	*index = (*index + src_size) & (buffer.size - 1);
+}
+
+static int enqueue_data(struct sw_driver_msg *msg, enum sw_wakeup_action action)
+{
+	size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
+	bool wrapped = false;
+
+	msg->tsc = 0;
+
+	READ_LOCK(sw_continuous_lock);
+	while (true) {
+		size_t old_write_index = buffer.write_index;
+		size_t new_write_index = (old_write_index + size) &
+						(buffer.size - 1);
+
+		if (get_space_available(&buffer) < size)
+			break;
+
+		if (CAS32(&buffer.write_index, old_write_index,
+				new_write_index)) {
+			msg->tsc = sw_timestamp();
+			wrapped = new_write_index <= old_write_index;
+			/* First copy header */
+			copy_wraparound((const char *)msg,
+				SW_DRIVER_MSG_HEADER_SIZE(), &old_write_index);
+			/* Then copy payload */
+			copy_wraparound((const char *)msg->p_payload,
+				msg->payload_len, &old_write_index);
+			break;
+		}
+	}
+	READ_UNLOCK(sw_continuous_lock);
+	if (!msg->tsc)
+		pw_pr_error("ERROR: couldn't enqueue data\n");
+	if (wrapped)
+		pw_pr_debug("DEBUG: wrapped!\n");
+
+	return msg->tsc ? 0 : -1;
+}
+
+/*
+ * Returns # of bytes successfully consumed on success
+ * 0 on EOF (no error condition)
+ */
+static size_t consume_buffer(void *dest, size_t bytes_to_read)
+{
+	size_t read_index = 0, write_index = 0, dst_index = 0;
+	size_t to_read = 0;
+	bool wrapped = false;
+	size_t read_size = bytes_to_read;
+	unsigned long bytes_not_copied = 0;
+	struct sw_driver_continuous_collect data = {0};
+
+	WRITE_LOCK(sw_continuous_lock);
+	smp_mb(); /* memory access ordering */
+	read_index = buffer.read_index;
+	write_index = buffer.write_index;
+	/* EXE sends size as header + payload; we only want payload */
+	read_size -= SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE();
+	data.collection_size = to_read =
+		MIN(read_size, get_data_available(&buffer));
+	pw_pr_debug(
+		"DEBUG: read = %zu, write = %zu, avail = %zu, to_read = %zu\n",
+		read_index, write_index, get_data_available(&buffer), to_read);
+	while (to_read) {
+		size_t curr_read = to_read;
+
+		if (read_index + to_read > buffer.size) {
+			curr_read = buffer.size - read_index;
+			wrapped = true;
+			pw_pr_debug(
+				"DEBUG: read = %zu, to_read = %zu, curr_read = %zu, buffer.size = %lu, WRAPPED!\n",
+				read_index, to_read, curr_read, buffer.size);
+		}
+		memcpy(&output_buffer[dst_index],
+			&buffer.data[read_index], curr_read);
+		read_index = (read_index + curr_read) & (buffer.size - 1);
+		to_read -= curr_read;
+		dst_index += curr_read;
+	}
+	buffer.read_index = read_index;
+	smp_mb(); /* memory access ordering */
+	pw_pr_debug("DEBUG: read at end of while = %zu\n", buffer.read_index);
+	WRITE_UNLOCK(sw_continuous_lock);
+
+	/*
+	 * Call 'copy_to_user' instead of 'sw_copy_to_user' since
+	 * sw_copy_to_user expects to see a 'struct uio' while this
+	 * is called from an IOCTL which does NOT have a 'struct uio'
+	 */
+	bytes_not_copied =
+	copy_to_user(dest, (char *)&data,
+		SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE());
+	if (bytes_not_copied)
+		return 0;
+
+	pw_pr_debug("DEBUG: collection size = %u\n", data.collection_size);
+	if (data.collection_size) {
+		bytes_not_copied =
+			copy_to_user(dest +
+				SW_DRIVER_CONTINUOUS_COLLECT_HEADER_SIZE(),
+				output_buffer, data.collection_size);
+		if (bytes_not_copied)
+			return 0;
+
+	}
+	return data.collection_size;
+}
+
+long initialize_circular_buffer(size_t size)
+{
+	size_t alloc_size = size, read_size = size;
+	/*
+	 * We require a power of two size
+	 */
+	pw_pr_debug("DEBUG: old alloc size = %zu\n", alloc_size);
+	if ((alloc_size & (alloc_size - 1)) != 0)
+		alloc_size = 1 << fls(alloc_size);
+
+	pw_pr_debug("DEBUG: new alloc size = %zu\n", alloc_size);
+	/* Create double-sized buffer */
+	alloc_size <<= 1;
+	pw_pr_debug("DEBUG: double alloc size = %zu\n", alloc_size);
+	memset(&buffer, 0, sizeof(buffer));
+	buffer.free_pages =
+		sw_allocate_pages(GFP_KERNEL | __GFP_ZERO, alloc_size);
+	if (!buffer.free_pages) {
+		pw_pr_error("Couldn't allocate space for buffer!\n");
+		return -ENOMEM;
+	}
+	buffer.read_index = buffer.write_index = 0;
+	buffer.size = alloc_size;
+	SW_INIT_RWLOCK(sw_continuous_lock);
+	/*
+	 * Create temp output buffer
+	 */
+	output_buffer = vmalloc(read_size);
+	if (!output_buffer) {
+		pw_pr_error(
+			"Couldn't create temporary buffer for data output!\n");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+void reset_output_buffers(void)
+{
+	buffer.read_index = buffer.write_index = 0;
+}
+
+
+void destroy_circular_buffer(void)
+{
+	if (buffer.free_pages) {
+		sw_release_pages(buffer.free_pages, buffer.size);
+		buffer.free_pages = 0;
+	}
+	if (output_buffer) {
+		vfree(output_buffer);
+		output_buffer = NULL;
+	}
+	SW_DESTROY_RWLOCK(sw_continuous_lock);
+	pw_pr_debug("DEBUG: read = %zu, write = %zu\n", buffer.read_index,
+	buffer.write_index);
+}
+
+/* *************************************************
+ * For per-cpu buffers (non circular)
+ * *************************************************
+ */
+
+static char *reserve_seg_space_i(size_t size, int cpu, bool *should_wakeup,
+	u64 *reservation_tsc)
+{
+	struct sw_output_buffer *buffer = GET_OUTPUT_BUFFER(cpu);
+	int i = 0;
+	int buff_index = buffer->buff_index;
+	char *dst = NULL;
+
+	if (buff_index < 0 || buff_index >= NUM_SEGS_PER_BUFFER)
+		goto prod_seg_done;
+
+	for_each_segment(i) {
+		struct sw_data_buffer *seg = &buffer->buffers[buff_index];
+
+		if (SEG_IS_FULL(seg) == false) {
+			if (SPACE_AVAIL(seg) >= size) {
+				*reservation_tsc = sw_timestamp();
+				dst = &seg->buffer[seg->bytes_written];
+				seg->bytes_written += size;
+				smp_mb(); /* memory access ordering */
+				buffer->buff_index = buff_index;
+				buffer->produced_samples++;
+				goto prod_seg_done;
+			}
+			SEG_SET_FULL(seg, sw_timestamp());
+		}
+		buff_index = CIRCULAR_INC(buff_index, NUM_SEGS_PER_BUFFER_MASK);
+		*should_wakeup = true;
+	}
+prod_seg_done:
+	if (!dst)
+		buffer->dropped_samples++;
+
+	return dst;
+};
+
+#ifdef CONFIG_PREEMPT_COUNT
+static int produce_polled_msg(struct sw_driver_msg *msg,
+	enum sw_wakeup_action action)
+{
+	int cpu = GET_POLLED_CPU();
+	bool should_wakeup = false;
+	int retVal = PW_SUCCESS;
+
+	if (!msg)
+		return -PW_ERROR;
+
+	pw_pr_debug("POLLED! cpu = %d\n", cpu);
+	LOCK(sw_polled_lock);
+	{
+		size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
+		char *dst = reserve_seg_space_i(size, cpu,
+						&should_wakeup, &msg->tsc);
+
+		if (dst) {
+			/*
+			 * Assign a special CPU number to this CPU.
+			 * This is OK, because messages enqueued in this buffer
+			 * are always CPU agnostic (otherwise they would
+			 * be invoked from within a preempt_disable()d context
+			 * in 'sw_handle_collector_node_i()', which ensures
+			 * they will be enqueued within the
+			 * 'sw_produce_generic_msg_on_cpu()' function).
+			 */
+			msg->cpuidx = cpu;
+			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
+			dst += SW_DRIVER_MSG_HEADER_SIZE();
+			memcpy(dst, msg->p_payload, msg->payload_len);
+		} else {
+			pw_pr_debug("NO space in polled msg!\n");
+			retVal = -PW_ERROR;
+		}
+	}
+	UNLOCK(sw_polled_lock);
+	if (unlikely(should_wakeup))
+		sw_wakeup_reader(action);
+
+	return retVal;
+};
+#endif /* CONFIG_PREEMPT_COUNT */
+
+static int sw_produce_generic_msg_i(struct sw_driver_msg *msg,
+	enum sw_wakeup_action action)
+{
+	int retval = PW_SUCCESS;
+	bool should_wakeup = false;
+	int cpu = -1;
+	unsigned long flags = 0;
+
+	if (!msg) {
+		pw_pr_error("ERROR: CANNOT produce a NULL msg!\n");
+		return -PW_ERROR;
+	}
+
+	/* Check if we need to use circular buffer */
+	if (output_buffer)
+		return enqueue_data(msg, action);
+
+#ifdef CONFIG_PREEMPT_COUNT
+	if (!in_atomic())
+		return produce_polled_msg(msg, action);
+#endif
+
+	cpu = sw_get_cpu(&flags);
+	{
+		size_t size = msg->payload_len +
+				SW_DRIVER_MSG_HEADER_SIZE();
+		char *dst = reserve_seg_space_i(size, cpu, &should_wakeup,
+						&msg->tsc);
+
+		if (likely(dst)) {
+			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
+			dst += SW_DRIVER_MSG_HEADER_SIZE();
+			memcpy(dst, msg->p_payload, msg->payload_len);
+		} else
+			retval = -PW_ERROR;
+	}
+	sw_put_cpu(flags);
+
+	if (unlikely(should_wakeup))
+		sw_wakeup_reader(action);
+
+	return retval;
+};
+
+int sw_produce_generic_msg(struct sw_driver_msg *msg,
+	enum sw_wakeup_action action)
+{
+	return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_produce_generic_msg_i,
+		msg, action);
+};
+
+static int sw_init_per_cpu_buffers_i(unsigned long per_cpu_mem_size)
+{
+	int cpu = -1;
+
+	per_cpu_output_buffers =
+	(struct sw_output_buffer *)sw_kmalloc(sizeof(struct sw_output_buffer) *
+	GET_NUM_OUTPUT_BUFFERS(), GFP_KERNEL | __GFP_ZERO);
+	if (per_cpu_output_buffers == NULL) {
+		pw_pr_error(
+			"ERROR allocating space for per-cpu output buffers!\n");
+		sw_destroy_per_cpu_buffers();
+		return -PW_ERROR;
+	}
+	for_each_output_buffer(cpu) {
+		struct sw_output_buffer *buffer = &per_cpu_output_buffers[cpu];
+		char *buff = NULL;
+		int i = 0;
+
+		buffer->mem_alloc_size = per_cpu_mem_size;
+		buffer->free_pages = sw_allocate_pages(GFP_KERNEL | __GFP_ZERO,
+			(unsigned int)per_cpu_mem_size);
+		if (buffer->free_pages == 0) {
+			pw_pr_error("ERROR allocating pages for buffer [%d]!\n",
+				cpu);
+			sw_destroy_per_cpu_buffers();
+			return -PW_ERROR;
+		}
+		buff = (char *)buffer->free_pages;
+		for_each_segment(i) {
+			buffer->buffers[i].buffer = (char *)buff;
+			buff += SW_SEG_DATA_SIZE;
+		}
+	}
+	pw_pr_debug("PER_CPU_MEM_SIZE = %lu, order = %u\n",
+	(unsigned long)per_cpu_mem_size, get_order(per_cpu_mem_size));
+	return PW_SUCCESS;
+};
+
+int sw_init_per_cpu_buffers(void)
+{
+	unsigned int per_cpu_mem_size = sw_get_output_buffer_size();
+
+	pw_pr_debug("Buffer alloc size = %ld\n", sw_buffer_alloc_size);
+
+	if (GET_NUM_OUTPUT_BUFFERS() <= 0) {
+		pw_pr_error("ERROR: max # output buffers= %d\n",
+			GET_NUM_OUTPUT_BUFFERS());
+		return -PW_ERROR;
+	}
+
+	pw_pr_debug("DEBUG: sw_max_num_cpus = %d, num output buffers = %d\n",
+	sw_max_num_cpus, GET_NUM_OUTPUT_BUFFERS());
+
+	/*
+	 * Try to allocate per-cpu buffers. If allocation fails, decrease
+	 * buffer size and retry. Stop trying if size drops below 2KB
+	 * (which means 1KB for each buffer).
+	 */
+	while (per_cpu_mem_size >= SW_MIN_OUTPUT_BUFFER_SIZE &&
+		sw_init_per_cpu_buffers_i(per_cpu_mem_size)) {
+		pw_pr_debug("WARNING: couldn't allocate per-cpu buffers with size %u -- trying smaller size!\n",
+			per_cpu_mem_size);
+		sw_buffer_alloc_size >>= 1;
+		per_cpu_mem_size = sw_get_output_buffer_size();
+	}
+
+	if (unlikely(per_cpu_output_buffers == NULL)) {
+		pw_pr_error("ERROR: couldn't allocate space for per-cpu output buffers!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Initialize our locks.
+	 */
+	SW_INIT_SPINLOCK(sw_polled_lock);
+
+	pw_pr_debug("OK, allocated per-cpu buffers with size = %lu\n",
+		(unsigned long)per_cpu_mem_size);
+
+	if (sw_init_reader_queue()) {
+		pw_pr_error("ERROR initializing reader subsys\n");
+		return -PW_ERROR;
+	}
+
+	return PW_SUCCESS;
+};
+
+void sw_destroy_per_cpu_buffers(void)
+{
+	int cpu = -1;
+
+	/*
+	 * Perform lock finalization.
+	 */
+	SW_DESTROY_SPINLOCK(sw_polled_lock);
+
+	if (per_cpu_output_buffers != NULL) {
+		for_each_output_buffer(cpu) {
+			struct sw_output_buffer *buffer =
+					&per_cpu_output_buffers[cpu];
+
+			if (buffer->free_pages != 0) {
+				sw_release_pages(buffer->free_pages,
+					buffer->mem_alloc_size);
+				buffer->free_pages = 0;
+			}
+		}
+		sw_kfree(per_cpu_output_buffers);
+		per_cpu_output_buffers = NULL;
+	}
+};
+
+void sw_reset_per_cpu_buffers(void)
+{
+	int cpu = 0, i = 0;
+
+	for_each_output_buffer(cpu) {
+		struct sw_output_buffer *buffer = GET_OUTPUT_BUFFER(cpu);
+
+		buffer->buff_index = buffer->dropped_samples =
+			buffer->produced_samples = 0;
+		buffer->last_seg_read = -1;
+
+		for_each_segment(i) {
+			struct sw_data_buffer *seg = &buffer->buffers[i];
+
+			memset(seg->buffer, 0, SW_SEG_DATA_SIZE);
+			SEG_SET_EMPTY(seg);
+		}
+	}
+	sw_last_cpu_read = -1;
+	sw_last_mask = -1;
+	pw_pr_debug("OK, reset per-cpu output buffers!\n");
+	/*
+	 * Reset circular buffer if it has been allocated
+	 */
+	if (output_buffer)
+		buffer.read_index = buffer.write_index = 0;
+
+};
+
+bool sw_any_seg_full(u32 *val, bool is_flush_mode)
+{
+	int num_visited = 0, i = 0;
+
+	if (!val) {
+		pw_pr_error("ERROR: NULL ptrs in %s!\n", __func__);
+		return false;
+	}
+
+	*val = SW_NO_DATA_AVAIL_MASK;
+	pw_pr_debug("Checking for full seg: val = %u, flush = %s\n",
+		 *val, GET_BOOL_STRING(is_flush_mode));
+	for_each_output_buffer(num_visited) {
+		int min_seg = EMPTY_SEG, non_empty_seg = EMPTY_SEG;
+		u64 min_tsc = EMPTY_TSC;
+		struct sw_output_buffer *buffer = NULL;
+
+		if (++sw_last_cpu_read >= GET_NUM_OUTPUT_BUFFERS())
+			sw_last_cpu_read = 0;
+
+		buffer = GET_OUTPUT_BUFFER(sw_last_cpu_read);
+		for_each_segment(i) {
+			struct sw_data_buffer *seg = &buffer->buffers[i];
+			u64 seg_tsc = seg->is_full;
+
+			if (SEG_IS_EMPTY(seg))
+				continue;
+
+			non_empty_seg = i;
+			if (seg_tsc < min_tsc) {
+				/*
+				 * Can only happen if seg was full, provided
+				 * 'EMPTY_TSC' is set to "(u64)-1"
+				 */
+				min_tsc = seg_tsc;
+				min_seg = i;
+			}
+		}
+		if (min_seg != EMPTY_SEG) {
+			*val = (sw_last_cpu_read & 0xffff) << 16 |
+				(min_seg & 0xffff);
+			return true;
+		} else if (is_flush_mode && non_empty_seg != EMPTY_SEG) {
+			*val = (sw_last_cpu_read & 0xffff) << 16 |
+				(non_empty_seg & 0xffff);
+			return true;
+		}
+	}
+	/*
+	 * Reaches here only if there's no data to be read.
+	 */
+	if (is_flush_mode) {
+		/*
+		 * We've drained all buffers and need to tell the userspace
+		 * application there isn't any data. Unfortunately, we can't
+		 * just return a 'zero' value for the mask (because that could
+		 * also indicate that segment # 0 of cpu #0 has data).
+		 */
+		*val = SW_ALL_WRITES_DONE_MASK;
+		return true;
+	}
+	return false;
+};
+
+/*
+ * Returns: number of bytes consumed on SUCCESS, 0 on EOF, negative
+ * error code on FAILURE
+ */
+ssize_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read)
+{
+	int which_cpu = -1, which_seg = -1;
+	unsigned long bytes_not_copied = 0;
+	struct sw_output_buffer *buff = NULL;
+	struct sw_data_buffer *seg = NULL;
+	size_t bytes_read = 0;
+
+	/* Check if we need to use circular buffer */
+	if (output_buffer)
+		return (ssize_t)consume_buffer(buffer, bytes_to_read);
+
+	if (!sw_check_output_buffer_params(buffer, bytes_to_read,
+			SW_SEG_DATA_SIZE)) {
+		pw_pr_error("ERROR: invalid params to \"%s\"!\n", __func__);
+		return -EIO;
+	}
+
+	which_cpu = mask >> 16; which_seg = mask & 0xffff;
+	pw_pr_debug("CONSUME: cpu = %d, seg = %d\n", which_cpu, which_seg);
+	if (which_seg >= NUM_SEGS_PER_BUFFER) {
+		pw_pr_error(
+			"Error: which_seg (%d) >= NUM_SEGS_PER_BUFFER (%d)\n",
+			which_seg, NUM_SEGS_PER_BUFFER);
+		return -EIO;
+	}
+	/*
+	 * OK to access unlocked; either the segment is FULL, or no collection
+	 * is ongoing. In either case, we're GUARANTEED no producer is touching
+	 * this segment.
+	 */
+	buff = GET_OUTPUT_BUFFER(which_cpu);
+	seg = &buff->buffers[which_seg];
+
+	bytes_not_copied = sw_copy_to_user(buffer,
+		seg->buffer, seg->bytes_written); /* dst, src */
+
+	if (likely(bytes_not_copied == 0))
+		bytes_read = seg->bytes_written;
+	else {
+		pw_pr_error("Warning: couldn't copy %lu bytes\n",
+			bytes_not_copied);
+		bytes_read = 0;
+	}
+	SEG_SET_EMPTY(seg);
+	return bytes_read;
+}
+
+unsigned int sw_get_output_buffer_size(void)
+{
+	return (sw_buffer_alloc_size * NUM_SEGS_PER_BUFFER);
+};
+
+void sw_count_samples_produced_dropped(void)
+{
+	int cpu = 0;
+
+	sw_num_samples_produced = sw_num_samples_dropped = 0;
+	if (per_cpu_output_buffers == NULL)
+		return;
+
+	for_each_output_buffer(cpu) {
+		struct sw_output_buffer *buff = GET_OUTPUT_BUFFER(cpu);
+
+		sw_num_samples_dropped += buff->dropped_samples;
+		sw_num_samples_produced += buff->produced_samples;
+	}
+};
+
+void sw_print_output_buffer_overheads(void)
+{
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_produce_generic_msg_i,
+		"PRODUCE_GENERIC_MSG");
+	sw_print_reader_stats();
+};
diff --git a/drivers/platform/x86/socwatch/sw_reader.c b/drivers/platform/x86/socwatch/sw_reader.c
index ea039c6fe72a..99e0b54747c0 100644
--- a/drivers/platform/x86/socwatch/sw_reader.c
+++ b/drivers/platform/x86/socwatch/sw_reader.c
@@ -1,159 +1,159 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#include "sw_internal.h"
-#include "sw_output_buffer.h"
-#include "sw_kernel_defines.h"
-
-/* delay buffer cleanup by 10^6 nsec i.e. 1 msec */
-#define SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC 1000000
-
-/*
- * The alarm queue.
- */
-wait_queue_head_t sw_reader_queue;
-/*
- * Reader wakeup timer.
- */
-static struct hrtimer s_reader_wakeup_timer;
-/*
- * Variable to track # timer fires.
- */
-static int s_num_timer_fires;
-
-/*
- * The alarm callback.
- */
-static enum hrtimer_restart sw_wakeup_callback_i(struct hrtimer *timer)
-{
-	++s_num_timer_fires;
-	wake_up_interruptible(&sw_reader_queue);
-	return HRTIMER_NORESTART;
-}
-
-/*
- * Init reader queue.
- */
-int sw_init_reader_queue(void)
-{
-	init_waitqueue_head(&sw_reader_queue);
-	/*
-	 * Also init wakeup timer (used in low-overhead mode).
-	 */
-	hrtimer_init(&s_reader_wakeup_timer,
-		CLOCK_MONOTONIC, HRTIMER_MODE_REL);
-	s_reader_wakeup_timer.function = &sw_wakeup_callback_i;
-
-	return PW_SUCCESS;
-}
-/*
- * Destroy reader queue.
- */
-void sw_destroy_reader_queue(void)
-{
-	/* NOP */
-}
-/*
- * Wakeup client waiting for a full buffer.
- */
-void sw_wakeup_reader(enum sw_wakeup_action action)
-{
-	if (waitqueue_active(&sw_reader_queue)) { /* direct mode */
-		switch (action) {
-		case SW_WAKEUP_ACTION_DIRECT:
-			wake_up_interruptible(&sw_reader_queue);
-			break;
-		case SW_WAKEUP_ACTION_TIMER:
-			if (!hrtimer_active(&s_reader_wakeup_timer)) {
-				ktime_t ktime = ns_to_ktime(
-					SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC);
-				/* TODO: possible race here --
-				 * introduce locks?
-				 */
-				hrtimer_start(&s_reader_wakeup_timer,
-					ktime, HRTIMER_MODE_REL);
-			}
-			break;
-		default:
-			break;
-		}
-	}
-}
-/*
- * Wakeup client waiting for a full buffer, and
- * cancel any timers initialized by the reader
- * subsys.
- */
-void sw_cancel_reader(void)
-{
-	/*
-	 * Cancel pending wakeup timer (used in low-overhead mode).
-	 */
-	if (hrtimer_active(&s_reader_wakeup_timer))
-		hrtimer_cancel(&s_reader_wakeup_timer);
-
-	/*
-	 * There might be a reader thread blocked on a read: wake
-	 * it up to give it a chance to respond to changed
-	 * conditions.
-	 */
-	sw_wakeup_reader(SW_WAKEUP_ACTION_DIRECT);
-}
-
-void sw_print_reader_stats(void)
-{
-#if DO_OVERHEAD_MEASUREMENTS
-	pw_pr_debug("# reader queue timer fires = %d\n", s_num_timer_fires);
-#endif /* OVERHEAD */
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_kernel_defines.h"
+
+/* delay buffer cleanup by 10^6 nsec i.e. 1 msec */
+#define SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC 1000000
+
+/*
+ * The alarm queue.
+ */
+wait_queue_head_t sw_reader_queue;
+/*
+ * Reader wakeup timer.
+ */
+static struct hrtimer s_reader_wakeup_timer;
+/*
+ * Variable to track # timer fires.
+ */
+static int s_num_timer_fires;
+
+/*
+ * The alarm callback.
+ */
+static enum hrtimer_restart sw_wakeup_callback_i(struct hrtimer *timer)
+{
+	++s_num_timer_fires;
+	wake_up_interruptible(&sw_reader_queue);
+	return HRTIMER_NORESTART;
+}
+
+/*
+ * Init reader queue.
+ */
+int sw_init_reader_queue(void)
+{
+	init_waitqueue_head(&sw_reader_queue);
+	/*
+	 * Also init wakeup timer (used in low-overhead mode).
+	 */
+	hrtimer_init(&s_reader_wakeup_timer,
+		CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	s_reader_wakeup_timer.function = &sw_wakeup_callback_i;
+
+	return PW_SUCCESS;
+}
+/*
+ * Destroy reader queue.
+ */
+void sw_destroy_reader_queue(void)
+{
+	/* NOP */
+}
+/*
+ * Wakeup client waiting for a full buffer.
+ */
+void sw_wakeup_reader(enum sw_wakeup_action action)
+{
+	if (waitqueue_active(&sw_reader_queue)) { /* direct mode */
+		switch (action) {
+		case SW_WAKEUP_ACTION_DIRECT:
+			wake_up_interruptible(&sw_reader_queue);
+			break;
+		case SW_WAKEUP_ACTION_TIMER:
+			if (!hrtimer_active(&s_reader_wakeup_timer)) {
+				ktime_t ktime = ns_to_ktime(
+					SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC);
+				/* TODO: possible race here --
+				 * introduce locks?
+				 */
+				hrtimer_start(&s_reader_wakeup_timer,
+					ktime, HRTIMER_MODE_REL);
+			}
+			break;
+		default:
+			break;
+		}
+	}
+}
+/*
+ * Wakeup client waiting for a full buffer, and
+ * cancel any timers initialized by the reader
+ * subsys.
+ */
+void sw_cancel_reader(void)
+{
+	/*
+	 * Cancel pending wakeup timer (used in low-overhead mode).
+	 */
+	if (hrtimer_active(&s_reader_wakeup_timer))
+		hrtimer_cancel(&s_reader_wakeup_timer);
+
+	/*
+	 * There might be a reader thread blocked on a read: wake
+	 * it up to give it a chance to respond to changed
+	 * conditions.
+	 */
+	sw_wakeup_reader(SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_print_reader_stats(void)
+{
+#if DO_OVERHEAD_MEASUREMENTS
+	pw_pr_debug("# reader queue timer fires = %d\n", s_num_timer_fires);
+#endif /* OVERHEAD */
+}
diff --git a/drivers/platform/x86/socwatch/sw_telem.c b/drivers/platform/x86/socwatch/sw_telem.c
index eb162b1b28e3..1bc0f63dfa20 100644
--- a/drivers/platform/x86/socwatch/sw_telem.c
+++ b/drivers/platform/x86/socwatch/sw_telem.c
@@ -1,873 +1,873 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/compiler.h>     /* Definition of __weak */
-#include <linux/version.h>      /* LINUX_VERSION_CODE */
-#include <linux/delay.h>        /* 'udelay' */
-#include <linux/io.h>           /* Definition of ioremap_nocache and iounmap */
-#include "sw_kernel_defines.h"  /* pw_pr_debug */
-#include "sw_mem.h"             /* sw_kmalloc/free */
-#include "sw_lock_defs.h"       /* Various lock-related definitions */
-#include "sw_telem.h"           /* Signatures of fn's exported from here. */
-
-/*
- * These functions and data structures are exported by the Telemetry
- * driver.  However, that file may not be available in the kernel for
- * which this driver is being built, so we re-define many of the same
- * things here.
- */
-/**
- * struct telemetry_evtlog - The "event log" returned by the kernel's
- *                           full-read telemetry driver.
- * @telem_evtid:   The 16-bit event ID.
- * @telem_evtlog:  The actual telemetry data.
- */
-struct telemetry_evtlog {
-	u32 telem_evtid;	/* Event ID of a data item. */
-	u64 telem_evtlog;   /* Counter data */
-};
-
-struct telemetry_evtconfig {
-	u32 *evtmap;	/* Array of Event-IDs to Enable */
-	u8 num_evts;	/* Number of Events (<29) in evtmap */
-	u8 period;	  /* Sampling period */
-};
-
-#define MAX_TELEM_EVENTS 28  /* Max telem events per unit */
-
-/* The enable bit is set when programming events, but is returned
- * cleared for queried events requests.
- */
-#define TELEM_EVENT_ENABLE 0x8000 /* Enabled when Event ID HIGH bit */
-
-/*
- * Sampling Period values.
- * The sampling period is encoded in an 7-bit value, where
- *	Period = (Value * 16^Exponent) usec where:
- *		bits[6:3] -> Value;
- *		bits [0:2]-> Exponent;
- * Here are some of the calculated possible values:
- * | Value  Val+Exp  | Value | Exponent | Period (usec) | Period (msec) |
- * |-----------------+-------+----------+---------------+---------------|
- * | 0xA = 000 1+010 |   1   |     2    |           256 |         0.256 |
- * | 0x12= 001 0+010 |   2   |     2    |           512 |         0.512 |
- * | 0x22= 010 0+010 |   4   |     2    |          1024 |         1.024 |
- * | 0xB = 000 1+011 |   1   |     3    |          4096 |         4.096 |
- * | 0x13= 001 0+011 |   2   |     3    |          8192 |         8.192 |
- * | 0x1B= 001 1+011 |   3   |     3    |         12288 |        12.288 |
- * | 0x0C= 000 1+100 |   1   |     4    |         65536 |        65.536 |
- * | 0x0D= 000 1+101 |   1   |     5    |       1048576 |      1048.576 |
- */
-#define TELEM_SAMPLING_1MS 0x22  /* Approximately 1 ms */
-#define TELEM_SAMPLING_1S  0x0D  /* Approximately 1 s */
-
-/* These functions make up the main APIs of the telemetry driver.  We
- * define all of them with weak linkage so that we can still compile
- * and load into kernels which don't have a telemetry driver.
- */
-extern int __weak telemetry_get_eventconfig(
-	struct telemetry_evtconfig *punit_config,
-	struct telemetry_evtconfig *pmc_config,
-	int  punit_len,
-	int  pmc_len);
-
-extern int __weak telemetry_reset_events(void);
-
-extern int __weak telemetry_set_sampling_period(
-	u8 punit_period,
-	u8 pmc_period);
-/*
- * Older kernels didn't have the p-unit/pmc ipc command interface
- */
-extern int __weak intel_punit_ipc_command(
-	u32 cmd, u32 para1, u32 para2, u32 *in, u32 *out);
-
-extern int __weak intel_pmc_ipc_command(
-	u32 cmd, u32 sub, u8 *in, u32 inlen, u32 *out, u32 outlen);
-/*
- * Spinlock to guard updates to the 'iters' values.
- */
-static SW_DEFINE_SPINLOCK(sw_telem_lock);
-
-
-/* ************************************************
- * Constants for P-unit/PMC telemetry interface
- *  ***********************************************
- */
-
-#define PUNIT_MAILBOX_INTERFACE_OFFSET		0x7084
-#define PUNIT_MAILBOX_DATA_OFFSET		0x7080
-
-#define PSS_TELEM_SSRAM_OFFSET			0x1A00
-#define IOSS_TELEM_SSRAM_OFFSET			0x1B00
-#define TELEM_SSRAM_SIZE			240
-
-#define PMC_IPC_CMD				0x0
-
-#define PMC_IPC_STATUS				0x04
-
-#define PMC_IPC_WRITE_BUFFER			0x80
-#define PMC_IPC_READ_BUFFER			0x90
-
-#define PMC_IPC_PMC_TELEMETRY_COMMAND		0xEB
-
-
-#define TELEM_READ_TIMEOUT_TRIAL		10
-#define TELEM_MAILBOX_STATUS_TIMEOUT		1000
-
-#define IPC_BIOS_PUNIT_CMD_BASE			0x00
-
-#define IPC_BIOS_PUNIT_CMD_READ_TELE_INFO				\
-					(IPC_BIOS_PUNIT_CMD_BASE + 0x09)
-#define IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL				\
-					(IPC_BIOS_PUNIT_CMD_BASE + 0x0c)
-#define IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL			\
-					(IPC_BIOS_PUNIT_CMD_BASE + 0x0d)
-#define IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT				\
-					(IPC_BIOS_PUNIT_CMD_BASE + 0x11)
-
-#define IOSS_TELEM_EVENT_WRITE			0x1
-#define IOSS_TELEM_INFO_READ			0x2
-#define IOSS_TELEM_EVENT_CTL_READ		0x7
-#define IOSS_TELEM_EVENT_CTL_WRITE		0x8
-
-#define IOSS_TELEM_EVT_CTRL_WRITE_SIZE		0x4
-#define IOSS_TELEM_READ_WORD			0x1
-#define IOSS_TELEM_EVT_WRITE_SIZE		0x3
-
-#ifndef BIT
-	#define BIT(x)				(1<<x)
-#endif /* BIT */
-
-#define TELEM_DISABLE(x)			((x) &= ~(BIT(31)))
-#define TELEM_ENABLE_SSRAM_EVT_TRACE(x)		((x) &= ~(BIT(30) | BIT(24)))
-#define TELEM_ENABLE_PERIODIC(x)	((x) |= (BIT(23) | BIT(31) | BIT(7)))
-#define TELEM_IOSS_EVTID_SHIFT			8
-
-#define TELEM_INFO_SSRAMEVTS_MASK		0xFF00
-#define TELEM_INFO_SSRAMEVTS_SHIFT		0x8
-
-#define TELEM_MIN_PERIOD(x)			((x) & 0x7F0000)
-#define TELEM_MAX_PERIOD(x)			((x) & 0x7F000000)
-#define TELEM_CLEAR_SAMPLE_PERIOD(x)		((x) &= ~0x7F)
-#define TELEM_DEFAULT_SAMPLING_PERIOD		TELEM_SAMPLING_1MS
-
-#define IS_TELEM_CONFIGURED()			\
-	(s_telemEventInfo[TELEM_PUNIT].idx > 0	\
-	|| s_telemEventInfo[TELEM_PMC].idx > 0)
-
-static u64 s_mchBarAddrs[3] = {0, 0, 0};
-
-static struct {
-	volatile u64 *ssram_virt_addr;
-	int idx, iters;
-	u32 events[MAX_TELEM_EVENTS];
-	u64 data_buffer[MAX_TELEM_EVENTS];
-} s_telemEventInfo[TELEM_UNIT_NONE] = {
-	[TELEM_PUNIT] = {NULL, 0, 0},
-	[TELEM_PMC] = {NULL, 0, 0},
-};
-
-static volatile u64 *s_punitInterfaceAddr;
-static volatile u64 *s_punitDataAddr;
-static volatile u64 *s_pmcIPCCmdAddr;
-static volatile u64 *s_pmcIPCStsAddr;
-static volatile u64 *s_pmcIPCWBufAddr;
-static volatile u64 *s_pmcIPCRBufAddr;
-
-/**
- * setup_punit_mbox -- Setup P-Unit virtual mappings
- *
- * Returns: true if setup successfully
- */
-static bool setup_punit_mbox(void)
-{
-	s_punitInterfaceAddr = (u64 *)ioremap_nocache(
-				(unsigned long)s_mchBarAddrs[TELEM_MCHBAR_CFG] +
-				PUNIT_MAILBOX_INTERFACE_OFFSET, 0x4);
-	s_punitDataAddr = (u64 *)ioremap_nocache(
-				(unsigned long)s_mchBarAddrs[TELEM_MCHBAR_CFG] +
-				PUNIT_MAILBOX_DATA_OFFSET, 0x4);
-	s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr = (u64 *)ioremap_nocache(
-				(unsigned long)
-					s_mchBarAddrs[TELEM_SSRAMBAR_CFG] +
-				PSS_TELEM_SSRAM_OFFSET, TELEM_SSRAM_SIZE);
-
-	return (s_punitInterfaceAddr && s_punitDataAddr &&
-		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr);
-}
-
-/**
- * destroy_punit_mbox -- Unmap p-unit virtual addresses
- */
-static void destroy_punit_mbox(void)
-{
-	if (s_punitInterfaceAddr) {
-		iounmap(s_punitInterfaceAddr);
-		s_punitInterfaceAddr = NULL;
-	}
-	if (s_punitDataAddr) {
-		iounmap(s_punitDataAddr);
-		s_punitDataAddr = NULL;
-	}
-	if (s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr) {
-		iounmap(s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr);
-		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr = NULL;
-	}
-}
-
-/**
- * setup_pmc_mbox -- Setup PMC virtual mappings
- *
- * Returns: true if setup successfully
- */
-static bool setup_pmc_mbox(void)
-{
-	s_pmcIPCCmdAddr = (u64 *)ioremap_nocache(
-			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
-			PMC_IPC_CMD, 0x4);
-	s_pmcIPCStsAddr = (u64 *)ioremap_nocache(
-			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
-			PMC_IPC_STATUS, 0x4);
-	s_pmcIPCWBufAddr = (u64 *)ioremap_nocache(
-			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
-			PMC_IPC_WRITE_BUFFER, 0x4);
-	s_pmcIPCRBufAddr = (u64 *)ioremap_nocache(
-			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
-			PMC_IPC_READ_BUFFER, 0x4);
-	s_telemEventInfo[TELEM_PMC].ssram_virt_addr = (u64 *)ioremap_nocache(
-			(unsigned long)s_mchBarAddrs[TELEM_SSRAMBAR_CFG] +
-			IOSS_TELEM_SSRAM_OFFSET, TELEM_SSRAM_SIZE);
-
-	return (s_pmcIPCCmdAddr && s_pmcIPCStsAddr &&
-		s_pmcIPCWBufAddr && s_pmcIPCRBufAddr &&
-		s_telemEventInfo[TELEM_PMC].ssram_virt_addr);
-}
-
-/**
- * destroy_pmc_mbox -- Unmap PMC virtual addresses
- */
-static void destroy_pmc_mbox(void)
-{
-	if (s_pmcIPCCmdAddr) {
-		iounmap(s_pmcIPCCmdAddr);
-		s_pmcIPCCmdAddr = NULL;
-	}
-	if (s_pmcIPCStsAddr) {
-		iounmap(s_pmcIPCStsAddr);
-		s_pmcIPCStsAddr = NULL;
-	}
-	if (s_pmcIPCWBufAddr) {
-		iounmap(s_pmcIPCWBufAddr);
-		s_pmcIPCWBufAddr = NULL;
-	}
-	if (s_pmcIPCRBufAddr) {
-		iounmap(s_pmcIPCRBufAddr);
-		s_pmcIPCRBufAddr = NULL;
-	}
-	if (s_telemEventInfo[TELEM_PMC].ssram_virt_addr) {
-		iounmap(s_telemEventInfo[TELEM_PMC].ssram_virt_addr);
-		s_telemEventInfo[TELEM_PMC].ssram_virt_addr = NULL;
-	}
-}
-
-/**
- * setup_telem - Setup telemetry interface
- *
- * Returns: 0 if setup successfully, 1 otherwise
- */
-int setup_telem(u64 addrs[3])
-{
-	/*
-	 * Don't setup if already done so
-	 */
-	if (s_mchBarAddrs[TELEM_MCHBAR_CFG])
-		return 0;
-
-	memcpy(s_mchBarAddrs, addrs, sizeof(s_mchBarAddrs));
-	/*
-	 * Setup Punit
-	 */
-	if (!setup_punit_mbox()) {
-		pw_pr_error("Couldn't setup PUNIT mbox\n");
-		return -1;
-	}
-	/*
-	 * Setup PMC
-	 */
-	if (!setup_pmc_mbox()) {
-		pw_pr_error("Couldn't setup PMC mbox\n");
-		return -1;
-	}
-	return 0;
-}
-
-/**
- * destroy_telem - Destroy telemetry interface
- */
-void destroy_telem(void)
-{
-	destroy_punit_mbox();
-	destroy_pmc_mbox();
-
-	memset(s_mchBarAddrs, 0, sizeof(s_mchBarAddrs));
-}
-
-/**
- * get_or_set_id - Add ID to list of events if not previously added
- *
- * Returns: 0 if setup successfully, 1 otherwise
- */
-static int get_or_set_id(u32 *events, u32 *unit_idx, u32 id)
-{
-	u32 i = 0;
-
-	if (*unit_idx >= MAX_TELEM_EVENTS)
-		return -1;
-
-	for (i = 0; i <  *unit_idx; ++i) {
-		if (events[i] == id)
-			return i;
-	}
-	events[*unit_idx] = id;
-	return (*unit_idx)++;
-}
-
-static int add_telem_id(enum telemetry_unit unit, u32 id)
-{
-	return get_or_set_id(
-		s_telemEventInfo[unit].events,
-		&s_telemEventInfo[unit].idx, id);
-}
-
-static void remove_telem_ids(void)
-{
-	memset(s_telemEventInfo, 0, sizeof(s_telemEventInfo));
-}
-
-
-static u64 read_telem_data(u64 *dst, volatile void *src, size_t num_events)
-{
-	u32 i, timeout = 0;
-	u64 prev_timestamp = 0, next_timestamp = 0, start_time = 0, event_data;
-
-	if (!dst)
-		return 0;
-
-	do {
-		u64 *_src = (u64 *)src;
-
-		prev_timestamp = *_src;
-		if (!prev_timestamp)
-			return 0;
-
-		start_time = *(_src + 1);
-
-		for (i = 0; i < num_events; ++i) {
-			event_data = *(_src + 2 + i);
-			dst[i] = event_data;
-		}
-		next_timestamp = *_src;
-
-		if (!next_timestamp)
-			return 0;
-
-		if (++timeout == TELEM_READ_TIMEOUT_TRIAL)
-			break;
-
-	} while (prev_timestamp != next_timestamp);
-	return prev_timestamp == next_timestamp ? start_time : 0;
-}
-
-/**
- * @returns timestamp (1st entry of SSRAM)
- */
-static u64 flush_telem_to_buffer(enum telemetry_unit unit)
-{
-	return read_telem_data(s_telemEventInfo[unit].data_buffer,
-			   s_telemEventInfo[unit].ssram_virt_addr,
-			   s_telemEventInfo[unit].idx);
-}
-
-static void read_telem_from_buffer(u64 *dst, enum telemetry_unit unit)
-{
-	memcpy(dst, s_telemEventInfo[unit].data_buffer,
-		s_telemEventInfo[unit].idx * sizeof(*dst));
-}
-
-static u64 read_event_from_buffer(enum telemetry_unit unit, int idx)
-{
-	if (idx < 0 || idx >= MAX_TELEM_EVENTS)
-		return SW_TELEM_READ_FAIL_VALUE;
-
-	return s_telemEventInfo[unit].data_buffer[idx];
-}
-
-static bool punit_start_telem(void)
-{
-	u32 telem_info = 0, telem_ctrl = 0, i;
-
-	/* Reset data buffer */
-	memset(s_telemEventInfo[TELEM_PUNIT].data_buffer, 0,
-		sizeof(s_telemEventInfo[TELEM_PUNIT].data_buffer));
-
-	/* Read basic config */
-	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_READ_TELE_INFO, 0, 0,
-			NULL, &telem_info))
-		pw_pr_warn("Could not execute P-unit IPC command to read telem info\n");
-
-	/* Debug info */
-	pw_pr_debug("DEBUG: Read P-Unit telem_info = 0x%x\n", telem_info);
-	pw_pr_debug("## SOCWATCHDRV ## PUNIT Telemetry info has events = %u\n",
-		(telem_info & TELEM_INFO_SSRAMEVTS_MASK) >>
-			TELEM_INFO_SSRAMEVTS_SHIFT);
-	pw_pr_debug(
-		"## SOCWATCHDRV ## PUNIT Telemetry info has event_regs = %u\n",
-		telem_info & TELEM_INFO_SSRAMEVTS_MASK);
-	pw_pr_debug(
-		"## SOCWATCHDRV ## PUNIT Telemetry info has min_period = %u\n",
-		TELEM_MIN_PERIOD(telem_info));
-	pw_pr_debug(
-		"## SOCWATCHDRV ## PUNIT Telemetry info has max_period = %u\n",
-		TELEM_MAX_PERIOD(telem_info));
-
-	/*TODO: check if #events or #event_regs is less than 28; exit if so */
-
-	/* Read control structure */
-	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL,
-			0, 0, NULL, &telem_ctrl))
-		pw_pr_warn("Could not execute P-unit IPC command to read telem ctrl structure\n");
-
-	/* Disable telem */
-	TELEM_DISABLE(telem_ctrl);
-	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL,
-			0, 0, &telem_ctrl, NULL))
-		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
-
-	/* Each event added requires a separate command */
-	for (i = 0; i < s_telemEventInfo[TELEM_PUNIT].idx; ++i) {
-		u32 event = s_telemEventInfo[TELEM_PUNIT].events[i] |
-			TELEM_EVENT_ENABLE;
-
-		pw_pr_debug("DEBUG: enabling PUNIT event 0x%x\n",
-		s_telemEventInfo[TELEM_PUNIT].events[i]);
-		if (intel_punit_ipc_command(
-				IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT, i, 0,
-				&event, NULL))
-			pw_pr_warn("Could not execute P-unit IPC command to write telem event\n");
-
-	}
-
-	TELEM_CLEAR_SAMPLE_PERIOD(telem_ctrl);
-	TELEM_ENABLE_SSRAM_EVT_TRACE(telem_ctrl);
-	TELEM_ENABLE_PERIODIC(telem_ctrl);
-	telem_ctrl |= TELEM_DEFAULT_SAMPLING_PERIOD;
-
-	/* Enable telemetry via control structure */
-	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL,
-			0, 0, &telem_ctrl, NULL))
-		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
-
-	return true;
-}
-
-static void punit_stop_telem(void)
-{
-	u32 telem_ctrl = 0;
-
-	if (intel_punit_ipc_command(
-			IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL, 0, 0,
-			NULL, &telem_ctrl))
-		pw_pr_warn("Could not execute P-unit IPC command to read telem ctrl structure\n");
-
-	/* Disable telem */
-	TELEM_DISABLE(telem_ctrl);
-	if (intel_punit_ipc_command(
-			IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL, 0, 0,
-			&telem_ctrl, NULL))
-		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
-}
-
-static bool pmc_start_telem(void)
-{
-	u32 telem_info = 0, telem_ctrl = 0, i;
-
-	/* Reset data buffer */
-	memset(s_telemEventInfo[TELEM_PMC].data_buffer,
-		0, sizeof(s_telemEventInfo[TELEM_PMC].data_buffer));
-
-	/* Read basic config */
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_INFO_READ, NULL, 0, &telem_info,
-			IOSS_TELEM_READ_WORD))
-		pw_pr_warn("Could not execute PMC IPC command to read telemetry info\n");
-
-	pw_pr_debug("DEBUG: Read PMC telem_info = 0x%x\n", telem_info);
-	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has events = %u\n",
-		(telem_info & TELEM_INFO_SSRAMEVTS_MASK) >>
-			TELEM_INFO_SSRAMEVTS_SHIFT);
-	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has event_regs = %u\n",
-		telem_info & TELEM_INFO_SSRAMEVTS_MASK);
-	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has min_period = %u\n",
-		TELEM_MIN_PERIOD(telem_info));
-	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has max_period = %u\n",
-		TELEM_MAX_PERIOD(telem_info));
-
-	/*TODO: check if #events or #event_regs is less than 28; exit if so */
-
-	/* Read control structure */
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_EVENT_CTL_READ, NULL, 0, &telem_ctrl,
-			IOSS_TELEM_READ_WORD))
-		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-
-	/* Disable telemetry */
-	TELEM_DISABLE(telem_ctrl);
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
-			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
-		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-
-
-	/* Each event added requires a separate command */
-	for (i = 0; i < s_telemEventInfo[TELEM_PMC].idx; ++i) {
-		u32 event =
-			s_telemEventInfo[TELEM_PMC].events[i] |
-			TELEM_EVENT_ENABLE;
-
-		event <<= TELEM_IOSS_EVTID_SHIFT;
-		event |= i; /* Set the index register */
-		pw_pr_debug("DEBUG: enabling PMC event 0x%x\n",
-			s_telemEventInfo[TELEM_PMC].events[i]);
-		if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-				IOSS_TELEM_EVENT_WRITE, (u8 *)&event,
-				IOSS_TELEM_EVT_WRITE_SIZE, NULL, 0))
-			pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-	}
-
-	TELEM_CLEAR_SAMPLE_PERIOD(telem_ctrl);
-	TELEM_ENABLE_SSRAM_EVT_TRACE(telem_ctrl);
-	TELEM_ENABLE_PERIODIC(telem_ctrl);
-	telem_ctrl |= TELEM_DEFAULT_SAMPLING_PERIOD;
-
-	/* Enable telemetry via control structure */
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
-			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
-		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-
-	return true;
-}
-
-static void pmc_stop_telem(void)
-{
-	u32 telem_ctrl = 0;
-
-	/* Read control structure */
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_EVENT_CTL_READ, NULL, 0, &telem_ctrl,
-			IOSS_TELEM_READ_WORD))
-		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-
-	/* Disable telemetry */
-	TELEM_DISABLE(telem_ctrl);
-	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
-			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
-			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
-		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
-}
-
-/**
- * Configurs events + starts counters
- * @returns  0 on success
- */
-static int start_telem(void)
-{
-	if (s_telemEventInfo[TELEM_PUNIT].idx) {
-		if (punit_start_telem() == false)
-			return -1;
-
-		/* Return value is don't care */
-		flush_telem_to_buffer(TELEM_PUNIT);
-	}
-
-	if (s_telemEventInfo[TELEM_PMC].idx) {
-		if (pmc_start_telem() == false)
-			return -1;
-
-		flush_telem_to_buffer(TELEM_PMC);
-	}
-	pw_pr_debug("OK, bypass telem started\n");
-	return 0;
-}
-
-static void stop_telem(void)
-{
-	if (s_telemEventInfo[TELEM_PUNIT].idx) {
-		punit_stop_telem();
-		s_telemEventInfo[TELEM_PUNIT].idx = 0;
-	}
-	if (s_telemEventInfo[TELEM_PMC].idx) {
-		pmc_stop_telem();
-		s_telemEventInfo[TELEM_PMC].idx = 0;
-	}
-	pw_pr_debug("OK, bypass telem stopped\n");
-}
-
-int read_telem(u64 *dst, enum telemetry_unit unit, bool should_retry)
-{
-	size_t num_iters = should_retry ? 10 : 0;
-	u64 timestamp = 0;
-
-	do {
-		timestamp = flush_telem_to_buffer(unit);
-	} while (!timestamp && should_retry && num_iters--);
-
-	if (timestamp) {
-		read_telem_from_buffer(dst, unit);
-		return 0;
-	}
-	return -1;
-}
-
-/**
- * builtin_telemetry_available - Determine if telemetry driver is present
- *
- * Returns: 1 if telemetry driver is present, 0 if not.
- */
-static int builtin_telemetry_available(void)
-{
-	int retval = 0;
-	struct telemetry_evtconfig punit_evtconfig;
-	struct telemetry_evtconfig pmc_evtconfig;
-	u32 punit_event_map[MAX_TELEM_EVENTS];
-	u32 pmc_event_map[MAX_TELEM_EVENTS];
-
-
-	/* The symbol below is weak.  We return 1 if we have a definition
-	 * for this telemetry-driver-supplied symbol, or 0 if only the
-	 * weak definition exists. This test will suffice to detect if
-	 * the telemetry driver is loaded.
-	 */
-	if (telemetry_get_eventconfig) {
-		/* OK, the telemetry driver is loaded. But it's possible it
-		 * hasn't been configured properly. To check that, retrieve
-		 * the number of events currently configured. This should never
-		 * be zero since the telemetry driver reserves some SSRAM slots
-		 * for its own use
-		 */
-		memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
-		memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
-
-		punit_evtconfig.evtmap = (u32 *) &punit_event_map;
-		pmc_evtconfig.evtmap = (u32 *) &pmc_event_map;
-
-		retval = telemetry_get_eventconfig(&punit_evtconfig, &pmc_evtconfig,
-						MAX_TELEM_EVENTS, MAX_TELEM_EVENTS);
-		return (retval == 0 && punit_evtconfig.num_evts > 0 &&
-			pmc_evtconfig.num_evts > 0);
-	}
-	return 0;
-}
-
-/**
- * was_telemetry_setup - Check if the P-unit and PMC addresses have been mapped
- *
- * Returns: true if successfully mapped
- */
-static bool was_telemetry_setup(void)
-{
-	return s_punitInterfaceAddr && s_punitDataAddr &&
-		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr /* P-unit */ &&
-		s_pmcIPCCmdAddr && s_pmcIPCStsAddr && s_pmcIPCWBufAddr &&
-		s_pmcIPCRBufAddr && s_telemEventInfo[TELEM_PMC].ssram_virt_addr;
-}
-
-
-/**
- * sw_telem_init_func - Set up the telemetry unit to retrieve a data item
- *						(e.g. counter).
- * @descriptor:  The IO descriptor containing the unit and ID
- *						of the telemetry info to gather.
- *
- * Because we don't (currently) control all of the counters, we
- * economize by seeing if it's already being collected before allocate
- * a slot for it.
- *
- * Returns: PW_SUCCESS  if the telem collector can collect the requested data.
- *		 -PW_ERROR   if the the addition of that item fails.
- */
-int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor)
-{
-	struct sw_driver_telem_io_descriptor *td =
-		&(descriptor->telem_descriptor);
-	u8  unit = td->unit;  /* Telemetry unit to use. */
-	u32 id; /* Event ID we want telemetry to track. */
-
-	if (!was_telemetry_setup())
-		return -ENXIO;
-
-	id = (u32)(td->id);
-
-	td->idx = add_telem_id(unit, id);
-	if (td->idx < 0) {
-		pw_pr_error("ERROR adding id 0x%x to unit %d\n", id, unit);
-		return -1;
-	}
-	pw_pr_debug("OK, added id 0x%x to unit %d at pos %d\n",
-			id, unit, td->idx);
-
-	return 0;
-}
-
-
-/**
- * sw_read_telem_info - Read a metric's data from the telemetry driver.
- * @dest:		Destination (storage for the read data)
- * @cpu:		Which CPU to read from (not used)
- * @descriptor:		The descriptor containing the data ID to read
- * @data_size_in_bytes: The # of bytes in the result (always 8)
- *
- * Returns: Nothing, but stores SW_TELEM_READ_FAIL_VALUE to dest if
- * the read fails.
- */
-void sw_read_telem_info(char *dest, int cpu,
-			  const sw_driver_io_descriptor_t *descriptor,
-			  u16 data_size_in_bytes)
-{
-	u64 *data_dest = (u64 *)dest;
-	const struct sw_driver_telem_io_descriptor *td =
-		&(descriptor->telem_descriptor);
-	u8 unit = td->unit;
-	bool needs_refresh = false;
-
-	/*
-	 * Check if we need to refresh the list of values
-	 */
-	LOCK(sw_telem_lock);
-	{
-		if (s_telemEventInfo[unit].iters == 0)
-			needs_refresh = true;
-
-		if (++s_telemEventInfo[unit].iters ==
-				s_telemEventInfo[unit].idx)
-			s_telemEventInfo[unit].iters = 0;
-	}
-
-	UNLOCK(sw_telem_lock);
-
-	if (needs_refresh) {
-		u64 timestamp = flush_telem_to_buffer(unit);
-
-		pw_pr_debug("DEBUG: unit %d refreshed, timestamp = %llu\n",
-			unit, timestamp);
-		if (!timestamp) { /* failure */
-			*data_dest = SW_TELEM_READ_FAIL_VALUE;
-			return;
-		}
-	} else
-		pw_pr_debug("DEBUG: unit %d NOT refreshed\n", unit);
-
-	*data_dest = read_event_from_buffer(unit, td->idx);
-}
-
-/**
- * sw_reset_telem - Stop collecting telemetry info.
- * @descriptor: Unused in this function
- *
- * Stop collecting anything extra, and give the driver back to
- * debugfs.  Because this driver increases the sampling rate, the
- * kernel's telemetry driver can't succesfully reset the driver unless
- * we first drop the rate back down to a much slower rate.  This is a
- * temporary measure, since the reset operation will then reset the
- * sampling interval to whatever the GMIN driver wants.
- *
- * Returns: 0
- */
-int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor)
-{
-	if (IS_TELEM_CONFIGURED()) {
-		stop_telem();
-		remove_telem_ids();
-		/* Return control to 'builtin' telemetry driver */
-		telemetry_set_sampling_period(TELEM_SAMPLING_1S,
-					  TELEM_SAMPLING_1S);
-		telemetry_reset_events();
-	}
-	return 0;
-}
-
-/**
- * sw_available_telem -- Decide if the telemetry subsystem is available for use
- */
-bool sw_telem_available(void)
-{
-	/*
-	 * Telemetry driver MUST be loaded; we perform this check because
-	 * on some systems an error with the p-unit/pmc IPC interface causes
-	 * kernel panics.
-	 */
-	return builtin_telemetry_available();
-};
-
-bool sw_telem_post_config(void)
-{
-	if (start_telem())
-		return false;
-
-	return true;
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/compiler.h>     /* Definition of __weak */
+#include <linux/version.h>      /* LINUX_VERSION_CODE */
+#include <linux/delay.h>        /* 'udelay' */
+#include <linux/io.h>           /* Definition of ioremap_nocache and iounmap */
+#include "sw_kernel_defines.h"  /* pw_pr_debug */
+#include "sw_mem.h"             /* sw_kmalloc/free */
+#include "sw_lock_defs.h"       /* Various lock-related definitions */
+#include "sw_telem.h"           /* Signatures of fn's exported from here. */
+
+/*
+ * These functions and data structures are exported by the Telemetry
+ * driver.  However, that file may not be available in the kernel for
+ * which this driver is being built, so we re-define many of the same
+ * things here.
+ */
+/**
+ * struct telemetry_evtlog - The "event log" returned by the kernel's
+ *                           full-read telemetry driver.
+ * @telem_evtid:   The 16-bit event ID.
+ * @telem_evtlog:  The actual telemetry data.
+ */
+struct telemetry_evtlog {
+	u32 telem_evtid;	/* Event ID of a data item. */
+	u64 telem_evtlog;   /* Counter data */
+};
+
+struct telemetry_evtconfig {
+	u32 *evtmap;	/* Array of Event-IDs to Enable */
+	u8 num_evts;	/* Number of Events (<29) in evtmap */
+	u8 period;	  /* Sampling period */
+};
+
+#define MAX_TELEM_EVENTS 28  /* Max telem events per unit */
+
+/* The enable bit is set when programming events, but is returned
+ * cleared for queried events requests.
+ */
+#define TELEM_EVENT_ENABLE 0x8000 /* Enabled when Event ID HIGH bit */
+
+/*
+ * Sampling Period values.
+ * The sampling period is encoded in an 7-bit value, where
+ *	Period = (Value * 16^Exponent) usec where:
+ *		bits[6:3] -> Value;
+ *		bits [0:2]-> Exponent;
+ * Here are some of the calculated possible values:
+ * | Value  Val+Exp  | Value | Exponent | Period (usec) | Period (msec) |
+ * |-----------------+-------+----------+---------------+---------------|
+ * | 0xA = 000 1+010 |   1   |     2    |           256 |         0.256 |
+ * | 0x12= 001 0+010 |   2   |     2    |           512 |         0.512 |
+ * | 0x22= 010 0+010 |   4   |     2    |          1024 |         1.024 |
+ * | 0xB = 000 1+011 |   1   |     3    |          4096 |         4.096 |
+ * | 0x13= 001 0+011 |   2   |     3    |          8192 |         8.192 |
+ * | 0x1B= 001 1+011 |   3   |     3    |         12288 |        12.288 |
+ * | 0x0C= 000 1+100 |   1   |     4    |         65536 |        65.536 |
+ * | 0x0D= 000 1+101 |   1   |     5    |       1048576 |      1048.576 |
+ */
+#define TELEM_SAMPLING_1MS 0x22  /* Approximately 1 ms */
+#define TELEM_SAMPLING_1S  0x0D  /* Approximately 1 s */
+
+/* These functions make up the main APIs of the telemetry driver.  We
+ * define all of them with weak linkage so that we can still compile
+ * and load into kernels which don't have a telemetry driver.
+ */
+extern int __weak telemetry_get_eventconfig(
+	struct telemetry_evtconfig *punit_config,
+	struct telemetry_evtconfig *pmc_config,
+	int  punit_len,
+	int  pmc_len);
+
+extern int __weak telemetry_reset_events(void);
+
+extern int __weak telemetry_set_sampling_period(
+	u8 punit_period,
+	u8 pmc_period);
+/*
+ * Older kernels didn't have the p-unit/pmc ipc command interface
+ */
+extern int __weak intel_punit_ipc_command(
+	u32 cmd, u32 para1, u32 para2, u32 *in, u32 *out);
+
+extern int __weak intel_pmc_ipc_command(
+	u32 cmd, u32 sub, u8 *in, u32 inlen, u32 *out, u32 outlen);
+/*
+ * Spinlock to guard updates to the 'iters' values.
+ */
+static SW_DEFINE_SPINLOCK(sw_telem_lock);
+
+
+/* ************************************************
+ * Constants for P-unit/PMC telemetry interface
+ *  ***********************************************
+ */
+
+#define PUNIT_MAILBOX_INTERFACE_OFFSET		0x7084
+#define PUNIT_MAILBOX_DATA_OFFSET		0x7080
+
+#define PSS_TELEM_SSRAM_OFFSET			0x1A00
+#define IOSS_TELEM_SSRAM_OFFSET			0x1B00
+#define TELEM_SSRAM_SIZE			240
+
+#define PMC_IPC_CMD				0x0
+
+#define PMC_IPC_STATUS				0x04
+
+#define PMC_IPC_WRITE_BUFFER			0x80
+#define PMC_IPC_READ_BUFFER			0x90
+
+#define PMC_IPC_PMC_TELEMETRY_COMMAND		0xEB
+
+
+#define TELEM_READ_TIMEOUT_TRIAL		10
+#define TELEM_MAILBOX_STATUS_TIMEOUT		1000
+
+#define IPC_BIOS_PUNIT_CMD_BASE			0x00
+
+#define IPC_BIOS_PUNIT_CMD_READ_TELE_INFO				\
+					(IPC_BIOS_PUNIT_CMD_BASE + 0x09)
+#define IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL				\
+					(IPC_BIOS_PUNIT_CMD_BASE + 0x0c)
+#define IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL			\
+					(IPC_BIOS_PUNIT_CMD_BASE + 0x0d)
+#define IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT				\
+					(IPC_BIOS_PUNIT_CMD_BASE + 0x11)
+
+#define IOSS_TELEM_EVENT_WRITE			0x1
+#define IOSS_TELEM_INFO_READ			0x2
+#define IOSS_TELEM_EVENT_CTL_READ		0x7
+#define IOSS_TELEM_EVENT_CTL_WRITE		0x8
+
+#define IOSS_TELEM_EVT_CTRL_WRITE_SIZE		0x4
+#define IOSS_TELEM_READ_WORD			0x1
+#define IOSS_TELEM_EVT_WRITE_SIZE		0x3
+
+#ifndef BIT
+	#define BIT(x)				(1<<x)
+#endif /* BIT */
+
+#define TELEM_DISABLE(x)			((x) &= ~(BIT(31)))
+#define TELEM_ENABLE_SSRAM_EVT_TRACE(x)		((x) &= ~(BIT(30) | BIT(24)))
+#define TELEM_ENABLE_PERIODIC(x)	((x) |= (BIT(23) | BIT(31) | BIT(7)))
+#define TELEM_IOSS_EVTID_SHIFT			8
+
+#define TELEM_INFO_SSRAMEVTS_MASK		0xFF00
+#define TELEM_INFO_SSRAMEVTS_SHIFT		0x8
+
+#define TELEM_MIN_PERIOD(x)			((x) & 0x7F0000)
+#define TELEM_MAX_PERIOD(x)			((x) & 0x7F000000)
+#define TELEM_CLEAR_SAMPLE_PERIOD(x)		((x) &= ~0x7F)
+#define TELEM_DEFAULT_SAMPLING_PERIOD		TELEM_SAMPLING_1MS
+
+#define IS_TELEM_CONFIGURED()			\
+	(s_telemEventInfo[TELEM_PUNIT].idx > 0	\
+	|| s_telemEventInfo[TELEM_PMC].idx > 0)
+
+static u64 s_mchBarAddrs[3] = {0, 0, 0};
+
+static struct {
+	volatile u64 *ssram_virt_addr;
+	int idx, iters;
+	u32 events[MAX_TELEM_EVENTS];
+	u64 data_buffer[MAX_TELEM_EVENTS];
+} s_telemEventInfo[TELEM_UNIT_NONE] = {
+	[TELEM_PUNIT] = {NULL, 0, 0},
+	[TELEM_PMC] = {NULL, 0, 0},
+};
+
+static volatile u64 *s_punitInterfaceAddr;
+static volatile u64 *s_punitDataAddr;
+static volatile u64 *s_pmcIPCCmdAddr;
+static volatile u64 *s_pmcIPCStsAddr;
+static volatile u64 *s_pmcIPCWBufAddr;
+static volatile u64 *s_pmcIPCRBufAddr;
+
+/**
+ * setup_punit_mbox -- Setup P-Unit virtual mappings
+ *
+ * Returns: true if setup successfully
+ */
+static bool setup_punit_mbox(void)
+{
+	s_punitInterfaceAddr = (u64 *)ioremap_nocache(
+				(unsigned long)s_mchBarAddrs[TELEM_MCHBAR_CFG] +
+				PUNIT_MAILBOX_INTERFACE_OFFSET, 0x4);
+	s_punitDataAddr = (u64 *)ioremap_nocache(
+				(unsigned long)s_mchBarAddrs[TELEM_MCHBAR_CFG] +
+				PUNIT_MAILBOX_DATA_OFFSET, 0x4);
+	s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr = (u64 *)ioremap_nocache(
+				(unsigned long)
+					s_mchBarAddrs[TELEM_SSRAMBAR_CFG] +
+				PSS_TELEM_SSRAM_OFFSET, TELEM_SSRAM_SIZE);
+
+	return (s_punitInterfaceAddr && s_punitDataAddr &&
+		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr);
+}
+
+/**
+ * destroy_punit_mbox -- Unmap p-unit virtual addresses
+ */
+static void destroy_punit_mbox(void)
+{
+	if (s_punitInterfaceAddr) {
+		iounmap(s_punitInterfaceAddr);
+		s_punitInterfaceAddr = NULL;
+	}
+	if (s_punitDataAddr) {
+		iounmap(s_punitDataAddr);
+		s_punitDataAddr = NULL;
+	}
+	if (s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr) {
+		iounmap(s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr);
+		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr = NULL;
+	}
+}
+
+/**
+ * setup_pmc_mbox -- Setup PMC virtual mappings
+ *
+ * Returns: true if setup successfully
+ */
+static bool setup_pmc_mbox(void)
+{
+	s_pmcIPCCmdAddr = (u64 *)ioremap_nocache(
+			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
+			PMC_IPC_CMD, 0x4);
+	s_pmcIPCStsAddr = (u64 *)ioremap_nocache(
+			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
+			PMC_IPC_STATUS, 0x4);
+	s_pmcIPCWBufAddr = (u64 *)ioremap_nocache(
+			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
+			PMC_IPC_WRITE_BUFFER, 0x4);
+	s_pmcIPCRBufAddr = (u64 *)ioremap_nocache(
+			(unsigned long)s_mchBarAddrs[TELEM_IPC1BAR_CFG] +
+			PMC_IPC_READ_BUFFER, 0x4);
+	s_telemEventInfo[TELEM_PMC].ssram_virt_addr = (u64 *)ioremap_nocache(
+			(unsigned long)s_mchBarAddrs[TELEM_SSRAMBAR_CFG] +
+			IOSS_TELEM_SSRAM_OFFSET, TELEM_SSRAM_SIZE);
+
+	return (s_pmcIPCCmdAddr && s_pmcIPCStsAddr &&
+		s_pmcIPCWBufAddr && s_pmcIPCRBufAddr &&
+		s_telemEventInfo[TELEM_PMC].ssram_virt_addr);
+}
+
+/**
+ * destroy_pmc_mbox -- Unmap PMC virtual addresses
+ */
+static void destroy_pmc_mbox(void)
+{
+	if (s_pmcIPCCmdAddr) {
+		iounmap(s_pmcIPCCmdAddr);
+		s_pmcIPCCmdAddr = NULL;
+	}
+	if (s_pmcIPCStsAddr) {
+		iounmap(s_pmcIPCStsAddr);
+		s_pmcIPCStsAddr = NULL;
+	}
+	if (s_pmcIPCWBufAddr) {
+		iounmap(s_pmcIPCWBufAddr);
+		s_pmcIPCWBufAddr = NULL;
+	}
+	if (s_pmcIPCRBufAddr) {
+		iounmap(s_pmcIPCRBufAddr);
+		s_pmcIPCRBufAddr = NULL;
+	}
+	if (s_telemEventInfo[TELEM_PMC].ssram_virt_addr) {
+		iounmap(s_telemEventInfo[TELEM_PMC].ssram_virt_addr);
+		s_telemEventInfo[TELEM_PMC].ssram_virt_addr = NULL;
+	}
+}
+
+/**
+ * setup_telem - Setup telemetry interface
+ *
+ * Returns: 0 if setup successfully, 1 otherwise
+ */
+int setup_telem(u64 addrs[3])
+{
+	/*
+	 * Don't setup if already done so
+	 */
+	if (s_mchBarAddrs[TELEM_MCHBAR_CFG])
+		return 0;
+
+	memcpy(s_mchBarAddrs, addrs, sizeof(s_mchBarAddrs));
+	/*
+	 * Setup Punit
+	 */
+	if (!setup_punit_mbox()) {
+		pw_pr_error("Couldn't setup PUNIT mbox\n");
+		return -1;
+	}
+	/*
+	 * Setup PMC
+	 */
+	if (!setup_pmc_mbox()) {
+		pw_pr_error("Couldn't setup PMC mbox\n");
+		return -1;
+	}
+	return 0;
+}
+
+/**
+ * destroy_telem - Destroy telemetry interface
+ */
+void destroy_telem(void)
+{
+	destroy_punit_mbox();
+	destroy_pmc_mbox();
+
+	memset(s_mchBarAddrs, 0, sizeof(s_mchBarAddrs));
+}
+
+/**
+ * get_or_set_id - Add ID to list of events if not previously added
+ *
+ * Returns: 0 if setup successfully, 1 otherwise
+ */
+static int get_or_set_id(u32 *events, u32 *unit_idx, u32 id)
+{
+	u32 i = 0;
+
+	if (*unit_idx >= MAX_TELEM_EVENTS)
+		return -1;
+
+	for (i = 0; i <  *unit_idx; ++i) {
+		if (events[i] == id)
+			return i;
+	}
+	events[*unit_idx] = id;
+	return (*unit_idx)++;
+}
+
+static int add_telem_id(enum telemetry_unit unit, u32 id)
+{
+	return get_or_set_id(
+		s_telemEventInfo[unit].events,
+		&s_telemEventInfo[unit].idx, id);
+}
+
+static void remove_telem_ids(void)
+{
+	memset(s_telemEventInfo, 0, sizeof(s_telemEventInfo));
+}
+
+
+static u64 read_telem_data(u64 *dst, volatile void *src, size_t num_events)
+{
+	u32 i, timeout = 0;
+	u64 prev_timestamp = 0, next_timestamp = 0, start_time = 0, event_data;
+
+	if (!dst)
+		return 0;
+
+	do {
+		u64 *_src = (u64 *)src;
+
+		prev_timestamp = *_src;
+		if (!prev_timestamp)
+			return 0;
+
+		start_time = *(_src + 1);
+
+		for (i = 0; i < num_events; ++i) {
+			event_data = *(_src + 2 + i);
+			dst[i] = event_data;
+		}
+		next_timestamp = *_src;
+
+		if (!next_timestamp)
+			return 0;
+
+		if (++timeout == TELEM_READ_TIMEOUT_TRIAL)
+			break;
+
+	} while (prev_timestamp != next_timestamp);
+	return prev_timestamp == next_timestamp ? start_time : 0;
+}
+
+/**
+ * @returns timestamp (1st entry of SSRAM)
+ */
+static u64 flush_telem_to_buffer(enum telemetry_unit unit)
+{
+	return read_telem_data(s_telemEventInfo[unit].data_buffer,
+			   s_telemEventInfo[unit].ssram_virt_addr,
+			   s_telemEventInfo[unit].idx);
+}
+
+static void read_telem_from_buffer(u64 *dst, enum telemetry_unit unit)
+{
+	memcpy(dst, s_telemEventInfo[unit].data_buffer,
+		s_telemEventInfo[unit].idx * sizeof(*dst));
+}
+
+static u64 read_event_from_buffer(enum telemetry_unit unit, int idx)
+{
+	if (idx < 0 || idx >= MAX_TELEM_EVENTS)
+		return SW_TELEM_READ_FAIL_VALUE;
+
+	return s_telemEventInfo[unit].data_buffer[idx];
+}
+
+static bool punit_start_telem(void)
+{
+	u32 telem_info = 0, telem_ctrl = 0, i;
+
+	/* Reset data buffer */
+	memset(s_telemEventInfo[TELEM_PUNIT].data_buffer, 0,
+		sizeof(s_telemEventInfo[TELEM_PUNIT].data_buffer));
+
+	/* Read basic config */
+	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_READ_TELE_INFO, 0, 0,
+			NULL, &telem_info))
+		pw_pr_warn("Could not execute P-unit IPC command to read telem info\n");
+
+	/* Debug info */
+	pw_pr_debug("DEBUG: Read P-Unit telem_info = 0x%x\n", telem_info);
+	pw_pr_debug("## SOCWATCHDRV ## PUNIT Telemetry info has events = %u\n",
+		(telem_info & TELEM_INFO_SSRAMEVTS_MASK) >>
+			TELEM_INFO_SSRAMEVTS_SHIFT);
+	pw_pr_debug(
+		"## SOCWATCHDRV ## PUNIT Telemetry info has event_regs = %u\n",
+		telem_info & TELEM_INFO_SSRAMEVTS_MASK);
+	pw_pr_debug(
+		"## SOCWATCHDRV ## PUNIT Telemetry info has min_period = %u\n",
+		TELEM_MIN_PERIOD(telem_info));
+	pw_pr_debug(
+		"## SOCWATCHDRV ## PUNIT Telemetry info has max_period = %u\n",
+		TELEM_MAX_PERIOD(telem_info));
+
+	/*TODO: check if #events or #event_regs is less than 28; exit if so */
+
+	/* Read control structure */
+	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL,
+			0, 0, NULL, &telem_ctrl))
+		pw_pr_warn("Could not execute P-unit IPC command to read telem ctrl structure\n");
+
+	/* Disable telem */
+	TELEM_DISABLE(telem_ctrl);
+	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL,
+			0, 0, &telem_ctrl, NULL))
+		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
+
+	/* Each event added requires a separate command */
+	for (i = 0; i < s_telemEventInfo[TELEM_PUNIT].idx; ++i) {
+		u32 event = s_telemEventInfo[TELEM_PUNIT].events[i] |
+			TELEM_EVENT_ENABLE;
+
+		pw_pr_debug("DEBUG: enabling PUNIT event 0x%x\n",
+		s_telemEventInfo[TELEM_PUNIT].events[i]);
+		if (intel_punit_ipc_command(
+				IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT, i, 0,
+				&event, NULL))
+			pw_pr_warn("Could not execute P-unit IPC command to write telem event\n");
+
+	}
+
+	TELEM_CLEAR_SAMPLE_PERIOD(telem_ctrl);
+	TELEM_ENABLE_SSRAM_EVT_TRACE(telem_ctrl);
+	TELEM_ENABLE_PERIODIC(telem_ctrl);
+	telem_ctrl |= TELEM_DEFAULT_SAMPLING_PERIOD;
+
+	/* Enable telemetry via control structure */
+	if (intel_punit_ipc_command(IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL,
+			0, 0, &telem_ctrl, NULL))
+		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
+
+	return true;
+}
+
+static void punit_stop_telem(void)
+{
+	u32 telem_ctrl = 0;
+
+	if (intel_punit_ipc_command(
+			IPC_BIOS_PUNIT_CMD_READ_TELE_EVENT_CTRL, 0, 0,
+			NULL, &telem_ctrl))
+		pw_pr_warn("Could not execute P-unit IPC command to read telem ctrl structure\n");
+
+	/* Disable telem */
+	TELEM_DISABLE(telem_ctrl);
+	if (intel_punit_ipc_command(
+			IPC_BIOS_PUNIT_CMD_WRITE_TELE_EVENT_CTRL, 0, 0,
+			&telem_ctrl, NULL))
+		pw_pr_warn("Could not execute P-unit IPC command to write telem ctrl structure\n");
+}
+
+static bool pmc_start_telem(void)
+{
+	u32 telem_info = 0, telem_ctrl = 0, i;
+
+	/* Reset data buffer */
+	memset(s_telemEventInfo[TELEM_PMC].data_buffer,
+		0, sizeof(s_telemEventInfo[TELEM_PMC].data_buffer));
+
+	/* Read basic config */
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_INFO_READ, NULL, 0, &telem_info,
+			IOSS_TELEM_READ_WORD))
+		pw_pr_warn("Could not execute PMC IPC command to read telemetry info\n");
+
+	pw_pr_debug("DEBUG: Read PMC telem_info = 0x%x\n", telem_info);
+	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has events = %u\n",
+		(telem_info & TELEM_INFO_SSRAMEVTS_MASK) >>
+			TELEM_INFO_SSRAMEVTS_SHIFT);
+	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has event_regs = %u\n",
+		telem_info & TELEM_INFO_SSRAMEVTS_MASK);
+	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has min_period = %u\n",
+		TELEM_MIN_PERIOD(telem_info));
+	pw_pr_debug("## SOCWATCHDRV ## PMC Telemetry info has max_period = %u\n",
+		TELEM_MAX_PERIOD(telem_info));
+
+	/*TODO: check if #events or #event_regs is less than 28; exit if so */
+
+	/* Read control structure */
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_EVENT_CTL_READ, NULL, 0, &telem_ctrl,
+			IOSS_TELEM_READ_WORD))
+		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+
+	/* Disable telemetry */
+	TELEM_DISABLE(telem_ctrl);
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
+			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
+		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+
+
+	/* Each event added requires a separate command */
+	for (i = 0; i < s_telemEventInfo[TELEM_PMC].idx; ++i) {
+		u32 event =
+			s_telemEventInfo[TELEM_PMC].events[i] |
+			TELEM_EVENT_ENABLE;
+
+		event <<= TELEM_IOSS_EVTID_SHIFT;
+		event |= i; /* Set the index register */
+		pw_pr_debug("DEBUG: enabling PMC event 0x%x\n",
+			s_telemEventInfo[TELEM_PMC].events[i]);
+		if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+				IOSS_TELEM_EVENT_WRITE, (u8 *)&event,
+				IOSS_TELEM_EVT_WRITE_SIZE, NULL, 0))
+			pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+	}
+
+	TELEM_CLEAR_SAMPLE_PERIOD(telem_ctrl);
+	TELEM_ENABLE_SSRAM_EVT_TRACE(telem_ctrl);
+	TELEM_ENABLE_PERIODIC(telem_ctrl);
+	telem_ctrl |= TELEM_DEFAULT_SAMPLING_PERIOD;
+
+	/* Enable telemetry via control structure */
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
+			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
+		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+
+	return true;
+}
+
+static void pmc_stop_telem(void)
+{
+	u32 telem_ctrl = 0;
+
+	/* Read control structure */
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_EVENT_CTL_READ, NULL, 0, &telem_ctrl,
+			IOSS_TELEM_READ_WORD))
+		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+
+	/* Disable telemetry */
+	TELEM_DISABLE(telem_ctrl);
+	if (intel_pmc_ipc_command(PMC_IPC_PMC_TELEMETRY_COMMAND,
+			IOSS_TELEM_EVENT_CTL_WRITE, (u8 *)&telem_ctrl,
+			IOSS_TELEM_EVT_CTRL_WRITE_SIZE, NULL, 0))
+		pw_pr_warn("Could not execute PMC IPC command to read telem control info\n");
+}
+
+/**
+ * Configurs events + starts counters
+ * @returns  0 on success
+ */
+static int start_telem(void)
+{
+	if (s_telemEventInfo[TELEM_PUNIT].idx) {
+		if (punit_start_telem() == false)
+			return -1;
+
+		/* Return value is don't care */
+		flush_telem_to_buffer(TELEM_PUNIT);
+	}
+
+	if (s_telemEventInfo[TELEM_PMC].idx) {
+		if (pmc_start_telem() == false)
+			return -1;
+
+		flush_telem_to_buffer(TELEM_PMC);
+	}
+	pw_pr_debug("OK, bypass telem started\n");
+	return 0;
+}
+
+static void stop_telem(void)
+{
+	if (s_telemEventInfo[TELEM_PUNIT].idx) {
+		punit_stop_telem();
+		s_telemEventInfo[TELEM_PUNIT].idx = 0;
+	}
+	if (s_telemEventInfo[TELEM_PMC].idx) {
+		pmc_stop_telem();
+		s_telemEventInfo[TELEM_PMC].idx = 0;
+	}
+	pw_pr_debug("OK, bypass telem stopped\n");
+}
+
+int read_telem(u64 *dst, enum telemetry_unit unit, bool should_retry)
+{
+	size_t num_iters = should_retry ? 10 : 0;
+	u64 timestamp = 0;
+
+	do {
+		timestamp = flush_telem_to_buffer(unit);
+	} while (!timestamp && should_retry && num_iters--);
+
+	if (timestamp) {
+		read_telem_from_buffer(dst, unit);
+		return 0;
+	}
+	return -1;
+}
+
+/**
+ * builtin_telemetry_available - Determine if telemetry driver is present
+ *
+ * Returns: 1 if telemetry driver is present, 0 if not.
+ */
+static int builtin_telemetry_available(void)
+{
+	int retval = 0;
+	struct telemetry_evtconfig punit_evtconfig;
+	struct telemetry_evtconfig pmc_evtconfig;
+	u32 punit_event_map[MAX_TELEM_EVENTS];
+	u32 pmc_event_map[MAX_TELEM_EVENTS];
+
+
+	/* The symbol below is weak.  We return 1 if we have a definition
+	 * for this telemetry-driver-supplied symbol, or 0 if only the
+	 * weak definition exists. This test will suffice to detect if
+	 * the telemetry driver is loaded.
+	 */
+	if (telemetry_get_eventconfig) {
+		/* OK, the telemetry driver is loaded. But it's possible it
+		 * hasn't been configured properly. To check that, retrieve
+		 * the number of events currently configured. This should never
+		 * be zero since the telemetry driver reserves some SSRAM slots
+		 * for its own use
+		 */
+		memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
+		memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
+
+		punit_evtconfig.evtmap = (u32 *) &punit_event_map;
+		pmc_evtconfig.evtmap = (u32 *) &pmc_event_map;
+
+		retval = telemetry_get_eventconfig(&punit_evtconfig, &pmc_evtconfig,
+						MAX_TELEM_EVENTS, MAX_TELEM_EVENTS);
+		return (retval == 0 && punit_evtconfig.num_evts > 0 &&
+			pmc_evtconfig.num_evts > 0);
+	}
+	return 0;
+}
+
+/**
+ * was_telemetry_setup - Check if the P-unit and PMC addresses have been mapped
+ *
+ * Returns: true if successfully mapped
+ */
+static bool was_telemetry_setup(void)
+{
+	return s_punitInterfaceAddr && s_punitDataAddr &&
+		s_telemEventInfo[TELEM_PUNIT].ssram_virt_addr /* P-unit */ &&
+		s_pmcIPCCmdAddr && s_pmcIPCStsAddr && s_pmcIPCWBufAddr &&
+		s_pmcIPCRBufAddr && s_telemEventInfo[TELEM_PMC].ssram_virt_addr;
+}
+
+
+/**
+ * sw_telem_init_func - Set up the telemetry unit to retrieve a data item
+ *						(e.g. counter).
+ * @descriptor:  The IO descriptor containing the unit and ID
+ *						of the telemetry info to gather.
+ *
+ * Because we don't (currently) control all of the counters, we
+ * economize by seeing if it's already being collected before allocate
+ * a slot for it.
+ *
+ * Returns: PW_SUCCESS  if the telem collector can collect the requested data.
+ *		 -PW_ERROR   if the the addition of that item fails.
+ */
+int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor)
+{
+	struct sw_driver_telem_io_descriptor *td =
+		&(descriptor->telem_descriptor);
+	u8  unit = td->unit;  /* Telemetry unit to use. */
+	u32 id; /* Event ID we want telemetry to track. */
+
+	if (!was_telemetry_setup())
+		return -ENXIO;
+
+	id = (u32)(td->id);
+
+	td->idx = add_telem_id(unit, id);
+	if (td->idx < 0) {
+		pw_pr_error("ERROR adding id 0x%x to unit %d\n", id, unit);
+		return -1;
+	}
+	pw_pr_debug("OK, added id 0x%x to unit %d at pos %d\n",
+			id, unit, td->idx);
+
+	return 0;
+}
+
+
+/**
+ * sw_read_telem_info - Read a metric's data from the telemetry driver.
+ * @dest:		Destination (storage for the read data)
+ * @cpu:		Which CPU to read from (not used)
+ * @descriptor:		The descriptor containing the data ID to read
+ * @data_size_in_bytes: The # of bytes in the result (always 8)
+ *
+ * Returns: Nothing, but stores SW_TELEM_READ_FAIL_VALUE to dest if
+ * the read fails.
+ */
+void sw_read_telem_info(char *dest, int cpu,
+			  const sw_driver_io_descriptor_t *descriptor,
+			  u16 data_size_in_bytes)
+{
+	u64 *data_dest = (u64 *)dest;
+	const struct sw_driver_telem_io_descriptor *td =
+		&(descriptor->telem_descriptor);
+	u8 unit = td->unit;
+	bool needs_refresh = false;
+
+	/*
+	 * Check if we need to refresh the list of values
+	 */
+	LOCK(sw_telem_lock);
+	{
+		if (s_telemEventInfo[unit].iters == 0)
+			needs_refresh = true;
+
+		if (++s_telemEventInfo[unit].iters ==
+				s_telemEventInfo[unit].idx)
+			s_telemEventInfo[unit].iters = 0;
+	}
+
+	UNLOCK(sw_telem_lock);
+
+	if (needs_refresh) {
+		u64 timestamp = flush_telem_to_buffer(unit);
+
+		pw_pr_debug("DEBUG: unit %d refreshed, timestamp = %llu\n",
+			unit, timestamp);
+		if (!timestamp) { /* failure */
+			*data_dest = SW_TELEM_READ_FAIL_VALUE;
+			return;
+		}
+	} else
+		pw_pr_debug("DEBUG: unit %d NOT refreshed\n", unit);
+
+	*data_dest = read_event_from_buffer(unit, td->idx);
+}
+
+/**
+ * sw_reset_telem - Stop collecting telemetry info.
+ * @descriptor: Unused in this function
+ *
+ * Stop collecting anything extra, and give the driver back to
+ * debugfs.  Because this driver increases the sampling rate, the
+ * kernel's telemetry driver can't succesfully reset the driver unless
+ * we first drop the rate back down to a much slower rate.  This is a
+ * temporary measure, since the reset operation will then reset the
+ * sampling interval to whatever the GMIN driver wants.
+ *
+ * Returns: 0
+ */
+int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor)
+{
+	if (IS_TELEM_CONFIGURED()) {
+		stop_telem();
+		remove_telem_ids();
+		/* Return control to 'builtin' telemetry driver */
+		telemetry_set_sampling_period(TELEM_SAMPLING_1S,
+					  TELEM_SAMPLING_1S);
+		telemetry_reset_events();
+	}
+	return 0;
+}
+
+/**
+ * sw_available_telem -- Decide if the telemetry subsystem is available for use
+ */
+bool sw_telem_available(void)
+{
+	/*
+	 * Telemetry driver MUST be loaded; we perform this check because
+	 * on some systems an error with the p-unit/pmc IPC interface causes
+	 * kernel panics.
+	 */
+	return builtin_telemetry_available();
+};
+
+bool sw_telem_post_config(void)
+{
+	if (start_telem())
+		return false;
+
+	return true;
+}
diff --git a/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c b/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
index e482a628d3ab..f9eb4ff2703d 100644
--- a/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
+++ b/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
@@ -1,2338 +1,2338 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-
-#include <linux/version.h> /* "LINUX_VERSION_CODE" */
-#include <linux/hrtimer.h>
-#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
-	#include <asm/cputime.h>
-#else
-	#include <linux/sched/cputime.h>
-#endif
-#include <asm/hardirq.h>
-#include <asm/local.h>
-
-#include <trace/events/power.h>
-#include <trace/events/irq.h>
-#include <trace/events/timer.h>
-#include <trace/events/power.h>
-#include <trace/events/sched.h>
-#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
-#include <asm/trace/irq_vectors.h> /* for the various APIC vector tracepoints
-				    *  (e.g. "thermal_apic",
-				    *  "local_timer" etc.)
-				    */
-#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
-struct pool_workqueue;
-struct cpu_workqueue_struct;
-#include <trace/events/workqueue.h>
-#include <linux/suspend.h> /* for 'pm_notifier' */
-#include <linux/cpufreq.h> /* for "cpufreq_notifier" */
-#include <linux/cpu.h> /* for 'CPU_UP_PREPARE' etc */
-
-#include "sw_kernel_defines.h"
-#include "sw_collector.h"
-#include "sw_overhead_measurements.h"
-#include "sw_tracepoint_handlers.h"
-#include "sw_output_buffer.h"
-#include "sw_mem.h"
-#include "sw_trace_notifier_provider.h"
-
-/* -------------------------------------------------
- * Compile time constants and useful macros.
- * -------------------------------------------------
- */
-#ifndef __get_cpu_var
-/*
- * Kernels >= 3.19 don't include a definition
- * of '__get_cpu_var'. Create one now.
- */
-#define __get_cpu_var(var) (*this_cpu_ptr(&var))
-#endif /* __get_cpu_var */
-
-#define BEGIN_LOCAL_IRQ_STATS_READ(p)                                          \
-	do {                                                                   \
-		p = &__get_cpu_var(irq_stat);
-
-#define END_LOCAL_IRQ_STATS_READ(p)                                            \
-	}                                                                      \
-	while (0)
-/*
- * CAS{32,64}
- */
-#define CAS32(p, o, n) (cmpxchg((p), (o), (n)) == (o))
-#define CAS64(p, o, n) (cmpxchg64((p), (o), (n)) == (o))
-/*
- * Timer start pid accessor macros
- */
-#ifdef CONFIG_TIMER_STATS
-#define GET_TIMER_THREAD_ID(t)                                                 \
-	((t)->start_pid) /* 'start_pid' is actually the thread ID
-			  * of the thread that initialized the timer
-			  */
-#else
-#define GET_TIMER_THREAD_ID(t) (-1)
-#endif /* CONFIG_TIMER_STATS */
-/*
- * Tracepoint probe register/unregister functions and
- * helper macros.
- */
-#if IS_ENABLED(CONFIG_TRACEPOINTS)
-#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
-#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
-	WARN_ON(register_trace_##name(probe))
-#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
-	unregister_trace_##name(probe)
-#elif KERNEL_VERSION(3, 15, 0) > LINUX_VERSION_CODE
-#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
-	WARN_ON(register_trace_##name(probe, NULL))
-#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
-	unregister_trace_##name(probe, NULL)
-#else
-#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
-	WARN_ON(tracepoint_probe_register(node->tp, probe, NULL))
-#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
-	tracepoint_probe_unregister(node->tp, probe, NULL)
-#endif
-#else /* CONFIG_TRACEPOINTS */
-#define DO_REGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
-#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
-#endif /* CONFIG_TRACEPOINTS */
-#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
-#define _DEFINE_PROBE_FUNCTION(name, ...) static void name(__VA_ARGS__)
-#else
-#define _DEFINE_PROBE_FUNCTION(name, ...)                                      \
-	static void name(void *ignore, __VA_ARGS__)
-#endif
-#define DEFINE_PROBE_FUNCTION(x) _DEFINE_PROBE_FUNCTION(x)
-
-/*
- * Tracepoint probe function parameters.
- * These tracepoint signatures depend on kernel version.
- */
-#if KERNEL_VERSION(2, 6, 36) > LINUX_VERSION_CODE
-#define PROBE_TPS_PARAMS                                                       \
-	sw_probe_power_start_i, unsigned int type, unsigned int state
-#elif KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-#define PROBE_TPS_PARAMS                                                       \
-	sw_probe_power_start_i, unsigned int type, unsigned int state,         \
-		unsigned int cpu_id
-#else
-#define PROBE_TPS_PARAMS                                                       \
-	sw_probe_cpu_idle_i, unsigned int state, unsigned int cpu_id
-#endif
-
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-#define PROBE_TPF_PARAMS                                                       \
-	sw_probe_power_frequency_i, unsigned int type, unsigned int state
-#else
-#define PROBE_TPF_PARAMS                                                       \
-	sw_probe_cpu_frequency_i, unsigned int new_freq, unsigned int cpu
-#endif
-
-#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
-#define PROBE_SCHED_WAKEUP_PARAMS                                              \
-	sw_probe_sched_wakeup_i, struct rq *rq, struct task_struct *task,      \
-		int success
-#else
-#define PROBE_SCHED_WAKEUP_PARAMS                                              \
-	sw_probe_sched_wakeup_i, struct task_struct *task, int success
-#endif
-
-#if IS_ENABLED(CONFIG_ANDROID)
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-#define PROBE_WAKE_LOCK_PARAMS sw_probe_wake_lock_i, struct wake_lock *lock
-#define PROBE_WAKE_UNLOCK_PARAMS                                               \
-	sw_probe_wake_unlock_i, struct wake_unlock *unlock
-#else
-#define PROBE_WAKE_LOCK_PARAMS                                                 \
-	sw_probe_wakeup_source_activate_i, const char *name, unsigned int state
-#define PROBE_WAKE_UNLOCK_PARAMS                                               \
-	sw_probe_wakeup_source_deactivate_i, const char *name,                 \
-		unsigned int state
-#endif /* version */
-#endif /* CONFIG_ANDROID */
-
-#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
-#define PROBE_WORKQUEUE_PARAMS                                                 \
-	sw_probe_workqueue_execution_i, struct task_struct *wq_thread,         \
-		struct work_struct *work
-#else
-#define PROBE_WORKQUEUE_PARAMS                                                 \
-	sw_probe_workqueue_execute_start_i, struct work_struct *work
-#endif
-
-#define PROBE_SCHED_SWITCH_PARAMS                                              \
-	sw_probe_sched_switch_i, struct task_struct *prev,                     \
-		struct task_struct *next
-/*
- * These tracepoint signatures are independent of kernel version.
- */
-#define PROBE_IRQ_PARAMS                                                       \
-	sw_probe_irq_handler_entry_i, int irq, struct irqaction *action
-#define PROBE_TIMER_ARGS sw_probe_timer_expire_entry_i, struct timer_list *t
-#define PROBE_HRTIMER_PARAMS                                                   \
-	sw_probe_hrtimer_expire_entry_i, struct hrtimer *hrt, ktime_t *now
-#define PROBE_PROCESS_FORK_PARAMS                                              \
-	sw_probe_sched_process_fork_i, struct task_struct *parent,             \
-		struct task_struct *child
-#define PROBE_SCHED_PROCESS_EXIT_PARAMS                                        \
-	sw_probe_sched_process_exit_i, struct task_struct *task
-#define PROBE_THERMAL_APIC_ENTRY_PARAMS                                        \
-	sw_probe_thermal_apic_entry_i, int vector
-#define PROBE_THERMAL_APIC_EXIT_PARAMS sw_probe_thermal_apic_exit_i, int vector
-
-#define IS_VALID_WAKEUP_EVENT(cpu)                                             \
-	({                                                                     \
-		bool *per_cpu_event =                                          \
-			&per_cpu(sw_is_valid_wakeup_event, (cpu));             \
-		bool old_value =                                               \
-			CAS32(per_cpu_event, true, sw_wakeup_event_flag);      \
-		old_value;                                                     \
-	})
-#define SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu) (IS_VALID_WAKEUP_EVENT(cpu))
-#define RESET_VALID_WAKEUP_EVENT_COUNTER(cpu)                                  \
-	(per_cpu(sw_is_valid_wakeup_event, (cpu)) = true)
-
-#define NUM_TRACEPOINT_NODES SW_ARRAY_SIZE(s_trace_collector_lists)
-#define NUM_VALID_TRACEPOINTS (NUM_TRACEPOINT_NODES - 1) /* "-1" for IPI */
-#define FOR_EACH_TRACEPOINT_NODE(idx, node)                                    \
-	for (idx = 0; idx < NUM_TRACEPOINT_NODES &&                            \
-		      (node = &s_trace_collector_lists[idx]);                  \
-	     ++idx)
-
-#define FOR_EACH_NOTIFIER_NODE(idx, node)                                      \
-	for (idx = 0; idx < SW_ARRAY_SIZE(s_notifier_collector_lists) &&       \
-		      (node = &s_notifier_collector_lists[idx]);               \
-	     ++idx)
-/*
- * Use these macros if all tracepoint ID numbers
- * ARE contiguous from 0 -- max tracepoint ID #
- */
-/* #if 0
-#define IS_VALID_TRACE_NOTIFIER_ID(id)                                         \
-	((id) >= 0 && (id) < SW_ARRAY_SIZE(s_trace_collector_lists))
-#define GET_COLLECTOR_TRACE_NODE(id) (&s_trace_collector_lists[id])
-#define FOR_EACH_trace_notifier_id(idx)                                        \
-	for (idx = 0; idx < SW_ARRAY_SIZE(s_trace_collector_lists); ++idx)
-#endif */
-/*
- * Use these macros if all tracepoint ID numbers
- * are NOT contiguous from 0 -- max tracepoint ID #
- */
-#define GET_COLLECTOR_TRACE_NODE(idx)                                          \
-	({                                                                     \
-		int __idx = 0;                                                 \
-		struct sw_trace_notifier_data *__node = NULL,                  \
-					      *__retVal = NULL;                \
-		FOR_EACH_TRACEPOINT_NODE(__idx, __node)                        \
-		{                                                              \
-			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
-				__retVal = __node;                             \
-				break;                                         \
-			}                                                      \
-		}                                                              \
-		__retVal;                                                      \
-	})
-#define IS_VALID_TRACE_NOTIFIER_ID(idx) (GET_COLLECTOR_TRACE_NODE(idx) != NULL)
-
-#define GET_COLLECTOR_NOTIFIER_NODE(idx)                                       \
-	({                                                                     \
-		int __idx = 0;                                                 \
-		struct sw_trace_notifier_data *__node = NULL,                  \
-					      *__retVal = NULL;                \
-		FOR_EACH_NOTIFIER_NODE(__idx, __node)                          \
-		{                                                              \
-			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
-				__retVal = __node;                             \
-				break;                                         \
-			}                                                      \
-		}                                                              \
-		__retVal;                                                      \
-	})
-#define IS_VALID_NOTIFIER_ID(idx) (GET_COLLECTOR_NOTIFIER_NODE(idx) != NULL)
-
-/* -------------------------------------------------
- * Local function declarations.
- * -------------------------------------------------
- */
-/*
- * The tracepoint registration functions.
- */
-int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
-#if KERNEL_VERSION(3,14,0) <= LINUX_VERSION_CODE
-    int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
-    int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
-    int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
-    int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
-#endif // KERNEL_VERSION(3,14,0) <= LINUX_VERSION_CODE
-#if IS_ENABLED(CONFIG_ANDROID)
-    #if KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
-        int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node);
-        int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node);
-        int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
-        int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
-    #else // KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
-        int sw_register_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
-        int sw_unregister_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
-        int sw_register_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
-        int sw_unregister_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
-    #endif // KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
-#endif // IS_ENABLED(CONFIG_ANDROID)
-int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
-int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node);
-int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node);
-int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node);
-int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node);
-int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
-int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
-int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
-int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
-void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu, int target_cpu);
-void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr, struct sw_trace_notifier_data *node,
-                                     pid_t tid);
-void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node);
-void sw_handle_workqueue_wakeup_helper_i(int cpu, struct sw_collector_data *node);
-void sw_handle_sched_switch_helper_i(void);
-void sw_tps_apic_i(int cpu);
-void sw_tps_tps_i(int cpu);
-void sw_tps_wakeup_i(int cpu);
-void sw_tps_i(void);
-void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node);
-void sw_process_fork_exit_helper_i(struct sw_collector_data *node, struct task_struct *task, bool is_fork);
-void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node, const char *name,
-                               int type, u64 timeout, int pid, int tid, const char *proc_name);
-u64 sw_my_local_arch_irq_stats_cpu_i(void);
-
-/*
- * The tracepoint probes.
- */
-/*
- * The tracepoint handlers.
- */
-void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node);
-void sw_handle_trace_notifier_on_cpu_i(int cpu, struct sw_trace_notifier_data *node);
-void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node);
-
-/* -------------------------------------------------
- * Variable definitions.
- * -------------------------------------------------
- */
-/*
- * For overhead measurements.
- */
-DECLARE_OVERHEAD_VARS(
-	sw_handle_timer_wakeup_helper_i); /* for the "timer_expire"
-					   *   family of probes
-					   */
-DECLARE_OVERHEAD_VARS(sw_handle_irq_wakeup_i); /* for IRQ wakeups */
-DECLARE_OVERHEAD_VARS(sw_handle_sched_wakeup_i); /* for SCHED */
-DECLARE_OVERHEAD_VARS(sw_tps_i); /* for TPS */
-DECLARE_OVERHEAD_VARS(sw_tpf_i); /* for TPF */
-DECLARE_OVERHEAD_VARS(sw_process_fork_exit_helper_i);
-#if IS_ENABLED(CONFIG_ANDROID)
-DECLARE_OVERHEAD_VARS(sw_handle_wakelock_i); /* for wake lock/unlock */
-#endif /* CONFIG_ANDROID */
-DECLARE_OVERHEAD_VARS(sw_handle_workqueue_wakeup_helper_i);
-DECLARE_OVERHEAD_VARS(sw_handle_sched_switch_helper_i);
-/*
- * Per-cpu wakeup counters.
- * Used to decide which wakeup event is the first to occur after a
- * core wakes up from a C-state.
- * Set to 'true' in TPS probe
- */
-static DEFINE_PER_CPU(bool, sw_is_valid_wakeup_event) = { true };
-/*
- * Per-cpu counts of the number of times the local APIC fired.
- * We need a separate count because some apic timer fires don't seem
- * to result in hrtimer/timer expires
- */
-static DEFINE_PER_CPU(u64, sw_num_local_apic_timer_inters);
-/*
- * Flag value to use to decide if the event is a valid wakeup event.
- * Set to 'false' in TPS probe.
- */
-static bool sw_wakeup_event_flag = true;
-
-#if IS_ENABLED(CONFIG_TRACEPOINTS)
-/*
- * Scheduler-based polling emulation.
- */
-static DEFINE_PER_CPU(unsigned long, sw_pcpu_polling_jiff);
-#endif /* CONFIG_TRACEPOINTS */
-
-pw_u16_t sw_min_polling_interval_msecs;
-
-/*
- * IDs for supported tracepoints.
- */
-enum sw_trace_id {
-	SW_TRACE_ID_CPU_IDLE,
-	SW_TRACE_ID_CPU_FREQUENCY,
-	SW_TRACE_ID_IRQ_HANDLER_ENTRY,
-	SW_TRACE_ID_TIMER_EXPIRE_ENTRY,
-	SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY,
-	SW_TRACE_ID_SCHED_WAKEUP,
-	SW_TRACE_ID_IPI,
-	SW_TRACE_ID_SCHED_PROCESS_FORK,
-	SW_TRACE_ID_SCHED_PROCESS_EXIT,
-	SW_TRACE_ID_THERMAL_APIC_ENTRY,
-	SW_TRACE_ID_THERMAL_APIC_EXIT,
-	SW_TRACE_ID_WAKE_LOCK,
-	SW_TRACE_ID_WAKE_UNLOCK,
-	SW_TRACE_ID_WORKQUEUE_EXECUTE_START,
-	SW_TRACE_ID_SCHED_SWITCH,
-};
-
-/*
- * IDs for supported notifiers.
- */
-enum sw_notifier_id {
-	SW_NOTIFIER_ID_SUSPEND, /* TODO: change name? */
-	SW_NOTIFIER_ID_SUSPEND_ENTER,
-	SW_NOTIFIER_ID_SUSPEND_EXIT,
-	SW_NOTIFIER_ID_HIBERNATE,
-	SW_NOTIFIER_ID_HIBERNATE_ENTER,
-	SW_NOTIFIER_ID_HIBERNATE_EXIT,
-	SW_NOTIFIER_ID_COUNTER_RESET,
-	SW_NOTIFIER_ID_CPUFREQ,
-	SW_NOTIFIER_ID_HOTCPU,
-};
-
-/*
- * Names for supported tracepoints. A tracepoint
- * 'name' consists of two strings: a "kernel" string
- * that is used to locate the tracepoint within the kernel
- * and an "abstract" string, that is used by Ring-3 to
- * specify which tracepoints to use during a collection.
- */
-static const struct sw_trace_notifier_name s_trace_names[] = {
-	[SW_TRACE_ID_CPU_IDLE] = { "cpu_idle", "CPU-IDLE" },
-	[SW_TRACE_ID_CPU_FREQUENCY] = { "cpu_frequency", "CPU-FREQUENCY" },
-	[SW_TRACE_ID_IRQ_HANDLER_ENTRY] = { "irq_handler_entry", "IRQ-ENTRY" },
-	[SW_TRACE_ID_TIMER_EXPIRE_ENTRY] = { "timer_expire_entry",
-					     "TIMER-ENTRY" },
-	[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY] = { "hrtimer_expire_entry",
-					       "HRTIMER-ENTRY" },
-	[SW_TRACE_ID_SCHED_WAKEUP] = { "sched_wakeup", "SCHED-WAKEUP" },
-	[SW_TRACE_ID_IPI] = { NULL, "IPI" },
-	[SW_TRACE_ID_SCHED_PROCESS_FORK] = { "sched_process_fork",
-					     "PROCESS-FORK" },
-	[SW_TRACE_ID_SCHED_PROCESS_EXIT] = { "sched_process_exit",
-					     "PROCESS-EXIT" },
-#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
-	[SW_TRACE_ID_THERMAL_APIC_ENTRY] = { "thermal_apic_entry",
-					     "THERMAL-THROTTLE-ENTRY" },
-	[SW_TRACE_ID_THERMAL_APIC_EXIT] = { "thermal_apic_exit",
-					    "THERMAL-THROTTLE-EXIT" },
-#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE  */
-#if IS_ENABLED(CONFIG_ANDROID)
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-	[SW_TRACE_ID_WAKE_LOCK] = { "wake_lock", "WAKE-LOCK" },
-	[SW_TRACE_ID_WAKE_UNLOCK] = { "wake_unlock", "WAKE-UNLOCK" },
-#else /* KERNEL_VERSION(3, 4, 0) <= LINUX_VERSION_CODE */
-	[SW_TRACE_ID_WAKE_LOCK] = { "wakeup_source_activate", "WAKE-LOCK" },
-	[SW_TRACE_ID_WAKE_UNLOCK] = { "wakeup_source_deactivate",
-				      "WAKE-UNLOCK" },
-#endif
-#endif
-	[SW_TRACE_ID_WORKQUEUE_EXECUTE_START] = { "workqueue_execute_start",
-						  "WORKQUEUE-START" },
-	[SW_TRACE_ID_SCHED_SWITCH] = { "sched_switch", "CONTEXT-SWITCH" },
-};
-
-/*
- * Names for supported notifiers. A notifier
- * 'name' consists of two strings: an unused "kernel" string
- * and an "abstract" string, that is used by Ring-3 to
- * specify which notifiers to use during a collection.
- */
-static const struct sw_trace_notifier_name s_notifier_names[] = {
-	[SW_NOTIFIER_ID_SUSPEND] = { "suspend_notifier" /* don't care */,
-				     "SUSPEND-NOTIFIER" },
-	[SW_NOTIFIER_ID_SUSPEND_ENTER] = { NULL, "SUSPEND-ENTER" },
-	[SW_NOTIFIER_ID_SUSPEND_EXIT] = { NULL, "SUSPEND-EXIT" },
-	[SW_NOTIFIER_ID_HIBERNATE] = { "hibernate_notifier" /* don't care */,
-				       "HIBERNATE-NOTIFIER" },
-	[SW_NOTIFIER_ID_HIBERNATE_ENTER] = { NULL, "HIBERNATE-ENTER" },
-	[SW_NOTIFIER_ID_HIBERNATE_EXIT] = { NULL, "HIBERNATE-EXIT" },
-	[SW_NOTIFIER_ID_COUNTER_RESET] = { NULL, "COUNTER-RESET" },
-	[SW_NOTIFIER_ID_CPUFREQ] = { "cpufreq_notifier" /* don't care */,
-				     "CPUFREQ-NOTIFIER" },
-	[SW_NOTIFIER_ID_HOTCPU] = { "hotcpu_notifier" /* don't care */,
-				    "HOTCPU-NOTIFIER" },
-};
-
-#if IS_ENABLED(CONFIG_TRACEPOINTS)
-/*
- * A list of supported tracepoints.
- */
-static struct sw_trace_notifier_data s_trace_collector_lists[] = {
-	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_CPU_IDLE],
-	  &sw_register_trace_cpu_idle_i, &sw_unregister_trace_cpu_idle_i,
-	  NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_CPU_FREQUENCY],
-	  &sw_register_trace_cpu_frequency_i,
-	  &sw_unregister_trace_cpu_frequency_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_IRQ_HANDLER_ENTRY],
-	  &sw_register_trace_irq_handler_entry_i,
-	  &sw_unregister_trace_irq_handler_entry_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_TIMER_EXPIRE_ENTRY],
-	  &sw_register_trace_timer_expire_entry_i,
-	  &sw_unregister_trace_timer_expire_entry_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY],
-	  &sw_register_trace_hrtimer_expire_entry_i,
-	  &sw_unregister_trace_hrtimer_expire_entry_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_SCHED_WAKEUP],
-	  &sw_register_trace_sched_wakeup_i,
-	  &sw_unregister_trace_sched_wakeup_i, NULL },
-	/* Placeholder for IPI -- no tracepoints associated with it! */
-	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_IPI], NULL,
-	  NULL, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_FORK],
-	  &sw_register_trace_sched_process_fork_i,
-	  &sw_unregister_trace_sched_process_fork_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_EXIT],
-	  &sw_register_trace_sched_process_exit_i,
-	  &sw_unregister_trace_sched_process_exit_i, NULL },
-#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
-	/*
-	 * For thermal throttling.
-	 * We probably only need one of either 'entry' or 'exit'. Use
-	 * both, until we decide which one to keep. Note that
-	 * tracepoint IDs for these, and subsequent tracepoints
-	 * (e.g. 'wake_lock') will change once we've picked which
-	 * one to use.
-	 */
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_ENTRY],
-	  &sw_register_trace_thermal_apic_entry_i,
-	  &sw_unregister_trace_thermal_apic_entry_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_EXIT],
-	  &sw_register_trace_thermal_apic_exit_i,
-	  &sw_unregister_trace_thermal_apic_exit_i, NULL },
-#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
-/* Wakelocks have multiple tracepoints, depending on kernel version */
-#if IS_ENABLED(CONFIG_ANDROID)
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
-	  &sw_register_trace_wake_lock_i, &sw_unregister_trace_wake_lock_i,
-	  NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
-	  &sw_register_trace_wake_unlock_i, &sw_unregister_trace_wake_unlock_i,
-	  NULL },
-#else /* KERNEL_VERSION(3, 4, 0) <= LINUX_VERSION_CODE  */
-	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
-	  &sw_register_trace_wakeup_source_activate_i,
-	  &sw_unregister_trace_wakeup_source_activate_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
-	  &sw_register_trace_wakeup_source_deactivate_i,
-	  &sw_unregister_trace_wakeup_source_deactivate_i, NULL },
-#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-#endif /* CONFIG_ANDROID */
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_WORKQUEUE_EXECUTE_START],
-	  &sw_register_trace_workqueue_execution_i,
-	  &sw_unregister_trace_workqueue_execution_i, NULL },
-	{ SW_TRACE_COLLECTOR_TRACEPOINT,
-	  &s_trace_names[SW_TRACE_ID_SCHED_SWITCH],
-	  &sw_register_trace_sched_switch_i,
-	  &sw_unregister_trace_sched_switch_i, NULL },
-};
-
-/*
- * List of supported notifiers.
- */
-static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND], &sw_register_pm_notifier_i,
-	  &sw_unregister_pm_notifier_i, NULL, true /* always register */ },
-	/* Placeholder for suspend enter/exit -- these will be called
-	 * from within the pm notifier
-	 */
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_ENTER], NULL, NULL, NULL },
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_EXIT], NULL, NULL, NULL },
-	/* Placeholder for hibernate enter/exit -- these will be called
-	 * from within the pm notifier
-	 */
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE], NULL, NULL, NULL },
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_ENTER], NULL, NULL, NULL },
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_EXIT], NULL, NULL, NULL },
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_COUNTER_RESET], NULL, NULL, NULL },
-	{ SW_TRACE_COLLECTOR_NOTIFIER,
-	  &s_notifier_names[SW_NOTIFIER_ID_CPUFREQ],
-	  &sw_register_cpufreq_notifier_i, &sw_unregister_cpufreq_notifier_i },
-};
-
-/*
- * Special entry for CPU notifier (i.e. "hotplug" notifier)
- * We don't want these to be visible to the user.
- */
-static struct sw_trace_notifier_data s_hotplug_notifier_data = {
-	SW_TRACE_COLLECTOR_NOTIFIER,
-	&s_notifier_names[SW_NOTIFIER_ID_HOTCPU],
-	&sw_register_hotcpu_notifier_i,
-	&sw_unregister_hotcpu_notifier_i,
-	NULL,
-	true /* always register */
-};
-#else /* !CONFIG_TRACEPOINTS */
-/*
- * A list of supported tracepoints.
- */
-static struct sw_trace_notifier_data s_trace_collector_lists[] = {
-	/* EMPTY */};
-/*
- * List of supported notifiers.
- */
-static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
-	/* EMPTY */ };
-
-static struct sw_trace_notifier_data s_hotplug_notifier_data = {
-	/* EMPTY */
-};
-
-
-#endif /* CONFIG_TRACEPOINTS */
-
-/*
- * Macros to retrieve tracepoint and notifier IDs.
- */
-#define GET_TRACE_ID_FROM_NODE(node) ((node)->name - s_trace_names)
-#define GET_NOTIFIER_ID_FROM_NODE(node) ((node)->name - s_notifier_names)
-
-#define GET_TRACE_NOTIFIER_ID(node)                                            \
-	(int)(((node)->type == SW_TRACE_COLLECTOR_TRACEPOINT) ?                \
-		      GET_TRACE_ID_FROM_NODE(node) :                           \
-		      GET_NOTIFIER_ID_FROM_NODE(node))
-
-/* -------------------------------------------------
- * Function definitions.
- * -------------------------------------------------
- */
-
-/*
- * Retrieve a TSC value
- */
-static inline u64 sw_tscval(void)
-{
-	unsigned int low, high;
-
-	asm volatile("rdtsc" : "=a"(low), "=d"(high));
-	return low | ((unsigned long long)high) << 32;
-};
-
-u64 sw_timestamp(void)
-{
-	struct timespec ts;
-
-	getnstimeofday(&ts);
-	return (ts.tv_sec * 1000000000ULL + ts.tv_nsec);
-}
-
-/*
- * Basically the same as arch/x86/kernel/irq.c --> "arch_irq_stat_cpu(cpu)"
- */
-u64 sw_my_local_arch_irq_stats_cpu_i(void)
-{
-	u64 sum = 0;
-	irq_cpustat_t *stats;
-#ifdef __arm__
-	int i = 0;
-#endif
-	BEGIN_LOCAL_IRQ_STATS_READ(stats);
-	{
-#ifndef __arm__
-		sum += stats->__nmi_count;
-#if IS_ENABLED(CONFIG_X86_LOCAL_APIC)
-		sum += stats->apic_timer_irqs;
-		sum += stats->irq_spurious_count;
-#endif
-#if KERNEL_VERSION(2, 6, 34) <= LINUX_VERSION_CODE
-		sum += stats->x86_platform_ipis;
-#endif /* 2,6,34 */
-		sum += stats->apic_perf_irqs;
-#if KERNEL_VERSION(3, 5, 0) <= LINUX_VERSION_CODE
-		sum += stats->apic_irq_work_irqs;
-#endif /* 3,5,0 */
-#ifdef CONFIG_SMP
-		sum += stats->irq_call_count;
-		sum += stats->irq_resched_count;
-		sum += stats->irq_tlb_count;
-#endif
-#ifdef CONFIG_X86_THERMAL_VECTOR
-		sum += stats->irq_thermal_count;
-#endif
-
-#else
-		sum += stats->__softirq_pending;
-#ifdef CONFIG_SMP
-		for (i = 0; i < NR_IPI; ++i)
-			sum += stats->ipi_irqs[i];
-
-#endif
-#ifdef CONFIG_X86_MCE
-		sum += stats->mce_exception_count;
-		sum += stats->mce_poll_count;
-#endif
-#endif
-	}
-	END_LOCAL_IRQ_STATS_READ(stats);
-	return sum;
-};
-
-/*
- * Generic tracepoint/notifier handling function.
- */
-void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node)
-{
-	struct sw_collector_data *curr = NULL;
-
-	if (!node)
-		return;
-
-	list_for_each_entry(curr, &node->list, list) {
-		pw_pr_debug("DEBUG: handling message\n");
-		sw_handle_per_cpu_msg(curr);
-	}
-};
-
-/*
- * Generic tracepoint/notifier handling function.
- */
-void sw_handle_trace_notifier_on_cpu_i(int cpu,
-				       struct sw_trace_notifier_data *node)
-{
-	struct sw_collector_data *curr = NULL;
-
-	if (!node)
-		return;
-
-	list_for_each_entry(curr, &node->list, list)
-		sw_handle_per_cpu_msg_on_cpu(cpu, curr);
-
-};
-
-void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node)
-{
-	struct sw_collector_data *curr = NULL;
-
-	if (!node)
-		return;
-
-	list_for_each_entry(curr, &node->list, list) {
-		pw_pr_debug("Handling message of unknown cpumask on cpu %d\n",
-			    RAW_CPU());
-		sw_schedule_work(&curr->cpumask, &sw_handle_per_cpu_msg, curr);
-	}
-}
-
-/*
- * Tracepoint helpers.
- */
-
-/*
- * TIMER wakeup handling function.
- */
-static void sw_handle_timer_wakeup_i(struct sw_collector_data *node, pid_t pid,
-			      pid_t tid)
-{
-	int cpu = RAW_CPU();
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
-	char *dst_vals = msg->p_payload;
-
-	/* msg->tsc = sw_timestamp(); */
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = cpu;
-
-	/*
-	 * TIMER handling ==> only return the pid, tid
-	 */
-	*((int *)dst_vals) = pid;
-	dst_vals += sizeof(pid);
-	*((int *)dst_vals) = tid;
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-	pw_pr_debug("HANDLED timer expire for %d, %d\n", pid, tid);
-};
-
-/*
- * Helper function for {hr}timer expires. Required for overhead tracking.
- */
-void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr,
-				     struct sw_trace_notifier_data *node,
-				     pid_t tid)
-{
-	pid_t pid = -1;
-
-	if (tid == 0)
-		pid = 0;
-	else {
-		struct task_struct *task =
-			pid_task(find_pid_ns(tid, &init_pid_ns), PIDTYPE_PID);
-		if (likely(task))
-			pid = task->tgid;
-	}
-	list_for_each_entry(curr, &node->list, list)
-		sw_handle_timer_wakeup_i(curr, pid, tid);
-};
-
-/*
- * SCHED wakeup handling function.
- */
-void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu,
-			      int target_cpu)
-{
-	int cpu = source_cpu;
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
-	char *dst_vals = msg->p_payload;
-
-	/* msg->tsc = sw_timestamp(); */
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = source_cpu;
-
-	/*
-	 * sched handling ==> only return the source, target CPUs
-	 */
-	*((int *)dst_vals) = source_cpu;
-	dst_vals += sizeof(source_cpu);
-	*((int *)dst_vals) = target_cpu;
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_NONE))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-};
-
-/*
- * APIC timer wakeup
- */
-void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node)
-{
-	/*
-	 * Send an empty message back to Ring-3
-	 */
-	int cpu = RAW_CPU();
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
-
-	/* msg->tsc = sw_timestamp(); */
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = cpu;
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-	pw_pr_debug("HANDLED APIC timer wakeup for cpu = %d\n", cpu);
-};
-
-/*
- * Helper function for workqueue executions. Required for overhead tracking.
- */
-void sw_handle_workqueue_wakeup_helper_i(int cpu,
-					 struct sw_collector_data *node)
-{
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-
-	/* msg->tsc = sw_timestamp(); */
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = cpu;
-
-	/*
-	 * Workqueue wakeup ==> empty message.
-	 */
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_error("WARNING: could NOT produce message!\n");
-};
-
-/*
- * Helper function for sched_switch. Required for overhead tracking.
- */
-void sw_handle_sched_switch_helper_i(void)
-{
-	static struct sw_trace_notifier_data *node;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_SWITCH);
-		pw_pr_debug("SCHED SWITCH NODE = %p\n", node);
-	}
-	if (!node)
-		return;
-
-	preempt_disable();
-	{
-		struct sw_collector_data *curr;
-
-		list_for_each_entry(curr, &node->list, list) {
-			unsigned long curr_jiff = jiffies,
-				      prev_jiff = curr->last_update_jiffies;
-			unsigned long delta_msecs =
-				jiffies_to_msecs(curr_jiff) -
-				jiffies_to_msecs(prev_jiff);
-			struct cpumask *mask = &curr->cpumask;
-			u16 timeout = curr->info->sampling_interval_msec;
-
-			if (!timeout)
-				timeout = sw_min_polling_interval_msecs;
-
-			/* Has there been enough time since the last
-			 * collection point?
-			 */
-			if (delta_msecs < timeout)
-				continue;
-
-			/* Update timestamp and handle message */
-			if (cpumask_test_cpu(
-				    RAW_CPU(),
-				    mask) /* This msg must be handled on
-					   * the current CPU
-					   */
-			    ||
-			    cpumask_empty(
-				    mask) /* This msg may be handled by
-					   * any CPU
-					   */) {
-				if (!CAS64(&curr->last_update_jiffies,
-					   prev_jiff, curr_jiff)) {
-					/*
-					 * CAS failure should only be possible
-					 * for messages that can be handled
-					 * on any CPU, in which case it
-					 * indicates a different CPU already
-					 * handled this message.
-					 */
-					continue;
-				}
-				sw_handle_per_cpu_msg_no_sched(curr);
-			}
-		}
-	}
-	preempt_enable();
-};
-
-/*
- * Probe functions.
- */
-
-/*
- * 1. TPS
- */
-
-/*
- * Check IPI wakeups within the cpu_idle tracepoint.
- */
-void sw_tps_apic_i(int cpu)
-{
-	static struct sw_trace_notifier_data *apic_timer_node;
-
-	if (unlikely(apic_timer_node == NULL)) {
-		apic_timer_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IPI);
-		pw_pr_debug("apic NODE = %p\n", apic_timer_node);
-	}
-	if (apic_timer_node) {
-		bool local_apic_timer_fired = false;
-		u64 curr_num_local_apic = sw_my_local_arch_irq_stats_cpu_i();
-		u64 *old_num_local_apic =
-			&__get_cpu_var(sw_num_local_apic_timer_inters);
-
-		if (*old_num_local_apic &&
-		    (*old_num_local_apic != curr_num_local_apic)) {
-			local_apic_timer_fired = true;
-		}
-		*old_num_local_apic = curr_num_local_apic;
-
-		if (local_apic_timer_fired &&
-		    SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
-			struct sw_collector_data *curr = NULL;
-
-			list_for_each_entry(curr, &apic_timer_node->list,
-					     list) {
-				sw_handle_apic_timer_wakeup_i(curr);
-			}
-		}
-	}
-};
-
-/*
- * Perform any user-defined tasks within the
- * cpu_idle tracepoint.
- */
-void sw_tps_tps_i(int cpu)
-{
-	static struct sw_trace_notifier_data *tps_node;
-
-	if (unlikely(tps_node == NULL)) {
-		tps_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_IDLE);
-		pw_pr_debug("TPS NODE = %p\n", tps_node);
-	}
-	sw_handle_trace_notifier_i(tps_node);
-};
-
-/*
- * Perform any wakeup-related tasks within the
- * cpu_idle tracepoint.
- */
-void sw_tps_wakeup_i(int cpu)
-{
-	/*
-	 * For now, assume we will always have to
-	 * do some wakeup book keeping. Later, we'll
-	 * need to detect if the user requested wakeups.
-	 */
-	sw_wakeup_event_flag = false;
-	RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
-};
-
-void sw_tps_i(void)
-{
-	/*
-	 * Update: FIRST handle IPI wakeups
-	 * THEN handle TPS
-	 */
-	int cpu = RAW_CPU();
-
-	sw_tps_apic_i(cpu);
-	sw_tps_tps_i(cpu);
-	sw_tps_wakeup_i(cpu);
-};
-
-/*
- * 2. TPF
- */
-
-/*
- * Helper function for overhead measurements.
- */
-void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node)
-{
-	sw_handle_trace_notifier_on_cpu_i((int)cpu, node);
-};
-
-#if IS_ENABLED(CONFIG_TRACEPOINTS)
-DEFINE_PROBE_FUNCTION(PROBE_TPS_PARAMS)
-{
-#if KERNEL_VERSION(2, 6, 38) <= LINUX_VERSION_CODE
-	if (state == PWR_EVENT_EXIT)
-		return;
-#endif
-	DO_PER_CPU_OVERHEAD_FUNC(sw_tps_i);
-};
-
-DEFINE_PROBE_FUNCTION(PROBE_TPF_PARAMS)
-{
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-	int cpu = RAW_CPU();
-#endif /* version < 2.6.38 */
-	static struct sw_trace_notifier_data *node;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_FREQUENCY);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
-};
-
-/*
- * IRQ wakeup handling function.
- */
-static void sw_handle_irq_wakeup_i(struct sw_collector_data *node, int irq)
-{
-	int cpu = RAW_CPU();
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
-	char *dst_vals = msg->p_payload;
-
-	/* msg->tsc = sw_timestamp(); */
-	/* msg TSC assigned when msg is written to buffer */
-	msg->cpuidx = cpu;
-
-	/*
-	 * IRQ handling ==> only return the irq number
-	 */
-	*((int *)dst_vals) = irq;
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-};
-
-/*
- * 3. IRQ handler entry
- */
-DEFINE_PROBE_FUNCTION(PROBE_IRQ_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-
-	struct sw_collector_data *curr = NULL;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IRQ_HANDLER_ENTRY);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
-		return;
-
-	list_for_each_entry(curr, &node->list, list)
-		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_irq_wakeup_i, curr, irq);
-
-};
-
-/*
- * 4. TIMER expire
- */
-DEFINE_PROBE_FUNCTION(PROBE_TIMER_ARGS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-
-	struct sw_collector_data *curr = NULL;
-	pid_t tid = GET_TIMER_THREAD_ID(t);
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_TIMER_EXPIRE_ENTRY);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-
-	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
-		return;
-
-	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
-				 tid);
-};
-
-/*
- * 5. HRTIMER expire
- */
-DEFINE_PROBE_FUNCTION(PROBE_HRTIMER_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-	struct sw_collector_data *curr = NULL;
-	pid_t tid = GET_TIMER_THREAD_ID(hrt);
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(
-			SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-
-	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
-		return;
-
-	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
-				 tid);
-};
-
-/*
- * 6. SCHED wakeup
- */
-DEFINE_PROBE_FUNCTION(PROBE_SCHED_WAKEUP_PARAMS)
-{
-	static struct sw_trace_notifier_data *node;
-	struct sw_collector_data *curr = NULL;
-	int target_cpu = task_cpu(task), source_cpu = RAW_CPU();
-	/*
-	 * "Self-sched" samples are "don't care".
-	 */
-	if (target_cpu == source_cpu)
-		return;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_WAKEUP);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	/*
-	 * Unlike other wakeup sources, we check the per-cpu flag
-	 * of the TARGET cpu to decide if we should produce a sample.
-	 */
-	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(target_cpu))
-		return;
-
-	list_for_each_entry(curr, &node->list, list) {
-		/* sw_handle_sched_wakeup_i(curr, source_cpu, target_cpu); */
-		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_wakeup_i, curr,
-					 source_cpu, target_cpu);
-	}
-};
-
-/*
- * 8. PROCESS fork
- */
-
-/*
- * Helper for PROCESS fork, PROCESS exit
- */
-void sw_process_fork_exit_helper_i(struct sw_collector_data *node,
-				   struct task_struct *task, bool is_fork)
-{
-	int cpu = RAW_CPU();
-	pid_t pid = task->tgid, tid = task->pid;
-	const char *name = task->comm;
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	char *dst_vals = msg->p_payload;
-
-	msg->cpuidx = cpu;
-
-	/*
-	 * Fork/Exit ==> return pid, tid
-	 * Fork ==> also return name
-	 */
-	*((int *)dst_vals) = pid;
-	dst_vals += sizeof(pid);
-	*((int *)dst_vals) = tid;
-	dst_vals += sizeof(tid);
-	if (is_fork)
-		memcpy(dst_vals, name, SW_MAX_PROC_NAME_SIZE);
-
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-	pw_pr_debug(
-		"HANDLED process %s event for task: pid = %d, tid = %d, name = %s\n",
-		is_fork ? "FORK" : "EXIT", pid, tid, name);
-};
-
-DEFINE_PROBE_FUNCTION(PROBE_PROCESS_FORK_PARAMS)
-{
-	static struct sw_trace_notifier_data *node;
-	struct sw_collector_data *curr = NULL;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_FORK);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	if (!node)
-		return;
-
-	list_for_each_entry(curr, &node->list, list) {
-		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
-					 child, true /* true ==> fork */);
-	}
-};
-
-/*
- * 9. PROCESS exit
- */
-DEFINE_PROBE_FUNCTION(PROBE_SCHED_PROCESS_EXIT_PARAMS)
-{
-	static struct sw_trace_notifier_data *node;
-	struct sw_collector_data *curr = NULL;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_EXIT);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	if (!node)
-		return;
-
-	list_for_each_entry(curr, &node->list, list) {
-		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
-					 task, false /* false ==> exit */);
-	}
-};
-
-#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
-/*
- * 10. THERMAL_APIC entry
- */
-DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_ENTRY_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_ENTRY);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
-};
-
-/*
- * 10. THERMAL_APIC exit
- */
-DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_EXIT_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_EXIT);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
-};
-#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
-
-#if IS_ENABLED(CONFIG_ANDROID)
-/*
- * 11. WAKE lock / WAKEUP source activate.
- */
-
-/*
- * Helper function to produce wake lock/unlock messages.
- */
-void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node,
-			       const char *name, int type, u64 timeout, int pid,
-			       int tid, const char *proc_name)
-{
-	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
-						    node->per_msg_payload_size);
-	char *dst_vals = msg->p_payload;
-
-	msg->cpuidx = cpu;
-
-	/*
-	 * Protocol:
-	 * wakelock_timeout, wakelock_type, wakelock_name,
-	 * proc_pid, proc_tid, proc_name
-	 */
-	*((u64 *)dst_vals) = timeout;
-	dst_vals += sizeof(timeout);
-	*((int *)dst_vals) = type;
-	dst_vals += sizeof(type);
-	strncpy(dst_vals, name, SW_MAX_KERNEL_WAKELOCK_NAME_SIZE);
-	dst_vals += SW_MAX_KERNEL_WAKELOCK_NAME_SIZE;
-
-	*((int *)dst_vals) = pid;
-	dst_vals += sizeof(pid);
-	*((int *)dst_vals) = tid;
-	dst_vals += sizeof(tid);
-	strncpy(dst_vals, proc_name, SW_MAX_PROC_NAME_SIZE);
-	dst_vals += SW_MAX_PROC_NAME_SIZE;
-
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_warn("WARNING: could NOT produce message!\n");
-
-};
-
-/*
- * Helper function to handle wake lock/unlock callbacks.
- */
-void sw_handle_wakelock_i(int cpu, struct sw_trace_notifier_data *node,
-			  const char *name, int type, u64 timeout)
-{
-	int pid = PID(), tid = TID();
-	const char *proc_name = NAME();
-	struct sw_collector_data *curr = NULL;
-
-	if (!node)
-		return;
-
-
-	list_for_each_entry(curr, &node->list, list) {
-		sw_produce_wakelock_msg_i(cpu, curr, name, type, timeout, pid,
-					  tid, proc_name);
-	}
-};
-
-DEFINE_PROBE_FUNCTION(PROBE_WAKE_LOCK_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-	enum sw_kernel_wakelock_type type = SW_WAKE_LOCK;
-	u64 timeout = 0;
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-	const char *name = lock->name;
-#endif
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_LOCK);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-	/*
-	 * Was this wakelock acquired with a timeout i.e.
-	 * is this an auto expire wakelock?
-	 */
-	if (lock->flags & (1U << 10)) {
-		type = SW_WAKE_LOCK_TIMEOUT;
-		timeout = jiffies_to_msecs(lock->expires - jiffies);
-	}
-#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
-				 (int)type, timeout);
-};
-
-/*
- * 11. WAKE unlock / WAKEUP source deactivate.
- */
-DEFINE_PROBE_FUNCTION(PROBE_WAKE_UNLOCK_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-	enum sw_kernel_wakelock_type type = SW_WAKE_UNLOCK;
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-	const char *name = lock->name;
-#endif
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_UNLOCK);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
-				 (int)type, 0 /*timeout*/);
-};
-#endif /* CONFIG_ANDROID */
-
-/*
- * 12. WORKQUEUE
- */
-DEFINE_PROBE_FUNCTION(PROBE_WORKQUEUE_PARAMS)
-{
-	int cpu = RAW_CPU();
-	static struct sw_trace_notifier_data *node;
-	struct sw_collector_data *curr = NULL;
-
-	if (unlikely(node == NULL)) {
-		node = GET_COLLECTOR_TRACE_NODE(
-			SW_TRACE_ID_WORKQUEUE_EXECUTE_START);
-		pw_pr_debug("NODE = %p\n", node);
-	}
-
-	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
-		return;
-
-	list_for_each_entry(curr, &node->list, list)
-		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_workqueue_wakeup_helper_i,
-					 cpu, curr);
-
-};
-
-/*
- * 13. SCHED switch
- */
-DEFINE_PROBE_FUNCTION(PROBE_SCHED_SWITCH_PARAMS)
-{
-	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_switch_helper_i);
-};
-
-/*
- * 1. SUSPEND notifier
- */
-static void sw_send_pm_notification_i(int value)
-{
-	struct sw_driver_msg *msg = NULL;
-	size_t buffer_len = sizeof(*msg) + sizeof(value);
-	char *buffer = vmalloc(buffer_len);
-
-	if (!buffer) {
-		pw_pr_error(
-			"couldn't allocate memory when sending suspend notification!\n");
-		return;
-	}
-	msg = (struct sw_driver_msg *)buffer;
-	msg->tsc = sw_timestamp();
-	msg->cpuidx = RAW_CPU();
-	msg->plugin_id = 0; /* "0" indicates a system message */
-	msg->metric_id = 1; /* "1" indicates a suspend/resume message (TODO) */
-	msg->msg_id = 0;
-	/* don't care; TODO: use the 'msg_id' to encode the 'value'? */
-	msg->payload_len = sizeof(value);
-	msg->p_payload = buffer + sizeof(*msg);
-	*((int *)msg->p_payload) = value;
-	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
-		pw_pr_error("couldn't produce generic message!\n");
-
-	vfree(buffer);
-}
-
-static u64 sw_pm_enter_tsc;
-static bool sw_is_reset_i(void)
-{
-	/*
-	 * TODO: rely on checking the IA32_FIXED_CTR2 instead?
-	 */
-	u64 curr_tsc = sw_tscval();
-	bool is_reset = sw_pm_enter_tsc > curr_tsc;
-
-	pw_pr_force("DEBUG: curr tsc = %llu, prev tsc = %llu, is reset = %s\n",
-		    curr_tsc, sw_pm_enter_tsc, is_reset ? "true" : "false");
-
-	return is_reset;
-}
-
-static void sw_probe_pm_helper_i(int id, int both_id, bool is_enter,
-				 enum sw_pm_action action, enum sw_pm_mode mode)
-{
-	struct sw_trace_notifier_data *node = GET_COLLECTOR_NOTIFIER_NODE(id);
-	struct sw_trace_notifier_data *both_node =
-		GET_COLLECTOR_NOTIFIER_NODE(both_id);
-	struct sw_trace_notifier_data *reset_node =
-		GET_COLLECTOR_NOTIFIER_NODE(SW_NOTIFIER_ID_COUNTER_RESET);
-	if (is_enter) {
-		/*
-		 * Entering HIBERNATION/SUSPEND
-		 */
-		sw_pm_enter_tsc = sw_tscval();
-	} else {
-		/*
-		 * Exitting HIBERNATION/SUSPEND
-		 */
-		if (sw_is_reset_i() && reset_node)
-			sw_handle_reset_messages_i(reset_node);
-
-	}
-	if (node)
-		sw_handle_trace_notifier_i(node);
-
-	if (both_node)
-		sw_handle_trace_notifier_i(both_node);
-
-	/* Send the suspend-resume notification */
-	sw_send_pm_notification_i(SW_PM_VALUE(mode, action));
-}
-
-static bool sw_is_suspend_via_firmware(void)
-{
-#if KERNEL_VERSION(4, 4, 0) <= LINUX_VERSION_CODE
-	/* 'pm_suspend_via_firmware' only available in kernel >= 4.4 */
-	return pm_suspend_via_firmware();
-#endif
-	return true;
-}
-
-static int sw_probe_pm_notifier_i(struct notifier_block *block,
-				  unsigned long state,
-				  void *dummy)
-{
-	static const struct {
-		enum sw_pm_action action;
-		int node_id;
-		int both_id;
-		bool is_enter;
-	} pm_data[PM_POST_RESTORE] = {
-		[PM_HIBERNATION_PREPARE] = { SW_PM_ACTION_HIBERNATE_ENTER,
-					     SW_NOTIFIER_ID_HIBERNATE_ENTER,
-					     SW_NOTIFIER_ID_HIBERNATE, true },
-		[PM_POST_HIBERNATION] = { SW_PM_ACTION_HIBERNATE_EXIT,
-					  SW_NOTIFIER_ID_HIBERNATE_EXIT,
-					  SW_NOTIFIER_ID_HIBERNATE, false },
-		[PM_SUSPEND_PREPARE] = { SW_PM_ACTION_SUSPEND_ENTER,
-					 SW_NOTIFIER_ID_SUSPEND_ENTER,
-					 SW_NOTIFIER_ID_SUSPEND, true },
-		[PM_POST_SUSPEND] = { SW_PM_ACTION_SUSPEND_EXIT,
-				      SW_NOTIFIER_ID_SUSPEND_EXIT,
-				      SW_NOTIFIER_ID_SUSPEND, false },
-	};
-	enum sw_pm_action action = pm_data[state].action;
-	enum sw_pm_mode mode = sw_is_suspend_via_firmware() ?
-				       SW_PM_MODE_FIRMWARE :
-				       SW_PM_MODE_NONE;
-	if (action != SW_PM_ACTION_NONE) {
-		int node_id = pm_data[state].node_id,
-		    both_id = pm_data[state].both_id;
-		bool is_enter = pm_data[state].is_enter;
-
-		sw_probe_pm_helper_i(node_id, both_id, is_enter, action, mode);
-	} else {
-		/* Not supported */
-		pw_pr_error(
-			"ERROR: unknown state %lu passed to SWA pm notifier!\n",
-			state);
-	}
-	return NOTIFY_DONE;
-}
-
-static void sw_store_topology_change_i(enum cpu_action type,
-				       int cpu, int core_id,
-				       int pkg_id)
-{
-	struct sw_topology_node *node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
-
-	if (!node) {
-		pw_pr_error(
-			"couldn't allocate a node for topology change tracking!\n");
-		return;
-	}
-	node->change.timestamp = sw_timestamp();
-	node->change.type = type;
-	node->change.cpu = cpu;
-	node->change.core = core_id;
-	node->change.pkg = pkg_id;
-
-	SW_LIST_ADD(&sw_topology_list, node, list);
-	++sw_num_topology_entries;
-}
-
-#if KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE
-int sw_probe_hotplug_notifier_i(struct notifier_block *block,
-				unsigned long action, void *pcpu)
-{
-	unsigned int cpu = (unsigned long)pcpu;
-	unsigned int pkg_id = topology_physical_package_id(cpu);
-	unsigned int core_id = topology_core_id(cpu);
-
-	switch (action) {
-	case CPU_UP_PREPARE:
-	case CPU_UP_PREPARE_FROZEN:
-		/* CPU is coming online -- store top change */
-		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE_PREPARE, cpu,
-					   core_id, pkg_id);
-		pw_pr_debug(
-			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) preparing to come online at tsc = %llu! Current cpu = %d\n",
-			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
-		break;
-	case CPU_ONLINE:
-	case CPU_ONLINE_FROZEN:
-		/* CPU is online -- first store top change
-		 * then take BEGIN snapshot
-		 */
-		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE, cpu, core_id,
-					   pkg_id);
-		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
-		pw_pr_debug(
-			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) online at tsc = %llu! Current cpu = %d\n",
-			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
-		break;
-	case CPU_DOWN_PREPARE:
-	case CPU_DOWN_PREPARE_FROZEN:
-		/* CPU is going offline -- take END snapshot */
-		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
-		pw_pr_debug(
-			"DEBUG: SoC Watch has cpu %d preparing to go offline at tsc = %llu! Current cpu = %d\n",
-			cpu, sw_timestamp(), RAW_CPU());
-		break;
-	case CPU_DEAD:
-	case CPU_DEAD_FROZEN:
-		/* CPU is offline -- store top change */
-		sw_store_topology_change_i(SW_CPU_ACTION_OFFLINE, cpu, core_id,
-					   pkg_id);
-		pw_pr_debug(
-			"DEBUG: SoC Watch has cpu %d offlined at tsc = %llu! Current cpu = %d\n",
-			cpu, sw_timestamp(), RAW_CPU());
-		break;
-	default:
-		break;
-	}
-	return NOTIFY_OK;
-};
-#else
-static void sw_probe_cpuhp_helper_i(unsigned int cpu, enum cpu_action action)
-{
-	unsigned int pkg_id = topology_physical_package_id(cpu);
-	unsigned int core_id = topology_core_id(cpu);
-
-	switch (action) {
-	case SW_CPU_ACTION_ONLINE_PREPARE:
-		/* CPU is coming online -- store top change */
-		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
-		break;
-	case SW_CPU_ACTION_ONLINE:
-		/* CPU is online -- first store top change
-		 * then take BEGIN snapshot
-		 */
-		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
-		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
-		break;
-	case SW_CPU_ACTION_OFFLINE:
-		/* CPU is preparing to go offline -- take
-		 * END snapshot then store top change
-		 */
-		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
-		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
-		break;
-	default:
-		break;
-	}
-}
-
-static int sw_probe_cpu_offline_i(unsigned int cpu)
-{
-	pw_pr_debug("DEBUG: offline notification for cpu %u at %llu\n",
-	       cpu, sw_tscval());
-	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_OFFLINE);
-	return 0;
-}
-
-static int sw_probe_cpu_online_i(unsigned int cpu)
-{
-	pw_pr_debug("DEBUG: online notification for cpu %u at %llu\n", cpu,
-	       sw_tscval());
-	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE_PREPARE);
-	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE);
-	return 0;
-}
-#endif /* KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE  */
-
-/*
- * 2. CPUFREQ notifier
- */
-static int sw_probe_cpufreq_notifier_i(struct notifier_block *block,
-				unsigned long state, void *data)
-{
-	struct cpufreq_freqs *freqs = data;
-	static struct sw_trace_notifier_data *node;
-#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
-	int cpu = freqs->cpu;
-#else
-	int cpu = freqs->policy->cpu;
-#endif /* KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE */
-
-	if (state == CPUFREQ_PRECHANGE) {
-		pw_pr_debug(
-			"CPU %d reports a CPUFREQ_PRECHANGE for target CPU %d at TSC = %llu\n",
-			RAW_CPU(), cpu, sw_timestamp());
-		if (unlikely(node == NULL)) {
-			node = GET_COLLECTOR_NOTIFIER_NODE(
-				SW_NOTIFIER_ID_CPUFREQ);
-			pw_pr_debug("NODE = %p\n", node);
-		}
-		/* Force an atomic context by disabling preemption */
-		get_cpu();
-		DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, cpu, node);
-		put_cpu();
-	}
-	return NOTIFY_DONE;
-}
-
-/*
- * 1. TPS.
- */
-int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_start,
-					sw_probe_power_start_i);
-#else /* kernel version >= 2.6.38 */
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
-#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_start,
-					  sw_probe_power_start_i);
-#else /* kernel version >= 2.6.38 */
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
-#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
-	return PW_SUCCESS;
-};
-
-/*
- * 2. TPF
- */
-int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
-					sw_probe_power_frequency_i);
-#else /* kernel version >= 2.6.38 */
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
-					sw_probe_cpu_frequency_i);
-#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
-					  sw_probe_power_frequency_i);
-#else /* kernel version >= 2.6.38 */
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
-					  sw_probe_cpu_frequency_i);
-#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
-	return PW_SUCCESS;
-};
-
-/*
- * 3. IRQ handler entry
- */
-int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
-					sw_probe_irq_handler_entry_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
-					  sw_probe_irq_handler_entry_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 4. TIMER expire.
- */
-int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
-					sw_probe_timer_expire_entry_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data
-					     *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
-					  sw_probe_timer_expire_entry_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 5. HRTIMER expire.
- */
-int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data
-					     *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
-					sw_probe_hrtimer_expire_entry_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_hrtimer_expire_entry_i(
-	struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
-					  sw_probe_hrtimer_expire_entry_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 6. SCHED wakeup
- */
-int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
-					sw_probe_sched_wakeup_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
-					  sw_probe_sched_wakeup_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 8. PROCESS fork
- */
-int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
-					sw_probe_sched_process_fork_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data
-					     *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
-					  sw_probe_sched_process_fork_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 9. PROCESS exit
- */
-int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
-					sw_probe_sched_process_exit_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data
-					     *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
-					  sw_probe_sched_process_exit_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 10. THERMAL_APIC entry
- */
-#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
-int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
-					sw_probe_thermal_apic_entry_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data
-					     *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
-					  sw_probe_thermal_apic_entry_i);
-	return PW_SUCCESS;
-};
-
-/*
- * 10. THERMAL_APIC exit
- */
-int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
-					sw_probe_thermal_apic_exit_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
-					  sw_probe_thermal_apic_exit_i);
-	return PW_SUCCESS;
-};
-#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
-
-/*
- * 11. WAKE lock / WAKEUP source activate.
- */
-#if IS_ENABLED(CONFIG_ANDROID)
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_lock, sw_probe_wake_lock_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_lock,
-					  sw_probe_wake_lock_i);
-	return PW_SUCCESS;
-};
-#else /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-int sw_register_trace_wakeup_source_activate_i(
-	struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
-					sw_probe_wakeup_source_activate_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_wakeup_source_activate_i(
-	struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
-					  sw_probe_wakeup_source_activate_i);
-	return PW_SUCCESS;
-};
-#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-
-/*
- * 11. WAKE unlock / WAKEUP source deactivate.
- */
-#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
-int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
-					sw_probe_wake_unlock_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
-					  sw_probe_wake_unlock_i);
-	return PW_SUCCESS;
-};
-
-#else /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-int sw_register_trace_wakeup_source_deactivate_i(
-	struct sw_trace_notifier_data *node)
-{
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
-					sw_probe_wakeup_source_deactivate_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_wakeup_source_deactivate_i(
-	struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
-					  sw_probe_wakeup_source_deactivate_i);
-	return PW_SUCCESS;
-};
-#endif /*  KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
-#endif /* CONFIG_ANDROID */
-
-/*
- * 12. WORKQUEUE execution.
- */
-int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
-					sw_probe_workqueue_execution_i);
-#else
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
-					sw_probe_workqueue_execute_start_i);
-#endif
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_workqueue_execution_i(
-	struct sw_trace_notifier_data *node)
-{
-#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
-					  sw_probe_workqueue_execution_i);
-#else
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
-					  sw_probe_workqueue_execute_start_i);
-#endif
-	return PW_SUCCESS;
-};
-
-/*
- * 13. SCHED switch
- */
-int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node)
-{
-	/*
-	 * Set polling tick time, in jiffies.
-	 * Used by the context switch tracepoint to decide
-	 * if enough time has elapsed since the last
-	 * collection point to read resources again.
-	 */
-	{
-		int cpu = 0;
-
-		for_each_present_cpu(cpu)
-			*(&per_cpu(sw_pcpu_polling_jiff, cpu)) = jiffies;
-
-	}
-	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
-					sw_probe_sched_switch_i);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node)
-{
-	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
-					  sw_probe_sched_switch_i);
-	return PW_SUCCESS;
-};
-
-/*
- * Notifier register/unregister functions.
- */
-
-/*
- * 1. SUSPEND notifier.
- */
-static struct notifier_block sw_pm_notifier = {
-	.notifier_call = &sw_probe_pm_notifier_i,
-};
-
-int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node)
-{
-	register_pm_notifier(&sw_pm_notifier);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node)
-{
-	unregister_pm_notifier(&sw_pm_notifier);
-	return PW_SUCCESS;
-};
-
-/*
- * 2. CPUFREQ notifier.
- */
-static struct notifier_block sw_cpufreq_notifier = {
-	.notifier_call = &sw_probe_cpufreq_notifier_i,
-};
-
-int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
-{
-	cpufreq_register_notifier(&sw_cpufreq_notifier,
-				  CPUFREQ_TRANSITION_NOTIFIER);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
-{
-	cpufreq_unregister_notifier(&sw_cpufreq_notifier,
-				    CPUFREQ_TRANSITION_NOTIFIER);
-	return PW_SUCCESS;
-};
-
-#if KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE
-/*
- * 3. CPU hot plug notifier.
- */
-struct notifier_block sw_cpu_hotplug_notifier = {
-	.notifier_call = &sw_probe_hotplug_notifier_i,
-};
-
-int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
-{
-	register_hotcpu_notifier(&sw_cpu_hotplug_notifier);
-	return PW_SUCCESS;
-};
-
-int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
-{
-	unregister_hotcpu_notifier(&sw_cpu_hotplug_notifier);
-	return PW_SUCCESS;
-};
-
-#else /* KERNEL_VERSION(4, 10, 0) <= LINUX_VERSION_CODE */
-static int sw_cpuhp_state = -1;
-int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
-{
-	sw_cpuhp_state = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
-						   "socwatch:online",
-						   &sw_probe_cpu_online_i,
-						   &sw_probe_cpu_offline_i);
-	if (sw_cpuhp_state < 0) {
-		pw_pr_error("couldn't register socwatch hotplug callbacks!\n");
-		return -EIO;
-	}
-	return 0;
-};
-
-int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
-{
-	if (sw_cpuhp_state >= 0)
-		cpuhp_remove_state_nocalls((enum cpuhp_state)sw_cpuhp_state);
-
-	return 0;
-};
-#endif /* KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE */
-
-/*
- * Tracepoint extraction routines.
- * Required for newer kernels (>=3.15)
- */
-#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE
-static void sw_extract_tracepoint_callback(struct tracepoint *tp, void *priv)
-{
-	struct sw_trace_notifier_data *node = NULL;
-	int i = 0;
-	int *numStructsFound = (int *)priv;
-
-	if (*numStructsFound == NUM_VALID_TRACEPOINTS) {
-		/*
-		 * We've found all the tracepoints we need.
-		 */
-		return;
-	}
-	if (tp) {
-		FOR_EACH_TRACEPOINT_NODE(i, node)
-		{
-			if (node->tp == NULL && node->name) {
-				const char *name =
-					sw_get_trace_notifier_kernel_name(node);
-				if (name && !strcmp(tp->name, name)) {
-					node->tp = tp;
-					++*numStructsFound;
-					pw_pr_debug("OK, found TP %s\n",
-						    tp->name);
-				}
-			}
-		}
-	}
-};
-#endif /* KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE */
-#endif /* CONFIG_TRACEPOINTS */
-
-/*
- * Retrieve the list of tracepoint structs to use
- * when registering and unregistering tracepoint handlers.
- */
-int sw_extract_trace_notifier_providers(void)
-{
-#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE  &&			\
-	IS_ENABLED(CONFIG_TRACEPOINTS)
-	int numCallbacks = 0;
-
-	for_each_kernel_tracepoint(&sw_extract_tracepoint_callback,
-				   &numCallbacks);
-	/*
-	 * Did we get the complete list?
-	 */
-	if (numCallbacks != NUM_VALID_TRACEPOINTS)
-		pw_pr_warn(
-		       "WARNING : Could NOT find tracepoint structs for some tracepoints !\n");
-#endif /* KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE */
-	return PW_SUCCESS;
-};
-
-void sw_reset_trace_notifier_providers(void)
-{
-	/*
-	 * Reset the wakeup flag. Not strictly required if we aren't probing
-	 * any of the wakeup tracepoints.
-	 */
-	{
-		int cpu = 0;
-
-		for_each_online_cpu(cpu)
-			RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
-	}
-	/*
-	 * Reset the wakeup event flag. Not strictly required if we
-	 * aren't probing any of the wakeup tracepoints. Will be reset
-	 * in the power_start tracepoint if user requested a c-state
-	 * collection.
-	 */
-	sw_wakeup_event_flag = true;
-};
-
-void sw_print_trace_notifier_provider_overheads(void)
-{
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tps_i, "TPS");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tpf_i, "TPF");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_irq_wakeup_i, "IRQ");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_timer_wakeup_helper_i,
-					 "TIMER_EXPIRE");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_wakeup_i,
-					 "SCHED WAKEUP");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_process_fork_exit_helper_i,
-					 "PROCESS FORK/EXIT");
-#if IS_ENABLED(CONFIG_ANDROID)
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_wakelock_i,
-					 "WAKE LOCK/UNLOCK");
-#endif /* CONFIG_ANDROID */
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_workqueue_wakeup_helper_i,
-					 "WORKQUEUE");
-	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_switch_helper_i,
-					 "SCHED SWITCH");
-};
-
-/*
- * Add all trace/notifier providers.
- */
-int sw_add_trace_notifier_providers(void)
-{
-	struct sw_trace_notifier_data *node = NULL;
-	int i = 0;
-
-	FOR_EACH_TRACEPOINT_NODE(i, node)
-	{
-		if (sw_register_trace_notify_provider(node)) {
-			pw_pr_error("ERROR : couldn't add a trace provider!\n");
-			return -EIO;
-		}
-	}
-	FOR_EACH_NOTIFIER_NODE(i, node)
-	{
-		if (sw_register_trace_notify_provider(node)) {
-			pw_pr_error(
-				"ERROR: couldn't add a notifier provider !\n");
-			return -EIO;
-		}
-	}
-	/*
-	 * Add the cpu hot plug notifier.
-	 */
-	{
-		if (sw_register_trace_notify_provider(
-			    &s_hotplug_notifier_data)) {
-			pw_pr_error(
-				"ERROR : couldn't add cpu notifier provider!\n");
-			return -EIO;
-		}
-	}
-	return PW_SUCCESS;
-}
-
-/*
- * Remove previously added providers.
- */
-void sw_remove_trace_notifier_providers(void)
-{ /* NOP */
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <linux/version.h> /* "LINUX_VERSION_CODE" */
+#include <linux/hrtimer.h>
+#if KERNEL_VERSION(4, 11, 0) > LINUX_VERSION_CODE
+	#include <asm/cputime.h>
+#else
+	#include <linux/sched/cputime.h>
+#endif
+#include <asm/hardirq.h>
+#include <asm/local.h>
+
+#include <trace/events/power.h>
+#include <trace/events/irq.h>
+#include <trace/events/timer.h>
+#include <trace/events/power.h>
+#include <trace/events/sched.h>
+#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
+#include <asm/trace/irq_vectors.h> /* for the various APIC vector tracepoints
+				    *  (e.g. "thermal_apic",
+				    *  "local_timer" etc.)
+				    */
+#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
+struct pool_workqueue;
+struct cpu_workqueue_struct;
+#include <trace/events/workqueue.h>
+#include <linux/suspend.h> /* for 'pm_notifier' */
+#include <linux/cpufreq.h> /* for "cpufreq_notifier" */
+#include <linux/cpu.h> /* for 'CPU_UP_PREPARE' etc */
+
+#include "sw_kernel_defines.h"
+#include "sw_collector.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_output_buffer.h"
+#include "sw_mem.h"
+#include "sw_trace_notifier_provider.h"
+
+/* -------------------------------------------------
+ * Compile time constants and useful macros.
+ * -------------------------------------------------
+ */
+#ifndef __get_cpu_var
+/*
+ * Kernels >= 3.19 don't include a definition
+ * of '__get_cpu_var'. Create one now.
+ */
+#define __get_cpu_var(var) (*this_cpu_ptr(&var))
+#endif /* __get_cpu_var */
+
+#define BEGIN_LOCAL_IRQ_STATS_READ(p)                                          \
+	do {                                                                   \
+		p = &__get_cpu_var(irq_stat);
+
+#define END_LOCAL_IRQ_STATS_READ(p)                                            \
+	}                                                                      \
+	while (0)
+/*
+ * CAS{32,64}
+ */
+#define CAS32(p, o, n) (cmpxchg((p), (o), (n)) == (o))
+#define CAS64(p, o, n) (cmpxchg64((p), (o), (n)) == (o))
+/*
+ * Timer start pid accessor macros
+ */
+#ifdef CONFIG_TIMER_STATS
+#define GET_TIMER_THREAD_ID(t)                                                 \
+	((t)->start_pid) /* 'start_pid' is actually the thread ID
+			  * of the thread that initialized the timer
+			  */
+#else
+#define GET_TIMER_THREAD_ID(t) (-1)
+#endif /* CONFIG_TIMER_STATS */
+/*
+ * Tracepoint probe register/unregister functions and
+ * helper macros.
+ */
+#if IS_ENABLED(CONFIG_TRACEPOINTS)
+#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(register_trace_##name(probe))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	unregister_trace_##name(probe)
+#elif KERNEL_VERSION(3, 15, 0) > LINUX_VERSION_CODE
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(register_trace_##name(probe, NULL))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	unregister_trace_##name(probe, NULL)
+#else
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(tracepoint_probe_register(node->tp, probe, NULL))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	tracepoint_probe_unregister(node->tp, probe, NULL)
+#endif
+#else /* CONFIG_TRACEPOINTS */
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+#endif /* CONFIG_TRACEPOINTS */
+#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
+#define _DEFINE_PROBE_FUNCTION(name, ...) static void name(__VA_ARGS__)
+#else
+#define _DEFINE_PROBE_FUNCTION(name, ...)                                      \
+	static void name(void *ignore, __VA_ARGS__)
+#endif
+#define DEFINE_PROBE_FUNCTION(x) _DEFINE_PROBE_FUNCTION(x)
+
+/*
+ * Tracepoint probe function parameters.
+ * These tracepoint signatures depend on kernel version.
+ */
+#if KERNEL_VERSION(2, 6, 36) > LINUX_VERSION_CODE
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_power_start_i, unsigned int type, unsigned int state
+#elif KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_power_start_i, unsigned int type, unsigned int state,         \
+		unsigned int cpu_id
+#else
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_cpu_idle_i, unsigned int state, unsigned int cpu_id
+#endif
+
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+#define PROBE_TPF_PARAMS                                                       \
+	sw_probe_power_frequency_i, unsigned int type, unsigned int state
+#else
+#define PROBE_TPF_PARAMS                                                       \
+	sw_probe_cpu_frequency_i, unsigned int new_freq, unsigned int cpu
+#endif
+
+#if KERNEL_VERSION(2, 6, 35) > LINUX_VERSION_CODE
+#define PROBE_SCHED_WAKEUP_PARAMS                                              \
+	sw_probe_sched_wakeup_i, struct rq *rq, struct task_struct *task,      \
+		int success
+#else
+#define PROBE_SCHED_WAKEUP_PARAMS                                              \
+	sw_probe_sched_wakeup_i, struct task_struct *task, int success
+#endif
+
+#if IS_ENABLED(CONFIG_ANDROID)
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+#define PROBE_WAKE_LOCK_PARAMS sw_probe_wake_lock_i, struct wake_lock *lock
+#define PROBE_WAKE_UNLOCK_PARAMS                                               \
+	sw_probe_wake_unlock_i, struct wake_unlock *unlock
+#else
+#define PROBE_WAKE_LOCK_PARAMS                                                 \
+	sw_probe_wakeup_source_activate_i, const char *name, unsigned int state
+#define PROBE_WAKE_UNLOCK_PARAMS                                               \
+	sw_probe_wakeup_source_deactivate_i, const char *name,                 \
+		unsigned int state
+#endif /* version */
+#endif /* CONFIG_ANDROID */
+
+#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
+#define PROBE_WORKQUEUE_PARAMS                                                 \
+	sw_probe_workqueue_execution_i, struct task_struct *wq_thread,         \
+		struct work_struct *work
+#else
+#define PROBE_WORKQUEUE_PARAMS                                                 \
+	sw_probe_workqueue_execute_start_i, struct work_struct *work
+#endif
+
+#define PROBE_SCHED_SWITCH_PARAMS                                              \
+	sw_probe_sched_switch_i, struct task_struct *prev,                     \
+		struct task_struct *next
+/*
+ * These tracepoint signatures are independent of kernel version.
+ */
+#define PROBE_IRQ_PARAMS                                                       \
+	sw_probe_irq_handler_entry_i, int irq, struct irqaction *action
+#define PROBE_TIMER_ARGS sw_probe_timer_expire_entry_i, struct timer_list *t
+#define PROBE_HRTIMER_PARAMS                                                   \
+	sw_probe_hrtimer_expire_entry_i, struct hrtimer *hrt, ktime_t *now
+#define PROBE_PROCESS_FORK_PARAMS                                              \
+	sw_probe_sched_process_fork_i, struct task_struct *parent,             \
+		struct task_struct *child
+#define PROBE_SCHED_PROCESS_EXIT_PARAMS                                        \
+	sw_probe_sched_process_exit_i, struct task_struct *task
+#define PROBE_THERMAL_APIC_ENTRY_PARAMS                                        \
+	sw_probe_thermal_apic_entry_i, int vector
+#define PROBE_THERMAL_APIC_EXIT_PARAMS sw_probe_thermal_apic_exit_i, int vector
+
+#define IS_VALID_WAKEUP_EVENT(cpu)                                             \
+	({                                                                     \
+		bool *per_cpu_event =                                          \
+			&per_cpu(sw_is_valid_wakeup_event, (cpu));             \
+		bool old_value =                                               \
+			CAS32(per_cpu_event, true, sw_wakeup_event_flag);      \
+		old_value;                                                     \
+	})
+#define SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu) (IS_VALID_WAKEUP_EVENT(cpu))
+#define RESET_VALID_WAKEUP_EVENT_COUNTER(cpu)                                  \
+	(per_cpu(sw_is_valid_wakeup_event, (cpu)) = true)
+
+#define NUM_TRACEPOINT_NODES SW_ARRAY_SIZE(s_trace_collector_lists)
+#define NUM_VALID_TRACEPOINTS (NUM_TRACEPOINT_NODES - 1) /* "-1" for IPI */
+#define FOR_EACH_TRACEPOINT_NODE(idx, node)                                    \
+	for (idx = 0; idx < NUM_TRACEPOINT_NODES &&                            \
+		      (node = &s_trace_collector_lists[idx]);                  \
+	     ++idx)
+
+#define FOR_EACH_NOTIFIER_NODE(idx, node)                                      \
+	for (idx = 0; idx < SW_ARRAY_SIZE(s_notifier_collector_lists) &&       \
+		      (node = &s_notifier_collector_lists[idx]);               \
+	     ++idx)
+/*
+ * Use these macros if all tracepoint ID numbers
+ * ARE contiguous from 0 -- max tracepoint ID #
+ */
+/* #if 0
+#define IS_VALID_TRACE_NOTIFIER_ID(id)                                         \
+	((id) >= 0 && (id) < SW_ARRAY_SIZE(s_trace_collector_lists))
+#define GET_COLLECTOR_TRACE_NODE(id) (&s_trace_collector_lists[id])
+#define FOR_EACH_trace_notifier_id(idx)                                        \
+	for (idx = 0; idx < SW_ARRAY_SIZE(s_trace_collector_lists); ++idx)
+#endif */
+/*
+ * Use these macros if all tracepoint ID numbers
+ * are NOT contiguous from 0 -- max tracepoint ID #
+ */
+#define GET_COLLECTOR_TRACE_NODE(idx)                                          \
+	({                                                                     \
+		int __idx = 0;                                                 \
+		struct sw_trace_notifier_data *__node = NULL,                  \
+					      *__retVal = NULL;                \
+		FOR_EACH_TRACEPOINT_NODE(__idx, __node)                        \
+		{                                                              \
+			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
+				__retVal = __node;                             \
+				break;                                         \
+			}                                                      \
+		}                                                              \
+		__retVal;                                                      \
+	})
+#define IS_VALID_TRACE_NOTIFIER_ID(idx) (GET_COLLECTOR_TRACE_NODE(idx) != NULL)
+
+#define GET_COLLECTOR_NOTIFIER_NODE(idx)                                       \
+	({                                                                     \
+		int __idx = 0;                                                 \
+		struct sw_trace_notifier_data *__node = NULL,                  \
+					      *__retVal = NULL;                \
+		FOR_EACH_NOTIFIER_NODE(__idx, __node)                          \
+		{                                                              \
+			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
+				__retVal = __node;                             \
+				break;                                         \
+			}                                                      \
+		}                                                              \
+		__retVal;                                                      \
+	})
+#define IS_VALID_NOTIFIER_ID(idx) (GET_COLLECTOR_NOTIFIER_NODE(idx) != NULL)
+
+/* -------------------------------------------------
+ * Local function declarations.
+ * -------------------------------------------------
+ */
+/*
+ * The tracepoint registration functions.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
+#if KERNEL_VERSION(3,14,0) <= LINUX_VERSION_CODE
+    int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
+    int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
+    int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+    int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+#endif // KERNEL_VERSION(3,14,0) <= LINUX_VERSION_CODE
+#if IS_ENABLED(CONFIG_ANDROID)
+    #if KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
+        int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+        int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+    #else // KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
+        int sw_register_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wakeup_source_activate_i(struct sw_trace_notifier_data *node);
+        int sw_register_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
+        int sw_unregister_trace_wakeup_source_deactivate_i(struct sw_trace_notifier_data *node);
+    #endif // KERNEL_VERSION(3,4,0) > LINUX_VERSION_CODE
+#endif // IS_ENABLED(CONFIG_ANDROID)
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu, int target_cpu);
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr, struct sw_trace_notifier_data *node,
+                                     pid_t tid);
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node);
+void sw_handle_workqueue_wakeup_helper_i(int cpu, struct sw_collector_data *node);
+void sw_handle_sched_switch_helper_i(void);
+void sw_tps_apic_i(int cpu);
+void sw_tps_tps_i(int cpu);
+void sw_tps_wakeup_i(int cpu);
+void sw_tps_i(void);
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node);
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node, struct task_struct *task, bool is_fork);
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node, const char *name,
+                               int type, u64 timeout, int pid, int tid, const char *proc_name);
+u64 sw_my_local_arch_irq_stats_cpu_i(void);
+
+/*
+ * The tracepoint probes.
+ */
+/*
+ * The tracepoint handlers.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_trace_notifier_on_cpu_i(int cpu, struct sw_trace_notifier_data *node);
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+/*
+ * For overhead measurements.
+ */
+DECLARE_OVERHEAD_VARS(
+	sw_handle_timer_wakeup_helper_i); /* for the "timer_expire"
+					   *   family of probes
+					   */
+DECLARE_OVERHEAD_VARS(sw_handle_irq_wakeup_i); /* for IRQ wakeups */
+DECLARE_OVERHEAD_VARS(sw_handle_sched_wakeup_i); /* for SCHED */
+DECLARE_OVERHEAD_VARS(sw_tps_i); /* for TPS */
+DECLARE_OVERHEAD_VARS(sw_tpf_i); /* for TPF */
+DECLARE_OVERHEAD_VARS(sw_process_fork_exit_helper_i);
+#if IS_ENABLED(CONFIG_ANDROID)
+DECLARE_OVERHEAD_VARS(sw_handle_wakelock_i); /* for wake lock/unlock */
+#endif /* CONFIG_ANDROID */
+DECLARE_OVERHEAD_VARS(sw_handle_workqueue_wakeup_helper_i);
+DECLARE_OVERHEAD_VARS(sw_handle_sched_switch_helper_i);
+/*
+ * Per-cpu wakeup counters.
+ * Used to decide which wakeup event is the first to occur after a
+ * core wakes up from a C-state.
+ * Set to 'true' in TPS probe
+ */
+static DEFINE_PER_CPU(bool, sw_is_valid_wakeup_event) = { true };
+/*
+ * Per-cpu counts of the number of times the local APIC fired.
+ * We need a separate count because some apic timer fires don't seem
+ * to result in hrtimer/timer expires
+ */
+static DEFINE_PER_CPU(u64, sw_num_local_apic_timer_inters);
+/*
+ * Flag value to use to decide if the event is a valid wakeup event.
+ * Set to 'false' in TPS probe.
+ */
+static bool sw_wakeup_event_flag = true;
+
+#if IS_ENABLED(CONFIG_TRACEPOINTS)
+/*
+ * Scheduler-based polling emulation.
+ */
+static DEFINE_PER_CPU(unsigned long, sw_pcpu_polling_jiff);
+#endif /* CONFIG_TRACEPOINTS */
+
+pw_u16_t sw_min_polling_interval_msecs;
+
+/*
+ * IDs for supported tracepoints.
+ */
+enum sw_trace_id {
+	SW_TRACE_ID_CPU_IDLE,
+	SW_TRACE_ID_CPU_FREQUENCY,
+	SW_TRACE_ID_IRQ_HANDLER_ENTRY,
+	SW_TRACE_ID_TIMER_EXPIRE_ENTRY,
+	SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY,
+	SW_TRACE_ID_SCHED_WAKEUP,
+	SW_TRACE_ID_IPI,
+	SW_TRACE_ID_SCHED_PROCESS_FORK,
+	SW_TRACE_ID_SCHED_PROCESS_EXIT,
+	SW_TRACE_ID_THERMAL_APIC_ENTRY,
+	SW_TRACE_ID_THERMAL_APIC_EXIT,
+	SW_TRACE_ID_WAKE_LOCK,
+	SW_TRACE_ID_WAKE_UNLOCK,
+	SW_TRACE_ID_WORKQUEUE_EXECUTE_START,
+	SW_TRACE_ID_SCHED_SWITCH,
+};
+
+/*
+ * IDs for supported notifiers.
+ */
+enum sw_notifier_id {
+	SW_NOTIFIER_ID_SUSPEND, /* TODO: change name? */
+	SW_NOTIFIER_ID_SUSPEND_ENTER,
+	SW_NOTIFIER_ID_SUSPEND_EXIT,
+	SW_NOTIFIER_ID_HIBERNATE,
+	SW_NOTIFIER_ID_HIBERNATE_ENTER,
+	SW_NOTIFIER_ID_HIBERNATE_EXIT,
+	SW_NOTIFIER_ID_COUNTER_RESET,
+	SW_NOTIFIER_ID_CPUFREQ,
+	SW_NOTIFIER_ID_HOTCPU,
+};
+
+/*
+ * Names for supported tracepoints. A tracepoint
+ * 'name' consists of two strings: a "kernel" string
+ * that is used to locate the tracepoint within the kernel
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which tracepoints to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_trace_names[] = {
+	[SW_TRACE_ID_CPU_IDLE] = { "cpu_idle", "CPU-IDLE" },
+	[SW_TRACE_ID_CPU_FREQUENCY] = { "cpu_frequency", "CPU-FREQUENCY" },
+	[SW_TRACE_ID_IRQ_HANDLER_ENTRY] = { "irq_handler_entry", "IRQ-ENTRY" },
+	[SW_TRACE_ID_TIMER_EXPIRE_ENTRY] = { "timer_expire_entry",
+					     "TIMER-ENTRY" },
+	[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY] = { "hrtimer_expire_entry",
+					       "HRTIMER-ENTRY" },
+	[SW_TRACE_ID_SCHED_WAKEUP] = { "sched_wakeup", "SCHED-WAKEUP" },
+	[SW_TRACE_ID_IPI] = { NULL, "IPI" },
+	[SW_TRACE_ID_SCHED_PROCESS_FORK] = { "sched_process_fork",
+					     "PROCESS-FORK" },
+	[SW_TRACE_ID_SCHED_PROCESS_EXIT] = { "sched_process_exit",
+					     "PROCESS-EXIT" },
+#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
+	[SW_TRACE_ID_THERMAL_APIC_ENTRY] = { "thermal_apic_entry",
+					     "THERMAL-THROTTLE-ENTRY" },
+	[SW_TRACE_ID_THERMAL_APIC_EXIT] = { "thermal_apic_exit",
+					    "THERMAL-THROTTLE-EXIT" },
+#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE  */
+#if IS_ENABLED(CONFIG_ANDROID)
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+	[SW_TRACE_ID_WAKE_LOCK] = { "wake_lock", "WAKE-LOCK" },
+	[SW_TRACE_ID_WAKE_UNLOCK] = { "wake_unlock", "WAKE-UNLOCK" },
+#else /* KERNEL_VERSION(3, 4, 0) <= LINUX_VERSION_CODE */
+	[SW_TRACE_ID_WAKE_LOCK] = { "wakeup_source_activate", "WAKE-LOCK" },
+	[SW_TRACE_ID_WAKE_UNLOCK] = { "wakeup_source_deactivate",
+				      "WAKE-UNLOCK" },
+#endif
+#endif
+	[SW_TRACE_ID_WORKQUEUE_EXECUTE_START] = { "workqueue_execute_start",
+						  "WORKQUEUE-START" },
+	[SW_TRACE_ID_SCHED_SWITCH] = { "sched_switch", "CONTEXT-SWITCH" },
+};
+
+/*
+ * Names for supported notifiers. A notifier
+ * 'name' consists of two strings: an unused "kernel" string
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which notifiers to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_notifier_names[] = {
+	[SW_NOTIFIER_ID_SUSPEND] = { "suspend_notifier" /* don't care */,
+				     "SUSPEND-NOTIFIER" },
+	[SW_NOTIFIER_ID_SUSPEND_ENTER] = { NULL, "SUSPEND-ENTER" },
+	[SW_NOTIFIER_ID_SUSPEND_EXIT] = { NULL, "SUSPEND-EXIT" },
+	[SW_NOTIFIER_ID_HIBERNATE] = { "hibernate_notifier" /* don't care */,
+				       "HIBERNATE-NOTIFIER" },
+	[SW_NOTIFIER_ID_HIBERNATE_ENTER] = { NULL, "HIBERNATE-ENTER" },
+	[SW_NOTIFIER_ID_HIBERNATE_EXIT] = { NULL, "HIBERNATE-EXIT" },
+	[SW_NOTIFIER_ID_COUNTER_RESET] = { NULL, "COUNTER-RESET" },
+	[SW_NOTIFIER_ID_CPUFREQ] = { "cpufreq_notifier" /* don't care */,
+				     "CPUFREQ-NOTIFIER" },
+	[SW_NOTIFIER_ID_HOTCPU] = { "hotcpu_notifier" /* don't care */,
+				    "HOTCPU-NOTIFIER" },
+};
+
+#if IS_ENABLED(CONFIG_TRACEPOINTS)
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = {
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_CPU_IDLE],
+	  &sw_register_trace_cpu_idle_i, &sw_unregister_trace_cpu_idle_i,
+	  NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_CPU_FREQUENCY],
+	  &sw_register_trace_cpu_frequency_i,
+	  &sw_unregister_trace_cpu_frequency_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_IRQ_HANDLER_ENTRY],
+	  &sw_register_trace_irq_handler_entry_i,
+	  &sw_unregister_trace_irq_handler_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_TIMER_EXPIRE_ENTRY],
+	  &sw_register_trace_timer_expire_entry_i,
+	  &sw_unregister_trace_timer_expire_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY],
+	  &sw_register_trace_hrtimer_expire_entry_i,
+	  &sw_unregister_trace_hrtimer_expire_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_WAKEUP],
+	  &sw_register_trace_sched_wakeup_i,
+	  &sw_unregister_trace_sched_wakeup_i, NULL },
+	/* Placeholder for IPI -- no tracepoints associated with it! */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_IPI], NULL,
+	  NULL, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_FORK],
+	  &sw_register_trace_sched_process_fork_i,
+	  &sw_unregister_trace_sched_process_fork_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_EXIT],
+	  &sw_register_trace_sched_process_exit_i,
+	  &sw_unregister_trace_sched_process_exit_i, NULL },
+#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
+	/*
+	 * For thermal throttling.
+	 * We probably only need one of either 'entry' or 'exit'. Use
+	 * both, until we decide which one to keep. Note that
+	 * tracepoint IDs for these, and subsequent tracepoints
+	 * (e.g. 'wake_lock') will change once we've picked which
+	 * one to use.
+	 */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_ENTRY],
+	  &sw_register_trace_thermal_apic_entry_i,
+	  &sw_unregister_trace_thermal_apic_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_EXIT],
+	  &sw_register_trace_thermal_apic_exit_i,
+	  &sw_unregister_trace_thermal_apic_exit_i, NULL },
+#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
+/* Wakelocks have multiple tracepoints, depending on kernel version */
+#if IS_ENABLED(CONFIG_ANDROID)
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
+	  &sw_register_trace_wake_lock_i, &sw_unregister_trace_wake_lock_i,
+	  NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
+	  &sw_register_trace_wake_unlock_i, &sw_unregister_trace_wake_unlock_i,
+	  NULL },
+#else /* KERNEL_VERSION(3, 4, 0) <= LINUX_VERSION_CODE  */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
+	  &sw_register_trace_wakeup_source_activate_i,
+	  &sw_unregister_trace_wakeup_source_activate_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
+	  &sw_register_trace_wakeup_source_deactivate_i,
+	  &sw_unregister_trace_wakeup_source_deactivate_i, NULL },
+#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+#endif /* CONFIG_ANDROID */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WORKQUEUE_EXECUTE_START],
+	  &sw_register_trace_workqueue_execution_i,
+	  &sw_unregister_trace_workqueue_execution_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_SWITCH],
+	  &sw_register_trace_sched_switch_i,
+	  &sw_unregister_trace_sched_switch_i, NULL },
+};
+
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND], &sw_register_pm_notifier_i,
+	  &sw_unregister_pm_notifier_i, NULL, true /* always register */ },
+	/* Placeholder for suspend enter/exit -- these will be called
+	 * from within the pm notifier
+	 */
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_ENTER], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_EXIT], NULL, NULL, NULL },
+	/* Placeholder for hibernate enter/exit -- these will be called
+	 * from within the pm notifier
+	 */
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_ENTER], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_EXIT], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_COUNTER_RESET], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_CPUFREQ],
+	  &sw_register_cpufreq_notifier_i, &sw_unregister_cpufreq_notifier_i },
+};
+
+/*
+ * Special entry for CPU notifier (i.e. "hotplug" notifier)
+ * We don't want these to be visible to the user.
+ */
+static struct sw_trace_notifier_data s_hotplug_notifier_data = {
+	SW_TRACE_COLLECTOR_NOTIFIER,
+	&s_notifier_names[SW_NOTIFIER_ID_HOTCPU],
+	&sw_register_hotcpu_notifier_i,
+	&sw_unregister_hotcpu_notifier_i,
+	NULL,
+	true /* always register */
+};
+#else /* !CONFIG_TRACEPOINTS */
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = {
+	/* EMPTY */};
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
+	/* EMPTY */ };
+
+static struct sw_trace_notifier_data s_hotplug_notifier_data = {
+	/* EMPTY */
+};
+
+
+#endif /* CONFIG_TRACEPOINTS */
+
+/*
+ * Macros to retrieve tracepoint and notifier IDs.
+ */
+#define GET_TRACE_ID_FROM_NODE(node) ((node)->name - s_trace_names)
+#define GET_NOTIFIER_ID_FROM_NODE(node) ((node)->name - s_notifier_names)
+
+#define GET_TRACE_NOTIFIER_ID(node)                                            \
+	(int)(((node)->type == SW_TRACE_COLLECTOR_TRACEPOINT) ?                \
+		      GET_TRACE_ID_FROM_NODE(node) :                           \
+		      GET_NOTIFIER_ID_FROM_NODE(node))
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+
+/*
+ * Retrieve a TSC value
+ */
+static inline u64 sw_tscval(void)
+{
+	unsigned int low, high;
+
+	asm volatile("rdtsc" : "=a"(low), "=d"(high));
+	return low | ((unsigned long long)high) << 32;
+};
+
+u64 sw_timestamp(void)
+{
+	struct timespec ts;
+
+	getnstimeofday(&ts);
+	return (ts.tv_sec * 1000000000ULL + ts.tv_nsec);
+}
+
+/*
+ * Basically the same as arch/x86/kernel/irq.c --> "arch_irq_stat_cpu(cpu)"
+ */
+u64 sw_my_local_arch_irq_stats_cpu_i(void)
+{
+	u64 sum = 0;
+	irq_cpustat_t *stats;
+#ifdef __arm__
+	int i = 0;
+#endif
+	BEGIN_LOCAL_IRQ_STATS_READ(stats);
+	{
+#ifndef __arm__
+		sum += stats->__nmi_count;
+#if IS_ENABLED(CONFIG_X86_LOCAL_APIC)
+		sum += stats->apic_timer_irqs;
+		sum += stats->irq_spurious_count;
+#endif
+#if KERNEL_VERSION(2, 6, 34) <= LINUX_VERSION_CODE
+		sum += stats->x86_platform_ipis;
+#endif /* 2,6,34 */
+		sum += stats->apic_perf_irqs;
+#if KERNEL_VERSION(3, 5, 0) <= LINUX_VERSION_CODE
+		sum += stats->apic_irq_work_irqs;
+#endif /* 3,5,0 */
+#ifdef CONFIG_SMP
+		sum += stats->irq_call_count;
+		sum += stats->irq_resched_count;
+		sum += stats->irq_tlb_count;
+#endif
+#ifdef CONFIG_X86_THERMAL_VECTOR
+		sum += stats->irq_thermal_count;
+#endif
+
+#else
+		sum += stats->__softirq_pending;
+#ifdef CONFIG_SMP
+		for (i = 0; i < NR_IPI; ++i)
+			sum += stats->ipi_irqs[i];
+
+#endif
+#ifdef CONFIG_X86_MCE
+		sum += stats->mce_exception_count;
+		sum += stats->mce_poll_count;
+#endif
+#endif
+	}
+	END_LOCAL_IRQ_STATS_READ(stats);
+	return sum;
+};
+
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node)
+		return;
+
+	list_for_each_entry(curr, &node->list, list) {
+		pw_pr_debug("DEBUG: handling message\n");
+		sw_handle_per_cpu_msg(curr);
+	}
+};
+
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_on_cpu_i(int cpu,
+				       struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node)
+		return;
+
+	list_for_each_entry(curr, &node->list, list)
+		sw_handle_per_cpu_msg_on_cpu(cpu, curr);
+
+};
+
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node)
+		return;
+
+	list_for_each_entry(curr, &node->list, list) {
+		pw_pr_debug("Handling message of unknown cpumask on cpu %d\n",
+			    RAW_CPU());
+		sw_schedule_work(&curr->cpumask, &sw_handle_per_cpu_msg, curr);
+	}
+}
+
+/*
+ * Tracepoint helpers.
+ */
+
+/*
+ * TIMER wakeup handling function.
+ */
+static void sw_handle_timer_wakeup_i(struct sw_collector_data *node, pid_t pid,
+			      pid_t tid)
+{
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
+	char *dst_vals = msg->p_payload;
+
+	/* msg->tsc = sw_timestamp(); */
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = cpu;
+
+	/*
+	 * TIMER handling ==> only return the pid, tid
+	 */
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+	pw_pr_debug("HANDLED timer expire for %d, %d\n", pid, tid);
+};
+
+/*
+ * Helper function for {hr}timer expires. Required for overhead tracking.
+ */
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr,
+				     struct sw_trace_notifier_data *node,
+				     pid_t tid)
+{
+	pid_t pid = -1;
+
+	if (tid == 0)
+		pid = 0;
+	else {
+		struct task_struct *task =
+			pid_task(find_pid_ns(tid, &init_pid_ns), PIDTYPE_PID);
+		if (likely(task))
+			pid = task->tgid;
+	}
+	list_for_each_entry(curr, &node->list, list)
+		sw_handle_timer_wakeup_i(curr, pid, tid);
+};
+
+/*
+ * SCHED wakeup handling function.
+ */
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu,
+			      int target_cpu)
+{
+	int cpu = source_cpu;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
+	char *dst_vals = msg->p_payload;
+
+	/* msg->tsc = sw_timestamp(); */
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = source_cpu;
+
+	/*
+	 * sched handling ==> only return the source, target CPUs
+	 */
+	*((int *)dst_vals) = source_cpu;
+	dst_vals += sizeof(source_cpu);
+	*((int *)dst_vals) = target_cpu;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_NONE))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+};
+
+/*
+ * APIC timer wakeup
+ */
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node)
+{
+	/*
+	 * Send an empty message back to Ring-3
+	 */
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
+
+	/* msg->tsc = sw_timestamp(); */
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = cpu;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+	pw_pr_debug("HANDLED APIC timer wakeup for cpu = %d\n", cpu);
+};
+
+/*
+ * Helper function for workqueue executions. Required for overhead tracking.
+ */
+void sw_handle_workqueue_wakeup_helper_i(int cpu,
+					 struct sw_collector_data *node)
+{
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+
+	/* msg->tsc = sw_timestamp(); */
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = cpu;
+
+	/*
+	 * Workqueue wakeup ==> empty message.
+	 */
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_error("WARNING: could NOT produce message!\n");
+};
+
+/*
+ * Helper function for sched_switch. Required for overhead tracking.
+ */
+void sw_handle_sched_switch_helper_i(void)
+{
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_SWITCH);
+		pw_pr_debug("SCHED SWITCH NODE = %p\n", node);
+	}
+	if (!node)
+		return;
+
+	preempt_disable();
+	{
+		struct sw_collector_data *curr;
+
+		list_for_each_entry(curr, &node->list, list) {
+			unsigned long curr_jiff = jiffies,
+				      prev_jiff = curr->last_update_jiffies;
+			unsigned long delta_msecs =
+				jiffies_to_msecs(curr_jiff) -
+				jiffies_to_msecs(prev_jiff);
+			struct cpumask *mask = &curr->cpumask;
+			u16 timeout = curr->info->sampling_interval_msec;
+
+			if (!timeout)
+				timeout = sw_min_polling_interval_msecs;
+
+			/* Has there been enough time since the last
+			 * collection point?
+			 */
+			if (delta_msecs < timeout)
+				continue;
+
+			/* Update timestamp and handle message */
+			if (cpumask_test_cpu(
+				    RAW_CPU(),
+				    mask) /* This msg must be handled on
+					   * the current CPU
+					   */
+			    ||
+			    cpumask_empty(
+				    mask) /* This msg may be handled by
+					   * any CPU
+					   */) {
+				if (!CAS64(&curr->last_update_jiffies,
+					   prev_jiff, curr_jiff)) {
+					/*
+					 * CAS failure should only be possible
+					 * for messages that can be handled
+					 * on any CPU, in which case it
+					 * indicates a different CPU already
+					 * handled this message.
+					 */
+					continue;
+				}
+				sw_handle_per_cpu_msg_no_sched(curr);
+			}
+		}
+	}
+	preempt_enable();
+};
+
+/*
+ * Probe functions.
+ */
+
+/*
+ * 1. TPS
+ */
+
+/*
+ * Check IPI wakeups within the cpu_idle tracepoint.
+ */
+void sw_tps_apic_i(int cpu)
+{
+	static struct sw_trace_notifier_data *apic_timer_node;
+
+	if (unlikely(apic_timer_node == NULL)) {
+		apic_timer_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IPI);
+		pw_pr_debug("apic NODE = %p\n", apic_timer_node);
+	}
+	if (apic_timer_node) {
+		bool local_apic_timer_fired = false;
+		u64 curr_num_local_apic = sw_my_local_arch_irq_stats_cpu_i();
+		u64 *old_num_local_apic =
+			&__get_cpu_var(sw_num_local_apic_timer_inters);
+
+		if (*old_num_local_apic &&
+		    (*old_num_local_apic != curr_num_local_apic)) {
+			local_apic_timer_fired = true;
+		}
+		*old_num_local_apic = curr_num_local_apic;
+
+		if (local_apic_timer_fired &&
+		    SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+			struct sw_collector_data *curr = NULL;
+
+			list_for_each_entry(curr, &apic_timer_node->list,
+					     list) {
+				sw_handle_apic_timer_wakeup_i(curr);
+			}
+		}
+	}
+};
+
+/*
+ * Perform any user-defined tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_tps_i(int cpu)
+{
+	static struct sw_trace_notifier_data *tps_node;
+
+	if (unlikely(tps_node == NULL)) {
+		tps_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_IDLE);
+		pw_pr_debug("TPS NODE = %p\n", tps_node);
+	}
+	sw_handle_trace_notifier_i(tps_node);
+};
+
+/*
+ * Perform any wakeup-related tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_wakeup_i(int cpu)
+{
+	/*
+	 * For now, assume we will always have to
+	 * do some wakeup book keeping. Later, we'll
+	 * need to detect if the user requested wakeups.
+	 */
+	sw_wakeup_event_flag = false;
+	RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+};
+
+void sw_tps_i(void)
+{
+	/*
+	 * Update: FIRST handle IPI wakeups
+	 * THEN handle TPS
+	 */
+	int cpu = RAW_CPU();
+
+	sw_tps_apic_i(cpu);
+	sw_tps_tps_i(cpu);
+	sw_tps_wakeup_i(cpu);
+};
+
+/*
+ * 2. TPF
+ */
+
+/*
+ * Helper function for overhead measurements.
+ */
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node)
+{
+	sw_handle_trace_notifier_on_cpu_i((int)cpu, node);
+};
+
+#if IS_ENABLED(CONFIG_TRACEPOINTS)
+DEFINE_PROBE_FUNCTION(PROBE_TPS_PARAMS)
+{
+#if KERNEL_VERSION(2, 6, 38) <= LINUX_VERSION_CODE
+	if (state == PWR_EVENT_EXIT)
+		return;
+#endif
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tps_i);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_TPF_PARAMS)
+{
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+	int cpu = RAW_CPU();
+#endif /* version < 2.6.38 */
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_FREQUENCY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+
+/*
+ * IRQ wakeup handling function.
+ */
+static void sw_handle_irq_wakeup_i(struct sw_collector_data *node, int irq)
+{
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	/* char *dst_vals = (char *)(unsigned long)msg->p_payload; */
+	char *dst_vals = msg->p_payload;
+
+	/* msg->tsc = sw_timestamp(); */
+	/* msg TSC assigned when msg is written to buffer */
+	msg->cpuidx = cpu;
+
+	/*
+	 * IRQ handling ==> only return the irq number
+	 */
+	*((int *)dst_vals) = irq;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+};
+
+/*
+ * 3. IRQ handler entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_IRQ_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IRQ_HANDLER_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
+		return;
+
+	list_for_each_entry(curr, &node->list, list)
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_irq_wakeup_i, curr, irq);
+
+};
+
+/*
+ * 4. TIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_TIMER_ARGS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	struct sw_collector_data *curr = NULL;
+	pid_t tid = GET_TIMER_THREAD_ID(t);
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_TIMER_EXPIRE_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
+		return;
+
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
+				 tid);
+};
+
+/*
+ * 5. HRTIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_HRTIMER_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+	pid_t tid = GET_TIMER_THREAD_ID(hrt);
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(
+			SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
+		return;
+
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
+				 tid);
+};
+
+/*
+ * 6. SCHED wakeup
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_WAKEUP_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+	int target_cpu = task_cpu(task), source_cpu = RAW_CPU();
+	/*
+	 * "Self-sched" samples are "don't care".
+	 */
+	if (target_cpu == source_cpu)
+		return;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_WAKEUP);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	/*
+	 * Unlike other wakeup sources, we check the per-cpu flag
+	 * of the TARGET cpu to decide if we should produce a sample.
+	 */
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(target_cpu))
+		return;
+
+	list_for_each_entry(curr, &node->list, list) {
+		/* sw_handle_sched_wakeup_i(curr, source_cpu, target_cpu); */
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_wakeup_i, curr,
+					 source_cpu, target_cpu);
+	}
+};
+
+/*
+ * 8. PROCESS fork
+ */
+
+/*
+ * Helper for PROCESS fork, PROCESS exit
+ */
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node,
+				   struct task_struct *task, bool is_fork)
+{
+	int cpu = RAW_CPU();
+	pid_t pid = task->tgid, tid = task->pid;
+	const char *name = task->comm;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+
+	msg->cpuidx = cpu;
+
+	/*
+	 * Fork/Exit ==> return pid, tid
+	 * Fork ==> also return name
+	 */
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+	dst_vals += sizeof(tid);
+	if (is_fork)
+		memcpy(dst_vals, name, SW_MAX_PROC_NAME_SIZE);
+
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+	pw_pr_debug(
+		"HANDLED process %s event for task: pid = %d, tid = %d, name = %s\n",
+		is_fork ? "FORK" : "EXIT", pid, tid, name);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_PROCESS_FORK_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_FORK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node)
+		return;
+
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
+					 child, true /* true ==> fork */);
+	}
+};
+
+/*
+ * 9. PROCESS exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_PROCESS_EXIT_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_EXIT);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node)
+		return;
+
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
+					 task, false /* false ==> exit */);
+	}
+};
+
+#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
+/*
+ * 10. THERMAL_APIC entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_ENTRY_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+
+/*
+ * 10. THERMAL_APIC exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_EXIT_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_EXIT);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
+
+#if IS_ENABLED(CONFIG_ANDROID)
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+
+/*
+ * Helper function to produce wake lock/unlock messages.
+ */
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node,
+			       const char *name, int type, u64 timeout, int pid,
+			       int tid, const char *proc_name)
+{
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+
+	msg->cpuidx = cpu;
+
+	/*
+	 * Protocol:
+	 * wakelock_timeout, wakelock_type, wakelock_name,
+	 * proc_pid, proc_tid, proc_name
+	 */
+	*((u64 *)dst_vals) = timeout;
+	dst_vals += sizeof(timeout);
+	*((int *)dst_vals) = type;
+	dst_vals += sizeof(type);
+	strncpy(dst_vals, name, SW_MAX_KERNEL_WAKELOCK_NAME_SIZE);
+	dst_vals += SW_MAX_KERNEL_WAKELOCK_NAME_SIZE;
+
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+	dst_vals += sizeof(tid);
+	strncpy(dst_vals, proc_name, SW_MAX_PROC_NAME_SIZE);
+	dst_vals += SW_MAX_PROC_NAME_SIZE;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+
+};
+
+/*
+ * Helper function to handle wake lock/unlock callbacks.
+ */
+void sw_handle_wakelock_i(int cpu, struct sw_trace_notifier_data *node,
+			  const char *name, int type, u64 timeout)
+{
+	int pid = PID(), tid = TID();
+	const char *proc_name = NAME();
+	struct sw_collector_data *curr = NULL;
+
+	if (!node)
+		return;
+
+
+	list_for_each_entry(curr, &node->list, list) {
+		sw_produce_wakelock_msg_i(cpu, curr, name, type, timeout, pid,
+					  tid, proc_name);
+	}
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_LOCK_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	enum sw_kernel_wakelock_type type = SW_WAKE_LOCK;
+	u64 timeout = 0;
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+	const char *name = lock->name;
+#endif
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_LOCK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+	/*
+	 * Was this wakelock acquired with a timeout i.e.
+	 * is this an auto expire wakelock?
+	 */
+	if (lock->flags & (1U << 10)) {
+		type = SW_WAKE_LOCK_TIMEOUT;
+		timeout = jiffies_to_msecs(lock->expires - jiffies);
+	}
+#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
+				 (int)type, timeout);
+};
+
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_UNLOCK_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	enum sw_kernel_wakelock_type type = SW_WAKE_UNLOCK;
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+	const char *name = lock->name;
+#endif
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_UNLOCK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
+				 (int)type, 0 /*timeout*/);
+};
+#endif /* CONFIG_ANDROID */
+
+/*
+ * 12. WORKQUEUE
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WORKQUEUE_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(
+			SW_TRACE_ID_WORKQUEUE_EXECUTE_START);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu))
+		return;
+
+	list_for_each_entry(curr, &node->list, list)
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_workqueue_wakeup_helper_i,
+					 cpu, curr);
+
+};
+
+/*
+ * 13. SCHED switch
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_SWITCH_PARAMS)
+{
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_switch_helper_i);
+};
+
+/*
+ * 1. SUSPEND notifier
+ */
+static void sw_send_pm_notification_i(int value)
+{
+	struct sw_driver_msg *msg = NULL;
+	size_t buffer_len = sizeof(*msg) + sizeof(value);
+	char *buffer = vmalloc(buffer_len);
+
+	if (!buffer) {
+		pw_pr_error(
+			"couldn't allocate memory when sending suspend notification!\n");
+		return;
+	}
+	msg = (struct sw_driver_msg *)buffer;
+	msg->tsc = sw_timestamp();
+	msg->cpuidx = RAW_CPU();
+	msg->plugin_id = 0; /* "0" indicates a system message */
+	msg->metric_id = 1; /* "1" indicates a suspend/resume message (TODO) */
+	msg->msg_id = 0;
+	/* don't care; TODO: use the 'msg_id' to encode the 'value'? */
+	msg->payload_len = sizeof(value);
+	msg->p_payload = buffer + sizeof(*msg);
+	*((int *)msg->p_payload) = value;
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT))
+		pw_pr_error("couldn't produce generic message!\n");
+
+	vfree(buffer);
+}
+
+static u64 sw_pm_enter_tsc;
+static bool sw_is_reset_i(void)
+{
+	/*
+	 * TODO: rely on checking the IA32_FIXED_CTR2 instead?
+	 */
+	u64 curr_tsc = sw_tscval();
+	bool is_reset = sw_pm_enter_tsc > curr_tsc;
+
+	pw_pr_force("DEBUG: curr tsc = %llu, prev tsc = %llu, is reset = %s\n",
+		    curr_tsc, sw_pm_enter_tsc, is_reset ? "true" : "false");
+
+	return is_reset;
+}
+
+static void sw_probe_pm_helper_i(int id, int both_id, bool is_enter,
+				 enum sw_pm_action action, enum sw_pm_mode mode)
+{
+	struct sw_trace_notifier_data *node = GET_COLLECTOR_NOTIFIER_NODE(id);
+	struct sw_trace_notifier_data *both_node =
+		GET_COLLECTOR_NOTIFIER_NODE(both_id);
+	struct sw_trace_notifier_data *reset_node =
+		GET_COLLECTOR_NOTIFIER_NODE(SW_NOTIFIER_ID_COUNTER_RESET);
+	if (is_enter) {
+		/*
+		 * Entering HIBERNATION/SUSPEND
+		 */
+		sw_pm_enter_tsc = sw_tscval();
+	} else {
+		/*
+		 * Exitting HIBERNATION/SUSPEND
+		 */
+		if (sw_is_reset_i() && reset_node)
+			sw_handle_reset_messages_i(reset_node);
+
+	}
+	if (node)
+		sw_handle_trace_notifier_i(node);
+
+	if (both_node)
+		sw_handle_trace_notifier_i(both_node);
+
+	/* Send the suspend-resume notification */
+	sw_send_pm_notification_i(SW_PM_VALUE(mode, action));
+}
+
+static bool sw_is_suspend_via_firmware(void)
+{
+#if KERNEL_VERSION(4, 4, 0) <= LINUX_VERSION_CODE
+	/* 'pm_suspend_via_firmware' only available in kernel >= 4.4 */
+	return pm_suspend_via_firmware();
+#endif
+	return true;
+}
+
+static int sw_probe_pm_notifier_i(struct notifier_block *block,
+				  unsigned long state,
+				  void *dummy)
+{
+	static const struct {
+		enum sw_pm_action action;
+		int node_id;
+		int both_id;
+		bool is_enter;
+	} pm_data[PM_POST_RESTORE] = {
+		[PM_HIBERNATION_PREPARE] = { SW_PM_ACTION_HIBERNATE_ENTER,
+					     SW_NOTIFIER_ID_HIBERNATE_ENTER,
+					     SW_NOTIFIER_ID_HIBERNATE, true },
+		[PM_POST_HIBERNATION] = { SW_PM_ACTION_HIBERNATE_EXIT,
+					  SW_NOTIFIER_ID_HIBERNATE_EXIT,
+					  SW_NOTIFIER_ID_HIBERNATE, false },
+		[PM_SUSPEND_PREPARE] = { SW_PM_ACTION_SUSPEND_ENTER,
+					 SW_NOTIFIER_ID_SUSPEND_ENTER,
+					 SW_NOTIFIER_ID_SUSPEND, true },
+		[PM_POST_SUSPEND] = { SW_PM_ACTION_SUSPEND_EXIT,
+				      SW_NOTIFIER_ID_SUSPEND_EXIT,
+				      SW_NOTIFIER_ID_SUSPEND, false },
+	};
+	enum sw_pm_action action = pm_data[state].action;
+	enum sw_pm_mode mode = sw_is_suspend_via_firmware() ?
+				       SW_PM_MODE_FIRMWARE :
+				       SW_PM_MODE_NONE;
+	if (action != SW_PM_ACTION_NONE) {
+		int node_id = pm_data[state].node_id,
+		    both_id = pm_data[state].both_id;
+		bool is_enter = pm_data[state].is_enter;
+
+		sw_probe_pm_helper_i(node_id, both_id, is_enter, action, mode);
+	} else {
+		/* Not supported */
+		pw_pr_error(
+			"ERROR: unknown state %lu passed to SWA pm notifier!\n",
+			state);
+	}
+	return NOTIFY_DONE;
+}
+
+static void sw_store_topology_change_i(enum cpu_action type,
+				       int cpu, int core_id,
+				       int pkg_id)
+{
+	struct sw_topology_node *node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+
+	if (!node) {
+		pw_pr_error(
+			"couldn't allocate a node for topology change tracking!\n");
+		return;
+	}
+	node->change.timestamp = sw_timestamp();
+	node->change.type = type;
+	node->change.cpu = cpu;
+	node->change.core = core_id;
+	node->change.pkg = pkg_id;
+
+	SW_LIST_ADD(&sw_topology_list, node, list);
+	++sw_num_topology_entries;
+}
+
+#if KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE
+int sw_probe_hotplug_notifier_i(struct notifier_block *block,
+				unsigned long action, void *pcpu)
+{
+	unsigned int cpu = (unsigned long)pcpu;
+	unsigned int pkg_id = topology_physical_package_id(cpu);
+	unsigned int core_id = topology_core_id(cpu);
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		/* CPU is coming online -- store top change */
+		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE_PREPARE, cpu,
+					   core_id, pkg_id);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) preparing to come online at tsc = %llu! Current cpu = %d\n",
+			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		/* CPU is online -- first store top change
+		 * then take BEGIN snapshot
+		 */
+		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE, cpu, core_id,
+					   pkg_id);
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) online at tsc = %llu! Current cpu = %d\n",
+			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_DOWN_PREPARE:
+	case CPU_DOWN_PREPARE_FROZEN:
+		/* CPU is going offline -- take END snapshot */
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d preparing to go offline at tsc = %llu! Current cpu = %d\n",
+			cpu, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/* CPU is offline -- store top change */
+		sw_store_topology_change_i(SW_CPU_ACTION_OFFLINE, cpu, core_id,
+					   pkg_id);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d offlined at tsc = %llu! Current cpu = %d\n",
+			cpu, sw_timestamp(), RAW_CPU());
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_OK;
+};
+#else
+static void sw_probe_cpuhp_helper_i(unsigned int cpu, enum cpu_action action)
+{
+	unsigned int pkg_id = topology_physical_package_id(cpu);
+	unsigned int core_id = topology_core_id(cpu);
+
+	switch (action) {
+	case SW_CPU_ACTION_ONLINE_PREPARE:
+		/* CPU is coming online -- store top change */
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		break;
+	case SW_CPU_ACTION_ONLINE:
+		/* CPU is online -- first store top change
+		 * then take BEGIN snapshot
+		 */
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+		break;
+	case SW_CPU_ACTION_OFFLINE:
+		/* CPU is preparing to go offline -- take
+		 * END snapshot then store top change
+		 */
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		break;
+	default:
+		break;
+	}
+}
+
+static int sw_probe_cpu_offline_i(unsigned int cpu)
+{
+	pw_pr_debug("DEBUG: offline notification for cpu %u at %llu\n",
+	       cpu, sw_tscval());
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_OFFLINE);
+	return 0;
+}
+
+static int sw_probe_cpu_online_i(unsigned int cpu)
+{
+	pw_pr_debug("DEBUG: online notification for cpu %u at %llu\n", cpu,
+	       sw_tscval());
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE_PREPARE);
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE);
+	return 0;
+}
+#endif /* KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE  */
+
+/*
+ * 2. CPUFREQ notifier
+ */
+static int sw_probe_cpufreq_notifier_i(struct notifier_block *block,
+				unsigned long state, void *data)
+{
+	struct cpufreq_freqs *freqs = data;
+	static struct sw_trace_notifier_data *node;
+#if KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE
+	int cpu = freqs->cpu;
+#else  /* KERNEL_VERSION(5, 2, 0) <= LINUX_VERSION_CODE */
+	int cpu = freqs->policy->cpu;
+#endif /* KERNEL_VERSION(5, 2, 0) > LINUX_VERSION_CODE */
+
+	if (state == CPUFREQ_PRECHANGE) {
+		pw_pr_debug(
+			"CPU %d reports a CPUFREQ_PRECHANGE for target CPU %d at TSC = %llu\n",
+			RAW_CPU(), cpu, sw_timestamp());
+		if (unlikely(node == NULL)) {
+			node = GET_COLLECTOR_NOTIFIER_NODE(
+				SW_NOTIFIER_ID_CPUFREQ);
+			pw_pr_debug("NODE = %p\n", node);
+		}
+		/* Force an atomic context by disabling preemption */
+		get_cpu();
+		DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, cpu, node);
+		put_cpu();
+	}
+	return NOTIFY_DONE;
+}
+
+/*
+ * 1. TPS.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_start,
+					sw_probe_power_start_i);
+#else /* kernel version >= 2.6.38 */
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_start,
+					  sw_probe_power_start_i);
+#else /* kernel version >= 2.6.38 */
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
+	return PW_SUCCESS;
+};
+
+/*
+ * 2. TPF
+ */
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
+					sw_probe_power_frequency_i);
+#else /* kernel version >= 2.6.38 */
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
+					sw_probe_cpu_frequency_i);
+#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
+					  sw_probe_power_frequency_i);
+#else /* kernel version >= 2.6.38 */
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
+					  sw_probe_cpu_frequency_i);
+#endif /* KERNEL_VERSION(2, 6, 38) > LINUX_VERSION_CODE */
+	return PW_SUCCESS;
+};
+
+/*
+ * 3. IRQ handler entry
+ */
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
+					sw_probe_irq_handler_entry_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
+					  sw_probe_irq_handler_entry_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 4. TIMER expire.
+ */
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
+					sw_probe_timer_expire_entry_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data
+					     *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
+					  sw_probe_timer_expire_entry_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 5. HRTIMER expire.
+ */
+int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data
+					     *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
+					sw_probe_hrtimer_expire_entry_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_hrtimer_expire_entry_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
+					  sw_probe_hrtimer_expire_entry_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 6. SCHED wakeup
+ */
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
+					sw_probe_sched_wakeup_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
+					  sw_probe_sched_wakeup_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 8. PROCESS fork
+ */
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
+					sw_probe_sched_process_fork_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data
+					     *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
+					  sw_probe_sched_process_fork_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 9. PROCESS exit
+ */
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
+					sw_probe_sched_process_exit_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data
+					     *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
+					  sw_probe_sched_process_exit_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 10. THERMAL_APIC entry
+ */
+#if KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE
+int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
+					sw_probe_thermal_apic_entry_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data
+					     *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
+					  sw_probe_thermal_apic_entry_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * 10. THERMAL_APIC exit
+ */
+int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
+					sw_probe_thermal_apic_exit_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
+					  sw_probe_thermal_apic_exit_i);
+	return PW_SUCCESS;
+};
+#endif /* KERNEL_VERSION(3, 14, 0) <= LINUX_VERSION_CODE */
+
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+#if IS_ENABLED(CONFIG_ANDROID)
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_lock, sw_probe_wake_lock_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_lock,
+					  sw_probe_wake_lock_i);
+	return PW_SUCCESS;
+};
+#else /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+int sw_register_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
+					sw_probe_wakeup_source_activate_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
+					  sw_probe_wakeup_source_activate_i);
+	return PW_SUCCESS;
+};
+#endif /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+#if KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE
+int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
+					sw_probe_wake_unlock_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
+					  sw_probe_wake_unlock_i);
+	return PW_SUCCESS;
+};
+
+#else /* KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+int sw_register_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
+					sw_probe_wakeup_source_deactivate_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
+					  sw_probe_wakeup_source_deactivate_i);
+	return PW_SUCCESS;
+};
+#endif /*  KERNEL_VERSION(3, 4, 0) > LINUX_VERSION_CODE */
+#endif /* CONFIG_ANDROID */
+
+/*
+ * 12. WORKQUEUE execution.
+ */
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
+					sw_probe_workqueue_execution_i);
+#else
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
+					sw_probe_workqueue_execute_start_i);
+#endif
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_workqueue_execution_i(
+	struct sw_trace_notifier_data *node)
+{
+#if KERNEL_VERSION(2, 6, 35) >= LINUX_VERSION_CODE
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
+					  sw_probe_workqueue_execution_i);
+#else
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
+					  sw_probe_workqueue_execute_start_i);
+#endif
+	return PW_SUCCESS;
+};
+
+/*
+ * 13. SCHED switch
+ */
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+	/*
+	 * Set polling tick time, in jiffies.
+	 * Used by the context switch tracepoint to decide
+	 * if enough time has elapsed since the last
+	 * collection point to read resources again.
+	 */
+	{
+		int cpu = 0;
+
+		for_each_present_cpu(cpu)
+			*(&per_cpu(sw_pcpu_polling_jiff, cpu)) = jiffies;
+
+	}
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
+					sw_probe_sched_switch_i);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
+					  sw_probe_sched_switch_i);
+	return PW_SUCCESS;
+};
+
+/*
+ * Notifier register/unregister functions.
+ */
+
+/*
+ * 1. SUSPEND notifier.
+ */
+static struct notifier_block sw_pm_notifier = {
+	.notifier_call = &sw_probe_pm_notifier_i,
+};
+
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+	register_pm_notifier(&sw_pm_notifier);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+	unregister_pm_notifier(&sw_pm_notifier);
+	return PW_SUCCESS;
+};
+
+/*
+ * 2. CPUFREQ notifier.
+ */
+static struct notifier_block sw_cpufreq_notifier = {
+	.notifier_call = &sw_probe_cpufreq_notifier_i,
+};
+
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+	cpufreq_register_notifier(&sw_cpufreq_notifier,
+				  CPUFREQ_TRANSITION_NOTIFIER);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+	cpufreq_unregister_notifier(&sw_cpufreq_notifier,
+				    CPUFREQ_TRANSITION_NOTIFIER);
+	return PW_SUCCESS;
+};
+
+#if KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE
+/*
+ * 3. CPU hot plug notifier.
+ */
+struct notifier_block sw_cpu_hotplug_notifier = {
+	.notifier_call = &sw_probe_hotplug_notifier_i,
+};
+
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	register_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+	return PW_SUCCESS;
+};
+
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	unregister_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+	return PW_SUCCESS;
+};
+
+#else /* KERNEL_VERSION(4, 10, 0) <= LINUX_VERSION_CODE */
+static int sw_cpuhp_state = -1;
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	sw_cpuhp_state = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
+						   "socwatch:online",
+						   &sw_probe_cpu_online_i,
+						   &sw_probe_cpu_offline_i);
+	if (sw_cpuhp_state < 0) {
+		pw_pr_error("couldn't register socwatch hotplug callbacks!\n");
+		return -EIO;
+	}
+	return 0;
+};
+
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	if (sw_cpuhp_state >= 0)
+		cpuhp_remove_state_nocalls((enum cpuhp_state)sw_cpuhp_state);
+
+	return 0;
+};
+#endif /* KERNEL_VERSION(4, 10, 0) > LINUX_VERSION_CODE */
+
+/*
+ * Tracepoint extraction routines.
+ * Required for newer kernels (>=3.15)
+ */
+#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE
+static void sw_extract_tracepoint_callback(struct tracepoint *tp, void *priv)
+{
+	struct sw_trace_notifier_data *node = NULL;
+	int i = 0;
+	int *numStructsFound = (int *)priv;
+
+	if (*numStructsFound == NUM_VALID_TRACEPOINTS) {
+		/*
+		 * We've found all the tracepoints we need.
+		 */
+		return;
+	}
+	if (tp) {
+		FOR_EACH_TRACEPOINT_NODE(i, node)
+		{
+			if (node->tp == NULL && node->name) {
+				const char *name =
+					sw_get_trace_notifier_kernel_name(node);
+				if (name && !strcmp(tp->name, name)) {
+					node->tp = tp;
+					++*numStructsFound;
+					pw_pr_debug("OK, found TP %s\n",
+						    tp->name);
+				}
+			}
+		}
+	}
+};
+#endif /* KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE */
+#endif /* CONFIG_TRACEPOINTS */
+
+/*
+ * Retrieve the list of tracepoint structs to use
+ * when registering and unregistering tracepoint handlers.
+ */
+int sw_extract_trace_notifier_providers(void)
+{
+#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE  &&			\
+	IS_ENABLED(CONFIG_TRACEPOINTS)
+	int numCallbacks = 0;
+
+	for_each_kernel_tracepoint(&sw_extract_tracepoint_callback,
+				   &numCallbacks);
+	/*
+	 * Did we get the complete list?
+	 */
+	if (numCallbacks != NUM_VALID_TRACEPOINTS)
+		pw_pr_warn(
+		       "WARNING : Could NOT find tracepoint structs for some tracepoints !\n");
+#endif /* KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE */
+	return PW_SUCCESS;
+};
+
+void sw_reset_trace_notifier_providers(void)
+{
+	/*
+	 * Reset the wakeup flag. Not strictly required if we aren't probing
+	 * any of the wakeup tracepoints.
+	 */
+	{
+		int cpu = 0;
+
+		for_each_online_cpu(cpu)
+			RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+	}
+	/*
+	 * Reset the wakeup event flag. Not strictly required if we
+	 * aren't probing any of the wakeup tracepoints. Will be reset
+	 * in the power_start tracepoint if user requested a c-state
+	 * collection.
+	 */
+	sw_wakeup_event_flag = true;
+};
+
+void sw_print_trace_notifier_provider_overheads(void)
+{
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tps_i, "TPS");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tpf_i, "TPF");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_irq_wakeup_i, "IRQ");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_timer_wakeup_helper_i,
+					 "TIMER_EXPIRE");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_wakeup_i,
+					 "SCHED WAKEUP");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_process_fork_exit_helper_i,
+					 "PROCESS FORK/EXIT");
+#if IS_ENABLED(CONFIG_ANDROID)
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_wakelock_i,
+					 "WAKE LOCK/UNLOCK");
+#endif /* CONFIG_ANDROID */
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_workqueue_wakeup_helper_i,
+					 "WORKQUEUE");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_switch_helper_i,
+					 "SCHED SWITCH");
+};
+
+/*
+ * Add all trace/notifier providers.
+ */
+int sw_add_trace_notifier_providers(void)
+{
+	struct sw_trace_notifier_data *node = NULL;
+	int i = 0;
+
+	FOR_EACH_TRACEPOINT_NODE(i, node)
+	{
+		if (sw_register_trace_notify_provider(node)) {
+			pw_pr_error("ERROR : couldn't add a trace provider!\n");
+			return -EIO;
+		}
+	}
+	FOR_EACH_NOTIFIER_NODE(i, node)
+	{
+		if (sw_register_trace_notify_provider(node)) {
+			pw_pr_error(
+				"ERROR: couldn't add a notifier provider !\n");
+			return -EIO;
+		}
+	}
+	/*
+	 * Add the cpu hot plug notifier.
+	 */
+	{
+		if (sw_register_trace_notify_provider(
+			    &s_hotplug_notifier_data)) {
+			pw_pr_error(
+				"ERROR : couldn't add cpu notifier provider!\n");
+			return -EIO;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * Remove previously added providers.
+ */
+void sw_remove_trace_notifier_providers(void)
+{ /* NOP */
+}
diff --git a/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c b/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
index b03155c89c14..6ecf02e7acd9 100644
--- a/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
+++ b/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
@@ -1,406 +1,406 @@
-/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
- *
- * This file is provided under a dual BSD/GPLv2 license.  When using or
- * redistributing this file, you may do so under either license.
- *
- * GPL LICENSE SUMMARY
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * Contact Information:
- * SoC Watch Developer Team <socwatchdevelopers@intel.com>
- * Intel Corporation,
- * 1300 S Mopac Expwy,
- * Austin, TX 78746
- *
- * BSD LICENSE
- *
- * Copyright(c) 2014 - 2019 Intel Corporation.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- *
- *   * Redistributions of source code must retain the above copyright
- *     notice, this list of conditions and the following disclaimer.
- *   * Redistributions in binary form must reproduce the above copyright
- *     notice, this list of conditions and the following disclaimer in
- *     the documentation and/or other materials provided with the
- *     distribution.
- *   * Neither the name of Intel Corporation nor the names of its
- *     contributors may be used to endorse or promote products derived
- *     from this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
- * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
- * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
- * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
- * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
- * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
- * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
- * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
- * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
- * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- */
-#include "sw_structs.h"
-#include "sw_kernel_defines.h"
-#include "sw_types.h"
-#include "sw_tracepoint_handlers.h"
-#include "sw_trace_notifier_provider.h"
-#include "sw_mem.h"
-
-/* -------------------------------------------------
- * Data structures and variable definitions.
- * -------------------------------------------------
- */
-struct sw_trace_list_node {
-	struct sw_trace_notifier_data *data;
-	int id;
-
-	SW_LIST_ENTRY(list, sw_trace_list_node);
-};
-static SW_DEFINE_LIST_HEAD(s_trace_list, sw_trace_list_node) =
-				SW_LIST_HEAD_INITIALIZER(s_trace_list);
-static SW_DEFINE_LIST_HEAD(s_notifier_list, sw_trace_list_node) =
-				SW_LIST_HEAD_INITIALIZER(s_notifier_list);
-static int s_trace_idx = -1, s_notifier_idx = -1;
-
-SW_DEFINE_LIST_HEAD(sw_topology_list, sw_topology_node) =
-				SW_LIST_HEAD_INITIALIZER(sw_topology_list);
-size_t sw_num_topology_entries;
-
-
-/* -------------------------------------------------
- * Function definitions.
- * -------------------------------------------------
- */
-int sw_extract_tracepoints(void)
-{
-	return sw_extract_trace_notifier_providers();
-}
-
-void sw_reset_trace_notifier_lists(void)
-{
-	sw_reset_trace_notifier_providers();
-}
-
-void sw_print_trace_notifier_overheads(void)
-{
-	sw_print_trace_notifier_provider_overheads();
-}
-
-static int sw_for_each_node_i(
-	void *list_head,
-	int (*func)(struct sw_trace_notifier_data *node, void *priv),
-	void *priv, bool return_on_error)
-{
-	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
-	int retval = PW_SUCCESS;
-	struct sw_trace_list_node *lnode = NULL;
-
-	SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
-		if ((*func)(lnode->data, priv)) {
-			retval = -EIO;
-			if (return_on_error)
-				break;
-		}
-	}
-	return retval;
-}
-
-int sw_for_each_tracepoint_node(
-	int (*func)(struct sw_trace_notifier_data *node, void *priv),
-	void *priv, bool return_on_error)
-{
-	if (func)
-		return sw_for_each_node_i(&s_trace_list,
-			func, priv, return_on_error);
-
-	return PW_SUCCESS;
-}
-
-int sw_for_each_notifier_node(
-	int (*func)(struct sw_trace_notifier_data *node, void *priv),
-	void *priv, bool return_on_error)
-{
-
-	if (func)
-		return sw_for_each_node_i(&s_notifier_list,
-						func, priv, return_on_error);
-
-	return PW_SUCCESS;
-}
-
-/*
- * Retrieve the ID for the corresponding tracepoint/notifier.
- */
-int sw_get_trace_notifier_id(struct sw_trace_notifier_data *tnode)
-{
-	struct sw_trace_list_node *lnode = NULL;
-
-	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = (void *)&s_trace_list;
-	if (!tnode) {
-		pw_pr_error(
-			"ERROR: cannot get ID for NULL trace/notifier data!\n");
-		return -EIO;
-	}
-	if (!(tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
-			tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
-		pw_pr_error(
-			"ERROR: cannot get ID for invalid trace/notifier data!\n");
-		return -EIO;
-	}
-	if (!tnode->name || !tnode->name->abstract_name) {
-		pw_pr_error(
-			"ERROR: cannot get ID for trace/notifier data without valid name!\n");
-		return -EIO;
-	}
-
-#if defined(LINUX_VERSION_CODE)
-#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE &&	\
-	defined(CONFIG_TRACEPOINTS)
-
-	if (tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT &&
-		tnode->name->kernel_name && !tnode->tp) {
-		/* No tracepoint structure found so no ID possible */
-		return -EIO;
-	}
-#endif
-#endif
-	if (tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)
-		head = (void *)&s_notifier_list;
-
-	SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
-		struct sw_trace_notifier_data *data = lnode->data;
-
-		if (!strcmp(
-			data->name->abstract_name, tnode->name->abstract_name))
-			return lnode->id;
-	}
-	return -1;
-}
-/*
- * Retrieve the "kernel" name for this tracepoint/notifier.
- */
-const char *sw_get_trace_notifier_kernel_name(
-		struct sw_trace_notifier_data *node)
-{
-	return node->name->kernel_name;
-};
-/*
- * Retrieve the "abstract" name for this tracepoint/notifier.
- */
-const char *sw_get_trace_notifier_abstract_name(
-			struct sw_trace_notifier_data *node)
-{
-	return node->name->abstract_name;
-};
-
-/*
- * Add a single TRACE/NOTIFY provider.
- */
-int sw_register_trace_notify_provider(struct sw_trace_notifier_data *data)
-{
-	struct sw_trace_list_node *lnode = NULL;
-
-	if (!data) {
-		pw_pr_error(
-			"ERROR: cannot add NULL trace/notifier provider!\n");
-		return -EIO;
-	}
-	if (!(data->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
-			data->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
-		pw_pr_error(
-			"ERROR: cannot add invalid trace/notifier data!\n");
-		return -EIO;
-	}
-	/*
-	 * Kernel name is allowed to be NULL, but abstract name
-	 * MUST be present!
-	 */
-	if (!data->name || !data->name->abstract_name) {
-		pw_pr_error(
-			"ERROR: cannot add trace/notifier provider without an abstract name!\n");
-		pw_pr_error("ERROR: data->name = %p\n", data->name);
-		return -EIO;
-	}
-	lnode = sw_kmalloc(sizeof(*lnode), GFP_KERNEL);
-	if (!lnode) {
-		pw_pr_error(
-			"ERROR: couldn't allocate a list node when adding a trace/notifier provider!\n");
-		return -ENOMEM;
-	}
-	lnode->data = data;
-	SW_LIST_ENTRY_INIT(lnode, list);
-	if (data->type == SW_TRACE_COLLECTOR_TRACEPOINT) {
-		lnode->id = ++s_trace_idx;
-		SW_LIST_ADD(&s_trace_list, lnode, list);
-	} else {
-		lnode->id = ++s_notifier_idx;
-		SW_LIST_ADD(&s_notifier_list, lnode, list);
-	}
-	return PW_SUCCESS;
-}
-/*
- * Add all TRACE/NOTIFY providers.
- */
-int sw_add_trace_notify(void)
-{
-	return sw_add_trace_notifier_providers();
-}
-
-static void sw_free_trace_notifier_list_i(void *list_head)
-{
-	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
-	while (!SW_LIST_EMPTY(head)) {
-		struct sw_trace_list_node *lnode =
-			SW_LIST_GET_HEAD_ENTRY(head, sw_trace_list_node, list);
-
-		SW_LIST_UNLINK(lnode, list);
-		sw_kfree(lnode);
-	}
-}
-/*
- * Remove TRACE/NOTIFY providers.
- */
-void sw_remove_trace_notify(void)
-{
-	/*
-	 * Free all nodes.
-	 */
-	sw_free_trace_notifier_list_i(&s_trace_list);
-	sw_free_trace_notifier_list_i(&s_notifier_list);
-	/*
-	 * Call our providers to deallocate resources.
-	 */
-	sw_remove_trace_notifier_providers();
-	/*
-	 * Clear out the topology list
-	 */
-	sw_clear_topology_list();
-}
-
-#define REG_FLAG (void *)1
-#define UNREG_FLAG (void *)2
-static int sw_reg_unreg_node_i(struct sw_trace_notifier_data *node,
-				void *is_reg)
-{
-	if (is_reg == REG_FLAG) {
-		/*
-		 * Do we have anything to collect?
-		 * Update: or were we asked to always register?
-		 */
-		if (SW_LIST_EMPTY(&node->list) && !node->always_register)
-			return PW_SUCCESS;
-
-		/*
-		 * Sanity: ensure we have a register AND an unregister function
-		 * before proceeding!
-		 */
-		if (node->probe_register == NULL ||
-				node->probe_unregister == NULL) {
-			pw_pr_debug(
-				"WARNING: invalid trace/notifier register/unregister function for %s\n",
-				 sw_get_trace_notifier_kernel_name(node));
-			/*
-			 * Don't flag this as an error -- some socwatch
-			 * trace providers don't have a register/unregister
-			 * function
-			 */
-			return PW_SUCCESS;
-		}
-		if ((*node->probe_register)(node))
-			return -EIO;
-
-		node->was_registered = true;
-		return PW_SUCCESS;
-	} else if (is_reg == UNREG_FLAG) {
-		if (node->was_registered) {
-			/*
-			 * No need to check for validity of probe unregister
-			 * function -- 'sw_register_notifiers_i()'
-			 * would already have done so!
-			 */
-			WARN_ON((*node->probe_unregister)(node));
-			node->was_registered = false;
-			pw_pr_debug("OK, unregistered trace/notifier for %s\n",
-				sw_get_trace_notifier_kernel_name(node));
-		}
-		return PW_SUCCESS;
-	}
-	pw_pr_error("ERROR: invalid reg/unreg flag value 0x%lx\n",
-		(unsigned long)is_reg);
-	return -EIO;
-}
-/*
- * Register all required tracepoints and notifiers.
- */
-int sw_register_trace_notifiers(void)
-{
-	/*
-	 * First, the tracepoints.
-	 */
-	if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i,
-			REG_FLAG, true /* return on error */)) {
-		pw_pr_error("ERROR registering some tracepoints\n");
-		return -EIO;
-	}
-	/*
-	 * And then the notifiers.
-	 */
-	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i,
-			REG_FLAG, true /* return on error */)) {
-		pw_pr_error("ERROR registering some tracepoints\n");
-		return -EIO;
-	}
-	return PW_SUCCESS;
-};
-/*
- * Unregister all previously registered tracepoints and notifiers.
- */
-int sw_unregister_trace_notifiers(void)
-{
-	/*
-	 * First, the notifiers.
-	 */
-	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, UNREG_FLAG,
-			true /* return on error */)) {
-		pw_pr_error("ERROR registering some tracepoints\n");
-		return -EIO;
-	}
-	/*
-	 * And then the tracepoints.
-	 */
-	if (sw_for_each_tracepoint_node(
-			&sw_reg_unreg_node_i,
-			UNREG_FLAG, true /* return on error */)) {
-		pw_pr_error("ERROR registering some tracepoints\n");
-		return -EIO;
-	}
-	return PW_SUCCESS;
-};
-
-void sw_clear_topology_list(void)
-{
-	SW_LIST_HEAD_VAR(sw_topology_node) * head = &sw_topology_list;
-	while (!SW_LIST_EMPTY(head)) {
-		struct sw_topology_node *lnode =
-			SW_LIST_GET_HEAD_ENTRY(head, sw_topology_node, list);
-
-		pw_pr_debug("Clearing topology node for cpu %d\n",
-			lnode->change.cpu);
-		SW_LIST_UNLINK(lnode, list);
-		sw_kfree(lnode);
-	}
-	sw_num_topology_entries  = 0;
-}
+/* SPDX-License-Identifier: GPL-2.0 AND BSD-3-Clause
+ *
+ * This file is provided under a dual BSD/GPLv2 license.  When using or
+ * redistributing this file, you may do so under either license.
+ *
+ * GPL LICENSE SUMMARY
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * Contact Information:
+ * SoC Watch Developer Team <socwatchdevelopers@intel.com>
+ * Intel Corporation,
+ * 1300 S Mopac Expwy,
+ * Austin, TX 78746
+ *
+ * BSD LICENSE
+ *
+ * Copyright(c) 2014 - 2019 Intel Corporation.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *   * Redistributions in binary form must reproduce the above copyright
+ *     notice, this list of conditions and the following disclaimer in
+ *     the documentation and/or other materials provided with the
+ *     distribution.
+ *   * Neither the name of Intel Corporation nor the names of its
+ *     contributors may be used to endorse or promote products derived
+ *     from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+#include "sw_structs.h"
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_trace_notifier_provider.h"
+#include "sw_mem.h"
+
+/* -------------------------------------------------
+ * Data structures and variable definitions.
+ * -------------------------------------------------
+ */
+struct sw_trace_list_node {
+	struct sw_trace_notifier_data *data;
+	int id;
+
+	SW_LIST_ENTRY(list, sw_trace_list_node);
+};
+static SW_DEFINE_LIST_HEAD(s_trace_list, sw_trace_list_node) =
+				SW_LIST_HEAD_INITIALIZER(s_trace_list);
+static SW_DEFINE_LIST_HEAD(s_notifier_list, sw_trace_list_node) =
+				SW_LIST_HEAD_INITIALIZER(s_notifier_list);
+static int s_trace_idx = -1, s_notifier_idx = -1;
+
+SW_DEFINE_LIST_HEAD(sw_topology_list, sw_topology_node) =
+				SW_LIST_HEAD_INITIALIZER(sw_topology_list);
+size_t sw_num_topology_entries;
+
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+int sw_extract_tracepoints(void)
+{
+	return sw_extract_trace_notifier_providers();
+}
+
+void sw_reset_trace_notifier_lists(void)
+{
+	sw_reset_trace_notifier_providers();
+}
+
+void sw_print_trace_notifier_overheads(void)
+{
+	sw_print_trace_notifier_provider_overheads();
+}
+
+static int sw_for_each_node_i(
+	void *list_head,
+	int (*func)(struct sw_trace_notifier_data *node, void *priv),
+	void *priv, bool return_on_error)
+{
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
+	int retval = PW_SUCCESS;
+	struct sw_trace_list_node *lnode = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
+		if ((*func)(lnode->data, priv)) {
+			retval = -EIO;
+			if (return_on_error)
+				break;
+		}
+	}
+	return retval;
+}
+
+int sw_for_each_tracepoint_node(
+	int (*func)(struct sw_trace_notifier_data *node, void *priv),
+	void *priv, bool return_on_error)
+{
+	if (func)
+		return sw_for_each_node_i(&s_trace_list,
+			func, priv, return_on_error);
+
+	return PW_SUCCESS;
+}
+
+int sw_for_each_notifier_node(
+	int (*func)(struct sw_trace_notifier_data *node, void *priv),
+	void *priv, bool return_on_error)
+{
+
+	if (func)
+		return sw_for_each_node_i(&s_notifier_list,
+						func, priv, return_on_error);
+
+	return PW_SUCCESS;
+}
+
+/*
+ * Retrieve the ID for the corresponding tracepoint/notifier.
+ */
+int sw_get_trace_notifier_id(struct sw_trace_notifier_data *tnode)
+{
+	struct sw_trace_list_node *lnode = NULL;
+
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = (void *)&s_trace_list;
+	if (!tnode) {
+		pw_pr_error(
+			"ERROR: cannot get ID for NULL trace/notifier data!\n");
+		return -EIO;
+	}
+	if (!(tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
+			tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+		pw_pr_error(
+			"ERROR: cannot get ID for invalid trace/notifier data!\n");
+		return -EIO;
+	}
+	if (!tnode->name || !tnode->name->abstract_name) {
+		pw_pr_error(
+			"ERROR: cannot get ID for trace/notifier data without valid name!\n");
+		return -EIO;
+	}
+
+#if defined(LINUX_VERSION_CODE)
+#if KERNEL_VERSION(3, 15, 0) <= LINUX_VERSION_CODE &&	\
+	defined(CONFIG_TRACEPOINTS)
+
+	if (tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT &&
+		tnode->name->kernel_name && !tnode->tp) {
+		/* No tracepoint structure found so no ID possible */
+		return -EIO;
+	}
+#endif
+#endif
+	if (tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)
+		head = (void *)&s_notifier_list;
+
+	SW_LIST_FOR_EACH_ENTRY(lnode, head, list) {
+		struct sw_trace_notifier_data *data = lnode->data;
+
+		if (!strcmp(
+			data->name->abstract_name, tnode->name->abstract_name))
+			return lnode->id;
+	}
+	return -1;
+}
+/*
+ * Retrieve the "kernel" name for this tracepoint/notifier.
+ */
+const char *sw_get_trace_notifier_kernel_name(
+		struct sw_trace_notifier_data *node)
+{
+	return node->name->kernel_name;
+};
+/*
+ * Retrieve the "abstract" name for this tracepoint/notifier.
+ */
+const char *sw_get_trace_notifier_abstract_name(
+			struct sw_trace_notifier_data *node)
+{
+	return node->name->abstract_name;
+};
+
+/*
+ * Add a single TRACE/NOTIFY provider.
+ */
+int sw_register_trace_notify_provider(struct sw_trace_notifier_data *data)
+{
+	struct sw_trace_list_node *lnode = NULL;
+
+	if (!data) {
+		pw_pr_error(
+			"ERROR: cannot add NULL trace/notifier provider!\n");
+		return -EIO;
+	}
+	if (!(data->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
+			data->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+		pw_pr_error(
+			"ERROR: cannot add invalid trace/notifier data!\n");
+		return -EIO;
+	}
+	/*
+	 * Kernel name is allowed to be NULL, but abstract name
+	 * MUST be present!
+	 */
+	if (!data->name || !data->name->abstract_name) {
+		pw_pr_error(
+			"ERROR: cannot add trace/notifier provider without an abstract name!\n");
+		pw_pr_error("ERROR: data->name = %p\n", data->name);
+		return -EIO;
+	}
+	lnode = sw_kmalloc(sizeof(*lnode), GFP_KERNEL);
+	if (!lnode) {
+		pw_pr_error(
+			"ERROR: couldn't allocate a list node when adding a trace/notifier provider!\n");
+		return -ENOMEM;
+	}
+	lnode->data = data;
+	SW_LIST_ENTRY_INIT(lnode, list);
+	if (data->type == SW_TRACE_COLLECTOR_TRACEPOINT) {
+		lnode->id = ++s_trace_idx;
+		SW_LIST_ADD(&s_trace_list, lnode, list);
+	} else {
+		lnode->id = ++s_notifier_idx;
+		SW_LIST_ADD(&s_notifier_list, lnode, list);
+	}
+	return PW_SUCCESS;
+}
+/*
+ * Add all TRACE/NOTIFY providers.
+ */
+int sw_add_trace_notify(void)
+{
+	return sw_add_trace_notifier_providers();
+}
+
+static void sw_free_trace_notifier_list_i(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_trace_list_node *lnode =
+			SW_LIST_GET_HEAD_ENTRY(head, sw_trace_list_node, list);
+
+		SW_LIST_UNLINK(lnode, list);
+		sw_kfree(lnode);
+	}
+}
+/*
+ * Remove TRACE/NOTIFY providers.
+ */
+void sw_remove_trace_notify(void)
+{
+	/*
+	 * Free all nodes.
+	 */
+	sw_free_trace_notifier_list_i(&s_trace_list);
+	sw_free_trace_notifier_list_i(&s_notifier_list);
+	/*
+	 * Call our providers to deallocate resources.
+	 */
+	sw_remove_trace_notifier_providers();
+	/*
+	 * Clear out the topology list
+	 */
+	sw_clear_topology_list();
+}
+
+#define REG_FLAG (void *)1
+#define UNREG_FLAG (void *)2
+static int sw_reg_unreg_node_i(struct sw_trace_notifier_data *node,
+				void *is_reg)
+{
+	if (is_reg == REG_FLAG) {
+		/*
+		 * Do we have anything to collect?
+		 * Update: or were we asked to always register?
+		 */
+		if (SW_LIST_EMPTY(&node->list) && !node->always_register)
+			return PW_SUCCESS;
+
+		/*
+		 * Sanity: ensure we have a register AND an unregister function
+		 * before proceeding!
+		 */
+		if (node->probe_register == NULL ||
+				node->probe_unregister == NULL) {
+			pw_pr_debug(
+				"WARNING: invalid trace/notifier register/unregister function for %s\n",
+				 sw_get_trace_notifier_kernel_name(node));
+			/*
+			 * Don't flag this as an error -- some socwatch
+			 * trace providers don't have a register/unregister
+			 * function
+			 */
+			return PW_SUCCESS;
+		}
+		if ((*node->probe_register)(node))
+			return -EIO;
+
+		node->was_registered = true;
+		return PW_SUCCESS;
+	} else if (is_reg == UNREG_FLAG) {
+		if (node->was_registered) {
+			/*
+			 * No need to check for validity of probe unregister
+			 * function -- 'sw_register_notifiers_i()'
+			 * would already have done so!
+			 */
+			WARN_ON((*node->probe_unregister)(node));
+			node->was_registered = false;
+			pw_pr_debug("OK, unregistered trace/notifier for %s\n",
+				sw_get_trace_notifier_kernel_name(node));
+		}
+		return PW_SUCCESS;
+	}
+	pw_pr_error("ERROR: invalid reg/unreg flag value 0x%lx\n",
+		(unsigned long)is_reg);
+	return -EIO;
+}
+/*
+ * Register all required tracepoints and notifiers.
+ */
+int sw_register_trace_notifiers(void)
+{
+	/*
+	 * First, the tracepoints.
+	 */
+	if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i,
+			REG_FLAG, true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	/*
+	 * And then the notifiers.
+	 */
+	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i,
+			REG_FLAG, true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	return PW_SUCCESS;
+};
+/*
+ * Unregister all previously registered tracepoints and notifiers.
+ */
+int sw_unregister_trace_notifiers(void)
+{
+	/*
+	 * First, the notifiers.
+	 */
+	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, UNREG_FLAG,
+			true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	/*
+	 * And then the tracepoints.
+	 */
+	if (sw_for_each_tracepoint_node(
+			&sw_reg_unreg_node_i,
+			UNREG_FLAG, true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	return PW_SUCCESS;
+};
+
+void sw_clear_topology_list(void)
+{
+	SW_LIST_HEAD_VAR(sw_topology_node) * head = &sw_topology_list;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_topology_node *lnode =
+			SW_LIST_GET_HEAD_ENTRY(head, sw_topology_node, list);
+
+		pw_pr_debug("Clearing topology node for cpu %d\n",
+			lnode->change.cpu);
+		SW_LIST_UNLINK(lnode, list);
+		sw_kfree(lnode);
+	}
+	sw_num_topology_entries  = 0;
+}
-- 
2.17.1

