From e50f3f8682348c57b5c51c23bcd0b69196e4e0a5 Mon Sep 17 00:00:00 2001
From: Zhou Furong <furong.zhou@intel.com>
Date: Fri, 24 Dec 2021 11:31:04 +0800
Subject: [PATCH 010/109] hantro: add squashed hantro driver

handro driver is media codec driver for KMB.
---
 drivers/gpu/drm/Kconfig                   |    2 +
 drivers/gpu/drm/Makefile                  |    1 +
 drivers/gpu/drm/drm_gem.c                 |    2 +-
 drivers/gpu/drm/hantro/Kconfig            |   26 +
 drivers/gpu/drm/hantro/Makefile           |    8 +
 drivers/gpu/drm/hantro/hantro_cache.c     |  536 +++++++
 drivers/gpu/drm/hantro/hantro_cache.h     |   22 +
 drivers/gpu/drm/hantro/hantro_cooling.c   |   94 ++
 drivers/gpu/drm/hantro/hantro_cooling.h   |   15 +
 drivers/gpu/drm/hantro/hantro_dec.c       | 1563 +++++++++++++++++++++
 drivers/gpu/drm/hantro/hantro_dec.h       |   21 +
 drivers/gpu/drm/hantro/hantro_dec400.c    |  202 +++
 drivers/gpu/drm/hantro/hantro_dec400.h    |   16 +
 drivers/gpu/drm/hantro/hantro_device.h    |  289 ++++
 drivers/gpu/drm/hantro/hantro_devicemgr.c |  563 ++++++++
 drivers/gpu/drm/hantro/hantro_devicemgr.h |  266 ++++
 drivers/gpu/drm/hantro/hantro_dmabuf.c    |   91 ++
 drivers/gpu/drm/hantro/hantro_drm.c       | 1550 ++++++++++++++++++++
 drivers/gpu/drm/hantro/hantro_drm.h       |  272 ++++
 drivers/gpu/drm/hantro/hantro_drv.c       |  846 +++++++++++
 drivers/gpu/drm/hantro/hantro_dwl_defs.h  |   89 ++
 drivers/gpu/drm/hantro/hantro_enc.c       |  783 +++++++++++
 drivers/gpu/drm/hantro/hantro_enc.h       |   24 +
 drivers/gpu/drm/hantro/hantro_fence.c     |  277 ++++
 drivers/gpu/drm/hantro/hantro_fs.c        |  476 +++++++
 drivers/gpu/drm/hantro/hantro_metadata.c  |   94 ++
 drivers/gpu/drm/hantro/hantro_metadata.h  |  153 ++
 drivers/gpu/drm/hantro/hantro_priv.h      |  250 ++++
 drivers/gpu/drm/hantro/hantro_slice.h     |  255 ++++
 drivers/gpu/drm/hantro/trace.c            |   41 +
 drivers/gpu/drm/hantro/trace.h            |  333 +++++
 31 files changed, 9159 insertions(+), 1 deletion(-)
 create mode 100644 drivers/gpu/drm/hantro/Kconfig
 create mode 100644 drivers/gpu/drm/hantro/Makefile
 create mode 100644 drivers/gpu/drm/hantro/hantro_cache.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_cache.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_cooling.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_cooling.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_dec.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_dec.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_dec400.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_dec400.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_device.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_devicemgr.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_devicemgr.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_dmabuf.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_drm.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_drm.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_drv.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_dwl_defs.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_enc.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_enc.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_fence.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_fs.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_metadata.c
 create mode 100644 drivers/gpu/drm/hantro/hantro_metadata.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_priv.h
 create mode 100644 drivers/gpu/drm/hantro/hantro_slice.h
 create mode 100644 drivers/gpu/drm/hantro/trace.c
 create mode 100644 drivers/gpu/drm/hantro/trace.h

diff --git a/drivers/gpu/drm/Kconfig b/drivers/gpu/drm/Kconfig
index cea777ae7fb9..f9a137145005 100644
--- a/drivers/gpu/drm/Kconfig
+++ b/drivers/gpu/drm/Kconfig
@@ -267,6 +267,8 @@ source "drivers/gpu/drm/nouveau/Kconfig"
 
 source "drivers/gpu/drm/i915/Kconfig"
 
+source "drivers/gpu/drm/hantro/Kconfig"
+
 source "drivers/gpu/drm/kmb/Kconfig"
 
 config DRM_VGEM
diff --git a/drivers/gpu/drm/Makefile b/drivers/gpu/drm/Makefile
index ad1112154898..c7a1387a2f05 100644
--- a/drivers/gpu/drm/Makefile
+++ b/drivers/gpu/drm/Makefile
@@ -74,6 +74,7 @@ obj-$(CONFIG_DRM_AMDGPU)+= amd/amdgpu/
 obj-$(CONFIG_DRM_MGA)	+= mga/
 obj-$(CONFIG_DRM_I810)	+= i810/
 obj-$(CONFIG_DRM_I915)	+= i915/
+obj-$(CONFIG_DRM_HANTRO_UNIFY)  += hantro/
 obj-$(CONFIG_DRM_KMB_DISPLAY)  += kmb/
 obj-$(CONFIG_DRM_MGAG200) += mgag200/
 obj-$(CONFIG_DRM_V3D)  += v3d/
diff --git a/drivers/gpu/drm/drm_gem.c b/drivers/gpu/drm/drm_gem.c
index 09c820045859..bd5816d09a96 100644
--- a/drivers/gpu/drm/drm_gem.c
+++ b/drivers/gpu/drm/drm_gem.c
@@ -341,7 +341,7 @@ int drm_gem_dumb_destroy(struct drm_file *file,
 {
 	return drm_gem_handle_delete(file, handle);
 }
-
+EXPORT_SYMBOL_GPL(drm_gem_dumb_destroy);
 /**
  * drm_gem_handle_create_tail - internal functions to create a handle
  * @file_priv: drm file-private structure to register the handle for
diff --git a/drivers/gpu/drm/hantro/Kconfig b/drivers/gpu/drm/hantro/Kconfig
new file mode 100644
index 000000000000..29d4c542a6f5
--- /dev/null
+++ b/drivers/gpu/drm/hantro/Kconfig
@@ -0,0 +1,26 @@
+# SPDX-License-Identifier: GPL-2.0-only
+config DRM_HANTRO_UNIFY
+	tristate "Hantro DRM"
+	depends on DRM
+	depends on ARM64
+	select DRM_PANEL
+	select DRM_KMS_HELPER
+	help
+	  Choose this option if you have a system that has "Keem
+	  Bay VPU" hardware which supports Verisilicon's Hantro
+	  Video Processor Unit (VPU) IP, a series of video decoder
+	  and encoder semiconductor IP cores which can be flexibly
+	  configured for video surveillance, multimedia consumer
+	  products, Internet of Things, cloud service products, data
+	  centers, aerial photography and recorders, thereby providing
+	  video transcoding and multi-channel HD video encoding and
+	  decoding.
+
+	  Hantro VC8000D allows 4K decoding that supports H264 and HEVC
+	  video formats. Hantro VC8000E allows 4K encoding that supports
+	  H264 and HEVC video formats.
+
+config DRM_HANTRO_TRACEPOINTS
+	bool "Hantro tracing"
+	help
+	  This option enables tracepoints for Keem Bay VPU DRM driver
diff --git a/drivers/gpu/drm/hantro/Makefile b/drivers/gpu/drm/hantro/Makefile
new file mode 100644
index 000000000000..28a223b576fd
--- /dev/null
+++ b/drivers/gpu/drm/hantro/Makefile
@@ -0,0 +1,8 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Hantro DRM media codec driver. This driver provides
+# support for Keem Bay Hantro VPU IP.
+hantro-objs := hantro_drv.o hantro_fs.o hantro_drm.o hantro_dmabuf.o hantro_enc.o hantro_dec.o hantro_fence.o hantro_cache.o hantro_dec400.o hantro_devicemgr.o hantro_cooling.o hantro_metadata.o
+hantro-$(CONFIG_DRM_HANTRO_TRACEPOINTS) += trace.o
+obj-$(CONFIG_DRM_HANTRO_UNIFY) += hantro.o
+CFLAGS_trace.o := -I$(src)
diff --git a/drivers/gpu/drm/hantro/hantro_cache.c b/drivers/gpu/drm/hantro/hantro_cache.c
new file mode 100644
index 000000000000..3a8af943e823
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_cache.c
@@ -0,0 +1,536 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro cache controller hardware driver.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_cache.h"
+
+/* Cache types */
+#define CCLIENT_TYPE_VC8000E	"_VC8000E"
+#define CCLIENT_TYPE_VC8000D0	"_VC8000D_0"
+#define CCLIENT_TYPE_VC8000D1	"_VC8000D_1"
+#define CCLIENT_TYPE_DECG10	"_DECODER_G1_0"
+#define CCLIENT_TYPE_DECG11	"_DECODER_G1_1"
+#define CCLIENT_TYPE_DECG20	"_DECODER_G2_0"
+#define CCLIENT_TYPE_DECG21	"_DECODER_G2_1"
+/* Cache directions */
+#define CC_DIR_READ	"_DIRRD"
+#define CC_DIR_WRITE	"_DIRWR"
+#define CC_DIR_BIDIR	"_DIRBI"
+
+static int reserve_io(struct cache_dev_t *);
+static void release_io(struct cache_dev_t *);
+static void reset_asic(struct cache_dev_t *dev);
+static irqreturn_t cache_isr(int irq, void *dev_id);
+
+static int check_cache_irq(struct cache_dev_t *dev)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+	unsigned long flags;
+	int rdy = 0;
+
+	spin_lock_irqsave(&pdevinfo->cache_owner_lock, flags);
+	if (dev->irq_received) {
+		/* reset the wait condition(s) */
+		dev->irq_received = 0;
+		rdy = 1;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+
+	return rdy;
+}
+
+static unsigned int wait_cache_ready(struct cache_dev_t *dev)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	if (wait_event_interruptible(pdevinfo->cache_wait_queue,
+				     check_cache_irq(dev))) {
+		PDEBUG("Cache wait_event_interruptible interrupted\n");
+		return -ERESTARTSYS;
+	}
+
+	return 0;
+}
+
+static int check_core_occupation(struct cache_dev_t *dev, struct file *file)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+	int ret = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pdevinfo->cache_owner_lock, flags);
+	if (!dev->is_reserved) {
+		dev->is_reserved = 1;
+		dev->cacheowner = file;
+		ret = 1;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+	return ret;
+}
+
+static long reserve_core(struct cache_dev_t *dev, struct file *file)
+{
+	struct device_info *pdevinfo;
+	int ret = 0;
+
+	START_TIME;
+	pdevinfo = dev->pdevinfo;
+	/* lock a core that has specified core id */
+	if (wait_event_interruptible(pdevinfo->cache_hw_queue,
+				     check_core_occupation(dev, file) != 0))
+		ret = -ERESTARTSYS;
+
+	trace_cache_reserve(pdevinfo->deviceid, (sched_clock() - start) / 1000);
+	return ret;
+}
+
+static void release_core(struct cache_dev_t *dev)
+{
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	/* release specified core id */
+	spin_lock_irqsave(&pdevinfo->cache_owner_lock, flags);
+	if (dev->is_reserved) {
+		dev->cacheowner = NULL;
+		dev->is_reserved = 0;
+	}
+
+	dev->irq_received = 0;
+	dev->irq_status = 0;
+	spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+	wake_up_interruptible_all(&pdevinfo->cache_hw_queue);
+	trace_cache_release(pdevinfo->deviceid);
+}
+
+long hantrocache_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int ret = 0;
+	u32 tmp, id, deviceid, node, type;
+	u32 core_id;
+	unsigned long long tmp64;
+	struct cache_dev_t *pccore;
+
+	if (hantro_drm.device_type != DEVICE_KEEMBAY && enable_dec400 == 0)
+		return -EFAULT;
+
+	switch (cmd) {
+	case CACHE_IOCGHWOFFSET:
+		__get_user(id, (int __user *)arg);
+		deviceid = DEVICE_ID(id);
+		node = KCORE(id);
+		pccore = get_cache_nodes(deviceid, node);
+		if (!pccore)
+			return -EFAULT;
+
+		__put_user(pccore->com_base_addr, (unsigned long long __user *)arg);
+		break;
+	case CACHE_IOCH_HW_RESERVE: {
+		enum driver_cache_dir dir;
+		enum cache_client_type client;
+		/* it's a little danger here since here's no protection of the chain */
+		__get_user(tmp64, (unsigned long long __user *)arg);
+		id = tmp64 >> 32;
+		deviceid = DEVICE_ID(id);
+		type = NODETYPE(id);
+		node = KCORE(id);
+		core_id = (u32)tmp64; /* get client and direction info */
+		dir = core_id & 0x01;
+		client = (core_id & 0x06) >> 1;
+		pccore = get_cache_nodes(deviceid, 0);
+
+		if (!pccore)
+			return -EFAULT;
+
+		while (pccore) {
+			/* a valid core supports such client and dir */
+			if (pccore->core_cfg.client == client &&
+			    pccore->core_cfg.dir == dir &&
+			    pccore->parentid == node && pccore->is_valid &&
+			    ((type == NODE_TYPE_DEC &&
+			      pccore->parenttype == CORE_DEC) ||
+			     (type == NODE_TYPE_ENC &&
+			      pccore->parenttype == CORE_ENC)))
+				break;
+
+			pccore = pccore->next;
+		}
+
+		if (!pccore)
+			return -EFAULT;
+
+		ret = reserve_core(pccore, file);
+		if (ret == 0)
+			__put_user(pccore->core_id, (unsigned long long __user *)arg);
+
+		break;
+	}
+	case CACHE_IOCH_HW_RELEASE:
+		core_id = (u32)arg;
+		deviceid = DEVICE_ID(core_id);
+		node = KCORE(core_id);
+		pccore = get_cache_nodes(deviceid, node);
+		if (!pccore)
+			return -EFAULT;
+
+		release_core(pccore);
+		break;
+	case CACHE_IOCG_ABORT_WAIT:
+		__get_user(core_id, (__u32 __user *)arg);
+		deviceid = DEVICE_ID(core_id);
+		node = KCORE(core_id);
+		pccore = get_cache_nodes(deviceid, node);
+		if (!pccore)
+			return -EFAULT;
+
+		tmp = wait_cache_ready(pccore);
+		if (tmp == 0)
+			__put_user(pccore->irq_status, (u32 __user *)arg);
+
+		break;
+	}
+	return 0;
+}
+
+int hantrocache_open(struct inode *inode, struct file *file)
+{
+	if (hantro_drm.device_type != DEVICE_KEEMBAY && enable_dec400 == 0)
+		return 0;
+
+	return 0;
+}
+
+void release_l2cache(struct cache_dev_t *dev, long core)
+{
+	/* If L2Cache is enabled, disable cache/shaper. */
+	u32 asic_id;
+
+	asic_id = ioread32(dev->hwregs);
+	asic_id = (asic_id >> 16) & 0x3;
+
+	if (asic_id != 2) {
+		/* Disable cache if it exists. */
+		PDEBUG(KERN_INFO "hantrodec: DEC[%li] disabled L2Cache\n", core);
+		iowrite32(0, (void *)(dev->hwregs + L2CACHE_E_OFF));
+	}
+
+	if (asic_id != 1) {
+		int i = 0, tmp;
+
+		/* Disable shaper if it exists. */
+		iowrite32(0, (void *)(dev->hwregs + SHAPER_E_OFF));
+		/* Check shaper abort interrupt */
+		for (i = 0; i < 100; i++) {
+			tmp = ioread32((void *)(dev->hwregs + SHAPER_INT_OFF));
+			if (tmp & 0x2) {
+				PDEBUG(KERN_INFO "hantrodec: DEC[%li] disabled shaper DONE\n", core);
+				iowrite32(tmp, (void *)(dev->hwregs + SHAPER_INT_OFF));
+				break;
+			}
+		}
+	}
+}
+
+void hantrocache_release(struct file *file)
+{
+	int i, devicecnt = get_device_count();
+	struct cache_dev_t *dev;
+
+	if (hantro_drm.device_type != DEVICE_KEEMBAY && enable_dec400 == 0)
+		return;
+
+	for (i = 0; i < devicecnt; i++) {
+		dev = get_cache_nodes(i, 0);
+		while (dev) {
+			if (dev->cacheowner == file && dev->is_reserved) {
+				PDEBUG("release cache core %d:%d", devicecnt, i);
+				release_l2cache(dev, dev->core_id);
+				release_core(dev);
+			}
+
+			dev = dev->next;
+		}
+	}
+}
+
+static void cache_get_cache_type(const char *name, int *client, int *dir)
+{
+	if (strstr(name, CCLIENT_TYPE_VC8000E))
+		*client = VC8000E;
+	else if (strstr(name, CCLIENT_TYPE_VC8000D0))
+		*client = VC8000D_0;
+	else if (strstr(name, CCLIENT_TYPE_VC8000D1))
+		*client = VC8000D_1;
+	else if (strstr(name, CCLIENT_TYPE_DECG10))
+		*client = DECODER_G1_0;
+	else if (strstr(name, CCLIENT_TYPE_DECG11))
+		*client = DECODER_G1_1;
+	else if (strstr(name, CCLIENT_TYPE_DECG20))
+		*client = DECODER_G2_0;
+	else if (strstr(name, CCLIENT_TYPE_DECG21))
+		*client = DECODER_G2_1;
+	else
+		*client = -1;
+
+	if (strstr(name, CC_DIR_READ))
+		*dir = DIR_RD;
+	else if (strstr(name, CC_DIR_WRITE))
+		*dir = DIR_WR;
+	else if (strstr(name, CC_DIR_BIDIR))
+		*dir = DIR_RD;
+	else
+		*dir = -1;
+}
+
+int hantrocache_probe(struct dtbnode *pnode)
+{
+	int result;
+	int i;
+	struct cache_dev_t *pccore;
+	int type, dir;
+	int ret;
+
+	if (hantro_drm.device_type != DEVICE_KEEMBAY && enable_dec400 == 0)
+		return 0;
+
+	cache_get_cache_type(pnode->ofnode->name, &type, &dir);
+	if (type == -1 || dir == -1)
+		return -EINVAL;
+
+	pccore = vmalloc(sizeof(*pccore));
+	if (!pccore)
+		return -ENOMEM;
+
+	memset(pccore, 0, sizeof(struct cache_dev_t));
+	pccore->com_base_addr = pnode->ioaddr;
+	pccore->core_cfg.base_addr = pnode->ioaddr;
+	pccore->core_cfg.iosize = pnode->iosize;
+	pccore->core_cfg.client = type;
+	pccore->core_cfg.dir = dir;
+
+	result = reserve_io(pccore);
+	if (result < 0) {
+		pr_err("cachecore: reserve reg 0x%llx-0x%llx fail\n",
+		       pnode->ioaddr, pnode->iosize);
+		ret = -ENODEV;
+		goto free_alloc;
+	}
+
+	reset_asic(pccore); /* reset hardware */
+	pccore->is_valid = 1;
+	for (i = 0; i < 4; i++)
+		pccore->irqlist[i] = -1;
+
+	if (enable_irqmode == 1) {
+		if (pnode->irq[0] > 0) {
+			strcpy(pccore->irq_name[0], pnode->irq_name[0]);
+			result = request_irq(pnode->irq[0], cache_isr,
+					     IRQF_SHARED, pccore->irq_name[0],
+					     (void *)pccore);
+			if (result == 0) {
+				pccore->irqlist[0] = pnode->irq[0];
+			} else {
+				pr_err("cachecore: request IRQ <%d> fail\n",
+				       pnode->irq[0]);
+				ret = -EINVAL;
+				goto free_io;
+			}
+		}
+	}
+
+	pccore->core_cfg.parentaddr = pnode->parentaddr;
+	add_cache_node(pnode->pdevinfo, pccore);
+	return 0;
+
+free_io:
+	release_io(pccore);
+free_alloc:
+	vfree(pccore);
+	return ret;
+}
+
+void hantrocache_remove(struct device_info *pdevinfo)
+{
+	struct cache_dev_t *pccore, *pnext;
+	int k;
+
+	pccore = get_cache_nodes(pdevinfo->deviceid, 0);
+	while (pccore) {
+		pnext = pccore->next;
+		writel(0, pccore->hwregs + 0x04); /* disable HW */
+		writel(0xF, pccore->hwregs + 0x14); /* clear IRQ */
+		/* free the encoder IRQ */
+		for (k = 0; k < 4; k++)
+			if (pccore->irqlist[k] > 0)
+				free_irq(pccore->irqlist[k], (void *)pccore);
+
+		release_io(pccore);
+		vfree(pccore);
+		pccore = pnext;
+	}
+}
+
+static int cache_get_hw_id(unsigned long base_addr, int *hwid)
+{
+	u8 __iomem *hwregs = NULL;
+
+	if (!request_mem_region(base_addr, 4, "hantro_cache")) {
+		PDEBUG(KERN_INFO
+		       "hantr_cache: failed to reserve HW regs,base_addr:%p\n",
+		       (void *)base_addr);
+		return -1;
+	}
+
+	hwregs = ioremap(base_addr, 4);
+	if (!hwregs) {
+		PDEBUG(KERN_INFO "hantr_cache: failed to ioremap HW regs\n");
+		release_mem_region(base_addr, 4);
+		return -1;
+	}
+
+	*hwid = readl(hwregs + 0x00);
+	PDEBUG(KERN_INFO "hantro_cache: hwid = %x, base_addr= %p\n", (int)*hwid,
+	       (void *)base_addr);
+
+	if (hwregs)
+		iounmap(hwregs);
+
+	release_mem_region(base_addr, 4);
+	return 0;
+}
+
+static int reserve_io(struct cache_dev_t *pccore)
+{
+	int hwid, hw_cfg;
+
+	if (cache_get_hw_id(pccore->core_cfg.base_addr, &hwid) < 0)
+		return -1;
+
+	hw_cfg = (hwid & 0xF0000) >> 16;
+
+	if (hw_cfg > 2)
+		return -1;
+
+	if (hw_cfg == 1 && pccore->core_cfg.dir == DIR_WR) /* cache only */
+		pccore->is_valid = 0;
+	else if (hw_cfg == 2 &&
+		 pccore->core_cfg.dir == DIR_RD) /* shaper only */
+		pccore->is_valid = 0;
+	else
+		pccore->is_valid = 1;
+
+	if (pccore->is_valid == 0)
+		return -1;
+
+	if (hwid == 0 && pccore->core_cfg.dir == DIR_RD) {
+		pccore->core_cfg.base_addr += CACHE_WITH_SHAPER_OFFSET;
+	} else if (hwid != 0) {
+		if (pccore->core_cfg.dir == DIR_WR)
+			pccore->core_cfg.base_addr += SHAPER_OFFSET;
+		else if (pccore->core_cfg.dir == DIR_RD && hw_cfg == 0)
+			pccore->core_cfg.base_addr += CACHE_WITH_SHAPER_OFFSET;
+		else if (pccore->core_cfg.dir == DIR_RD && hw_cfg == 1)
+			pccore->core_cfg.base_addr += CACHE_ONLY_OFFSET;
+	}
+
+	if (!request_mem_region(pccore->core_cfg.base_addr,
+				pccore->core_cfg.iosize, pccore->reg_name)) {
+		PDEBUG(KERN_INFO
+		       "hantr_cache: failed to reserve HW regs,core:%x\n",
+		       hwid);
+		pccore->is_valid = 0;
+		return -1;
+	}
+
+	pccore->hwregs = ioremap(pccore->core_cfg.base_addr,
+				 pccore->core_cfg.iosize);
+
+	if (!pccore->hwregs) {
+		PDEBUG(KERN_INFO
+		       "hantr_cache: failed to ioremap HW regs,core:%x\n",
+		       hwid);
+		release_mem_region(pccore->core_cfg.base_addr,
+				   pccore->core_cfg.iosize);
+		pccore->is_valid = 0;
+		return -1;
+	}
+
+	pr_info("hantrocache: HW at base <0x%llx> with ID 0x%x\n",
+		pccore->core_cfg.base_addr, hwid);
+
+	return 0;
+}
+
+static void release_io(struct cache_dev_t *pccore)
+{
+	if (pccore->is_valid == 0)
+		return;
+
+	if (pccore->hwregs)
+		iounmap(pccore->hwregs);
+
+	release_mem_region(pccore->core_cfg.base_addr, pccore->core_cfg.iosize);
+}
+
+static irqreturn_t cache_isr(int irq, void *dev_id)
+{
+	unsigned int handled = 0;
+	struct cache_dev_t *dev = (struct cache_dev_t *)dev_id;
+	u32 irq_status;
+	unsigned long flags;
+	u32 irq_triggered = 0;
+	struct device_info *pdevinfo;
+
+	pdevinfo = dev->pdevinfo;
+	/* If core is not reserved by any user, but irq is received, just ignore it */
+	spin_lock_irqsave(&pdevinfo->cache_owner_lock, flags);
+	if (!dev->is_reserved) {
+		spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+		return IRQ_HANDLED;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+	if (dev->core_cfg.dir == DIR_RD) {
+		irq_status = readl(dev->hwregs + 0x04);
+		if (irq_status & 0x28) {
+			irq_triggered = 1;
+			writel(irq_status, dev->hwregs + 0x04); /* clear irq */
+		}
+	} else {
+		irq_status = readl(dev->hwregs + 0x0C);
+		if (irq_status) {
+			irq_triggered = 1;
+			writel(irq_status, dev->hwregs + 0x0C); /* clear irq */
+		}
+	}
+
+	if (irq_triggered == 1) {
+		/* clear all IRQ bits. IRQ is cleared by writing 1 */
+		spin_lock_irqsave(&pdevinfo->cache_owner_lock, flags);
+		dev->irq_received = 1;
+		dev->irq_status = irq_status;
+		spin_unlock_irqrestore(&pdevinfo->cache_owner_lock, flags);
+		wake_up_interruptible_all(&pdevinfo->cache_wait_queue);
+		handled++;
+	}
+
+	return IRQ_HANDLED;
+}
+
+static void reset_asic(struct cache_dev_t *dev)
+{
+	int i;
+
+	if (dev->is_valid == 0)
+		return;
+
+	for (i = 0; i < dev->core_cfg.iosize; i += 4)
+		writel(0, dev->hwregs + i);
+}
diff --git a/drivers/gpu/drm/hantro/hantro_cache.h b/drivers/gpu/drm/hantro/hantro_cache.h
new file mode 100644
index 000000000000..8e0ae756bd63
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_cache.h
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro cache controller header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_CACHE_H__
+#define __HANTRO_CACHE_H__
+
+#define L2CACHE_E_OFF 0x4
+#define SHAPER_E_OFF 0x0
+#define SHAPER_INT_OFF 0xC
+
+long hantrocache_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+int hantrocache_probe(struct dtbnode *pnode);
+void hantrocache_remove(struct device_info *pdevinfo);
+int hantrocache_open(struct inode *inode, struct file *file);
+void hantrocache_release(struct file *file);
+
+#endif /* __HANTRO_CACHE_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_cooling.c b/drivers/gpu/drm/hantro/hantro_cooling.c
new file mode 100644
index 000000000000..c65077369f1b
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_cooling.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro thermal cooling.
+ *
+ *    Copyright (c) 2021, Intel Corporation
+ */
+
+#include "hantro_cooling.h"
+
+static int hantro_cooling_get_max_state(struct thermal_cooling_device *cdev,
+					unsigned long *state)
+{
+	struct device_info *pdevinfo = cdev->devdata;
+
+	if (!pdevinfo)
+		return -EINVAL;
+
+	*state = pdevinfo->thermal_data.media_clk_max_state;
+	return 0;
+}
+
+static int hantro_cooling_set_cur_state(struct thermal_cooling_device *cdev,
+					unsigned long state)
+{
+	struct device_info *pdevinfo = cdev->devdata;
+
+	if (!pdevinfo || state > pdevinfo->thermal_data.media_clk_max_state)
+		return -EINVAL;
+
+	if (state == pdevinfo->thermal_data.media_clk_state ||
+	    state > 2) //only	3 states supported
+		return 0;
+
+	pdevinfo->thermal_data.media_clk_state = state;
+
+	if (hantro_drm.device_type == DEVICE_KEEMBAY)
+		pdevinfo->thermal_data.clk_freq = kmb_freq_table[state];
+	else if (hantro_drm.device_type == DEVICE_THUNDERBAY)
+		pdevinfo->thermal_data.clk_freq = tbh_freq_table[state];
+
+	pr_info("set_cur_state: %ld for device %d\n",
+		pdevinfo->thermal_data.clk_freq, pdevinfo->deviceid);
+	return 0;
+}
+
+static int hantro_cooling_get_cur_state(struct thermal_cooling_device *cdev,
+					unsigned long *state)
+{
+	struct device_info *pdevinfo = cdev->devdata;
+
+	if (!pdevinfo)
+		return -EINVAL;
+
+	*state = pdevinfo->thermal_data.media_clk_state;
+	return 0;
+}
+
+static const struct thermal_cooling_device_ops hantro_cooling_ops = {
+	.get_max_state = hantro_cooling_get_max_state,
+	.get_cur_state = hantro_cooling_get_cur_state,
+	.set_cur_state = hantro_cooling_set_cur_state,
+};
+
+int setup_thermal_cooling(struct device_info *pdevinfo)
+{
+	int result;
+	char thermal_str[64];
+	struct device_node *np;
+	struct thermal_cooling_device *cdev;
+
+	if (!pdevinfo) {
+		pr_warn("Device info NULL\n");
+		return -EINVAL;
+	}
+
+	np = pdevinfo->dev->of_node;
+	pdevinfo->thermal_data.media_clk_max_state = 3;
+	if (hantro_drm.device_type == DEVICE_KEEMBAY)
+		pdevinfo->thermal_data.clk_freq = kmb_freq_table[0];
+	else if (hantro_drm.device_type == DEVICE_THUNDERBAY)
+		pdevinfo->thermal_data.clk_freq = tbh_freq_table[0];
+
+	sprintf(thermal_str, "media-cooling%d", pdevinfo->deviceid);
+	cdev = devm_thermal_of_cooling_device_register(pdevinfo->dev, np, thermal_str, pdevinfo,
+						       &hantro_cooling_ops);
+	if (IS_ERR(cdev)) {
+		result = PTR_ERR(cdev);
+		dev_err(pdevinfo->dev,
+			"failed to register thermal zone device %d", result);
+	}
+
+	pdevinfo->thermal_data.cooling_dev = cdev;
+	return 0;
+}
diff --git a/drivers/gpu/drm/hantro/hantro_cooling.h b/drivers/gpu/drm/hantro/hantro_cooling.h
new file mode 100644
index 000000000000..1336138d21ff
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_cooling.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro thermal cooling header file.
+ *
+ *    Copyright (c) 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_COOLING_H__
+#define __HANTRO_COOLING_H__
+
+#include "hantro_priv.h"
+
+int setup_thermal_cooling(struct device_info *pdevinfo);
+
+#endif /* __HANTRO_COOLING_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_dec.c b/drivers/gpu/drm/hantro/hantro_dec.c
new file mode 100644
index 000000000000..a59852a80ba8
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dec.c
@@ -0,0 +1,1563 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro decoder hardware driver.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_dec.h"
+#include "hantro_dwl_defs.h"
+
+#define KMB_VC8000D_PAGE_LUT           0x20889000
+
+/* hantro G1 regs config including dec and pp */
+#define HANTRO_PP_ORG_REGS		41
+
+#define HANTRO_PP_EXT_REGS		9
+
+#define HANTRO_PP_TOTAL_REGS		(HANTRO_PP_ORG_REGS + HANTRO_PP_EXT_REGS)
+
+#define HANTRO_PP_ORG_FIRST_REG		60
+#define HANTRO_PP_ORG_LAST_REG		100
+#define HANTRO_PP_EXT_FIRST_REG		146
+#define HANTRO_PP_EXT_LAST_REG		154
+
+/* hantro VC8000D reg config */
+#define HANTRO_VC8000D_LAST_REG		(HANTRO_VC8000D_REGS - 1)
+
+#define HANTRO_VC8KD_REG_BWREAD		300
+#define HANTRO_VC8KD_REG_BWWRITE	304
+#define VC8KD_BURSTWIDTH		16
+
+#define IS_G1(hw_id)		(((hw_id) == 0x6731) ? 1 : 0)
+#define IS_G2(hw_id)		(((hw_id) == 0x6732) ? 1 : 0)
+#define IS_VC8000D(hw_id)	(((hw_id) == 0x8001) ? 1 : 0)
+
+static const int dec_hwid[] = { 0x6731, /* G1 */
+				0x6732, /* G2 */
+				0x8001 };
+#undef PDEBUG
+#define PDEBUG(fmt, arg...)                                                    \
+	do {                                                                   \
+		if (verbose)                                                   \
+			pr_info(fmt, ##arg);                                   \
+	} while (0)
+
+static int reserve_io(struct hantrodec_t *core, struct hantrodec_t **auxcore);
+static void release_io(struct hantrodec_t *dev);
+static void reset_asic(struct hantrodec_t *dev);
+
+/* IRQ handler */
+static irqreturn_t hantrodec_isr(int irq, void *dev_id);
+
+static atomic_t irq_rx = ATOMIC_INIT(0);
+static atomic_t irq_tx = ATOMIC_INIT(0);
+
+#define DWL_CLIENT_TYPE_H264_DEC	1U
+#define DWL_CLIENT_TYPE_MPEG4_DEC	2U
+#define DWL_CLIENT_TYPE_JPEG_DEC	3U
+#define DWL_CLIENT_TYPE_PP		4U
+#define DWL_CLIENT_TYPE_VC1_DEC		5U
+#define DWL_CLIENT_TYPE_MPEG2_DEC	6U
+#define DWL_CLIENT_TYPE_VP6_DEC		7U
+#define DWL_CLIENT_TYPE_AVS_DEC		8U
+#define DWL_CLIENT_TYPE_RV_DEC		9U
+#define DWL_CLIENT_TYPE_VP8_DEC		10U
+#define DWL_CLIENT_TYPE_VP9_DEC		11U
+#define DWL_CLIENT_TYPE_HEVC_DEC	12U
+
+static struct hantrodec_t *get_core_ctrl(u32 id)
+{
+	struct hantrodec_t *pcore;
+	u32 deviceid = DEVICE_ID(id);
+	u32 node = KCORE(id);
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	pcore = get_dec_node_by_device_id(deviceid, node);
+	return pcore;
+}
+
+u32 hantrodec_read_bandwidth(struct device_info *pdevinfo, int is_read_bw)
+{
+	int i, devcnt = get_device_count();
+	u32 bandwidth = 0;
+	struct hantrodec_t *dev;
+
+	if (!pdevinfo) {
+		for (i = 0; i < devcnt; i++) {
+			dev = get_dec_node_by_device_id(i, 0);
+			while (dev) {
+				if (is_read_bw)
+					bandwidth +=
+						ioread32((dev->hwregs +
+								  HANTRO_VC8KD_REG_BWREAD * 4));
+				else
+					bandwidth +=
+						ioread32((dev->hwregs +
+								  HANTRO_VC8KD_REG_BWWRITE * 4));
+
+				dev = dev->next;
+			}
+		}
+	} else {
+		dev = get_dec_node(pdevinfo, 0);
+		while (dev) {
+			if (is_read_bw)
+				bandwidth +=
+					ioread32((dev->hwregs +
+							  HANTRO_VC8KD_REG_BWREAD * 4));
+			else
+				bandwidth +=
+					ioread32((dev->hwregs +
+							  HANTRO_VC8KD_REG_BWWRITE * 4));
+
+			dev = dev->next;
+		}
+	}
+	return bandwidth * VC8KD_BURSTWIDTH;
+}
+
+static void read_core_config(struct hantrodec_t *dev)
+{
+	int c = dev->core_id;
+	u32 reg, tmp, mask;
+	struct hantrodec_t *next;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	dev->cfg = 0;
+
+	/* Decoder configuration */
+	if (IS_G1(dev->hw_id)) {
+		reg = ioread32((dev->hwregs + HANTRODEC_SYNTH_CFG * 4));
+
+		tmp = (reg >> DWL_H264_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has H264\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_H264_DEC : 0;
+
+		tmp = (reg >> DWL_JPEG_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has JPEG\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_JPEG_DEC : 0;
+
+		tmp = (reg >> DWL_MPEG4_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has MPEG4\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_MPEG4_DEC : 0;
+
+		tmp = (reg >> DWL_VC1_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VC1\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VC1_DEC : 0;
+
+		tmp = (reg >> DWL_MPEG2_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has MPEG2\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_MPEG2_DEC : 0;
+
+		tmp = (reg >> DWL_VP6_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VP6\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP6_DEC : 0;
+
+		reg = ioread32((dev->hwregs +
+					HANTRODEC_SYNTH_CFG_2 * 4));
+
+		/* VP7 and WEBP is part of VP8 */
+		mask = (1 << DWL_VP8_E) | (1 << DWL_VP7_E) | (1 << DWL_WEBP_E);
+		tmp = (reg & mask);
+		if (tmp & (1 << DWL_VP8_E))
+			PDEBUG("hantrodec: core[%d] has VP8\n", c);
+
+		if (tmp & (1 << DWL_VP7_E))
+			PDEBUG("hantrodec: core[%d] has VP7\n", c);
+
+		if (tmp & (1 << DWL_WEBP_E))
+			PDEBUG("hantrodec: core[%d] has WebP\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP8_DEC : 0;
+
+		tmp = (reg >> DWL_AVS_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has AVS\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_AVS_DEC : 0;
+
+		tmp = (reg >> DWL_RV_E) & 0x03U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has RV\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_RV_DEC : 0;
+
+		/* Post-processor configuration */
+		reg = ioread32((dev->hwregs + HANTROPP_SYNTH_CFG * 4));
+
+		tmp = (reg >> DWL_G1_PP_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has PP\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_PP : 0;
+	} else if ((IS_G2(dev->hw_id))) {
+		reg = ioread32((dev->hwregs + HANTRODEC_CFG_STAT * 4));
+
+		tmp = (reg >> DWL_G2_HEVC_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has HEVC\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_HEVC_DEC : 0;
+
+		tmp = (reg >> DWL_G2_VP9_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VP9\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP9_DEC : 0;
+
+		/* Post-processor configuration */
+		reg = ioread32((dev->hwregs +
+					HANTRODECPP_SYNTH_CFG * 4));
+
+		tmp = (reg >> DWL_G2_PP_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has PP\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_PP : 0;
+	} else if ((IS_VC8000D(dev->hw_id)) && !dev->its_main_core_id) {
+		reg = ioread32((dev->hwregs + HANTRODEC_SYNTH_CFG * 4));
+
+		tmp = (reg >> DWL_H264_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has H264\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_H264_DEC : 0;
+
+		tmp = (reg >> DWL_H264HIGH10_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has H264HIGH10\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_H264_DEC : 0;
+
+		tmp = (reg >> DWL_JPEG_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has JPEG\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_JPEG_DEC : 0;
+
+		tmp = (reg >> DWL_MPEG4_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has MPEG4\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_MPEG4_DEC : 0;
+
+		tmp = (reg >> DWL_VC1_E) & 0x3U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VC1\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VC1_DEC : 0;
+
+		tmp = (reg >> DWL_MPEG2_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has MPEG2\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_MPEG2_DEC : 0;
+
+		tmp = (reg >> DWL_VP6_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VP6\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP6_DEC : 0;
+
+		reg = ioread32((dev->hwregs +
+					HANTRODEC_SYNTH_CFG_2 * 4));
+
+		/* VP7 and WEBP is part of VP8 */
+		mask = (1 << DWL_VP8_E) | (1 << DWL_VP7_E) | (1 << DWL_WEBP_E);
+		tmp = (reg & mask);
+		if (tmp & (1 << DWL_VP8_E))
+			PDEBUG("hantrodec: core[%d] has VP8\n", c);
+
+		if (tmp & (1 << DWL_VP7_E))
+			PDEBUG("hantrodec: core[%d] has VP7\n", c);
+
+		if (tmp & (1 << DWL_WEBP_E))
+			PDEBUG("hantrodec: core[%d] has WebP\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP8_DEC : 0;
+
+		tmp = (reg >> DWL_AVS_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has AVS\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_AVS_DEC : 0;
+
+		tmp = (reg >> DWL_RV_E) & 0x03U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has RV\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_RV_DEC : 0;
+
+		reg = ioread32((dev->hwregs +
+					HANTRODEC_SYNTH_CFG_3 * 4));
+
+		tmp = (reg >> DWL_HEVC_E) & 0x07U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has HEVC\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_HEVC_DEC : 0;
+
+		tmp = (reg >> DWL_VP9_E) & 0x07U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has VP9\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_VP9_DEC : 0;
+
+		/* Post-processor configuration */
+		reg = ioread32((dev->hwregs +
+					HANTRODECPP_CFG_STAT * 4));
+
+		tmp = (reg >> DWL_PP_E) & 0x01U;
+		if (tmp)
+			PDEBUG("hantrodec: core[%d] has PP\n", c);
+
+		dev->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_PP : 0;
+
+		if (dev->its_aux_core_id) {
+			/* set main_core_id and aux_core_id */
+			next = dev->its_aux_core_id;
+			reg = ioread32((next->hwregs +
+						HANTRODEC_SYNTH_CFG_2 * 4));
+
+			tmp = (reg >> DWL_H264_PIPELINE_E) & 0x01U;
+			if (tmp)
+				PDEBUG("hantrodec: core[%d] has pipeline H264\n",
+				       c);
+
+			next->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_H264_DEC : 0;
+
+			tmp = (reg >> DWL_JPEG_PIPELINE_E) & 0x01U;
+			if (tmp)
+				PDEBUG("hantrodec: core[%d] has pipeline JPEG\n",
+				       c);
+
+			next->cfg |= tmp ? 1 << DWL_CLIENT_TYPE_JPEG_DEC : 0;
+		}
+	}
+	dev->cfg_backup = dev->cfg;
+}
+
+static int core_has_format(const u32 cfg, u32 format)
+{
+	return (cfg & (1 << format)) ? 1 : 0;
+}
+
+static int get_dec_core(long core, struct hantrodec_t *dev, struct file *file,
+			unsigned long format)
+{
+	int success = 0;
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	if (core_has_format(dev->cfg, format) && !dev->dec_owner) {
+		dev->dec_owner = file;
+		((struct device_info *)(dev->pdevinfo))->dec_irq &=
+			~(1 << core);
+		success = 1;
+		/*
+		 * If one main core takes one format which doesn't supported
+		 * by aux core, set aux core's cfg to none video format support,
+		 * else if aux support, set aux core's cfg only support the format
+		 * which main core takes
+		 */
+		if (dev->its_aux_core_id) {
+			if (!core_has_format(dev->its_aux_core_id->cfg, format))
+				dev->its_aux_core_id->cfg = 0;
+			else
+				dev->its_aux_core_id->cfg = (1 << format);
+		}
+
+		/*
+		 * If one aux core takes one format,
+		 * set main core's cfg only support the format which aux core takes
+		 */
+		else if (dev->its_main_core_id)
+			dev->its_main_core_id->cfg = (1 << format);
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	return success;
+}
+
+static int get_dec_core_any(long *core, struct hantrodec_t *dev,
+			    struct file *file, unsigned long format)
+{
+	int success = 0;
+	long c = 0;
+
+	*core = -1;
+
+	while (dev) {
+		/* a free core that has format */
+		if (get_dec_core(c, dev, file, format)) {
+			success = 1;
+			*core = c;
+			PDEBUG("get core %ld:%d,fp=%lx, pid=%d", c,
+			       dev->core_id, (unsigned long)file,
+			       (int)current->pid);
+			break;
+		}
+
+		c++;
+		dev = dev->next;
+	}
+
+	return success;
+}
+
+static int get_dec_core_id(struct hantrodec_t *dev, struct file *file,
+			   unsigned long format)
+{
+	long c = 0;
+	unsigned long flags;
+	int core_id = -1;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	while (dev) {
+		/* a core that has format */
+		spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+		if (core_has_format(dev->cfg_backup, format)) {
+			core_id = c;
+			spin_unlock_irqrestore(&pdevinfo->owner_lock,
+					       flags);
+			break;
+		}
+
+		spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+		dev = dev->next;
+		c++;
+	}
+
+	return core_id;
+}
+
+static long reserve_decoder(struct hantrodec_t *dev, struct file *file,
+			    unsigned long format)
+{
+	long core = -1;
+	struct device_info *pdevinfo = dev->pdevinfo;
+	struct hantrodec_t *reserved_core = NULL;
+
+	START_TIME;
+	/* reserve a core */
+	if (down_interruptible(&pdevinfo->dec_core_sem)) {
+		core = -ERESTARTSYS;
+		goto out;
+	}
+
+	/* lock a core that has specific format*/
+	if (wait_event_interruptible(pdevinfo->hw_queue,
+				     get_dec_core_any(&core, dev, file,
+						      format) != 0)) {
+		core = -ERESTARTSYS;
+		goto out;
+	}
+
+	reserved_core = get_dec_node(pdevinfo, KCORE(core));
+	if (!reserved_core) {
+		pr_err("Core not found");
+		goto out;
+	}
+
+	if (reserved_core->dev_clk &&
+	    pdevinfo->thermal_data.clk_freq != reserved_core->clk_freq) {
+		PDEBUG("Reserve decoder:  setting to %ld for device %d, core %ld\n",
+		       pdevinfo->thermal_data.clk_freq,
+		       pdevinfo->deviceid, core);
+		clk_set_rate(reserved_core->dev_clk,
+			     pdevinfo->thermal_data.clk_freq);
+		reserved_core->clk_freq = pdevinfo->thermal_data.clk_freq;
+	}
+
+	reserved_core->perf_data.last_resv = sched_clock();
+out:
+	trace_dec_reserve(pdevinfo->deviceid, core, (sched_clock() - start) / 1000);
+	return core;
+}
+
+static void release_decoder(struct hantrodec_t *dev, long core)
+{
+	u32 status;
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+	struct hantrodec_t *reserved_core = NULL;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	reserved_core = get_dec_node(pdevinfo, KCORE(core));
+	reserved_core->perf_data.count++;
+	reserved_core->perf_data.totaltime +=
+		(sched_clock() - (reserved_core->perf_data.last_resv == 0 ?
+					  sched_clock() :
+					  reserved_core->perf_data.last_resv));
+	status = ioread32((dev->hwregs + HANTRODEC_IRQ_STAT_DEC_OFF));
+	/* make sure HW is disabled */
+	if (status & HANTRODEC_DEC_E) {
+		pr_info("hantrodec: DEC[%lx] still enabled -> reset, status = 0x%x [offset=%x]\n",
+			core, status, HANTRODEC_IRQ_STAT_DEC_OFF);
+		/* abort decoder */
+		status |= HANTRODEC_DEC_ABORT | HANTRODEC_DEC_IRQ_DISABLE;
+		iowrite32(status,
+			  (dev->hwregs + HANTRODEC_IRQ_STAT_DEC_OFF));
+	}
+
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	/* If aux core released, revert main core's config back */
+	if (dev->its_main_core_id)
+		dev->its_main_core_id->cfg = dev->its_main_core_id->cfg_backup;
+
+	/* If main core released, revert aux core's config back */
+	if (dev->its_aux_core_id)
+		dev->its_aux_core_id->cfg = dev->its_aux_core_id->cfg_backup;
+
+	dev->dec_owner = NULL;
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	up(&pdevinfo->dec_core_sem);
+	wake_up_interruptible_all(&pdevinfo->hw_queue);
+	trace_dec_release(pdevinfo->deviceid, KCORE(core));
+}
+
+static long reserve_post_processor(struct hantrodec_t *dev, struct file *file)
+{
+	unsigned long flags;
+	long core = 0;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	/* single core PP only */
+	if (down_interruptible(&pdevinfo->pp_core_sem))
+		return -ERESTARTSYS;
+
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	dev->pp_owner = file;
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	return core;
+}
+
+static void release_post_processor(struct hantrodec_t *dev, long core)
+{
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	u32 status = ioread32((dev->hwregs + HANTRO_IRQ_STAT_PP_OFF));
+
+	/* make sure HW is disabled */
+	if (status & HANTRO_PP_E) {
+		pr_info("hantrodec: PP[%li] still enabled -> reset\n", core);
+		iowrite32(0x10, (dev->hwregs + HANTRO_IRQ_STAT_PP_OFF));
+	}
+
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	dev->pp_owner = NULL;
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	up(&pdevinfo->pp_core_sem);
+}
+
+static long dec_flush_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long ret = 0;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	ret = copy_from_user(dev->dec_regs, core->regs,
+			     HANTRO_VC8000D_REGS * 4);
+	if (ret) {
+		pr_info("copy_from_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	/* write all regs but the status reg[1] to hardware */
+	iowrite32(0x0, (dev->hwregs + 4));
+	memcpy(((dev->hwregs + 0x8)), &dev->dec_regs[2],
+	       (HANTRO_VC8000D_LAST_REG - 2) * 4);
+
+	/* write the status register, which may start the decoder */
+	iowrite32(dev->dec_regs[1], (dev->hwregs + 4));
+	return 0;
+}
+
+static long dec_refresh_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long ret;
+
+	memcpy(dev->dec_regs, dev->hwregs, HANTRO_VC8000D_LAST_REG * 4);
+	ret = copy_to_user(core->regs, dev->dec_regs,
+			   HANTRO_VC8000D_LAST_REG * 4);
+	dev->perf_data.hwcycles += ioread32(dev->hwregs + (63 * 4));
+	if (ret) {
+		pr_info("copy_to_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static long dec_write_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long ret = 0, i;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	i = core->reg_id;
+	ret = copy_from_user(dev->dec_regs + core->reg_id,
+			     core->regs + core->reg_id, 4);
+	if (ret) {
+		pr_info("copy_from_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	iowrite32(dev->dec_regs[i], dev->hwregs + i * 4);
+	return 0;
+}
+
+static long dec_read_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long ret, i;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+
+	/* read specific registers from hardware */
+	i = core->reg_id;
+	dev->dec_regs[i] = ioread32(dev->hwregs + i * 4);
+
+	/* put registers to user space */
+	ret = copy_to_user(core->regs + core->reg_id,
+			   dev->dec_regs + core->reg_id, 4);
+	if (ret) {
+		pr_info("copy_to_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static long pp_flush_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long ret = 0;
+	u32 i;
+
+	/* copy original dec regs to kernel space */
+	ret = copy_from_user(dev->dec_regs + HANTRO_PP_ORG_FIRST_REG,
+			     core->regs + HANTRO_PP_ORG_FIRST_REG,
+			     HANTRO_PP_ORG_REGS * 4);
+	if (ret) {
+		pr_err("copy_from_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	/* write all regs but the status reg[1] to hardware */
+	/* both original and extended regs need to be written */
+	for (i = HANTRO_PP_ORG_FIRST_REG + 1; i <= HANTRO_PP_ORG_LAST_REG; i++)
+		iowrite32(dev->dec_regs[i], dev->hwregs + i * 4);
+
+	/* write the stat reg, which may start the PP */
+	iowrite32(dev->dec_regs[HANTRO_PP_ORG_FIRST_REG],
+		  dev->hwregs + HANTRO_PP_ORG_FIRST_REG * 4);
+	return 0;
+}
+
+static long pp_refresh_regs(struct hantrodec_t *dev, struct core_desc *core)
+{
+	long i, ret;
+	/* user has to know exactly what they are asking for */
+	if (core->size != (HANTRO_PP_ORG_REGS * 4))
+		return -EFAULT;
+
+	/* read all registers from hardware */
+	/* both original and extended regs need to be read */
+	for (i = HANTRO_PP_ORG_FIRST_REG; i <= HANTRO_PP_ORG_LAST_REG; i++)
+		dev->dec_regs[i] = ioread32(dev->hwregs + i * 4);
+
+	/* put registers to user space*/
+	/* put original registers to user space*/
+	ret = copy_to_user(core->regs + HANTRO_PP_ORG_FIRST_REG,
+			   dev->dec_regs + HANTRO_PP_ORG_FIRST_REG,
+			   HANTRO_PP_ORG_REGS * 4);
+	if (ret) {
+		pr_err("copy_to_user failed, returned %li\n", ret);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+static int check_pp_irq(struct hantrodec_t *dev, int id)
+{
+	unsigned long flags;
+	int rdy = 0;
+	struct device_info *pdevinfo = dev->pdevinfo;
+	const u32 irq_mask = (1 << id);
+
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	if (pdevinfo->pp_irq & irq_mask) {
+		/* reset the wait condition(s) */
+		pdevinfo->pp_irq &= ~irq_mask;
+		rdy = 1;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	return rdy;
+}
+
+static long wait_pp_ready_and_refresh_regs(struct hantrodec_t *dev,
+					   struct core_desc *core)
+{
+	u32 id = core->id;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	PDEBUG("wait_event_interruptible PP[%d]\n", id);
+	if (wait_event_interruptible(pdevinfo->pp_wait_queue,
+				     check_pp_irq(dev, id))) {
+		pr_err("PP[%d]  failed to wait_event_interruptible interrupted\n",
+		       id);
+		return -ERESTARTSYS;
+	}
+
+	atomic_inc(&irq_tx);
+	/* refresh registers */
+	return pp_refresh_regs(dev, core);
+}
+
+static int check_core_irq(struct hantrodec_t *dev, const struct file *file,
+			  u32 id)
+{
+	unsigned long flags;
+	int rdy = 0, n = 0;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	while (dev) {
+		u32 irq_mask = (1 << n);
+
+		spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+		if (pdevinfo->dec_irq & irq_mask) {
+			if (id == n) {
+				/* we have an IRQ for our client */
+				/* reset the wait condition(s) */
+				pdevinfo->dec_irq &= ~irq_mask;
+				/* signal ready Core no. for our client */
+				rdy = 1;
+				spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+				break;
+			} else if (!dev->dec_owner) {
+				/* zombie IRQ */
+				pr_info("IRQ on Core[%d], but no owner!!!\n",
+					n);
+				/* reset the wait condition(s) */
+				pdevinfo->dec_irq &= ~irq_mask;
+			}
+		}
+
+		spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+		n++; /* next Core */
+		dev = dev->next;
+	}
+
+	return rdy;
+}
+
+static long wait_core_ready(struct hantrodec_t *dev, const struct file *file,
+			    u32 id)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	PDEBUG("wait_event_interruptible CORE\n");
+
+	if (wait_event_interruptible(pdevinfo->dec_wait_queue,
+				     check_core_irq(dev, file, id))) {
+		pr_err("CORE  failed to wait_event_interruptible interrupted\n");
+		return -ERESTARTSYS;
+	}
+
+	atomic_inc(&irq_tx);
+	return 0;
+}
+
+long hantrodec_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int i, ret;
+	u32 hw_id, id;
+	u32 deviceid, node;
+	long tmp = 0;
+	unsigned long long tmp64;
+	struct core_desc core;
+	struct hantrodec_t *pcore;
+
+	if (enable_decode == 0)
+		return -EFAULT;
+
+	switch (_IOC_NR(cmd)) {
+	case _IOC_NR(HANTRODEC_IOC_CLI): {
+		id = arg;
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return -EFAULT;
+
+		for (i = 0; i < 4; i++)
+			if (pcore->irqlist[i] > 0)
+				disable_irq(pcore->irqlist[i]);
+
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOC_STI): {
+		id = arg;
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return -EFAULT;
+
+		for (i = 0; i < 4; i++)
+			if (pcore->irqlist[i] > 0)
+				enable_irq(pcore->irqlist[i]);
+
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCGHWOFFSET): {
+		__get_user(id, (unsigned long __user *)arg);
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return -EFAULT;
+
+		__put_user(pcore->multicorebase_actual,
+			   (unsigned long long __user *)arg);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCGHWIOSIZE): {
+		__u32 io_size;
+
+		__get_user(id, (__u32 __user *)arg);
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return -EFAULT;
+
+		io_size = pcore->iosize;
+		__put_user(io_size, (u32 __user *)arg);
+
+		return 0;
+	}
+	case _IOC_NR(HANTRODEC_IOC_MC_OFFSETS): {
+		__get_user(deviceid, (__u32 __user *)arg);
+		pcore = get_dec_node_by_device_id(deviceid, 0);
+		if (!pcore)
+			return -EFAULT;
+
+		i = 0;
+		while (pcore) {
+			tmp = copy_to_user(((unsigned long long __user *)arg) + i,
+					   &pcore->multicorebase_actual,
+					   sizeof(pcore->multicorebase_actual));
+			if (tmp) {
+				pr_err("copy_to_user failed, returned %li\n",
+				       tmp);
+				return -EFAULT;
+			}
+
+			pcore = pcore->next;
+			i++;
+		}
+
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOC_MC_CORES):
+		__get_user(id, (__u32 __user *)arg);
+		id = get_device_core_num(id, CORE_DEC);
+		PDEBUG("cores=%d\n", id);
+		__put_user(id, (u32 __user *)arg);
+		break;
+	case _IOC_NR(HANTRODEC_IOCS_DEC_PUSH_REG): {
+		/* get registers from user space */
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_info("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		dec_flush_regs(pcore, &core);
+		break;
+	}
+
+	case _IOC_NR(HANTRODEC_IOCS_DEC_WRITE_REG): {
+		/* get registers from user space */
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_info("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		dec_write_regs(pcore, &core);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCS_PP_PUSH_REG): {
+		/* get registers from user space */
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_err("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		pp_flush_regs(pcore, &core);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCS_DEC_PULL_REG): {
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_err("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		return dec_refresh_regs(pcore, &core);
+	}
+	case _IOC_NR(HANTRODEC_IOCS_DEC_READ_REG): {
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_info("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		return dec_read_regs(pcore, &core);
+	}
+	case _IOC_NR(HANTRODEC_IOCS_PP_PULL_REG): {
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_err("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		return pp_refresh_regs(pcore, &core);
+	}
+	case _IOC_NR(HANTRODEC_IOCH_DEC_RESERVE): {
+		__get_user(tmp64, (unsigned long long __user *)arg);
+		deviceid = tmp64 >> 32;
+		PDEBUG("Reserve DEC core, format = %i\n", (u32)tmp64);
+		pcore = get_core_ctrl(deviceid << 16);
+		if (!pcore)
+			return -EFAULT;
+
+		ret = reserve_decoder(pcore, file, tmp64 & 0xffffffff);
+		if (ret < 0)
+			return -EFAULT;
+
+		__put_user((ret | (deviceid << 16)), (unsigned long long __user *)arg);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCT_DEC_RELEASE): {
+		pcore = get_core_ctrl((u32)arg);
+		if (!pcore)
+			return -EFAULT;
+
+		if (pcore->dec_owner != file) {
+			pr_err("bogus DEC release, Core = %li\n", arg);
+			return -EFAULT;
+		}
+
+		PDEBUG("Release DEC, core = %li\n", arg);
+		release_decoder(pcore, arg);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCQ_PP_RESERVE):
+		id = (u32)arg;
+		pcore = get_dec_node_by_device_id(DEVICE_ID(id), 0);
+		if (!pcore)
+			return -EFAULT;
+
+		return reserve_post_processor(pcore, file);
+	case _IOC_NR(HANTRODEC_IOCT_PP_RELEASE): {
+		pcore = get_core_ctrl(arg);
+		if (!pcore)
+			return -EFAULT;
+
+		if (arg != 0 || pcore->pp_owner != file) {
+			pr_err("bogus PP release %li\n", arg);
+			return -EFAULT;
+		}
+
+		release_post_processor(pcore, arg);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOCX_PP_WAIT): {
+		tmp = copy_from_user(&core, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (tmp) {
+			pr_err("copy_from_user failed, returned %li\n", tmp);
+			return -EFAULT;
+		}
+
+		pcore = get_core_ctrl(core.id);
+		if (!pcore)
+			return -EFAULT;
+
+		return wait_pp_ready_and_refresh_regs(pcore, &core);
+	}
+	case _IOC_NR(HANTRODEC_IOCG_CORE_WAIT): {
+		id = (u32)arg;
+		deviceid = DEVICE_ID(id);
+		node = KCORE(id);
+		pcore = get_dec_node_by_device_id(deviceid, 0);
+		if (!pcore)
+			return -EFAULT;
+
+		tmp = wait_core_ready(pcore, file, node);
+		return tmp;
+	}
+	case _IOC_NR(HANTRODEC_IOX_ASIC_ID):
+		__get_user(id, (__u32 __user *)arg);
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return 0;
+
+		id = ioread32(pcore->hwregs);
+		__put_user(id, (u32 __user *)arg);
+		break;
+	case _IOC_NR(HANTRODEC_IOCG_CORE_ID): {
+		PDEBUG("Get DEC Core_id, format = %li\n", arg);
+		__get_user(tmp64, (unsigned long long __user *)arg);
+		deviceid = tmp64 >> 32;
+		pcore = get_dec_node_by_device_id(deviceid, 0);
+		if (!pcore)
+			return -EFAULT;
+
+		tmp = get_dec_core_id(pcore, file, tmp64 & 0xffffffff);
+		__put_user(tmp, (unsigned long long __user *)arg);
+		break;
+	}
+	case _IOC_NR(HANTRODEC_IOX_ASIC_BUILD_ID): {
+		__get_user(id, (int __user *)arg);
+		pcore = get_core_ctrl(id);
+		if (!pcore)
+			return -EFAULT;
+
+		hw_id = ioread32((pcore->hwregs));
+		if (IS_G1(hw_id >> 16) || IS_G2(hw_id >> 16)) {
+			__put_user(hw_id, (u32 __user *)arg);
+		} else {
+			hw_id = ioread32((pcore->hwregs +
+						  HANTRODEC_HW_BUILD_ID_OFF));
+			__put_user(hw_id, (u32 __user *)arg);
+		}
+
+		return 0;
+	}
+	case _IOC_NR(HANTRODEC_DEBUG_STATUS): {
+		struct device_info *pdevinfo;
+
+		PDEBUG("hantrodec: IRQs received/sent2user = %d / %d\n",
+		       atomic_read(&irq_rx), atomic_read(&irq_tx));
+		deviceid = get_device_count();
+		for (i = 0; i < (int)deviceid; i++) {
+			pcore = get_dec_node_by_device_id(i, 0);
+			if (!pcore)
+				continue;
+
+			pdevinfo = pcore->pdevinfo;
+			PDEBUG("hantrodec: device %d dec_irq     = 0x%08x\n", i,
+			       pdevinfo->dec_irq);
+			PDEBUG("hantrodec: device %d pp_irq      = 0x%08x\n", i,
+			       pdevinfo->pp_irq);
+			id = 0;
+			while (pcore) {
+				PDEBUG("hantrodec: device %d dec_core[%i] %s\n",
+				       i, id,
+				       !pcore->dec_owner ? "FREE" : "RESERVED");
+				PDEBUG("hantrodec: device %d pp_core[%i]  %s\n",
+				       i, id,
+				       !pcore->pp_owner ? "FREE" : "RESERVED");
+				pcore = pcore->next;
+				id++;
+			}
+		}
+
+		break;
+	}
+	default:
+		return -ENOTTY;
+	}
+
+	return 0;
+}
+
+int hantrodec_release(struct file *file)
+{
+	int n, i, devicecnt = get_device_count();
+	struct hantrodec_t *pcore;
+
+	if (enable_decode == 0)
+		return 0;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	for (i = 0; i < devicecnt; i++) {
+		pcore = get_dec_node_by_device_id(i, 0);
+		n = 0;
+		while (pcore) {
+			if (pcore->dec_owner == file) {
+				PDEBUG("releasing device %d dec Core %i lock\n",
+				       i, n);
+				release_decoder(pcore, n);
+			}
+
+			n++;
+			pcore = pcore->next;
+		}
+
+		pcore = get_dec_node_by_device_id(i, 0);
+		if (pcore && pcore->pp_owner == file) {
+			PDEBUG("releasing device %d pp Core %i lock\n", i, 0);
+			release_post_processor(pcore, n);
+		}
+	}
+
+	return 0;
+}
+
+int hantrodec_open(struct inode *inode, struct file *file)
+{
+	if (enable_decode == 0)
+		return 0;
+
+	return 0;
+}
+
+static void setup_dec_lut(void)
+{
+	u8 __iomem *dec_page_lut_regs;
+
+	if (hantro_drm.dec_page_lut_regs) {
+		pr_info("hantrodec: page_lut already reserved\n");
+		return;
+	}
+
+	/* Register and set the page lookup table for read */
+	if (!request_mem_region(KMB_VC8000D_PAGE_LUT, 0x100,
+				"hantrodec_pagelut_read")) {
+		pr_err("hantrodec: failed to reserve page lookup table registers\n");
+		return;
+	}
+
+	dec_page_lut_regs = ioremap(KMB_VC8000D_PAGE_LUT, 0x100);
+	if (!dec_page_lut_regs) {
+		pr_err("hantrodec: failed to ioremap page lookup table registers\n");
+		return;
+	}
+
+	/* Set VDEC RD Page LUT AXI ID 0-15 to 0x4 */
+	iowrite32(0x04040404, dec_page_lut_regs);
+	pr_info("hantrodec: RD AXI ID 3:0 = %x\n",
+		ioread32(dec_page_lut_regs));
+	iowrite32(0x04040404, dec_page_lut_regs + 0x4);
+	pr_info("hantrodec: RD AXI ID 7:4 = %x\n",
+		ioread32(dec_page_lut_regs + 0x4));
+	iowrite32(0x04040404, dec_page_lut_regs + 0x8);
+	pr_info("hantrodec: RD AXI ID 11:8 = %x\n",
+		ioread32(dec_page_lut_regs + 0x8));
+	iowrite32(0x04040404, dec_page_lut_regs + 0xc);
+	pr_info("hantrodec: RD AXI ID 15:12 = %x\n",
+		ioread32(dec_page_lut_regs + 0xc));
+
+#ifdef STATIC_AXI_WR
+	iowrite32(0x04, dec_page_lut_regs + 0x10);
+	pr_info("hantrodec: WR AXI ID 0 = %x\n",
+		ioread32(dec_page_lut_regs + 0x10));
+#else /* dynamic WR AXI ID */
+	/* Set WR Page LUT AXI ID 0-3, 6-15 to 0x4 and WR Page LUT AXI ID 4,5 to 0x0 */
+	iowrite32(0x04040400, dec_page_lut_regs + 0x10);
+	pr_info("hantrodec: page_lut_regs WR AXI ID 3:0= %x\n",
+		ioread32(dec_page_lut_regs + 0x10));
+	iowrite32(0x04040000, dec_page_lut_regs + 0x14);
+	pr_info("hantrodec: page_lut_regs WR AXI ID 7:4= %x\n",
+		ioread32(dec_page_lut_regs + 0x14));
+	iowrite32(0x04040404, dec_page_lut_regs + 0x18);
+	pr_info("hantrodec: page_lut_regs WR AXI ID 11:8= %x\n",
+		ioread32(dec_page_lut_regs + 0x18));
+	iowrite32(0x04040404, dec_page_lut_regs + 0x1c);
+	pr_info("hantrodec: page_lut_regs WR AXI ID 15:12= %x\n",
+		ioread32(dec_page_lut_regs + 0x1c));
+#endif
+	pr_info("hantrodec: page_lut reserved\n");
+
+	hantro_drm.dec_page_lut_regs = dec_page_lut_regs;
+}
+
+int hantrodec_init(void)
+{
+	if (hantro_drm.device_type == DEVICE_KEEMBAY && enable_lut)
+		setup_dec_lut();
+
+	return 0;
+}
+
+int hantrodec_cleanup(void)
+{
+	if (hantro_drm.dec_page_lut_regs) {
+		iounmap(hantro_drm.dec_page_lut_regs);
+		hantro_drm.dec_page_lut_regs = NULL;
+		release_mem_region(KMB_VC8000D_PAGE_LUT, 0x100);
+	}
+
+	return 0;
+}
+
+static void disable_dec_clock(struct hantrodec_t *pcore)
+{
+	if (!pcore->dev_clk)
+		return;
+
+	clk_disable_unprepare(pcore->dev_clk);
+	clk_put(pcore->dev_clk);
+	pcore->dev_clk = NULL;
+}
+
+static int init_dec_clock(struct hantrodec_t *pcore, struct dtbnode *pnode)
+{
+	if (strlen(pnode->clock_name) == 0)
+		return 0;
+
+	pcore->dev_clk = clk_get(pnode->pdevinfo->dev, pnode->clock_name);
+	if (IS_ERR(pcore->dev_clk) || !pcore->dev_clk) {
+		pr_err("%s: clock %s not found. err = %ld", __func__,
+		       pnode->clock_name, PTR_ERR(pcore->dev_clk));
+		pcore->dev_clk = NULL;
+		return -EINVAL;
+	}
+
+	clk_prepare_enable(pcore->dev_clk);
+	pcore->clk_freq = pnode->pdevinfo->thermal_data.clk_freq;
+	clk_set_rate(pcore->dev_clk, pcore->clk_freq);
+	return 0;
+}
+
+int hantrodec_probe(struct dtbnode *pnode)
+{
+	int i, result = 0;
+	struct hantrodec_t *pcore, *auxcore;
+	int irqn;
+
+	if (enable_decode == 0)
+		return 0;
+
+	pcore = vmalloc(sizeof(*pcore));
+	if (!pcore)
+		return -ENOMEM;
+
+	memset(pcore, 0, sizeof(struct hantrodec_t));
+	pcore->multicorebase = pnode->ioaddr;
+	pcore->multicorebase_actual = pnode->ioaddr;
+	pcore->iosize = pnode->iosize;
+	auxcore = NULL;
+	result = reserve_io(pcore, &auxcore);
+	if (result < 0) {
+		vfree(pcore);
+		return -ENODEV;
+	}
+
+	read_core_config(pcore);
+	reset_asic(pcore);
+	pcore->dec_owner = NULL;
+	pcore->pp_owner = NULL;
+
+	if (auxcore) {
+		read_core_config(auxcore);
+		reset_asic(auxcore);
+		auxcore->dec_owner = NULL;
+		auxcore->pp_owner = NULL;
+	}
+
+	irqn = 0;
+	for (i = 0; i < 4; i++)
+		pcore->irqlist[i] = -1;
+	if (enable_irqmode == 1) {
+		for (i = 0; i < 4; i++) {
+			if (pnode->irq[i] > 0) {
+				strcpy(pcore->irq_name[i], pnode->irq_name[i]);
+				result = request_irq(pnode->irq[i],
+						     hantrodec_isr, IRQF_SHARED,
+						     pcore->irq_name[i],
+						     (void *)pcore);
+				if (result != 0) {
+					pr_err("dec can't reserve irq %d\n",
+					       pnode->irq[i]);
+					release_io(pcore);
+					vfree(pcore);
+					if (auxcore) {
+						release_io(auxcore);
+						vfree(auxcore);
+					}
+
+					return -ENODEV;
+				}
+
+				pcore->irqlist[irqn] = pnode->irq[i];
+				irqn++;
+			}
+		}
+	}
+
+	init_dec_clock(pcore, pnode);
+	add_dec_node(pnode->pdevinfo, pcore);
+	if (auxcore)
+		add_dec_node(pnode->pdevinfo, auxcore);
+
+	return 0;
+}
+
+void hantrodec_remove(struct device_info *pdevinfo)
+{
+	struct hantrodec_t *pcore, *next;
+	int i;
+
+	/* free the IRQ */
+	pcore = get_dec_node(pdevinfo, 0);
+	while (pcore) {
+		/* reset hardware */
+		disable_dec_clock(pcore);
+		reset_asic(pcore);
+		for (i = 0; i < 4; i++)
+			if (pcore->irqlist[i] > 0)
+				free_irq(pcore->irqlist[i], (void *)pcore);
+
+		release_io(pcore);
+		next = pcore->next;
+		vfree(pcore);
+		pcore = next;
+	}
+}
+
+static int check_hw_id(struct hantrodec_t *dev)
+{
+	long hwid;
+	size_t num_hw = sizeof(dec_hwid) / sizeof(*dec_hwid);
+	int found = 0;
+
+	hwid = readl(dev->hwregs);
+	hwid = (hwid >> 16) & 0xFFFF;
+
+	while (num_hw--) {
+		if (hwid == dec_hwid[num_hw]) {
+			pr_info("hantrodec: HW at base <0x%llx> with ID 0x%lx\n",
+				dev->multicorebase_actual, hwid);
+			found = 1;
+			dev->hw_id = hwid;
+			break;
+		}
+	}
+
+	if (!found) {
+		pr_info("hantrodec: HW at base <0x%llx> with ID 0x%lx\n",
+			dev->multicorebase_actual, hwid);
+		pr_info("hantrodec: Unknown HW found at 0x%llx\n",
+			dev->multicorebase_actual);
+		return 0;
+	}
+	return 1;
+}
+
+static int reserve_io(struct hantrodec_t *core, struct hantrodec_t **auxcore)
+{
+	int result;
+	long hwid;
+	u32 reg;
+	struct hantrodec_t *auxcore_tmp = NULL;
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	if (!request_mem_region(core->multicorebase_actual, core->iosize,
+				core->reg_name)) {
+		pr_info("hantrodec: failed to reserve HW regs %llx, %x\n",
+			core->multicorebase_actual, core->iosize);
+		return -EBUSY;
+	}
+
+	core->hwregs = ioremap(core->multicorebase_actual, core->iosize);
+	if (!core->hwregs) {
+		pr_info("hantrodec: failed to ioremap HW regs\n");
+		release_mem_region(core->multicorebase_actual, core->iosize);
+		return -EBUSY;
+	}
+
+	core->its_main_core_id = NULL;
+	core->its_aux_core_id = NULL;
+	/* check for correct HW */
+	result = check_hw_id(core);
+	if (!result) {
+		result = -ENXIO;
+		goto error;
+	}
+
+	hwid = ((readl(core->hwregs)) >> 16) & 0xFFFF;
+	if (IS_VC8000D(hwid)) {
+		reg = readl(core->hwregs + HANTRODEC_SYNTH_CFG_2_OFF);
+		if (((reg >> DWL_H264_PIPELINE_E) & 0x01U) ||
+		    ((reg >> DWL_JPEG_PIPELINE_E) & 0x01U)) {
+			auxcore_tmp = vmalloc(sizeof(*auxcore_tmp));
+			if (!auxcore_tmp) {
+				result = -ENOMEM;
+				goto error;
+			}
+
+			memset(auxcore_tmp, 0, sizeof(struct hantrodec_t));
+			auxcore_tmp->multicorebase_actual =
+				core->multicorebase_actual + 0x800;
+			auxcore_tmp->multicorebase =
+				auxcore_tmp->multicorebase_actual;
+			auxcore_tmp->iosize = core->iosize;
+
+			if (!request_mem_region(auxcore_tmp->multicorebase_actual,
+						auxcore_tmp->iosize, "hantrodec0")) {
+				pr_info("hantrodec: failed to reserve HW regs\n");
+				result = -EBUSY;
+				goto free_auxcore;
+			}
+
+			auxcore_tmp->hwregs =
+				ioremap(auxcore_tmp->multicorebase_actual,
+					auxcore_tmp->iosize);
+			if (!auxcore_tmp->hwregs) {
+				pr_info("hantrodec: failed to ioremap HW regs\n");
+				release_mem_region(auxcore_tmp->multicorebase_actual,
+						   auxcore_tmp->iosize);
+				result = -EBUSY;
+				goto free_auxcore;
+			}
+
+			core->its_aux_core_id = auxcore_tmp;
+			auxcore_tmp->its_main_core_id = core;
+			auxcore_tmp->its_aux_core_id = NULL;
+		}
+	}
+
+	if (auxcore_tmp) {
+		result = check_hw_id(auxcore_tmp);
+		if (!result) {
+			result = -ENXIO;
+			goto release_auxcore;
+		}
+	}
+
+	*auxcore = auxcore_tmp;
+	return 0;
+
+release_auxcore:
+	release_io(auxcore_tmp);
+free_auxcore:
+	vfree(auxcore_tmp);
+	auxcore_tmp = NULL;
+error:
+	release_io(core);
+	return result;
+}
+
+static void release_io(struct hantrodec_t *dev)
+{
+	PDEBUG("hantrodec: %s\n", __func__);
+	if (dev->hwregs)
+		iounmap(dev->hwregs);
+
+	release_mem_region(dev->multicorebase_actual, dev->iosize);
+}
+
+static irqreturn_t hantrodec_isr(int irq, void *dev_id)
+{
+	unsigned long flags;
+	unsigned int handled = 0;
+	int i = 0;
+	struct hantrodec_t *dev = (struct hantrodec_t *)dev_id;
+	u32 irq_status_dec;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	dev = get_first_dec_nodes(pdevinfo);
+	spin_lock_irqsave(&pdevinfo->owner_lock, flags);
+	while (dev) {
+		u8 __iomem *hwregs = dev->hwregs;
+
+		/* interrupt status register read */
+		irq_status_dec =
+			ioread32(hwregs + HANTRODEC_IRQ_STAT_DEC_OFF);
+		if (irq_status_dec & HANTRODEC_DEC_IRQ) {
+			/* clear dec IRQ */
+			irq_status_dec &= (~HANTRODEC_DEC_IRQ);
+			iowrite32(irq_status_dec,
+				  hwregs + HANTRODEC_IRQ_STAT_DEC_OFF);
+
+			PDEBUG("decoder IRQ received! Core %d\n", i);
+
+			atomic_inc(&irq_rx);
+
+			pdevinfo->dec_irq |= (1 << i);
+
+			wake_up_interruptible_all(&pdevinfo->dec_wait_queue);
+			handled++;
+		}
+
+		i++;
+		dev = dev->next;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->owner_lock, flags);
+	if (!handled)
+		PDEBUG("IRQ received, but not hantrodec's!\n");
+
+	return IRQ_RETVAL(handled);
+}
+
+static void reset_asic(struct hantrodec_t *dev)
+{
+	int i;
+	u32 status;
+	int size = MIN(DEC_IO_SIZE_MAX, dev->iosize);
+
+	PDEBUG("hantrodec: %s\n", __func__);
+	status = ioread32(dev->hwregs + HANTRODEC_IRQ_STAT_DEC_OFF);
+	if (status & HANTRODEC_DEC_E) {
+		pr_info("hantrodec: %s abort with IRQ disabled\n", __func__);
+		/* abort with IRQ disabled */
+		status = HANTRODEC_DEC_ABORT | HANTRODEC_DEC_IRQ_DISABLE;
+		iowrite32(status,
+			  dev->hwregs + HANTRODEC_IRQ_STAT_DEC_OFF);
+	}
+
+	if (IS_G1(dev->hw_id))
+		/* reset PP */
+		iowrite32(0, dev->hwregs + HANTRO_IRQ_STAT_PP_OFF);
+
+	for (i = 4; i < size; i += 4)
+		iowrite32(0, dev->hwregs + i);
+}
diff --git a/drivers/gpu/drm/hantro/hantro_dec.h b/drivers/gpu/drm/hantro/hantro_dec.h
new file mode 100644
index 000000000000..4f50fbcc6800
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dec.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro decoder hardware driver header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_DEC_H__
+#define __HANTRO_DEC_H__
+
+int hantrodec_release(struct file *file);
+int hantrodec_init(void);
+int hantrodec_cleanup(void);
+int hantrodec_probe(struct dtbnode *pnode);
+void hantrodec_remove(struct device_info *pdevinfo);
+long hantrodec_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+int hantrodec_open(struct inode *inode, struct file *file);
+u32 hantrodec_read_bandwidth(struct device_info *pdevinfo, int is_read_bw);
+
+#endif /* __HANTRO_DEC_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_dec400.c b/drivers/gpu/drm/hantro/hantro_dec400.c
new file mode 100644
index 000000000000..7027df87c4ba
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dec400.c
@@ -0,0 +1,202 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro dec400 controller hardware driver.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_dec400.h"
+
+static void dec400_reset_asic(struct dec400_t *dev)
+{
+	int i;
+
+	for (i = 0; i < dev->core_cfg.iosize; i += sizeof(u32))
+		iowrite32(0, dev->hwregs + i);
+}
+
+static int hantro_dec400_write_regs(struct dec400_t *dev,
+				    struct core_desc *core)
+{
+	u32 data;
+	int ret = 0;
+
+	ret = copy_from_user(&data, core->regs + core->reg_id, sizeof(u32));
+	if (ret) {
+		pr_err("copy_from_user failed, returned %d\n", ret);
+		return -EFAULT;
+	}
+
+	iowrite32(data, (dev->hwregs + core->reg_id * sizeof(u32)));
+	return 0;
+}
+
+static int hantro_dec400_read_regs(struct dec400_t *dev, struct core_desc *core)
+{
+	u32 data;
+	int ret = 0;
+
+	data = ioread32((dev->hwregs + core->reg_id * sizeof(u32)));
+	ret = copy_to_user(core->regs + core->reg_id, &data, sizeof(u32));
+	if (ret) {
+		pr_err("copy_to_user failed, returned %d\n", ret);
+		return -EFAULT;
+	}
+
+	return 0;
+}
+
+long hantrodec400_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	int ret;
+	u32 id, deviceid, node, type;
+	struct dec400_t *pdec400;
+	struct core_desc coredesc;
+
+	if (enable_dec400 == 0)
+		return -EFAULT;
+
+	switch (cmd) {
+	case DEC400_IOCGHWOFFSET:
+		ret = copy_from_user(&id, (void const __user *)arg, sizeof(u32));
+		if (ret) {
+			pr_err("copy_from_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		deviceid = DEVICE_ID(id);
+		type = NODETYPE(id);
+		node = KCORE(id);
+		pdec400 = get_dec400_node_by_type(deviceid, type, node);
+		if (!pdec400)
+			return -EINVAL;
+
+		ret = __put_user(pdec400->core_cfg.dec400corebase,
+				 (unsigned long long __user *)arg);
+		if (ret) {
+			pr_err("copy_to_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		return 0;
+	case DEC400_IOCGHWIOSIZE:
+		ret = copy_from_user(&id, (void const __user *)arg, sizeof(u32));
+		if (ret) {
+			pr_err("copy_from_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		deviceid = DEVICE_ID(id);
+		type = NODETYPE(id);
+		node = KCORE(id);
+		pdec400 = get_dec400_node_by_type(deviceid, type, node);
+		if (!pdec400)
+			return -EFAULT;
+
+		ret = __put_user(pdec400->core_cfg.iosize, (unsigned int __user *)arg);
+		if (ret) {
+			pr_err("copy_to_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		return 0;
+	case DEC400_IOCS_DEC_WRITE_REG:
+		ret = copy_from_user(&coredesc, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (ret) {
+			pr_err("copy_from_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		deviceid = DEVICE_ID(coredesc.id);
+		type = NODETYPE(coredesc.id);
+		node = KCORE(coredesc.id);
+		pdec400 = get_dec400_node_by_type(deviceid, type, node);
+		if (!pdec400)
+			return -EFAULT;
+
+		return hantro_dec400_write_regs(pdec400, &coredesc);
+	case DEC400_IOCS_DEC_READ_REG:
+		ret = copy_from_user(&coredesc, (void const __user *)arg,
+				     sizeof(struct core_desc));
+		if (ret) {
+			pr_err("copy_from_user failed, returned %d\n", ret);
+			return -EFAULT;
+		}
+
+		deviceid = DEVICE_ID(coredesc.id);
+		type = NODETYPE(coredesc.id);
+		node = KCORE(coredesc.id);
+		pdec400 = get_dec400_node_by_type(deviceid, type, node);
+		if (!pdec400)
+			return -EFAULT;
+
+		return hantro_dec400_read_regs(pdec400, &coredesc);
+	default:
+		return -EINVAL;
+	}
+	return 0;
+}
+
+int hantrodec400_probe(struct dtbnode *pnode)
+{
+	struct dec400_t *pdec400;
+	int ret;
+
+	if (enable_dec400 == 0)
+		return 0;
+
+	pdec400 = vmalloc(sizeof(*pdec400));
+	if (!pdec400)
+		return -EINVAL;
+
+	pdec400->core_cfg.dec400corebase = pnode->ioaddr;
+	pdec400->core_cfg.iosize = pnode->iosize;
+	pdec400->core_cfg.parentaddr = pnode->parentaddr;
+	if (!request_mem_region(pdec400->core_cfg.dec400corebase,
+				pdec400->core_cfg.iosize, "hantrodec400")) {
+		pr_err("DEC400: HW regs busy\n");
+		ret = -ENODEV;
+		goto free_alloc;
+	}
+
+	pdec400->hwregs = ioremap(pdec400->core_cfg.dec400corebase,
+				  pdec400->core_cfg.iosize);
+	if (!pdec400->hwregs) {
+		release_mem_region(pdec400->core_cfg.dec400corebase,
+				   pdec400->core_cfg.iosize);
+		pr_err("DEC400: failed to map HW regs\n");
+		ret = -ENODEV;
+		goto free_alloc;
+	}
+
+	dec400_reset_asic(pdec400);
+	add_dec400_node(pnode->pdevinfo, pdec400);
+	pr_info("hantrodec400: HW at base <0x%llx>\n",
+		pdec400->core_cfg.dec400corebase);
+	return 0;
+
+free_alloc:
+	vfree(pdec400);
+	return ret;
+}
+
+void hantrodec400_remove(struct device_info *pdevinfo)
+{
+	struct dec400_t *dev, *pp;
+
+	dev = get_dec400_nodes(pdevinfo->deviceid, 0);
+	while (dev) {
+		if (dev->hwregs) {
+			iounmap(dev->hwregs);
+			release_mem_region(dev->core_cfg.dec400corebase,
+					   dev->core_cfg.iosize);
+		}
+
+		pp = dev->next;
+		vfree(dev);
+		dev = pp;
+	}
+}
diff --git a/drivers/gpu/drm/hantro/hantro_dec400.h b/drivers/gpu/drm/hantro/hantro_dec400.h
new file mode 100644
index 000000000000..544edc17264f
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dec400.h
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro dec400 controller hardware driver header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_DEC400_H__
+#define __HANTRO_DEC400_H__
+
+int hantrodec400_probe(struct dtbnode *pnode);
+void hantrodec400_remove(struct device_info *pdevinfo);
+long hantrodec400_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+
+#endif /* __HANTRO_DEC400_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_device.h b/drivers/gpu/drm/hantro/hantro_device.h
new file mode 100644
index 000000000000..ec0911606bd2
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_device.h
@@ -0,0 +1,289 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro device header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_DEVICE_H__
+#define __HANTRO_DEVICE_H__
+
+#include "hantro_drm.h"
+
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
+#define MIN(a, b) (((a) < (b)) ? (a) : (b))
+
+typedef enum {
+	DEVICE_UNKNOWN = 0,
+	DEVICE_KEEMBAY,
+	DEVICE_THUNDERBAY
+} hantro_device_type;
+
+/* supported core type */
+typedef enum {
+	CORE_UNKNOWN = -1,
+	CORE_DEVICE = 0,
+	CORE_DEC,
+	CORE_ENC,
+	CORE_CACHE,
+	CORE_DEC400,
+} device_coretype;
+
+// structure to hold data performance data per device
+typedef struct performance_data {
+	int count;
+	u64 last_resv;
+	u64 totaltime;
+	u64 hwcycles;
+} performance_data;
+
+struct cache_core_config {
+	cache_client_type client;
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	driver_cache_dir dir;
+	u32 deviceidx;
+	unsigned long long parentaddr;
+};
+
+struct cache_dev_t {
+	struct cache_core_config core_cfg; /* config of each core,such as base addr, irq,etc */
+	unsigned long hw_id; /* hw id to indicate project */
+	u32 core_id; /* core id for driver and sw internal use */
+	u32 is_valid; /* indicate this core is hantro's core or not */
+	u32 is_reserved; /* indicate this core is occupied by user or not */
+	struct file *cacheowner; /* indicate which process is occupying the core */
+	u32 irq_received; /* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 *hwregs;
+	char reg_name[32];
+	unsigned long long com_base_addr; /* common base addr of each L2 */
+	int irqlist[4];
+	char irq_name[4][32];
+	device_coretype parenttype;
+	u32 parentid; /* parent codec core's core_id */
+	void *parentcore; /* either struct hantroenc_t or struct hantrodec_t, or device itself */
+	void *parentdevice;
+	int deviceidx;
+	struct cache_dev_t *next;
+};
+
+struct dec400_core_cfg {
+	unsigned long long dec400corebase;
+
+	unsigned int iosize;
+	u32 deviceidx;
+	unsigned long long parentaddr;
+};
+
+struct dec400_t {
+	struct dec400_core_cfg core_cfg;
+
+	u32 core_id;
+	u8 *hwregs;
+
+	char reg_name[32];
+	device_coretype parenttype;
+	u32 parentid; /* parent codec core's core_id */
+	void *parentcore; /* either struct hantroenc_t or struct hantrodec_t, or device itself */
+	void *parentdevice;
+	struct dec400_t *next;
+};
+
+typedef struct {
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	/*
+	 * resource_shared indicate core share resources with other cores.
+	 * If 1, cores can not work at same time.
+	 */
+	u32 resource_shared;
+	u32 deviceidx;
+} CORE_CONFIG;
+
+struct hantroenc_t {
+	CORE_CONFIG core_cfg; /* config of each core,such as base addr, irq,etc */
+	u32 hw_id; /* hw id to indicate project */
+	u32 core_id; /* core id for driver and sw internal use */
+	u32 is_reserved; /* indicate this core is occupied by user or not */
+	int pid; /* indicate which process is occupying the core */
+	u32 irq_received; /* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 *hwregs;
+	char reg_name[32];
+	struct clk *dev_clk;
+	unsigned long clk_freq;
+	struct fasync_struct *async_queue;
+	int irqlist[4];
+	char irq_name[4][32];
+	void *parentdevice;
+	int deviceidx;
+	performance_data perf_data;
+	struct hantroenc_t *next;
+};
+
+#define HANTRO_G1_DEC_REGS	155 /* G1 total regs */
+#define HANTRO_G2_DEC_REGS	337 /* G2 total regs */
+#define HANTRO_VC8000D_REGS	393 /* VC8000D total regs */
+#define DEC_IO_SIZE_MAX \
+	(MAX(MAX(HANTRO_G2_DEC_REGS, HANTRO_G1_DEC_REGS), \
+	     HANTRO_VC8000D_REGS) * 4)
+
+struct hantrodec_t {
+	u32 cfg;
+	int core_id;
+	unsigned int iosize;
+	u32 cfg_backup;
+	/* indicate if main core exist */
+	struct hantrodec_t *its_main_core_id;
+	/* indicate if aux core exist */
+	struct hantrodec_t *its_aux_core_id;
+	/*
+	 * all access to hwregs are through readl/writel
+	 * so volatile is removed according to doc "volatile is evil"
+	 */
+	u8 *hwregs;
+	int hw_id;
+	u8 *page_lut_regs_read;
+	char reg_name[32];
+	unsigned long long multicorebase;
+	/*
+	 * Because one core may contain multi-pipeline,
+	 * so multicore base may be changed
+	 */
+	unsigned long long multicorebase_actual;
+
+	u32 dec_regs[DEC_IO_SIZE_MAX / 4];
+	int irqlist[4];
+	char irq_name[4][32];
+	struct clk *dev_clk;
+	unsigned long clk_freq;
+	struct file *dec_owner;
+	struct file *pp_owner;
+	void *parentdevice;
+	u32 deviceidx;
+	performance_data perf_data;
+	struct hantrodec_t *next;
+};
+
+struct hantro_cooling_data {
+	unsigned int media_clk_state;
+	unsigned int media_clk_max_state;
+	unsigned long clk_freq;
+	struct thermal_cooling_device *cooling_dev;
+};
+
+#define NODE_TYPES 2  //encoder & decoder
+#define MAX_CORES 4   // max core per device
+#define NODE_TYPE_DEC	1
+#define NODE_TYPE_ENC	2
+
+/*
+ * current internal device data structure will look like this:
+ *	devicehdr->	device 0		-> device 1 -> ...
+ *				|
+ *		________________________________________
+ *		|				|			|			|
+ *	dec core 0		enc core 0	dec400 0		cache core 0
+ *		|				|			|			|
+ *	dec core 1		enc core 1	dec400 1		cache core 1
+ *		|				|			|			|
+ *	.......				......		   .....		     .....
+ *
+ * Each core node contains its own info: io region, irq, hwid, direction, etc,
+ * depending on its type.
+ * Each dec400 core has pointer to a dec or enc core.
+ * Each dec/enc core has a pointer to dec400 core.
+ * Each cache core has pointer to a dec or enc core.
+ * Each dec/enc core has pointers to dec400 core.
+ * We do it this way since we don't know which one will be probed first,
+ * dec/enc or dec400/cache.
+ * And only ID to connect dec/enc to dec400/cache core is their HW address.
+ */
+
+struct device_info {
+	struct device *dev; /* related dev, for drm usage */
+	struct drm_device *drm_dev;
+	struct device *codec_rsvmem;
+	struct dentry *debugfs_root;
+	struct resource mem_res[2];
+	phys_addr_t rsvmem_addr;
+	phys_addr_t memsize;
+	int deviceid;
+	u32 config;
+	struct hantro_cooling_data thermal_data;
+	int deccore_num;
+	int enccore_num;
+	int dec400core_num;
+	int cachecore_num;
+
+	struct hantrodec_t *dechdr;
+	struct hantroenc_t *enchdr;
+	struct cache_dev_t *cachehdr;
+	struct dec400_t *dec400hdr;
+
+	/* orig cache global vars */
+	wait_queue_head_t cache_hw_queue;
+	wait_queue_head_t cache_wait_queue;
+	/* spinlock for cache owner */
+	spinlock_t cache_owner_lock;
+
+	/* orig enc global vars */
+	struct semaphore enc_core_sem;
+	wait_queue_head_t enc_hw_queue;
+	/* spinlock for enc owner */
+	spinlock_t enc_owner_lock;
+	wait_queue_head_t enc_wait_queue;
+
+	/* orig dec global vars */
+	int dec_irq;
+	int pp_irq;
+	/* spinlock for dec owner */
+	spinlock_t owner_lock;
+	wait_queue_head_t dec_wait_queue;
+	wait_queue_head_t pp_wait_queue;
+	wait_queue_head_t hw_queue;
+	struct semaphore dec_core_sem;
+	struct semaphore pp_core_sem;
+	void *priv_data;
+	struct idr clients;
+	struct idr allocations;
+	/* alloc mutex struct */
+	struct mutex alloc_mutex;
+	struct device_info *next;
+};
+
+struct device_info *getdevicenode(u32 deviceid);
+int get_devicecorenum(u32 deviceindex, device_coretype type);
+struct hantrodec_t *get_decnode(struct device_info *pdevice, u32 nodeidx);
+struct hantrodec_t *get_decnode_bydeviceid(u32 deviceindex, u32 nodeidx);
+struct hantrodec_t *getfirst_decnodes(struct device_info *pdevice);
+struct hantroenc_t *get_encnode(struct device_info *pdevice, u32 nodeidx);
+struct hantroenc_t *get_encnode_bydeviceid(u32 deviceid, u32 nodeidx);
+struct cache_dev_t *get_cachenodes(u32 deviceid, u32 nodeidx);
+struct cache_dev_t *get_cachenodebytype(u32 deviceid, u32 parenttype,
+					u32 parentnodeidx);
+struct dec400_t *get_dec400nodes(u32 deviceid, u32 nodeidx);
+struct dec400_t *get_dec400nodebytype(u32 deviceid, u32 parenttype,
+				      u32 parentnodeidx);
+int add_decnode(struct device_info *pdevice, struct hantrodec_t *deccore);
+int add_encnode(struct device_info *pdevice, struct hantroenc_t *enccore);
+int add_dec400node(struct device_info *splice, struct dec400_t *dec400core);
+int add_cachenode(struct device_info *splice, struct cache_dev_t *cachecore);
+int get_devicecount(void);
+struct device_info *getparentdevice(void *node, int type);
+int device_remove(void);
+int device_init(void);
+void device_init_finish(void);
+long hantrodevice_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+
+void device_printdebug(void);
+
+#endif /* __HANTRO_DEVICE_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_devicemgr.c b/drivers/gpu/drm/hantro/hantro_devicemgr.c
new file mode 100644
index 000000000000..9e5404936c1a
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_devicemgr.c
@@ -0,0 +1,563 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro device manager.
+ *
+ *    Copyright (c) 2018 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+
+int get_device_core_num(u32 deviceid, enum device_coretype type)
+{
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return 0;
+
+	switch (type) {
+	case CORE_ENC:
+		return hdr->enccore_num;
+	case CORE_DEC:
+		return hdr->deccore_num;
+	case CORE_CACHE:
+		return hdr->cachecore_num;
+	case CORE_DEC400:
+		return hdr->dec400core_num;
+	default:
+		return 0;
+	}
+}
+
+/* get dec nodes list hdr of a device */
+struct hantrodec_t *get_dec_node(struct device_info *pdevinfo, u32 nodeidx)
+{
+	int i;
+	struct hantrodec_t *p;
+
+	if (!pdevinfo)
+		return NULL;
+
+	p = pdevinfo->dechdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+/* get dec nodes list hdr of a device */
+struct hantrodec_t *get_dec_node_by_device_id(u32 deviceid, u32 nodeidx)
+{
+	int i;
+	struct hantrodec_t *p;
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return NULL;
+
+	p = hdr->dechdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+struct hantrodec_t *get_first_dec_nodes(struct device_info *pdevinfo)
+{
+	if (pdevinfo)
+		return pdevinfo->dechdr;
+
+	return NULL;
+}
+
+/* get enc nodes list hdr of a device */
+struct hantroenc_t *get_enc_node(struct device_info *pdevinfo, u32 nodeidx)
+{
+	int i;
+	struct hantroenc_t *p;
+
+	if (!pdevinfo)
+		return NULL;
+
+	p = pdevinfo->enchdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+/* get enc nodes list hdr of a device */
+struct hantroenc_t *get_enc_node_by_device_id(u32 deviceid, u32 nodeidx)
+{
+	int i;
+	struct hantroenc_t *p;
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return NULL;
+
+	p = hdr->enchdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+/* get cache nodes list hdr of a device */
+struct cache_dev_t *get_cache_nodes(u32 deviceid, u32 nodeidx)
+{
+	int i;
+	struct cache_dev_t *p;
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return NULL;
+
+	p = hdr->cachehdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+/* get dec400 nodes list hdr of a device */
+struct dec400_t *get_dec400_nodes(u32 deviceid, u32 nodeidx)
+{
+	int i;
+	struct dec400_t *p;
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return NULL;
+
+	p = hdr->dec400hdr;
+	for (i = 0; i < nodeidx; i++) {
+		if (!p)
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+/* get dec400 nodes by parent type and parent core num */
+struct dec400_t *get_dec400_node_by_type(u32 deviceid, u32 parenttype,
+					 u32 parentnodeidx)
+{
+	struct dec400_t *p;
+	struct device_info *hdr = get_device_info(deviceid);
+
+	if (!hdr)
+		return NULL;
+
+	p = hdr->dec400hdr;
+	while (p) {
+		if (p->parentid == parentnodeidx &&
+		    ((parenttype == NODE_TYPE_DEC &&
+		      p->parenttype == CORE_DEC) ||
+		     (parenttype == NODE_TYPE_ENC && p->parenttype == CORE_ENC)))
+			break;
+
+		p = p->next;
+	}
+
+	return p;
+}
+
+int add_dec_node(struct device_info *pdevinfo, struct hantrodec_t *deccore)
+{
+	struct hantrodec_t *pdec;
+
+	if (!pdevinfo)
+		return -EINVAL;
+
+	pdec = pdevinfo->dechdr;
+	if (!pdec) {
+		pdevinfo->dechdr = deccore;
+	} else {
+		while (pdec->next)
+			pdec = pdec->next;
+
+		pdec->next = deccore;
+	}
+
+	deccore->next = NULL;
+	pdevinfo->deccore_num++;
+	deccore->core_id = pdevinfo->deccore_num - 1;
+	deccore->pdevinfo = pdevinfo;
+	pdevinfo->config |= CONFIG_HWDEC;
+
+	sema_init(&pdevinfo->dec_core_sem, pdevinfo->deccore_num);
+	return 0;
+}
+
+int add_enc_node(struct device_info *pdevinfo, struct hantroenc_t *enccore)
+{
+	struct hantroenc_t *penc;
+
+	if (!pdevinfo)
+		return -EINVAL;
+
+	penc = pdevinfo->enchdr;
+	if (!penc) {
+		pdevinfo->enchdr = enccore;
+	} else {
+		while (penc->next)
+			penc = penc->next;
+
+		penc->next = enccore;
+	}
+
+	enccore->next = NULL;
+	pdevinfo->enccore_num++;
+	enccore->core_id = pdevinfo->enccore_num - 1;
+	enccore->pdevinfo = pdevinfo;
+	pdevinfo->config |= CONFIG_HWENC;
+	return 0;
+}
+
+int add_dec400_node(struct device_info *pdevinfo, struct dec400_t *dec400core)
+{
+	struct dec400_t *pdec400;
+	struct hantrodec_t *pdec;
+	struct hantroenc_t *penc;
+
+	if (!pdevinfo)
+		return -EINVAL;
+
+	pdec400 = pdevinfo->dec400hdr;
+	if (!pdec400) {
+		pdevinfo->dec400hdr = dec400core;
+	} else {
+		while (pdec400->next)
+			pdec400 = pdec400->next;
+
+		pdec400->next = dec400core;
+	}
+
+	dec400core->next = NULL;
+	pdevinfo->dec400core_num++;
+	dec400core->core_id = pdevinfo->dec400core_num - 1;
+	pdevinfo->config |= CONFIG_DEC400;
+	/* set default */
+	dec400core->parentcore = pdevinfo;
+	dec400core->parentid = CORE_DEVICE;
+	dec400core->pdevinfo = pdevinfo;
+
+	if (dec400core->core_cfg.parentaddr == pdevinfo->rsvmem_addr) {
+		dec400core->parentcore = pdevinfo;
+		dec400core->parenttype = CORE_DEVICE;
+		goto end;
+	}
+
+	penc = pdevinfo->enchdr;
+	while (penc) {
+		if ((unsigned long long)penc->core_cfg.base_addr ==
+		    dec400core->core_cfg.parentaddr) {
+			dec400core->parentcore = penc;
+			dec400core->parentid = penc->core_id;
+			dec400core->parenttype = CORE_ENC;
+			goto end;
+		}
+
+		penc = penc->next;
+	}
+
+	pdec = pdevinfo->dechdr;
+	while (pdec) {
+		if ((unsigned long long)pdec->multicorebase ==
+		    dec400core->core_cfg.parentaddr) {
+			dec400core->parentcore = pdec;
+			dec400core->parentid = pdec->core_id;
+			dec400core->parenttype = CORE_DEC;
+			goto end;
+		}
+
+		pdec = pdec->next;
+	}
+
+end:
+	return 0;
+}
+
+int add_cache_node(struct device_info *pdevinfo, struct cache_dev_t *cachecore)
+{
+	struct cache_dev_t *pcache;
+	struct hantrodec_t *pdec;
+	struct hantroenc_t *penc;
+
+	if (!pdevinfo)
+		return -ENODEV;
+
+	pcache = pdevinfo->cachehdr;
+	if (!pcache) {
+		pdevinfo->cachehdr = cachecore;
+	} else {
+		while (pcache->next)
+			pcache = pcache->next;
+
+		pcache->next = cachecore;
+	}
+
+	cachecore->next = NULL;
+	pdevinfo->cachecore_num++;
+	cachecore->core_id = pdevinfo->cachecore_num - 1;
+	pdevinfo->config |= CONFIG_L2CACHE;
+
+	/* set default */
+	cachecore->parentcore = pdevinfo;
+	cachecore->parentid = pdevinfo->deviceid;
+	cachecore->parenttype = CORE_DEVICE;
+	cachecore->pdevinfo = pdevinfo;
+
+	if (cachecore->core_cfg.client == VC8000E) {
+		penc = pdevinfo->enchdr;
+		while (penc) {
+			if ((unsigned long long)penc->core_cfg.base_addr ==
+			    cachecore->core_cfg.parentaddr) {
+				cachecore->parentcore = penc;
+				cachecore->parentid = penc->core_id;
+				cachecore->parenttype = CORE_ENC;
+				break;
+			}
+
+			penc = penc->next;
+		}
+	} else {
+		pdec = pdevinfo->dechdr;
+		while (pdec) {
+			if ((unsigned long long)pdec->multicorebase ==
+			    cachecore->core_cfg.parentaddr) {
+				cachecore->parentcore = pdec;
+				cachecore->parentid = pdec->core_id;
+				cachecore->parenttype = CORE_DEC;
+				break;
+			}
+
+			pdec = pdec->next;
+		}
+	}
+
+	return 0;
+}
+
+int get_device_count(void)
+{
+	return atomic_read(&hantro_drm.devicecount);
+}
+
+long hantrodevice_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+	case DRM_IOCTL_HANTRO_GET_DEVICENUM:
+		return get_device_count();
+	default:
+		return -EINVAL;
+	}
+}
+
+void device_print_debug(void)
+{
+	struct hantrodec_t *pdec, *pdec2;
+	struct hantroenc_t *penc, *penc2;
+	struct device_info *pdevinfo;
+	struct cache_dev_t *pcache, *pcache2;
+	struct dec400_t *pdec400, *pdec400_2;
+	int i, n = get_device_count(), k;
+	int decn, encn, cachen, dec400n;
+	struct device_info *pdevinfo0, *pdevinfo1;
+
+	pr_info("device info start");
+	pr_info("device num = %d", n);
+	pdevinfo0 = hantro_drm.pdevice_list;
+	for (i = 0; i < n; i++) {
+		pr_info("device %d:%lx:%llx:%lld:%x", i,
+			(unsigned long)pdevinfo0->dev, pdevinfo0->rsvmem_addr,
+			pdevinfo0->memsize, pdevinfo0->config);
+
+		decn = get_device_core_num(i, CORE_DEC);
+		pr_info("dec num = %d", decn);
+		encn = get_device_core_num(i, CORE_ENC);
+		pr_info("enc num = %d", encn);
+		cachen = get_device_core_num(i, CORE_CACHE);
+		pr_info("cache  num = %d", cachen);
+		dec400n = get_device_core_num(i, CORE_DEC400);
+		pr_info("dec400n num = %d", dec400n);
+
+		pdec = get_dec_node_by_device_id(i, 0);
+		k = 0;
+		while (pdec) {
+			pr_info("dec core %d", k);
+			pdec2 = get_dec_node_by_device_id(i, k);
+			pdevinfo1 = pdec->pdevinfo;
+			if (pdec != pdec2)
+				pr_info("get_decnodes fails @ %d", k);
+
+			if (pdevinfo0 != pdevinfo1)
+				pr_info("get_parent_device fails @ dec %d:%d", i,
+					k);
+
+			pr_info("addr=%llx, size=%d", pdec->multicorebase,
+				pdec->iosize);
+			pr_info("irq0=%d, irq1=%d", pdec->irqlist[0],
+				pdec->irqlist[1]);
+			if (pdec->its_main_core_id) {
+				pdec2 = pdec->its_main_core_id;
+				pr_info("main core = %d:%d", pdevinfo1->deviceid,
+					pdec2->core_id);
+			}
+
+			if (pdec->its_aux_core_id) {
+				pdec2 = pdec->its_aux_core_id;
+				pr_info("aux core = %d:%d", pdec2->pdevinfo->deviceid,
+					pdec2->core_id);
+			}
+
+			pdec = pdec->next;
+			k++;
+		}
+
+		penc = get_enc_node_by_device_id(i, 0);
+		k = 0;
+		while (penc) {
+			pr_info("enc core %d:", k);
+			penc2 = get_enc_node_by_device_id(i, k);
+			pdevinfo1 = penc->pdevinfo;
+			if (penc != penc2)
+				pr_info("get_encnodes fails @ %d", k);
+
+			if (pdevinfo0 != pdevinfo1)
+				pr_info("get_parent_device fails @ enc %d:%d", i,
+					k);
+
+			pr_info("addr=%llx, size=%d", penc->core_cfg.base_addr,
+				penc->core_cfg.iosize);
+			pr_info("irq0=%d, irq1=%d", penc->irqlist[0],
+				penc->irqlist[1]);
+			penc = penc->next;
+			k++;
+		}
+
+		pcache = get_cache_nodes(i, 0);
+		k = 0;
+		while (pcache) {
+			pr_info("cache core %d:", k);
+			pcache2 = get_cache_nodes(i, k);
+			pdevinfo1 = pcache->pdevinfo;
+			if (pcache != pcache2)
+				pr_info("get_cache_nodes fails @ %d", k);
+
+			if (pdevinfo0 != pdevinfo1)
+				pr_info("get_parent_device fails @ cache %d:%d",
+					i, k);
+
+			pr_info("addr=%llx, size=%d, type=%d, dir=%d",
+				pcache->core_cfg.base_addr,
+				pcache->core_cfg.iosize,
+				pcache->core_cfg.client, pcache->core_cfg.dir);
+			pr_info("irq0=%d, irq1=%d", pcache->irqlist[0],
+				pcache->irqlist[1]);
+			pr_info("parent addr=%llx",
+				pcache->core_cfg.parentaddr);
+			if (pcache->parentcore) {
+				if (pcache->core_cfg.client == VC8000E) {
+					penc = (struct hantroenc_t *)
+						       pcache->parentcore;
+					pr_info("parent enc core = %d:%d,addr %llx",
+						penc->pdevinfo->deviceid,
+						penc->core_id,
+						penc->core_cfg.base_addr);
+				} else {
+					pdec = (struct hantrodec_t *)
+						       pcache->parentcore;
+					pr_info("parent dec core = %d:%d,addr %llx",
+						pdec->pdevinfo->deviceid, pdec->core_id,
+						pdec->multicorebase);
+				}
+
+			} else {
+				pr_info("parent core = NULL");
+			}
+
+			pcache = pcache->next;
+			k++;
+		}
+		pdec400 = get_dec400_nodes(i, 0);
+		k = 0;
+		while (pdec400) {
+			pr_info("dec400 core %d:", k);
+			pdec400_2 = get_dec400_nodes(i, k);
+			pdevinfo1 = pdec400->pdevinfo;
+			if (pdec400 != pdec400_2)
+				pr_info("get_dec400_nodes fails @ %d", k);
+
+			if (pdevinfo0 != pdevinfo1)
+				pr_info("get_parent_device fails @ dec400 %d:%d",
+					i, k);
+
+			pr_info("addr=%llx, size=%d",
+				pdec400->core_cfg.dec400corebase,
+				pdec400->core_cfg.iosize);
+			pr_info("parent addr=%llx",
+				pdec400->core_cfg.parentaddr);
+			if (pdec400->parentcore) {
+				switch (pdec400->parenttype) {
+				case CORE_ENC:
+					penc = (struct hantroenc_t *)
+						       pdec400->parentcore;
+					pr_info("parent enc core = %d:%d,addr %llx",
+						penc->pdevinfo->deviceid,
+						penc->core_id,
+						penc->core_cfg.base_addr);
+					break;
+				case CORE_DEC:
+					pdec = (struct hantrodec_t *)
+						       pdec400->parentcore;
+					pr_info("parent dec core = %d:%d,addr %llx",
+						pdec->pdevinfo->deviceid, pdec->core_id,
+						pdec->multicorebase);
+					break;
+				case CORE_DEVICE:
+					pdevinfo = (struct device_info *)
+							  pdec400->parentcore;
+					pr_info("parent device addr %llx",
+						pdevinfo->rsvmem_addr);
+					break;
+				default:
+					pr_info("error: dec400 parent type unknown");
+					break;
+				}
+			} else {
+				pr_info("parent core = NULL");
+			}
+
+			pdec400 = pdec400->next;
+			k++;
+		}
+
+		pdevinfo0 = pdevinfo0->next;
+	}
+
+	pr_info("device info finish");
+}
diff --git a/drivers/gpu/drm/hantro/hantro_devicemgr.h b/drivers/gpu/drm/hantro/hantro_devicemgr.h
new file mode 100644
index 000000000000..5cf28dd99d4c
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_devicemgr.h
@@ -0,0 +1,266 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro device manager header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_DEVICE_H__
+#define __HANTRO_DEVICE_H__
+
+#include "hantro_drm.h"
+
+#define MAX(a, b) ({ \
+	typeof(a) a_ = (a); \
+	typeof(b) b_ = (b); \
+	(a_ > b_) ? a_ : b_; \
+})
+#define MIN(a, b) ({ \
+	typeof(a) a_ = (a); \
+	typeof(b) b_ = (b); \
+	(a_ < b_) ? a_ : b_; \
+})
+
+enum hantro_device_type {
+	DEVICE_UNKNOWN = 0,
+	DEVICE_KEEMBAY,
+	DEVICE_THUNDERBAY
+};
+
+/* supported core type */
+enum device_coretype {
+	CORE_UNKNOWN = -1,
+	CORE_DEVICE = 0,
+	CORE_DEC,
+	CORE_ENC,
+	CORE_CACHE,
+	CORE_DEC400,
+};
+
+/* structure to hold data performance data per device */
+struct performance_data {
+	int count;
+	u64 last_resv;
+	u64 totaltime;
+	u64 hwcycles;
+};
+
+struct cache_core_config {
+	enum cache_client_type client;
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	enum driver_cache_dir dir;
+	unsigned long long parentaddr;
+};
+
+struct cache_dev_t {
+	struct cache_core_config core_cfg;	/* each core config, such as base addr, irq, etc */
+	unsigned long hw_id;			/* hw id to indicate project */
+	u32 core_id;				/* core id for driver and sw internal use */
+	u32 is_valid;				/* indicate this core is hantro's core or not */
+	u32 is_reserved;			/* indicate this core is occupied by user or not */
+	struct file *cacheowner;		/* indicate which process is occupying the core */
+	u32 irq_received;			/* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 __iomem *hwregs;
+	char reg_name[32];
+	unsigned long long com_base_addr;	/* common base addr of each L2 */
+	int irqlist[4];
+	char irq_name[4][32];
+	enum device_coretype parenttype;
+	u32 parentid;				/* parent codec core's core_id */
+	void *parentcore;			/*
+						 * either struct hantroenc_t or struct hantrodec_t,
+						 * or device itself
+						 */
+	struct device_info *pdevinfo;
+	struct cache_dev_t *next;
+};
+
+struct dec400_core_cfg {
+	unsigned long long dec400corebase;
+
+	unsigned int iosize;
+	unsigned long long parentaddr;
+};
+
+struct dec400_t {
+	struct dec400_core_cfg core_cfg;
+
+	u32 core_id;
+	u8 __iomem *hwregs;
+
+	char reg_name[32];
+	enum device_coretype parenttype;
+	u32 parentid;				/* parent codec core's core_id */
+	void *parentcore;			/*
+						 * either struct hantroenc_t or struct hantrodec_t,
+						 * or device itself
+						 */
+	struct device_info *pdevinfo;
+	struct dec400_t *next;
+};
+
+struct core_config {
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	/*
+	 * resource_shared indicate core share resources with other cores.
+	 * If 1, cores can not work at same time.
+	 */
+	u32 resource_shared;
+};
+
+struct hantroenc_t {
+	struct core_config core_cfg;	/* each core config ,such as base addr, irq, etc */
+	u32 hw_id;			/* hw id to indicate project */
+	u32 core_id;			/* core id for driver and sw internal use */
+	u32 is_reserved;		/* indicate this core is occupied by user or not */
+	int pid;			/* indicate which process is occupying the core */
+	u32 irq_received;		/* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 __iomem *hwregs;
+	char reg_name[32];
+	struct clk *dev_clk;
+	unsigned long clk_freq;
+	struct fasync_struct *async_queue;
+	int irqlist[4];
+	char irq_name[4][32];
+	struct device_info *pdevinfo;
+	struct performance_data perf_data;
+	struct hantroenc_t *next;
+};
+
+#define HANTRO_VC8000D_REGS	393 /* VC8000D total regs */
+#define DEC_IO_SIZE_MAX		(HANTRO_VC8000D_REGS * 4)
+
+struct hantrodec_t {
+	u32 cfg;
+	int core_id;
+	unsigned int iosize;
+	u32 cfg_backup;
+	/* indicate if main core exist */
+	struct hantrodec_t *its_main_core_id;
+	/* indicate if aux core exist */
+	struct hantrodec_t *its_aux_core_id;
+	/*
+	 * all access to hwregs are through readl/writel
+	 * so volatile is removed according to doc "volatile is evil"
+	 */
+	u8 __iomem *hwregs;
+	int hw_id;
+	u8 __iomem *page_lut_regs_read;
+	char reg_name[32];
+	unsigned long long multicorebase;
+	/*
+	 * Because one core may contain multi-pipeline,
+	 * so multicore base may be changed
+	 */
+	unsigned long long multicorebase_actual;
+
+	u32 dec_regs[DEC_IO_SIZE_MAX / 4];
+	int irqlist[4];
+	char irq_name[4][32];
+	struct clk *dev_clk;
+	unsigned long clk_freq;
+	struct file *dec_owner;
+	struct file *pp_owner;
+	struct device_info *pdevinfo;
+	struct performance_data perf_data;
+	struct hantrodec_t *next;
+};
+
+struct hantro_cooling_data {
+	unsigned int media_clk_state;
+	unsigned int media_clk_max_state;
+	unsigned long clk_freq;
+	struct thermal_cooling_device *cooling_dev;
+};
+
+#define NODE_TYPES 2  /* encoder & decoder */
+#define MAX_CORES 4   /* max core per device */
+#define NODE_TYPE_DEC	1
+#define NODE_TYPE_ENC	2
+
+struct device_info {
+	struct device *dev; /* related dev, for drm usage */
+	struct drm_device *drm_dev;
+	struct device *codec_rsvmem;
+	struct dentry *debugfs_root;
+	struct resource mem_res[2];
+	phys_addr_t rsvmem_addr;
+	phys_addr_t memsize;
+	int deviceid;
+	u32 config;
+	struct hantro_cooling_data thermal_data;
+	int deccore_num;
+	int enccore_num;
+	int dec400core_num;
+	int cachecore_num;
+
+	struct hantrodec_t *dechdr;
+	struct hantroenc_t *enchdr;
+	struct cache_dev_t *cachehdr;
+	struct dec400_t *dec400hdr;
+
+	/* orig cache global vars */
+	wait_queue_head_t cache_hw_queue;
+	wait_queue_head_t cache_wait_queue;
+	/* spinlock for cache owner */
+	spinlock_t cache_owner_lock;
+
+	/* orig enc global vars */
+	struct semaphore enc_core_sem;
+	wait_queue_head_t enc_hw_queue;
+	/* spinlock for enc owner */
+	spinlock_t enc_owner_lock;
+	wait_queue_head_t enc_wait_queue;
+
+	/* orig dec global vars */
+	int dec_irq;
+	int pp_irq;
+	/* spinlock for dec owner */
+	spinlock_t owner_lock;
+	wait_queue_head_t dec_wait_queue;
+	wait_queue_head_t pp_wait_queue;
+	wait_queue_head_t hw_queue;
+	struct semaphore dec_core_sem;
+	struct semaphore pp_core_sem;
+	void *priv_data;
+	struct idr clients;
+	struct idr allocations;
+	/* alloc mutex struct */
+	struct mutex alloc_mutex;
+	struct device_info *next;
+};
+
+struct device_info *get_device_node(u32 deviceid);
+int get_device_core_num(u32 deviceid, enum device_coretype type);
+struct hantrodec_t *get_dec_node(struct device_info *pdevinfo, u32 nodeidx);
+struct hantrodec_t *get_dec_node_by_device_id(u32 deviceid, u32 nodeidx);
+struct hantrodec_t *get_first_dec_nodes(struct device_info *pdevinfo);
+struct hantroenc_t *get_enc_node(struct device_info *pdevinfo, u32 nodeidx);
+struct hantroenc_t *get_enc_node_by_device_id(u32 deviceid, u32 nodeidx);
+struct cache_dev_t *get_cache_nodes(u32 deviceid, u32 nodeidx);
+struct dec400_t *get_dec400_nodes(u32 deviceid, u32 nodeidx);
+struct dec400_t *get_dec400_node_by_type(u32 deviceid, u32 parenttype, u32 parentnodeidx);
+int add_dec_node(struct device_info *pdevinfo, struct hantrodec_t *deccore);
+int add_enc_node(struct device_info *pdevinfo, struct hantroenc_t *enccore);
+int add_dec400_node(struct device_info *pdevinfo, struct dec400_t *dec400core);
+int add_cache_node(struct device_info *pdevinfo, struct cache_dev_t *cachecore);
+int get_device_count(void);
+int device_remove(void);
+int device_init(void);
+void device_init_finish(void);
+long hantrodevice_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+
+void device_print_debug(void);
+
+#endif /* __HANTRO_DEVICE_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_dmabuf.c b/drivers/gpu/drm/hantro/hantro_dmabuf.c
new file mode 100644
index 000000000000..e3f0ad693119
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dmabuf.c
@@ -0,0 +1,91 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver DMA_BUF operation.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+
+static void hantro_gem_dmabuf_release(struct dma_buf *dma_buf)
+{
+	struct drm_gem_object *obj = hantro_get_gem_from_dmabuf(dma_buf);
+
+	if (obj) {
+		drm_gem_object_put(obj);
+		drm_dev_put(obj->dev);
+	}
+}
+
+static struct sg_table *
+hantro_gem_map_dma_buf(struct dma_buf_attachment *attach,
+		       enum dma_data_direction dir)
+{
+	struct drm_gem_object *obj = hantro_get_gem_from_dmabuf(attach->dmabuf);
+	struct sg_table *sgt;
+
+	if (WARN_ON(dir == DMA_NONE))
+		return ERR_PTR(-EINVAL);
+
+	if (WARN_ON(!obj))
+		return ERR_PTR(-EINVAL);
+
+	sgt = obj->funcs->get_sg_table(obj);
+	if (!dma_map_sg_attrs(attach->dev, sgt->sgl, sgt->nents, dir,
+			      DMA_ATTR_SKIP_CPU_SYNC)) {
+		sg_free_table(sgt);
+		kfree(sgt);
+		sgt = ERR_PTR(-ENOMEM);
+	}
+
+	return sgt;
+}
+
+static int hantro_gem_dmabuf_mmap(struct dma_buf *dma_buf,
+				  struct vm_area_struct *vma)
+{
+	struct drm_gem_object *obj = hantro_get_gem_from_dmabuf(dma_buf);
+	struct drm_device *dev;
+
+	if (!obj)
+		return -EINVAL;
+
+	dev = obj->dev;
+	if (!dev->driver->gem_prime_mmap)
+		return -EOPNOTSUPP;
+
+	return dev->driver->gem_prime_mmap(obj, vma);
+}
+
+static int hantro_gem_dmabuf_vmap(struct dma_buf *dma_buf, struct dma_buf_map *map)
+{
+	struct drm_gem_object *obj = hantro_get_gem_from_dmabuf(dma_buf);
+
+	return obj->funcs->vmap(obj, map);
+}
+
+static void hantro_gem_dmabuf_vunmap(struct dma_buf *dma_buf, struct dma_buf_map *vaddr)
+{
+	struct drm_gem_object *obj = hantro_get_gem_from_dmabuf(dma_buf);
+
+	if (!vaddr)
+		return;
+
+	if (obj) {
+		if (obj->funcs && obj->funcs->vunmap)
+			obj->funcs->vunmap(obj, vaddr);
+		else if (obj->funcs->vunmap)
+			obj->funcs->vunmap(obj, vaddr);
+	}
+}
+
+const struct dma_buf_ops hantro_dmabuf_ops = {
+	.cache_sgt_mapping = true,
+	.map_dma_buf = hantro_gem_map_dma_buf,
+	.unmap_dma_buf = drm_gem_unmap_dma_buf,
+	.release = hantro_gem_dmabuf_release,
+	.mmap = hantro_gem_dmabuf_mmap,
+	.vmap = hantro_gem_dmabuf_vmap,
+	.vunmap = hantro_gem_dmabuf_vunmap,
+};
diff --git a/drivers/gpu/drm/hantro/hantro_drm.c b/drivers/gpu/drm/hantro/hantro_drm.c
new file mode 100644
index 000000000000..f6c2cec83abf
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_drm.c
@@ -0,0 +1,1550 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver main DRM file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_dec.h"
+#include "hantro_enc.h"
+#include "hantro_cache.h"
+#include "hantro_dec400.h"
+#include "hantro_metadata.h"
+
+static struct drm_gem_object_funcs hantro_gem_object_funcs;
+
+static int hantro_record_mem(struct device_info *pdevinfo, void *obj, int size)
+{
+	int ret;
+
+	mutex_lock(&pdevinfo->alloc_mutex);
+	ret = idr_alloc(&pdevinfo->allocations, obj, 1, 0, GFP_KERNEL);
+	mutex_unlock(&pdevinfo->alloc_mutex);
+	return (ret > 0 ? 0 : -ENOMEM);
+}
+
+static void hantro_unrecord_mem(struct device_info *pdevinfo, void *obj)
+{
+	int id;
+	void *cma_obj;
+
+	mutex_lock(&pdevinfo->alloc_mutex);
+	idr_for_each_entry(&pdevinfo->allocations, cma_obj, id) {
+		if (cma_obj == obj) {
+			idr_remove(&pdevinfo->allocations, id);
+			break;
+		}
+	}
+	mutex_unlock(&pdevinfo->alloc_mutex);
+}
+
+static void hantro_drm_fb_destroy(struct drm_framebuffer *fb)
+{
+	struct hantro_drm_fb *vsi_fb = (struct hantro_drm_fb *)fb;
+	int i;
+
+	for (i = 0; i < 4; i++)
+		hantro_unref_drmobj(vsi_fb->obj[i]);
+
+	drm_framebuffer_cleanup(fb);
+	kfree(vsi_fb);
+}
+
+static int hantro_drm_fb_create_handle(struct drm_framebuffer *fb,
+				       struct drm_file *file_priv,
+				       unsigned int *handle)
+{
+	struct hantro_drm_fb *vsi_fb = (struct hantro_drm_fb *)fb;
+	int ret;
+
+	ret = drm_gem_handle_create(file_priv, vsi_fb->obj[0], handle);
+	trace_gem_handle_create(*handle);
+	return ret;
+}
+
+static int hantro_drm_fb_dirty(struct drm_framebuffer *fb,
+			       struct drm_file *file, unsigned int flags,
+			       unsigned int color, struct drm_clip_rect *clips,
+			       unsigned int num_clips)
+{
+	/* nothing to do now */
+	return 0;
+}
+
+static const struct drm_framebuffer_funcs hantro_drm_fb_funcs = {
+	.destroy = hantro_drm_fb_destroy,
+	.create_handle = hantro_drm_fb_create_handle,
+	.dirty = hantro_drm_fb_dirty,
+};
+
+static int hantro_gem_dumb_create_internal(struct drm_file *file_priv,
+					   struct drm_device *dev,
+					   struct drm_mode_create_dumb *args)
+{
+	struct drm_gem_hantro_object *cma_obj = NULL;
+	struct drm_gem_object *obj;
+	struct device_info *pdevinfo;
+	int min_pitch = DIV_ROUND_UP(args->width * args->bpp, 8);
+	unsigned int deviceidx = (args->flags & 0xf);
+	unsigned int region = (args->flags >> 0x8) & 0xf;
+	int ret = 0;
+
+	pdevinfo = get_device_info(deviceidx);
+	if (!pdevinfo)
+		return -EINVAL;
+
+	if (mutex_lock_interruptible(&dev->struct_mutex))
+		return -EBUSY;
+
+	cma_obj = kzalloc(sizeof(*cma_obj), GFP_KERNEL);
+	if (!cma_obj) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	args->handle = 0;
+	args->pitch = ALIGN(min_pitch, 64);
+	args->size = (__u64)args->pitch * (__u64)args->height;
+	args->size = PAGE_ALIGN(args->size);
+
+	obj = &cma_obj->base;
+	obj->funcs = &hantro_gem_object_funcs;
+	cma_obj->dmapriv.self = cma_obj;
+	cma_obj->num_pages = args->size >> PAGE_SHIFT;
+	cma_obj->pdevinfo = pdevinfo;
+
+	if (region == CODEC_RESERVED && pdevinfo->codec_rsvmem)
+		cma_obj->memdev = pdevinfo->codec_rsvmem;
+
+	if (!cma_obj->memdev) {
+		cma_obj->memdev = pdevinfo->dev;
+		region = PIXEL_CMA;
+	}
+
+	cma_obj->vaddr =
+		dma_alloc_coherent(cma_obj->memdev, args->size, &cma_obj->paddr,
+				   GFP_KERNEL | GFP_DMA);
+	if (region == CODEC_RESERVED && !cma_obj->vaddr) {
+		cma_obj->memdev = pdevinfo->dev;
+		region = PIXEL_CMA;
+		cma_obj->vaddr = dma_alloc_coherent(cma_obj->memdev, args->size,
+						    &cma_obj->paddr,
+						    GFP_KERNEL | GFP_DMA);
+	}
+
+	if (!cma_obj->vaddr) {
+		int used_mem[2] = { 0 }, alloc_count[2] = { 0 };
+
+		mem_usage_internal(deviceidx, pdevinfo->dev, &used_mem[0],
+				   &alloc_count[0], NULL);
+		if (pdevinfo->codec_rsvmem)
+			mem_usage_internal(deviceidx, pdevinfo->codec_rsvmem,
+					   &used_mem[1], &alloc_count[1], NULL);
+
+		__trace_hantro_err("Device %d out of memory; Requested region = %d;  CMA 0: %dK in %d allocations\n CMA 1: %dK in %d allocations",
+				   deviceidx, ((args->flags >> 0x8) & 0xf), used_mem[0] / 1024,
+				   alloc_count[0], used_mem[0] / 1024, alloc_count[0]);
+		ret = -ENOMEM;
+		goto fail_out;
+	}
+
+	obj = &cma_obj->base;
+	drm_gem_object_init(dev, obj, args->size);
+	ret = drm_gem_handle_create(file_priv, obj, &args->handle);
+	if (ret) {
+		drm_gem_object_put(obj);
+		dma_free_coherent(cma_obj->memdev, args->size, cma_obj->vaddr,
+				  cma_obj->paddr);
+		goto fail_out;
+	}
+
+	init_hantro_resv(&cma_obj->kresv, cma_obj);
+	cma_obj->handle = args->handle;
+	cma_obj->dmapriv.magic_num = HANTRO_IMAGE_VIV_META_DATA_MAGIC;
+	cma_obj->fd = -1;
+	cma_obj->dmapriv.self = cma_obj;
+	cma_obj->file_priv = file_priv;
+
+	hantro_record_mem(pdevinfo, cma_obj, args->size);
+	drm_gem_object_put(obj);
+	trace_gem_handle_create(args->handle);
+	trace_hantro_cma_alloc(deviceidx, region, (void *)cma_obj->paddr, args->handle,
+			       args->size);
+	goto out;
+
+fail_out:
+	kfree(cma_obj);
+out:
+	mutex_unlock(&dev->struct_mutex);
+	return ret;
+}
+
+static int hantro_gem_dumb_create(struct drm_device *dev, void *data,
+				  struct drm_file *file_priv)
+{
+	return hantro_gem_dumb_create_internal(file_priv, dev,
+					       (struct drm_mode_create_dumb *)data);
+}
+
+static int hantro_gem_dumb_map_offset(struct drm_file *file_priv,
+				      struct drm_device *dev, u32 handle,
+				      uint64_t *offset)
+{
+	struct drm_gem_object *obj;
+	int ret;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, handle);
+	if (!obj)
+		return -EINVAL;
+
+	ret = drm_gem_create_mmap_offset(obj);
+	if (ret == 0)
+		*offset = drm_vma_node_offset_addr(&obj->vma_node);
+
+	hantro_unref_drmobj(obj);
+	return ret;
+}
+
+static int hantro_destroy_dumb(struct drm_device *dev, void *data,
+			       struct drm_file *file_priv)
+{
+	struct drm_mode_destroy_dumb *args = data;
+	struct drm_gem_object *obj;
+
+	if (mutex_lock_interruptible(&dev->struct_mutex))
+		return -EBUSY;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, args->handle);
+	if (!obj) {
+		mutex_unlock(&dev->struct_mutex);
+		return -EINVAL;
+	}
+
+	hantro_unref_drmobj(obj);
+	drm_gem_handle_delete(file_priv, args->handle);
+	trace_gem_handle_delete(args->handle);
+	mutex_unlock(&dev->struct_mutex);
+	return 0;
+}
+
+static struct sg_table *
+hantro_gem_prime_get_sg_table(struct drm_gem_object *obj)
+{
+	struct drm_gem_hantro_object *cma_obj = to_drm_gem_hantro_obj(obj);
+	struct sg_table *sgt;
+	int ret;
+
+	sgt = kzalloc(sizeof(*sgt), GFP_KERNEL);
+	if (!sgt)
+		return NULL;
+
+	ret = dma_get_sgtable(cma_obj->memdev, sgt, cma_obj->vaddr,
+			      cma_obj->paddr, obj->size);
+	if (ret < 0)
+		goto out;
+
+	return sgt;
+out:
+	kfree(sgt);
+	return NULL;
+}
+
+static struct drm_gem_object *
+hantro_gem_prime_import_sg_table(struct drm_device *dev,
+				 struct dma_buf_attachment *attach,
+				 struct sg_table *sgt)
+{
+	struct drm_gem_hantro_object *cma_obj;
+	struct drm_gem_object *obj;
+	struct dma_buf_map map;
+	int ret;
+
+	cma_obj = kzalloc(sizeof(*cma_obj), GFP_KERNEL);
+	if (!cma_obj)
+		return ERR_PTR(-ENOMEM);
+
+	obj = &cma_obj->base;
+	obj->funcs = &hantro_gem_object_funcs;
+	if (sgt->nents > 1) {
+		/* check if the entries in the sg_table are contiguous */
+		dma_addr_t next_addr = sg_dma_address(sgt->sgl);
+		struct scatterlist *s;
+		unsigned int i;
+
+		for_each_sg(sgt->sgl, s, sgt->nents, i) {
+			/*
+			 * sg_dma_address(s) is only valid for entries
+			 * that have sg_dma_len(s) != 0
+			 */
+			if (!sg_dma_len(s))
+				continue;
+
+			if (sg_dma_address(s) != next_addr) {
+				kfree(cma_obj);
+				return ERR_PTR(-EINVAL);
+			}
+
+			next_addr = sg_dma_address(s) + sg_dma_len(s);
+		}
+	}
+
+	if (drm_gem_object_init(dev, obj, attach->dmabuf->size) != 0) {
+		__trace_hantro_err("import sg table failed");
+		kfree(cma_obj);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	cma_obj->paddr = sg_dma_address(sgt->sgl);
+	ret = dma_buf_vmap(attach->dmabuf, &map);
+	if (ret)
+		return ERR_PTR(ret);
+
+	cma_obj->vaddr = map.vaddr;
+	cma_obj->sgt = sgt;
+	cma_obj->flag |= HANTRO_GEM_FLAG_FOREIGN_IMPORTED;
+	cma_obj->num_pages = attach->dmabuf->size >> PAGE_SHIFT;
+	cma_obj->dmapriv.meta_data_info =
+		*((struct viv_vidmem_metadata *)attach->dmabuf->priv);
+	cma_obj->dmapriv.self = cma_obj;
+	cma_obj->dmapriv.magic_num = HANTRO_IMAGE_VIV_META_DATA_MAGIC;
+	return obj;
+}
+
+static int hantro_gem_prime_vmap(struct drm_gem_object *obj, struct dma_buf_map *map)
+{
+	struct drm_gem_hantro_object *cma_obj = to_drm_gem_hantro_obj(obj);
+
+	dma_buf_map_set_vaddr(map, cma_obj->vaddr);
+	return 0;
+}
+
+static void hantro_gem_prime_vunmap(struct drm_gem_object *obj, struct dma_buf_map *map)
+{
+}
+
+static int hantro_gem_prime_mmap(struct drm_gem_object *obj,
+				 struct vm_area_struct *vma)
+{
+	struct drm_gem_hantro_object *cma_obj;
+	unsigned long page_num = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	int ret = 0;
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	if (page_num > cma_obj->num_pages)
+		return -EINVAL;
+
+	if ((unsigned long)cma_obj->vaddr == 0)
+		return -EINVAL;
+
+	ret = drm_gem_mmap_obj(obj, obj->size, vma);
+	if (ret < 0)
+		return ret;
+
+	vma->vm_flags &= ~VM_PFNMAP;
+	vma->vm_pgoff = 0;
+	if (mutex_lock_interruptible(&hantro_drm.drm_dev->struct_mutex))
+		return -EBUSY;
+
+	if (dma_mmap_coherent(obj->dev->dev, vma, cma_obj->vaddr,
+			      cma_obj->paddr, vma->vm_end - vma->vm_start)) {
+		drm_gem_vm_close(vma);
+		mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+		return -EAGAIN;
+	}
+
+	mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+	vma->vm_private_data = cma_obj;
+	return ret;
+}
+
+static void hantro_gem_free_object(struct drm_gem_object *gem_obj)
+{
+	struct drm_gem_hantro_object *cma_obj;
+	struct device_info *pdevinfo;
+	struct dma_buf_map map;
+
+	/*
+	 * dma buf imported from others,
+	 * release data structures allocated by ourselves
+	 */
+	cma_obj = to_drm_gem_hantro_obj(gem_obj);
+	if (cma_obj->pages) {
+		int i;
+
+		for (i = 0; i < cma_obj->num_pages; i++)
+			unref_page(cma_obj->pages[i]);
+
+		kfree(cma_obj->pages);
+		cma_obj->pages = NULL;
+	}
+
+	map.vaddr = cma_obj->vaddr;
+	map.is_iomem = false;
+
+	if (gem_obj->import_attach) {
+		if (cma_obj->vaddr)
+			dma_buf_vunmap(gem_obj->import_attach->dmabuf,
+				       &map);
+
+		drm_prime_gem_destroy(gem_obj, cma_obj->sgt);
+	} else if (cma_obj->vaddr) {
+		pdevinfo = cma_obj->pdevinfo;
+		if (!pdevinfo)
+			return;
+
+		dma_free_coherent(cma_obj->memdev, cma_obj->base.size,
+				  cma_obj->vaddr, cma_obj->paddr);
+		hantro_unrecord_mem(cma_obj->pdevinfo, cma_obj);
+		trace_hantro_cma_free(pdevinfo->deviceid, (void *)cma_obj->paddr,
+				      cma_obj->handle);
+	}
+
+	drm_gem_object_release(gem_obj);
+	kfree(cma_obj);
+}
+
+static int hantro_gem_close(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct drm_gem_close *args = data;
+	struct drm_gem_object *obj =
+		hantro_gem_object_lookup(dev, file_priv, args->handle);
+	int ret = 0;
+
+	if (!obj)
+		return -EINVAL;
+
+	ret = drm_gem_handle_delete(file_priv, args->handle);
+	trace_gem_handle_delete(args->handle);
+	hantro_unref_drmobj(obj);
+	return ret;
+}
+
+static int hantro_gem_open(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	int ret;
+	u32 handle;
+	struct drm_gem_open *openarg;
+	struct drm_gem_object *obj;
+
+	openarg = (struct drm_gem_open *)data;
+	obj = idr_find(&dev->object_name_idr, (int)openarg->name);
+	if (obj)
+		hantro_ref_drmobj(obj);
+	else
+		return -ENOENT;
+
+	ret = drm_gem_handle_create(file_priv, obj, &handle);
+	trace_gem_handle_create(handle);
+	hantro_unref_drmobj(obj);
+	if (ret)
+		return ret;
+
+	openarg->handle = handle;
+	openarg->size = obj->size;
+	return ret;
+}
+
+static int hantro_map_vaddr(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct hantro_addrmap *pamap = data;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, pamap->handle);
+	if (!obj)
+		return -EINVAL;
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	pamap->vm_addr = (unsigned long long)cma_obj->vaddr;
+	pamap->phy_addr = cma_obj->paddr;
+	hantro_unref_drmobj(obj);
+	return 0;
+}
+
+static int hantro_get_hwcfg(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct device_info *pdevinfo;
+	u32 config = 0;
+	int i;
+
+	for (i = 0; i < get_device_count(); i++) {
+		pdevinfo = get_device_info(i);
+		config |= pdevinfo->config;
+	}
+
+	return config;
+}
+
+static int hantro_get_devicenum(struct drm_device *dev, void *data,
+				struct drm_file *file_priv)
+{
+	return get_device_count();
+}
+
+static int hantro_add_client(struct drm_device *dev, void *data,
+			     struct drm_file *file_priv)
+{
+	int ret;
+	struct hantro_client *attrib = data;
+	struct file_data *priv_data =
+		(struct file_data *)file_priv->driver_priv;
+	struct hantro_client *client = NULL;
+	struct device_info *pdevinfo;
+
+	if (!data)
+		return -EINVAL;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		return -ENOMEM;
+
+	pdevinfo = get_device_info(attrib->deviceid);
+	if (!pdevinfo)
+		return -EINVAL;
+
+	client->clientid = attrib->clientid;
+	client->deviceid = attrib->deviceid;
+	client->codec = attrib->codec;
+	client->profile = attrib->profile;
+	client->height = attrib->height;
+	client->width = attrib->width;
+	client->file = file_priv;
+
+	mutex_lock(&pdevinfo->alloc_mutex);
+	ret = idr_alloc(&pdevinfo->clients, client, 1, 0, GFP_KERNEL);
+	mutex_unlock(&pdevinfo->alloc_mutex);
+	trace_client_add((void *)priv_data, attrib->deviceid, attrib->clientid,
+			 attrib->codec, attrib->profile, attrib->width,
+			 attrib->height);
+	return (ret > 0 ? 0 : -ENOMEM);
+}
+
+static int hantro_remove_client(struct drm_device *dev, void *data,
+				struct drm_file *file_priv)
+{
+	struct hantro_client *attrib = data;
+	struct file_data *priv_data =
+		(struct file_data *)file_priv->driver_priv;
+	struct hantro_client *client = NULL;
+	struct device_info *pdevinfo;
+	int id;
+
+	if (!data)
+		return -EINVAL;
+
+	pdevinfo = get_device_info(attrib->deviceid);
+	if (!pdevinfo)
+		return -EINVAL;
+
+	mutex_lock(&pdevinfo->alloc_mutex);
+	idr_for_each_entry(&pdevinfo->clients, client, id) {
+		if (client && client->clientid == attrib->clientid &&
+		    client->file == file_priv) {
+			idr_remove(&pdevinfo->clients, id);
+			kfree(client);
+			break;
+		}
+	}
+
+	trace_client_remove((void *)priv_data, attrib->clientid);
+	mutex_unlock(&pdevinfo->alloc_mutex);
+	return 0;
+}
+
+static int hantro_gem_flink(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct drm_gem_flink *args = data;
+	struct drm_gem_object *obj;
+	int ret;
+
+	if (!drm_core_check_feature(dev, DRIVER_GEM))
+		return -ENODEV;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, args->handle);
+	if (!obj)
+		return -ENOENT;
+
+	mutex_lock(&dev->object_name_lock);
+	/* prevent races with concurrent gem_close. */
+	if (obj->handle_count == 0) {
+		ret = -ENOENT;
+		goto err;
+	}
+
+	if (!obj->name) {
+		ret = idr_alloc(&dev->object_name_idr, obj, 1, 0, GFP_KERNEL);
+		if (ret < 0)
+			goto err;
+
+		obj->name = ret;
+	}
+
+	args->name = (uint64_t)obj->name;
+	ret = 0;
+err:
+	mutex_unlock(&dev->object_name_lock);
+	hantro_unref_drmobj(obj);
+	return ret;
+}
+
+static int hantro_map_dumb(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_mode_map_dumb *temparg = (struct drm_mode_map_dumb *)data;
+
+	ret = hantro_gem_dumb_map_offset(file_priv, dev, temparg->handle,
+					 &temparg->offset);
+	return ret;
+}
+
+static int hantro_drm_open(struct drm_device *dev, struct drm_file *file)
+{
+	trace_drm_file_open((void *)dev, (void *)file);
+	return 0;
+}
+
+/*
+ * This function is used to treat abnormal condition such as Ctrl^c or assert.
+ * We can't release memory or drm resources in normal way.
+ * In kernel document it's suggested driver_priv
+ * be used in this call back. In abnormal situation many kernel data
+ * structures might be unavailable, e.g. hantro_gem_object_lookup is not
+ * working. So we have to save every gem obj info by ourselves.
+ */
+static void hantro_drm_postclose(struct drm_device *dev, struct drm_file *file)
+{
+}
+
+static struct drm_gem_object *
+hantro_drm_gem_prime_import(struct drm_device *dev, struct dma_buf *dma_buf)
+{
+	struct device *attach_dev = dev->dev;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj;
+	int ret;
+
+	if (dma_buf->ops == &hantro_dmabuf_ops) {
+		obj = hantro_get_gem_from_dmabuf(dma_buf);
+		if (obj && obj->dev == dev) {
+			drm_gem_object_get(obj);
+			return obj;
+		}
+	}
+
+	if (!dev->driver->gem_prime_import_sg_table)
+		return ERR_PTR(-EINVAL);
+
+	attach = dma_buf_attach(dma_buf, attach_dev);
+	if (IS_ERR(attach))
+		return ERR_CAST(attach);
+
+	get_dma_buf(dma_buf);
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		ret = PTR_ERR(sgt);
+		goto fail_detach;
+	}
+
+	obj = dev->driver->gem_prime_import_sg_table(dev, attach, sgt);
+	if (IS_ERR(obj)) {
+		ret = PTR_ERR(obj);
+		goto fail_unmap;
+	}
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	if (!attach->priv)
+		attach->priv = &cma_obj->dmapriv.meta_data_info;
+	else
+		cma_obj->dmapriv.meta_data_info.magic = 0;
+
+	obj->import_attach = attach;
+	obj->resv = dma_buf->resv;
+	return obj;
+
+fail_unmap:
+	dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+fail_detach:
+	dma_buf_detach(dma_buf, attach);
+	dma_buf_put(dma_buf);
+
+	return ERR_PTR(ret);
+}
+
+static struct dma_buf *hantro_prime_export(struct drm_gem_object *obj,
+					   int flags)
+{
+	struct dma_buf *dma_buf;
+	struct drm_gem_hantro_object *cma_obj;
+	struct drm_device *dev = obj->dev;
+	struct dma_buf_export_info exp_info = {
+		.exp_name = KBUILD_MODNAME,
+		.owner = dev->driver->fops->owner,
+		.ops = &hantro_dmabuf_ops,
+		.flags = flags,
+	};
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	exp_info.resv = &cma_obj->kresv;
+	exp_info.size = cma_obj->num_pages << PAGE_SHIFT;
+	exp_info.priv = &cma_obj->dmapriv.meta_data_info;
+
+	dma_buf = dma_buf_export(&exp_info);
+	if (IS_ERR(dma_buf))
+		return dma_buf;
+
+	drm_dev_get(dev);
+	drm_gem_object_get(&cma_obj->base);
+	trace_prime_dmabuf_export((void *)cma_obj->paddr, cma_obj->handle,
+				  (void *)dma_buf);
+	return dma_buf;
+}
+
+static int hantro_handle_to_fd(struct drm_device *dev, void *data,
+			       struct drm_file *file_priv)
+{
+	int ret;
+	struct drm_prime_handle *primeargs = (struct drm_prime_handle *)data;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, primeargs->handle);
+	if (!obj) {
+		__trace_hantro_err("handle not found!! (handle = %-3d, fd = %-3d,)",
+				   primeargs->handle, -1);
+		return -ENOENT;
+	}
+
+	ret = drm_gem_prime_handle_to_fd(dev, file_priv, primeargs->handle,
+					 primeargs->flags, &primeargs->fd);
+
+	if (ret == 0) {
+		cma_obj = to_drm_gem_hantro_obj(obj);
+		cma_obj->flag |= HANTRO_GEM_FLAG_EXPORT;
+		cma_obj->fd = primeargs->fd;
+	}
+
+	trace_prime_handle_to_fd(obj, primeargs->handle, primeargs->fd, ret);
+	hantro_unref_drmobj(obj);
+	return ret;
+}
+
+static int hantro_fd_to_handle(struct drm_device *dev, void *data,
+			       struct drm_file *file_priv)
+{
+	struct drm_prime_handle *primeargs = (struct drm_prime_handle *)data;
+	s32 ret = 0;
+
+	primeargs->flags = 0;
+	ret = drm_gem_prime_fd_to_handle(dev, file_priv, primeargs->fd,
+					 &primeargs->handle);
+
+	trace_prime_fd_to_handle(primeargs->fd, primeargs->handle, ret);
+	return ret;
+}
+
+static int hantro_fb_create2(struct drm_device *dev, void *data,
+			     struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd2 *mode_cmd = (struct drm_mode_fb_cmd2 *)data;
+	struct hantro_drm_fb *vsifb;
+	struct drm_gem_object *objs[4];
+	struct drm_gem_object *obj;
+	const struct drm_format_info *info = drm_get_format_info(dev, mode_cmd);
+	unsigned int hsub;
+	unsigned int vsub;
+	int num_planes;
+	int ret;
+	int i;
+
+	hsub = info->hsub;
+	vsub = info->vsub;
+	num_planes = min_t(int, info->num_planes, 4);
+	for (i = 0; i < num_planes; i++) {
+		unsigned int width = mode_cmd->width / (i ? hsub : 1);
+		unsigned int height = mode_cmd->height / (i ? vsub : 1);
+		unsigned int min_size;
+
+		obj = hantro_gem_object_lookup(dev, file_priv,
+					       mode_cmd->handles[i]);
+		if (!obj) {
+			ret = -ENXIO;
+			goto err_gem_object_unreference;
+		}
+
+		hantro_unref_drmobj(obj);
+		min_size = (height - 1) * mode_cmd->pitches[i] +
+			   mode_cmd->offsets[i] + width * info->cpp[i];
+
+		if (obj->size < min_size) {
+			ret = -EINVAL;
+			goto err_gem_object_unreference;
+		}
+
+		objs[i] = obj;
+	}
+
+	vsifb = kzalloc(sizeof(*vsifb), GFP_KERNEL);
+	if (!vsifb)
+		return -ENOMEM;
+
+	drm_helper_mode_fill_fb_struct(dev, &vsifb->fb, mode_cmd);
+	for (i = 0; i < num_planes; i++)
+		vsifb->obj[i] = objs[i];
+
+	ret = drm_framebuffer_init(dev, &vsifb->fb, &hantro_drm_fb_funcs);
+	if (ret)
+		kfree(vsifb);
+
+	return ret;
+
+err_gem_object_unreference:
+	return ret;
+}
+
+static int hantro_fb_create(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct drm_mode_fb_cmd *or = data;
+	struct drm_mode_fb_cmd2 r = {};
+	int ret;
+
+	/* convert to new format and call new ioctl */
+	r.fb_id = or->fb_id;
+	r.width = or->width;
+	r.height = or->height;
+	r.pitches[0] = or->pitch;
+	r.pixel_format = drm_mode_legacy_fb_format(or->bpp, or->depth);
+	r.handles[0] = or->handle;
+
+	ret = hantro_fb_create2(dev, &r, file_priv);
+	if (ret)
+		return ret;
+
+	or->fb_id = r.fb_id;
+	return 0;
+}
+
+static int hantro_get_version(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	struct drm_version *pversion;
+	char *sname = DRIVER_NAME;
+	char *sdesc = DRIVER_DESC;
+	char *sdate = DRIVER_DATE;
+
+	pversion = (struct drm_version *)data;
+	pversion->version_major = dev->driver->major;
+	pversion->version_minor = dev->driver->minor;
+	pversion->version_patchlevel = 0;
+	pversion->name_len = strlen(DRIVER_NAME);
+	pversion->desc_len = strlen(DRIVER_DESC);
+	pversion->date_len = strlen(DRIVER_DATE);
+	if (pversion->name)
+		if (copy_to_user(pversion->name, sname, pversion->name_len))
+			return -EFAULT;
+
+	if (pversion->date)
+		if (copy_to_user(pversion->date, sdate, pversion->date_len))
+			return -EFAULT;
+
+	if (pversion->desc)
+		if (copy_to_user(pversion->desc, sdesc, pversion->desc_len))
+			return -EFAULT;
+
+	return 0;
+}
+
+static int hantro_get_cap(struct drm_device *dev, void *data,
+			  struct drm_file *file_priv)
+{
+	struct drm_get_cap *req = (struct drm_get_cap *)data;
+
+	req->value = 0;
+	/* some values should be reset */
+	switch (req->capability) {
+	case DRM_CAP_PRIME:
+		req->value |= dev->driver->prime_fd_to_handle ?
+				      DRM_PRIME_CAP_IMPORT :
+				      0;
+		req->value |= dev->driver->prime_handle_to_fd ?
+				      DRM_PRIME_CAP_EXPORT :
+				      0;
+		return 0;
+	case DRM_CAP_DUMB_BUFFER:
+		req->value = 1;
+		break;
+	case DRM_CAP_VBLANK_HIGH_CRTC:
+		req->value = 1;
+		break;
+	case DRM_CAP_DUMB_PREFERRED_DEPTH:
+		req->value = dev->mode_config.preferred_depth;
+		break;
+	case DRM_CAP_DUMB_PREFER_SHADOW:
+		req->value = dev->mode_config.prefer_shadow;
+		break;
+	case DRM_CAP_ASYNC_PAGE_FLIP:
+		req->value = dev->mode_config.async_page_flip;
+		break;
+	case DRM_CAP_CURSOR_WIDTH:
+		if (dev->mode_config.cursor_width)
+			req->value = dev->mode_config.cursor_width;
+		else
+			req->value = 64;
+
+		break;
+	case DRM_CAP_CURSOR_HEIGHT:
+		if (dev->mode_config.cursor_height)
+			req->value = dev->mode_config.cursor_height;
+		else
+			req->value = 64;
+
+		break;
+	case DRM_CAP_ADDFB2_MODIFIERS:
+		req->value = dev->mode_config.allow_fb_modifiers;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/* just a test API for any purpose */
+static int hantro_test(struct drm_device *dev, void *data,
+		       struct drm_file *file_priv)
+{
+	unsigned int *input = data;
+	int handle = *input;
+	struct drm_gem_object *obj;
+	struct dma_fence *pfence;
+	int ret = 10 * HZ; /* timeout */
+
+	obj = hantro_gem_object_lookup(dev, file_priv, handle);
+	if (!obj)
+		return -EINVAL;
+
+	pfence = dma_resv_excl_fence(obj->dma_buf->resv);
+	while (ret > 0)
+		ret = schedule_timeout(ret);
+
+	hantro_fence_signal(pfence);
+	hantro_unref_drmobj(obj);
+	return 0;
+}
+
+static int hantro_getprimeaddr(struct drm_device *dev, void *data,
+			       struct drm_file *file_priv)
+{
+	unsigned long *input = data;
+	int fd = *input;
+	struct drm_gem_hantro_object *cma_obj;
+	struct dma_buf *dma_buf;
+
+	dma_buf = dma_buf_get(fd);
+	if (IS_ERR(dma_buf))
+		return PTR_ERR(dma_buf);
+
+	cma_obj = (struct drm_gem_hantro_object *)dma_buf->priv;
+	*input = cma_obj->paddr;
+	dma_buf_put(dma_buf);
+	trace_prime_dmabuf_put((void *)cma_obj->paddr, (void *)dma_buf, fd);
+	return 0;
+}
+
+static int hantro_ptr_to_phys(struct drm_device *dev, void *data,
+			      struct drm_file *file_priv)
+{
+	unsigned long *arg = data;
+	struct vm_area_struct *vma;
+	struct drm_gem_hantro_object *cma_obj;
+	unsigned long vaddr = *arg;
+
+	vma = find_vma(current->mm, vaddr);
+	if (!vma)
+		return -EFAULT;
+
+	cma_obj = (struct drm_gem_hantro_object *)vma->vm_private_data;
+	if (!cma_obj)
+		return -EFAULT;
+
+	if (cma_obj->base.dev != dev)
+		return -EFAULT;
+
+	if (vaddr < vma->vm_start ||
+	    vaddr >= vma->vm_start + (cma_obj->num_pages << PAGE_SHIFT))
+		return -EFAULT;
+
+	*arg = (phys_addr_t)(vaddr - vma->vm_start) + cma_obj->paddr;
+	return 0;
+}
+
+static int hantro_getmagic(struct drm_device *dev, void *data,
+			   struct drm_file *file_priv)
+{
+	struct drm_auth *auth = data;
+	int ret = 0;
+
+	mutex_lock(&dev->struct_mutex);
+	if (!file_priv->magic) {
+		ret = idr_alloc(&file_priv->master->magic_map, file_priv, 1, 0,
+				GFP_KERNEL);
+		if (ret >= 0)
+			file_priv->magic = ret;
+	}
+
+	auth->magic = file_priv->magic;
+	mutex_unlock(&dev->struct_mutex);
+	return ret < 0 ? ret : 0;
+}
+
+static int hantro_authmagic(struct drm_device *dev, void *data,
+			    struct drm_file *file_priv)
+{
+	struct drm_auth *auth = data;
+	struct drm_file *file;
+
+	mutex_lock(&dev->struct_mutex);
+	file = idr_find(&file_priv->master->magic_map, auth->magic);
+	if (file) {
+		file->authenticated = 1;
+		idr_replace(&file_priv->master->magic_map, NULL, auth->magic);
+	}
+
+	mutex_unlock(&dev->struct_mutex);
+	return file ? 0 : -EINVAL;
+}
+
+#define DRM_IOCTL_DEF(ioctl, _func, _flags)                                    \
+	[DRM_IOCTL_NR(ioctl)] = {                                              \
+		.cmd = ioctl, .func = _func, .flags = _flags, .name = #ioctl   \
+	}
+
+/* Ioctl table */
+static const struct drm_ioctl_desc hantro_ioctls[] = {
+	DRM_IOCTL_DEF(DRM_IOCTL_VERSION, hantro_get_version,
+		      DRM_UNLOCKED | DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_UNIQUE, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_MAGIC, hantro_getmagic, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_IRQ_BUSID, drm_invalid_op,
+		      DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_MAP, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_CLIENT, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_STATS, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_CAP, hantro_get_cap,
+		      DRM_UNLOCKED | DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_CLIENT_CAP, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_VERSION, drm_invalid_op,
+		      DRM_UNLOCKED | DRM_MASTER),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_UNIQUE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_BLOCK, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_UNBLOCK, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AUTH_MAGIC, hantro_authmagic,
+		      DRM_AUTH | DRM_UNLOCKED | DRM_MASTER),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_ADD_MAP, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_RM_MAP, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_SAREA_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_SAREA_CTX, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_SET_MASTER, drm_invalid_op,
+		      DRM_UNLOCKED | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_DROP_MASTER, drm_invalid_op,
+		      DRM_UNLOCKED | DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_ADD_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_RM_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MOD_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_GET_CTX, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_SWITCH_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_NEW_CTX, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_RES_CTX, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_ADD_DRAW, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_RM_DRAW, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_LOCK, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_UNLOCK, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_FINISH, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_ADD_BUFS, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_MARK_BUFS, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_INFO_BUFS, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_MAP_BUFS, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_FREE_BUFS, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_DMA, drm_invalid_op, DRM_AUTH),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_CONTROL, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+
+#if IS_ENABLED(CONFIG_AGP)
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_ACQUIRE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_RELEASE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_ENABLE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_INFO, drm_invalid_op, DRM_AUTH),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_ALLOC, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_FREE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_BIND, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_AGP_UNBIND, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+#endif
+
+	DRM_IOCTL_DEF(DRM_IOCTL_SG_ALLOC, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+	DRM_IOCTL_DEF(DRM_IOCTL_SG_FREE, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_WAIT_VBLANK, drm_invalid_op, DRM_UNLOCKED),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MODESET_CTL, drm_invalid_op, 0),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_UPDATE_DRAW, drm_invalid_op,
+		      DRM_AUTH | DRM_MASTER | DRM_ROOT_ONLY),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_GEM_CLOSE, hantro_gem_close,
+		      DRM_UNLOCKED | DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_GEM_FLINK, hantro_gem_flink,
+		      DRM_AUTH | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_GEM_OPEN, hantro_gem_open,
+		      DRM_AUTH | DRM_UNLOCKED),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETRESOURCES, drm_invalid_op,
+		      DRM_UNLOCKED),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_PRIME_HANDLE_TO_FD, hantro_handle_to_fd,
+		      DRM_AUTH | DRM_UNLOCKED | DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF(DRM_IOCTL_PRIME_FD_TO_HANDLE, hantro_fd_to_handle,
+		      DRM_AUTH | DRM_UNLOCKED | DRM_RENDER_ALLOW),
+
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANERESOURCES, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCRTC, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETCRTC, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPLANE, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETPLANE, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CURSOR, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETGAMMA, drm_invalid_op, DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETGAMMA, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETENCODER, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETCONNECTOR, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATTACHMODE, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DETACHMODE, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPERTY, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_SETPROPERTY, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETPROPBLOB, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_GETFB, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ADDFB, hantro_fb_create,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ADDFB2, hantro_fb_create2,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_RMFB, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_PAGE_FLIP, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DIRTYFB, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CREATE_DUMB, hantro_gem_dumb_create,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_MAP_DUMB, hantro_map_dumb,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DESTROY_DUMB, hantro_destroy_dumb,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_OBJ_GETPROPERTIES, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_OBJ_SETPROPERTY, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CURSOR2, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_ATOMIC, drm_invalid_op,
+		      DRM_MASTER | DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_CREATEPROPBLOB, drm_invalid_op,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_MODE_DESTROYPROPBLOB, drm_invalid_op,
+		      DRM_UNLOCKED),
+
+	/*hantro specific ioctls*/
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_TESTCMD, hantro_test,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_GETPADDR, hantro_map_vaddr,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_HWCFG, hantro_get_hwcfg,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_TESTREADY, hantro_testbufvalid,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_SETDOMAIN, hantro_setdomain,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_ACQUIREBUF, hantro_acquirebuf,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_RELEASEBUF, hantro_releasebuf,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_GETPRIMEADDR, hantro_getprimeaddr,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_PTR_PHYADDR, hantro_ptr_to_phys,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_GET_DEVICENUM, hantro_get_devicenum,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_ADD_CLIENT, hantro_add_client,
+		      DRM_UNLOCKED),
+	DRM_IOCTL_DEF(DRM_IOCTL_HANTRO_REMOVE_CLIENT, hantro_remove_client,
+		      DRM_UNLOCKED),
+};
+
+#define HANTRO_IOCTL_COUNT ARRAY_SIZE(hantro_ioctls)
+static long hantro_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct drm_file *file_priv = file->private_data;
+	struct drm_device *dev = hantro_drm.drm_dev;
+	const struct drm_ioctl_desc *ioctl = NULL;
+	drm_ioctl_t *func;
+	unsigned int nr = DRM_IOCTL_NR(cmd);
+	int retcode = 0;
+	char stack_kdata[256];
+	char *kdata = stack_kdata;
+	unsigned int in_size, out_size;
+
+	if (drm_dev_is_unplugged(dev))
+		return -ENODEV;
+
+	out_size = _IOC_SIZE(cmd);
+	in_size = _IOC_SIZE(cmd);
+	if (in_size > 0) {
+		if (_IOC_DIR(cmd) & _IOC_READ)
+			retcode = !hantro_access_ok(VERIFY_WRITE, (void const __user *)arg,
+						    in_size);
+		else if (_IOC_DIR(cmd) & _IOC_WRITE)
+			retcode = !hantro_access_ok(VERIFY_READ, (void const __user *)arg,
+						    in_size);
+		if (retcode)
+			return -EFAULT;
+	}
+	if (nr >= DRM_IOCTL_NR(HX280ENC_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HX280ENC_IOC_END)) {
+		if (enable_encode)
+			return hantroenc_ioctl(file, cmd, arg);
+
+		if (cmd == HX280ENC_IOCG_CORE_NUM) {
+			int corenum = 0;
+
+			__put_user(corenum, (unsigned int __user *)arg);
+		} else {
+			return -EFAULT;
+		}
+	}
+
+	if (nr >= DRM_IOCTL_NR(HANTRODEC_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HANTRODEC_IOC_END)) {
+		return hantrodec_ioctl(file, cmd, arg);
+	}
+
+	if (nr >= DRM_IOCTL_NR(HANTROCACHE_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HANTROCACHE_IOC_END)) {
+		return hantrocache_ioctl(file, cmd, arg);
+	}
+
+	if (nr >= DRM_IOCTL_NR(HANTRODEC400_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HANTRODEC400_IOC_END)) {
+		return hantrodec400_ioctl(file, cmd, arg);
+	}
+
+	if (nr >= DRM_IOCTL_NR(HANTRODEVICE_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HANTRODEVICE_IOC_END)) {
+		return hantrodevice_ioctl(file, cmd, arg);
+	}
+
+	if (nr >= DRM_IOCTL_NR(HANTROMETADATA_IOC_START) &&
+	    nr <= DRM_IOCTL_NR(HANTROMETADATA_IOC_END)) {
+		return hantrometadata_ioctl(file, cmd, arg);
+	}
+
+	if (nr >= HANTRO_IOCTL_COUNT)
+		return -EINVAL;
+
+	ioctl = &hantro_ioctls[nr];
+
+	if (copy_from_user(kdata, (void __user *)arg, in_size) != 0)
+		return -EFAULT;
+
+	if (cmd == DRM_IOCTL_MODE_SETCRTC ||
+	    cmd == DRM_IOCTL_MODE_GETRESOURCES ||
+	    cmd == DRM_IOCTL_SET_CLIENT_CAP || cmd == DRM_IOCTL_MODE_GETCRTC ||
+	    cmd == DRM_IOCTL_MODE_GETENCODER ||
+	    cmd == DRM_IOCTL_MODE_GETCONNECTOR || cmd == DRM_IOCTL_MODE_GETFB) {
+		retcode = drm_ioctl(file, cmd, arg);
+		return retcode;
+	}
+
+	func = ioctl->func;
+	if (!func)
+		return -EINVAL;
+
+	retcode = func(dev, kdata, file_priv);
+
+	if (copy_to_user((void __user *)arg, kdata, out_size) != 0)
+		retcode = -EFAULT;
+
+	return retcode;
+}
+
+static int hantro_device_open(struct inode *inode, struct file *file)
+{
+	int ret;
+
+	ret = drm_open(inode, file);
+	hantrodec_open(inode, file);
+	hantrocache_open(inode, file);
+	return ret;
+}
+
+static int hantro_device_release(struct inode *inode, struct file *file)
+{
+	hantrodec_release(file);
+	hantroenc_release();
+	hantrocache_release(file);
+	return drm_release(inode, file);
+}
+
+static int hantro_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int ret = 0;
+	struct drm_gem_object *obj = NULL;
+	struct drm_gem_hantro_object *cma_obj;
+	struct drm_vma_offset_node *node;
+	unsigned long page_num = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+	unsigned long address = 0;
+	struct page **pages = NULL;
+	struct device_info *pdevinfo;
+	struct device *dev;
+
+	if (mutex_lock_interruptible(&hantro_drm.drm_dev->struct_mutex))
+		return -EBUSY;
+
+	drm_vma_offset_lock_lookup(hantro_drm.drm_dev->vma_offset_manager);
+	node = drm_vma_offset_exact_lookup_locked(hantro_drm.drm_dev->vma_offset_manager,
+						  vma->vm_pgoff, vma_pages(vma));
+
+	if (likely(node)) {
+		obj = container_of(node, struct drm_gem_object, vma_node);
+		if (!kref_get_unless_zero(&obj->refcount))
+			obj = NULL;
+	}
+
+	drm_vma_offset_unlock_lookup(hantro_drm.drm_dev->vma_offset_manager);
+	if (!obj) {
+		mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+		return -EINVAL;
+	}
+
+	hantro_unref_drmobj(obj);
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	if (page_num > cma_obj->num_pages) {
+		mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+		return -EINVAL;
+	}
+
+	if ((cma_obj->flag & HANTRO_GEM_FLAG_IMPORT) == 0) {
+		pdevinfo = cma_obj->pdevinfo;
+		if (!pdevinfo) {
+			mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+			return -EINVAL;
+		}
+
+		dev = cma_obj->memdev;
+	} else {
+		dev = obj->dev->dev;
+	}
+
+	if ((cma_obj->flag & HANTRO_GEM_FLAG_IMPORT) == 0) {
+		address = (unsigned long)cma_obj->vaddr;
+		if (address == 0) {
+			mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+			return -EINVAL;
+		}
+
+		ret = drm_gem_mmap_obj(obj,
+				       drm_vma_node_size(node) << PAGE_SHIFT, vma);
+
+		if (ret) {
+			mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+			return ret;
+		}
+	} else {
+		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+		/* else mmap report uncached error for some importer, e.g. i915 */
+	}
+
+	vma->vm_pgoff = 0;
+	if (dma_mmap_coherent(dev, vma, cma_obj->vaddr, cma_obj->paddr,
+			      page_num << PAGE_SHIFT)) {
+		__trace_hantro_err("unable to map memory; paddr = %p, handle = %d", cma_obj->paddr,
+				   cma_obj->handle);
+		mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+		return -EAGAIN;
+	}
+
+	vma->vm_private_data = cma_obj;
+	cma_obj->pages = pages;
+	mutex_unlock(&hantro_drm.drm_dev->struct_mutex);
+	return ret;
+}
+
+/* VFS methods */
+static const struct file_operations hantro_fops = {
+	.owner = THIS_MODULE,
+	.open = hantro_device_open,
+	.mmap = hantro_mmap,
+	.release = hantro_device_release,
+	.poll = drm_poll,
+	.read = drm_read,
+	.unlocked_ioctl = hantro_ioctl,
+	.compat_ioctl = drm_compat_ioctl,
+};
+
+static void hantro_gem_vm_close(struct vm_area_struct *vma)
+{
+	int i;
+	struct drm_gem_hantro_object *obj =
+		(struct drm_gem_hantro_object *)vma->vm_private_data;
+	/* unmap callback */
+
+	if (obj->pages) {
+		for (i = 0; i < obj->num_pages; i++)
+			unref_page(obj->pages[i]);
+
+		kfree(obj->pages);
+		obj->pages = NULL;
+	}
+
+	drm_gem_vm_close(vma);
+}
+
+static void hantro_release(struct drm_device *dev)
+{
+}
+
+static int hantro_gem_prime_handle_to_fd(struct drm_device *dev,
+					 struct drm_file *file, u32 handle,
+					 u32 flags, int *prime_fd)
+{
+	int ret;
+
+	ret = drm_gem_prime_handle_to_fd(dev, file, handle, flags, prime_fd);
+	trace_prime_handle_to_fd(NULL, handle, *prime_fd, ret);
+	return ret;
+}
+
+static vm_fault_t hantro_vm_fault(struct vm_fault *vmf)
+{
+	return VM_FAULT_SIGBUS;
+}
+
+static const struct vm_operations_struct hantro_drm_gem_cma_vm_ops = {
+	.open = drm_gem_vm_open,
+	.close = hantro_gem_vm_close,
+	.fault = hantro_vm_fault,
+};
+
+/* temp no usage now */
+static u32 hantro_vblank_no_hw_counter(struct drm_device *dev,
+				       unsigned int pipe)
+{
+	return 0;
+}
+
+static struct drm_driver hantro_drm_driver = {
+	/* these two are related with controlD and renderD */
+	.driver_features = DRIVER_GEM | DRIVER_RENDER,
+	.get_vblank_counter = hantro_vblank_no_hw_counter,
+	.open = hantro_drm_open,
+	.postclose = hantro_drm_postclose,
+	.release = hantro_release,
+	.dumb_create = hantro_gem_dumb_create_internal,
+	.dumb_map_offset = hantro_gem_dumb_map_offset,
+	.gem_prime_import = hantro_drm_gem_prime_import,
+	.prime_handle_to_fd = hantro_gem_prime_handle_to_fd,
+	.prime_fd_to_handle = drm_gem_prime_fd_to_handle,
+	.gem_prime_import_sg_table = hantro_gem_prime_import_sg_table,
+	.gem_prime_mmap = hantro_gem_prime_mmap,
+	.fops = &hantro_fops,
+	.name = DRIVER_NAME,
+	.desc = DRIVER_DESC,
+	.date = DRIVER_DATE,
+	.major = DRIVER_MAJOR,
+	.minor = DRIVER_MINOR,
+};
+
+static struct drm_gem_object_funcs hantro_gem_object_funcs = {
+	.free = hantro_gem_free_object,
+	.export = hantro_prime_export,
+	.get_sg_table = hantro_gem_prime_get_sg_table,
+	.vmap = hantro_gem_prime_vmap,
+	.vunmap = hantro_gem_prime_vunmap,
+	.vm_ops = &hantro_drm_gem_cma_vm_ops,
+};
+
+struct drm_device *create_hantro_drm(struct device *dev)
+{
+	struct drm_device *ddev;
+	int result;
+
+	ddev = drm_dev_alloc(&hantro_drm_driver, dev);
+	if (IS_ERR(ddev))
+		return ddev;
+
+	ddev->dev = dev;
+	drm_mode_config_init(ddev);
+	result = drm_dev_register(ddev, 0);
+	if (result < 0) {
+		drm_dev_unregister(ddev);
+		drm_dev_put(ddev);
+		return NULL;
+	}
+
+	return ddev;
+}
diff --git a/drivers/gpu/drm/hantro/hantro_drm.h b/drivers/gpu/drm/hantro/hantro_drm.h
new file mode 100644
index 000000000000..6eedeb2abf0a
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_drm.h
@@ -0,0 +1,272 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro driver public header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_H__
+#define __HANTRO_H__
+
+#include <linux/platform_device.h>
+#include <drm/drm.h>
+#include <drm/drm_drv.h>
+#include "hantro_metadata.h"
+
+#define CONFIG_HWDEC	BIT(0)
+#define CONFIG_HWENC	BIT(1)
+#define CONFIG_L2CACHE	BIT(2)
+#define CONFIG_DEC400	BIT(3)
+
+#define KCORE(id)	((u32)(id) & 0xff)
+#define NODETYPE(id)	(((u32)(id) >> 8) & 0xff)
+#define DEVICE_ID(id)	((u32)(id) >> 16)
+
+/*
+ * device index definition is unchanged.
+ * for dec400/cache NODE(id) refers to its parent core number based on NODETYPE
+ * for dec/enc, NODE(id) refers to its core num, and NODETYPE is useless.
+ */
+/* node type for NODETYPE(id), apply to be expanded */
+
+enum cache_client_type {
+	VC8000E,
+	VC8000D_0,
+	VC8000D_1,
+	DECODER_G1_0,
+	DECODER_G1_1,
+	DECODER_G2_0,
+	DECODER_G2_1,
+};
+
+enum driver_cache_dir {
+	DIR_RD = 0,
+	DIR_WR,
+	DIR_BI
+};
+
+struct hantro_drm_fb {
+	struct drm_framebuffer fb;
+	struct drm_gem_object *obj[4];
+};
+
+struct drm_gem_hantro_object {
+	/* base of gem object */
+	struct drm_gem_object base;
+
+	/* following is private data for hantro object */
+	dma_addr_t paddr;
+	struct sg_table *sgt;
+	struct device *memdev;
+
+	/* For objects with DMA memory allocated by GEM CMA */
+	void *vaddr;
+	struct page *pageaddr;
+	struct page **pages;
+	unsigned long num_pages;
+	/* fence ref */
+	struct dma_resv kresv;
+	unsigned int ctxno;
+	int handle;
+	int fd;
+	struct device_info *pdevinfo;
+	struct drm_file *file_priv;
+	int flag;
+	/* common meta information for dec400, MUST next to base */
+	struct dmapriv dmapriv;
+};
+
+struct hantro_fencecheck {
+	unsigned int handle;
+	int ready;
+};
+
+struct hantro_domainset {
+	unsigned int handle;
+	unsigned int writedomain;
+	unsigned int readdomain;
+};
+
+struct hantro_addrmap {
+	unsigned int handle;
+	unsigned long long vm_addr;
+	unsigned long long phy_addr;
+};
+
+struct hantro_regtransfer {
+	unsigned long coreid;
+	unsigned long offset;
+	unsigned long size;
+	const void *data;
+	int benc; /* encoder core or decoder core */
+	int direction; /* 0=read, 1=write */
+};
+
+struct hantro_corenum {
+	unsigned int deccore;
+	unsigned int enccore;
+};
+
+#define HANTRO_FENCE_WRITE 1
+struct hantro_acquirebuf {
+	unsigned long handle;
+	unsigned long flags;
+	unsigned long timeout;
+	unsigned long fence_handle;
+};
+
+struct hantro_releasebuf {
+	unsigned long fence_handle;
+};
+
+struct core_desc {
+	__u32 id; /* id of the core */
+	__u32 __user *regs; /* pointer to user registers */
+	__u32 size; /* size of register space */
+	__u32 reg_id;
+};
+
+struct hantro_client {
+	int clientid;
+	int deviceid;
+	unsigned long width; /*buffer size*/
+	unsigned long height;
+	int profile;
+	int codec;
+	struct drm_file *file;
+};
+
+/* Define Cache&Shaper Offset from common base */
+#define SHAPER_OFFSET			(0x8 << 2)
+#define CACHE_ONLY_OFFSET		(0x8 << 2)
+#define CACHE_WITH_SHAPER_OFFSET	(0x80 << 2)
+
+/* Ioctl definitions */
+/* hantro drm related */
+#define HANTRO_IOCTL_START (DRM_COMMAND_BASE)
+#define DRM_IOCTL_HANTRO_TESTCMD DRM_IOWR(HANTRO_IOCTL_START, unsigned int)
+#define DRM_IOCTL_HANTRO_GETPADDR                                              \
+	DRM_IOWR(HANTRO_IOCTL_START + 1, struct hantro_addrmap)
+#define DRM_IOCTL_HANTRO_HWCFG DRM_IO(HANTRO_IOCTL_START + 2)
+#define DRM_IOCTL_HANTRO_TESTREADY                                             \
+	DRM_IOWR(HANTRO_IOCTL_START + 3, struct hantro_fencecheck)
+#define DRM_IOCTL_HANTRO_SETDOMAIN                                             \
+	DRM_IOWR(HANTRO_IOCTL_START + 4, struct hantro_domainset)
+#define DRM_IOCTL_HANTRO_ACQUIREBUF                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 6, struct hantro_acquirebuf)
+#define DRM_IOCTL_HANTRO_RELEASEBUF                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 7, struct hantro_releasebuf)
+#define DRM_IOCTL_HANTRO_GETPRIMEADDR                                          \
+	DRM_IOWR(HANTRO_IOCTL_START + 8, unsigned long *)
+#define DRM_IOCTL_HANTRO_PTR_PHYADDR                                           \
+	DRM_IOWR(HANTRO_IOCTL_START + 9, unsigned long *)
+
+/* hantro metadata related */
+#define HANTROMETADATA_IOC_START DRM_IO(HANTRO_IOCTL_START + 10)
+#define DRM_IOCTL_HANTRO_QUERY_METADATA                                        \
+	DRM_IOWR(HANTRO_IOCTL_START + 10, struct hantro_metainfo_params)
+#define DRM_IOCTL_HANTRO_UPDATE_METADATA                                       \
+	DRM_IOWR(HANTRO_IOCTL_START + 11, struct hantro_metainfo_params)
+#define HANTROMETADATA_IOC_END DRM_IO(HANTRO_IOCTL_START + 11)
+
+#define DRM_IOCTL_HANTRO_ADD_CLIENT                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 12, struct hantro_client)
+#define DRM_IOCTL_HANTRO_REMOVE_CLIENT                                         \
+	DRM_IOWR(HANTRO_IOCTL_START + 13, struct hantro_client)
+
+/* hantro enc related */
+#define HX280ENC_IOC_START DRM_IO(HANTRO_IOCTL_START + 17)
+#define HX280ENC_IOCGHWOFFSET                                                  \
+	DRM_IOR(HANTRO_IOCTL_START + 17, unsigned long long *)
+#define HX280ENC_IOCGHWIOSIZE DRM_IOWR(HANTRO_IOCTL_START + 18, unsigned long *)
+#define HX280ENC_IOC_CLI DRM_IO(HANTRO_IOCTL_START + 19)
+#define HX280ENC_IOC_STI DRM_IO(HANTRO_IOCTL_START + 20)
+#define HX280ENC_IOCHARDRESET                                                  \
+	DRM_IO(HANTRO_IOCTL_START + 21) /* debugging tool */
+#define HX280ENC_IOCGSRAMOFFSET                                                \
+	DRM_IOR(HANTRO_IOCTL_START + 22, unsigned long long *)
+#define HX280ENC_IOCGSRAMEIOSIZE                                               \
+	DRM_IOR(HANTRO_IOCTL_START + 23, unsigned int *)
+#define HX280ENC_IOCH_ENC_RESERVE                                              \
+	DRM_IOWR(HANTRO_IOCTL_START + 24, unsigned long *)
+#define HX280ENC_IOCH_ENC_RELEASE                                              \
+	DRM_IOW(HANTRO_IOCTL_START + 25, unsigned long *)
+#define HX280ENC_IOCG_CORE_NUM DRM_IO(HANTRO_IOCTL_START + 26)
+#define HX280ENC_IOCG_CORE_WAIT                                                \
+	DRM_IOWR(HANTRO_IOCTL_START + 27, unsigned int *)
+#define HX280ENC_IOC_END DRM_IO(HANTRO_IOCTL_START + 32)
+
+/* device related */
+#define HANTRODEVICE_IOC_START DRM_IO(HANTRO_IOCTL_START + 33)
+#define DRM_IOCTL_HANTRO_GET_DEVICENUM DRM_IO(HANTRO_IOCTL_START + 33)
+#define HANTRODEVICE_IOC_END DRM_IO(HANTRO_IOCTL_START + 40)
+
+/* hantro dec related */
+#define HANTRODEC_IOC_START DRM_IO(HANTRO_IOCTL_START + 41)
+#define HANTRODEC_PP_INSTANCE DRM_IO(HANTRO_IOCTL_START + 41)
+#define HANTRODEC_HW_PERFORMANCE DRM_IO(HANTRO_IOCTL_START + 42)
+#define HANTRODEC_IOCGHWOFFSET                                                 \
+	DRM_IOWR(HANTRO_IOCTL_START + 43, unsigned long long *)
+#define HANTRODEC_IOCGHWIOSIZE DRM_IOWR(HANTRO_IOCTL_START + 44, unsigned int *)
+#define HANTRODEC_IOC_CLI DRM_IO(HANTRO_IOCTL_START + 45)
+#define HANTRODEC_IOC_STI DRM_IO(HANTRO_IOCTL_START + 46)
+#define HANTRODEC_IOC_MC_OFFSETS                                               \
+	DRM_IOWR(HANTRO_IOCTL_START + 47, unsigned long long *)
+#define HANTRODEC_IOC_MC_CORES DRM_IO(HANTRO_IOCTL_START + 48)
+#define HANTRODEC_IOCS_DEC_PUSH_REG                                            \
+	DRM_IOW(HANTRO_IOCTL_START + 49, struct core_desc *)
+#define HANTRODEC_IOCS_PP_PUSH_REG                                             \
+	DRM_IOW(HANTRO_IOCTL_START + 50, struct core_desc *)
+#define HANTRODEC_IOCH_DEC_RESERVE                                             \
+	DRM_IOW(HANTRO_IOCTL_START + 51, unsigned long long *)
+#define HANTRODEC_IOCT_DEC_RELEASE DRM_IO(HANTRO_IOCTL_START + 52)
+#define HANTRODEC_IOCQ_PP_RESERVE DRM_IO(HANTRO_IOCTL_START + 53)
+#define HANTRODEC_IOCT_PP_RELEASE DRM_IO(HANTRO_IOCTL_START + 54)
+#define HANTRODEC_IOCX_DEC_WAIT                                                \
+	DRM_IOW(HANTRO_IOCTL_START + 55, struct core_desc *)
+#define HANTRODEC_IOCX_PP_WAIT                                                 \
+	DRM_IOWR(HANTRO_IOCTL_START + 56, struct core_desc *)
+#define HANTRODEC_IOCS_DEC_PULL_REG                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 57, struct core_desc *)
+#define HANTRODEC_IOCS_PP_PULL_REG                                             \
+	DRM_IOWR(HANTRO_IOCTL_START + 58, struct core_desc *)
+#define HANTRODEC_IOCG_CORE_WAIT DRM_IO(HANTRO_IOCTL_START + 59)
+#define HANTRODEC_IOX_ASIC_ID DRM_IO(HANTRO_IOCTL_START + 60)
+#define HANTRODEC_IOCG_CORE_ID                                                 \
+	DRM_IOW(HANTRO_IOCTL_START + 61, unsigned long long *)
+#define HANTRODEC_IOCS_DEC_WRITE_REG                                           \
+	DRM_IOW(HANTRO_IOCTL_START + 62, struct core_desc *)
+#define HANTRODEC_IOCS_DEC_READ_REG                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 63, struct core_desc *)
+#define HANTRODEC_DEBUG_STATUS DRM_IO(HANTRO_IOCTL_START + 64)
+#define HANTRODEC_IOX_ASIC_BUILD_ID                                            \
+	DRM_IOWR(HANTRO_IOCTL_START + 65, unsigned int *)
+
+#define HANTRODEC_IOC_END DRM_IO(HANTRO_IOCTL_START + 79)
+
+/* hantro cache related */
+#define HANTROCACHE_IOC_START DRM_IO(HANTRO_IOCTL_START + 80)
+#define CACHE_IOCGHWOFFSET                                                     \
+	DRM_IOR(HANTRO_IOCTL_START + 80, unsigned long long *)
+#define CACHE_IOCGHWIOSIZE DRM_IO(HANTRO_IOCTL_START + 81)
+#define CACHE_IOCHARDRESET DRM_IO(HANTRO_IOCTL_START + 82) /* debugging tool */
+#define CACHE_IOCH_HW_RESERVE                                                  \
+	DRM_IOW(HANTRO_IOCTL_START + 83, unsigned long long *)
+#define CACHE_IOCH_HW_RELEASE DRM_IO(HANTRO_IOCTL_START + 84)
+#define CACHE_IOCG_CORE_NUM DRM_IO(HANTRO_IOCTL_START + 85)
+#define CACHE_IOCG_ABORT_WAIT DRM_IO(HANTRO_IOCTL_START + 86)
+#define HANTROCACHE_IOC_END DRM_IO(HANTRO_IOCTL_START + 89)
+#define HANTRODEC400_IOC_START DRM_IO(HANTRO_IOCTL_START + 90)
+#define DEC400_IOCGHWIOSIZE DRM_IO(HANTRO_IOCTL_START + 90)
+#define DEC400_IOCS_DEC_WRITE_REG                                              \
+	DRM_IOW(HANTRO_IOCTL_START + 91, struct core_desc *)
+#define DEC400_IOCS_DEC_READ_REG                                               \
+	DRM_IOWR(HANTRO_IOCTL_START + 92, struct core_desc *)
+#define DEC400_IOCS_DEC_PUSH_REG                                               \
+	DRM_IOW(HANTRO_IOCTL_START + 93, struct core_desc *)
+#define DEC400_IOCGHWOFFSET                                                    \
+	DRM_IOWR(HANTRO_IOCTL_START + 94, unsigned long long *)
+#define HANTRODEC400_IOC_END DRM_IO(HANTRO_IOCTL_START + 99)
+
+#endif /* __HANTRO_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_drv.c b/drivers/gpu/drm/hantro/hantro_drv.c
new file mode 100644
index 000000000000..ff4a583d6c4f
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_drv.c
@@ -0,0 +1,846 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver main entrance.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+/* Our header */
+#include "hantro_priv.h"
+#include "hantro_enc.h"
+#include "hantro_dec.h"
+#include "hantro_cache.h"
+#include "hantro_dec400.h"
+#include "hantro_cooling.h"
+
+struct hantro_drm_handle hantro_drm;
+
+long kmb_freq_table[3] = { 700000000, 500000000, 250000000 };
+long tbh_freq_table[3] = { 800000000, 400000000, 200000000 };
+
+bool verbose;
+module_param(verbose, bool, 0);
+MODULE_PARM_DESC(verbose, "Verbose log operations (default 0)");
+
+bool enable_lut;
+module_param(enable_lut, bool, 0);
+MODULE_PARM_DESC(enable_lut, "Enable Page Table LUT(default 0)");
+
+bool enable_encode = 1;
+module_param(enable_encode, bool, 0);
+MODULE_PARM_DESC(enable_encode, "Enable Encode(default 1)");
+
+bool enable_decode = 1;
+module_param(enable_decode, bool, 0);
+MODULE_PARM_DESC(enable_decode, "Enable Decode(default 1)");
+
+bool enable_dec400;
+module_param(enable_dec400, bool, 0);
+MODULE_PARM_DESC(enable_dec400, "Enable DEC400/L2(default 0)");
+
+bool enable_irqmode = 1;
+module_param(enable_irqmode, bool, 0);
+MODULE_PARM_DESC(enable_irqmode, "Enable IRQ Mode(default 1)");
+
+static int link_device_drm(struct device_info *pdevinfo)
+{
+	struct device_info *tmpdev;
+
+	if (!pdevinfo)
+		return -1;
+
+	if (mutex_lock_interruptible(&hantro_drm.hantro_mutex))
+		return -EBUSY;
+
+	if (!hantro_drm.pdevice_list) {
+		hantro_drm.pdevice_list = pdevinfo;
+	} else {
+		tmpdev = hantro_drm.pdevice_list;
+		while (tmpdev->next)
+			tmpdev = tmpdev->next;
+
+		tmpdev->next = pdevinfo;
+	}
+
+	mutex_unlock(&hantro_drm.hantro_mutex);
+	return 0;
+}
+
+static int unlink_device_drm(struct device_info *pdevinfo)
+{
+	struct device_info *tmpdev;
+
+	if (!pdevinfo)
+		return -1;
+
+	if (!hantro_drm.pdevice_list)
+		return -1;
+
+	if (mutex_lock_interruptible(&hantro_drm.hantro_mutex))
+		return -EBUSY;
+
+	if (hantro_drm.pdevice_list == pdevinfo) {
+		hantro_drm.pdevice_list = pdevinfo->next;
+	} else {
+		tmpdev = hantro_drm.pdevice_list;
+
+		while (tmpdev->next && tmpdev->next != pdevinfo)
+			tmpdev = tmpdev->next;
+	}
+
+	mutex_unlock(&hantro_drm.hantro_mutex);
+	return 0;
+}
+
+struct device_info *get_device_info(int deviceid)
+{
+	struct device_info *tmpdev;
+
+	if (mutex_lock_interruptible(&hantro_drm.hantro_mutex))
+		return NULL;
+
+	tmpdev = hantro_drm.pdevice_list;
+	while (tmpdev && tmpdev->deviceid != deviceid)
+		tmpdev = tmpdev->next;
+
+	mutex_unlock(&hantro_drm.hantro_mutex);
+	return tmpdev;
+}
+
+static int get_clock_name(struct dtbnode *pnode)
+{
+	const char *nodename;
+
+	if (!pnode || !pnode->ofnode)
+		return -EINVAL;
+
+	nodename = pnode->ofnode->name;
+
+	if (hantro_drm.device_type == DEVICE_KEEMBAY) {
+		if (strstr(nodename, "decoderA") == nodename)
+			sprintf(pnode->clock_name, "clk_xin_vdec");
+
+		if (strstr(nodename, "decoderB") == nodename)
+			sprintf(pnode->clock_name, "clk_xin_vdec");
+
+		if (strstr(nodename, "encoderA") == nodename)
+			sprintf(pnode->clock_name, "clk_xin_venc");
+
+		if (strstr(nodename, "encoderB") == nodename)
+			sprintf(pnode->clock_name, "clk_xin_jpeg");
+	} else if (hantro_drm.device_type == DEVICE_THUNDERBAY) {
+		if (strstr(nodename, "decoderA") == nodename)
+			sprintf(pnode->clock_name, "vc8000da_aclk_computess%d",
+				pnode->pdevinfo->deviceid);
+
+		if (strstr(nodename, "decoderB") == nodename)
+			sprintf(pnode->clock_name, "vc8000db_aclk_computess%d",
+				pnode->pdevinfo->deviceid);
+
+		if (strstr(nodename, "encoderA") == nodename)
+			sprintf(pnode->clock_name, "vc8000ej_aclk_computess%d",
+				pnode->pdevinfo->deviceid);
+
+		if (strstr(nodename, "encoderB") == nodename)
+			sprintf(pnode->clock_name, "vc8000e_aclk_computess%d",
+				pnode->pdevinfo->deviceid);
+	}
+
+	return -EINVAL;
+}
+
+static int get_node_type(const char *name)
+{
+	if (strstr(name, NODENAME_DECODER) == name)
+		return CORE_DEC;
+
+	if (strstr(name, NODENAME_ENCODER) == name)
+		return CORE_ENC;
+
+	if (strstr(name, NODENAME_CACHE) == name)
+		return CORE_CACHE;
+
+	if (strstr(name, NODENAME_DEC400) == name)
+		return CORE_DEC400;
+
+	return CORE_UNKNOWN;
+}
+
+static struct dtbnode *try_create_node(struct platform_device *pdev,
+				       struct device_node *ofnode,
+				       struct device_info *pdevinfo, int parenttype,
+				       phys_addr_t parentaddr)
+{
+	struct fwnode_handle *fwnode;
+	struct resource r;
+	int i, na, ns, ret = 0;
+	int endian = of_device_is_big_endian(ofnode);
+	u32 reg_u32[4];
+	const char *reg_name;
+	u64 ioaddress, iosize;
+	struct dtbnode *pnode = kzalloc(sizeof(*pnode), GFP_KERNEL);
+
+	if (!pnode)
+		return NULL;
+
+	pnode->type = get_node_type(ofnode->name);
+	pnode->parentaddr = parentaddr;
+	pnode->parenttype = parenttype;
+	pnode->pdevinfo = pdevinfo;
+	pnode->ofnode = ofnode;
+	fwnode = &ofnode->fwnode;
+	get_clock_name(pnode);
+
+	na = of_n_addr_cells(ofnode);
+	ns = of_n_size_cells(ofnode);
+	if (na > 2 || ns > 2) {
+		pr_err("cell size too big");
+		kfree(pnode);
+		return NULL;
+	}
+
+	fwnode_property_read_u32_array(fwnode, "reg", reg_u32, na + ns);
+	if (na == 2) {
+		if (!endian) {
+			ioaddress = reg_u32[0];
+			ioaddress <<= 32;
+			ioaddress |= reg_u32[1];
+		} else {
+			ioaddress = reg_u32[1];
+			ioaddress <<= 32;
+			ioaddress |= reg_u32[0];
+		}
+	} else {
+		ioaddress = reg_u32[0];
+	}
+
+	if (ns == 2) {
+		if (!endian) {
+			iosize = reg_u32[na];
+			iosize <<= 32;
+			iosize |= reg_u32[na + 1];
+		} else {
+			iosize = reg_u32[na + 1];
+			iosize <<= 32;
+			iosize |= reg_u32[na];
+		}
+	} else {
+		iosize = reg_u32[na];
+	}
+
+	pnode->ioaddr = ioaddress;
+	pnode->iosize = iosize;
+	fwnode_property_read_string(fwnode, "reg-names", &reg_name);
+
+	if (strlen(reg_name))
+		strcpy(pnode->reg_name, reg_name);
+	else
+		strcpy(pnode->reg_name, "hantro_reg");
+
+	for (i = 0; i < 4; i++) {
+		if (of_irq_to_resource(ofnode, i, &r) > 0) {
+			pnode->irq[i] = r.start;
+			if (strlen(r.name))
+				strcpy(pnode->irq_name[i], r.name);
+			else
+				strcpy(pnode->irq_name[i], "hantro_irq");
+
+			PDEBUG("irq %s: mapping = %lld\n", r.name, r.end);
+		} else {
+			pnode->irq[i] = -1;
+		}
+	}
+
+	switch (pnode->type) {
+	case CORE_DEC:
+		ret = hantrodec_probe(pnode);
+		break;
+	case CORE_ENC:
+		ret = hantroenc_probe(pnode);
+		break;
+	case CORE_CACHE:
+		ret = hantrocache_probe(pnode);
+		break;
+	case CORE_DEC400:
+		ret = hantrodec400_probe(pnode);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+
+	if (ret < 0) {
+		kfree(pnode);
+		pnode = NULL;
+	}
+
+	return pnode;
+}
+
+/* hantro_mmu_control is used to check whether media MMU is enabled or disabled */
+static void hantro_mmu_control(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	u64 mmu_tcu_smmu_cr0;
+	u8 __iomem *mmu_tcu_smmu_cr0_register;
+	int is_mmu_enabled, count = 0;
+
+	count = device_property_read_u64(dev, "mmu-tcu-reg",
+					 &mmu_tcu_smmu_cr0);
+
+	if (count == 0) {
+		if (!request_mem_region(mmu_tcu_smmu_cr0, 0x32,
+					"mmu_tcu_smmu_cr0")) {
+			pr_info("mmu_tcu_smmu_cr0: failed to request mem region\n");
+		}
+
+		mmu_tcu_smmu_cr0_register =
+			ioremap(mmu_tcu_smmu_cr0, 0x32);
+		if (!mmu_tcu_smmu_cr0_register) {
+			pr_info("mmu_tcu_smmu_cr0_register: failed to ioremap mmu_tcu_smmu_cr0_register\n");
+		} else {
+			is_mmu_enabled = ioread32(mmu_tcu_smmu_cr0_register);
+			if (is_mmu_enabled)
+				pr_info("hantro_init: Media MMU600 is enabled, is_mmu_enabled = %d\n",
+					is_mmu_enabled);
+			else
+				pr_info("hantro_init: Media MMU600 is disabled, is_mmu_enabled = %d\n",
+					is_mmu_enabled);
+
+			release_mem_region(mmu_tcu_smmu_cr0, 0x32);
+		}
+	}
+}
+
+/* hantro_enable_clock to enable/disable the media SS clocks */
+static int hantro_clock_control(struct device *dev, bool enable)
+{
+	struct clk *dev_clk = NULL;
+	const char **clock_names;
+	int i = 0, ret = 0, count = 0;
+	unsigned long rate;
+
+	if ((hantro_drm.device_type != DEVICE_KEEMBAY) &&
+			(hantro_drm.device_type != DEVICE_THUNDERBAY))
+		return -EINVAL;
+
+	/* Read clock names */
+	count = device_property_read_string_array(dev, "clock-names", NULL, 0);
+	if (count > 0) {
+		clock_names = kcalloc(count, sizeof(*clock_names), GFP_KERNEL);
+		if (!clock_names)
+			return 0;
+
+		ret = device_property_read_string_array(dev, "clock-names",
+							clock_names, count);
+		if (ret < 0) {
+			pr_err("failed to read clock names\n");
+			kfree(clock_names);
+			return 0;
+		}
+
+		for (i = 0; i < count; i++) {
+			PDEBUG("hantro: clock_name = %s\n", clock_names[i]);
+			dev_clk = clk_get(dev, clock_names[i]);
+			if (enable) {
+				clk_prepare_enable(dev_clk);
+
+				if (verbose)
+					pr_info("hantro: default clock frequency of clock_name = %s is %ld\n"
+						, clock_names[i],
+						clk_get_rate(dev_clk));
+
+				if (hantro_drm.device_type == DEVICE_KEEMBAY)
+					rate = kmb_freq_table[0];
+				else if (hantro_drm.device_type == DEVICE_THUNDERBAY)
+					rate = tbh_freq_table[0];
+
+				clk_set_rate(dev_clk, rate);
+				if (verbose)
+					pr_info("hantro: set %lu Mhz clock frequency of clock_name = %s is %ld\n"
+					, rate, clock_names[i], clk_get_rate(dev_clk));
+
+			} else {
+				clk_disable_unprepare(dev_clk);
+			}
+
+			clk_put(dev_clk);
+		}
+
+		kfree(clock_names);
+	}
+
+	return 0;
+}
+
+/* hantro_reset_clock to de-assert/assert the media SS cores and MMU */
+static int hantro_reset_control(struct platform_device *pdev, bool deassert)
+{
+	struct device *dev = &pdev->dev;
+	const char **reset_names;
+	struct reset_control *dev_reset = NULL;
+	int i = 0, ret = 0, count = 0;
+
+	/* Read reset names */
+	count = device_property_read_string_array(dev, "reset-names", NULL, 0);
+	if (count > 0) {
+		reset_names = kcalloc(count, sizeof(*reset_names), GFP_KERNEL);
+		if (!reset_names)
+			return 0;
+
+		ret = device_property_read_string_array(dev, "reset-names",
+							reset_names, count);
+		if (ret < 0) {
+			pr_err("failed to read clock names\n");
+			kfree(reset_names);
+			return 0;
+		}
+
+		for (i = 0; i < count; i++) {
+			if (verbose)
+				pr_info("hantro: reset_name = %s\n",
+					reset_names[i]);
+
+			dev_reset = devm_reset_control_get(dev, reset_names[i]);
+			if (deassert) {
+				ret = reset_control_deassert(dev_reset);
+				if (ret < 0) {
+					pr_err("failed to deassert reset : %s, %d\n",
+					       reset_names[i], ret);
+				}
+
+			} else {
+				ret = reset_control_assert(dev_reset);
+				if (ret < 0) {
+					pr_err("failed to assert reset : %s, %d\n",
+					       reset_names[i], ret);
+				}
+			}
+		}
+
+		kfree(reset_names);
+	}
+
+	return 0;
+}
+
+static int hantro_analyze_subnode(struct platform_device *pdev,
+				  struct device_node *pofnode,
+				  struct device_info *pdevinfo)
+{
+	struct dtbnode *head, *nhead, *newtail, *node;
+
+	head = kzalloc(sizeof(*head), GFP_KERNEL);
+	if (!head)
+		return -ENOMEM;
+
+	head->type = CORE_DEVICE;
+	head->parenttype = CORE_DEVICE;
+	head->ofnode = pofnode;
+	head->pdevinfo = pdevinfo;
+	head->ioaddr = -1;
+	head->iosize = 0;
+	head->next = NULL;
+	/* this is a wide first tree structure iteration, result is stored in device info */
+	while (head) {
+		nhead = NULL;
+		newtail = NULL;
+		while (head) {
+			struct device_node *child, *ofnode = head->ofnode;
+
+			for_each_child_of_node(ofnode, child) {
+				node = try_create_node(pdev, child, pdevinfo,
+						       head->type, head->ioaddr);
+				if (node) {
+					if (!nhead) {
+						nhead = node;
+						newtail = node;
+					} else {
+						newtail->next = node;
+					}
+
+					node->next = NULL;
+					newtail = node;
+				}
+			}
+
+			node = head->next;
+			kfree(head);
+			head = node;
+		}
+
+		head = nhead;
+	}
+
+	return 0;
+}
+
+static int init_codec_rsvd_mem(struct device *dev, struct device_info *pdevice,
+			       const char *mem_name, unsigned int mem_idx)
+{
+	struct device *mem_dev;
+	int rc = -1;
+
+	/* Create a child device (of dev) to own the reserved memory. */
+	mem_dev =
+		devm_kzalloc(dev, sizeof(struct device), GFP_KERNEL | GFP_DMA);
+	if (!mem_dev)
+		return -ENOMEM;
+
+	device_initialize(mem_dev);
+	dev_set_name(mem_dev, "%s:%s", dev_name(dev), mem_name);
+	mem_dev->parent = dev;
+	mem_dev->dma_mask = dev->dma_mask;
+	mem_dev->coherent_dma_mask = dev->coherent_dma_mask;
+	/* Set up DMA configuration using information from parent's DT node. */
+	mem_dev->release = of_reserved_mem_device_release;
+	rc = device_add(mem_dev);
+	if (rc)
+		goto err;
+
+	/* Initialized the device reserved memory region. */
+	rc = of_reserved_mem_device_init_by_idx(mem_dev, dev->of_node, mem_idx);
+	if (rc) {
+		device_del(mem_dev);
+		goto err;
+	} else {
+		dev_info(dev,
+			 "Success: Codec reserved memory found at idx = %d, ret=%d\n",
+			 mem_idx, rc);
+	}
+
+	pdevice->codec_rsvmem = mem_dev;
+	return 0;
+err:
+	put_device(mem_dev);
+	return rc;
+}
+
+static void set_device_type(struct device_info *pdevinfo)
+{
+	struct device_node *ofnode = NULL;
+	struct fwnode_handle *fwnode;
+	const char *compat_name;
+
+	if (!pdevinfo || !pdevinfo->dev)
+		return;
+
+	ofnode = pdevinfo->dev->of_node;
+	if (!ofnode)
+		return;
+
+	fwnode = &ofnode->fwnode;
+	fwnode_property_read_string(fwnode, "compatible", &compat_name);
+	if (strstr(compat_name, "kmb"))
+		hantro_drm.device_type = DEVICE_KEEMBAY;
+	else if (strstr(compat_name, "thunderbay"))
+		hantro_drm.device_type = DEVICE_THUNDERBAY;
+}
+
+static int init_device_info(struct device *dev, struct device_info *pdevinfo)
+{
+	if (!pdevinfo)
+		return -EINVAL;
+
+	pdevinfo->dev = dev;
+	dev_set_drvdata(dev, pdevinfo);
+	pdevinfo->drm_dev = hantro_drm.drm_dev;
+	pdevinfo->config = 0;
+	pdevinfo->next = NULL;
+	pdevinfo->deccore_num = 0;
+	pdevinfo->enccore_num = 0;
+	pdevinfo->cachecore_num = 0;
+	pdevinfo->dec400core_num = 0;
+	pdevinfo->dechdr = NULL;
+	pdevinfo->enchdr = NULL;
+	pdevinfo->cachehdr = NULL;
+	pdevinfo->dec400hdr = NULL;
+
+	init_waitqueue_head(&pdevinfo->cache_hw_queue);
+	init_waitqueue_head(&pdevinfo->cache_wait_queue);
+	spin_lock_init(&pdevinfo->cache_owner_lock);
+
+	sema_init(&pdevinfo->enc_core_sem, 1);
+	init_waitqueue_head(&pdevinfo->enc_hw_queue);
+	spin_lock_init(&pdevinfo->enc_owner_lock);
+	init_waitqueue_head(&pdevinfo->enc_wait_queue);
+
+	pdevinfo->dec_irq = 0;
+	pdevinfo->pp_irq = 0;
+	spin_lock_init(&pdevinfo->owner_lock);
+	init_waitqueue_head(&pdevinfo->dec_wait_queue);
+	init_waitqueue_head(&pdevinfo->pp_wait_queue);
+	init_waitqueue_head(&pdevinfo->hw_queue);
+	sema_init(&pdevinfo->pp_core_sem, 1);
+	pdevinfo->deviceid = atomic_read(&hantro_drm.devicecount);
+	atomic_inc(&hantro_drm.devicecount);
+	set_device_type(pdevinfo);
+	return 0;
+}
+
+static int get_reserved_mem_size(struct device_info *pdevice)
+{
+	struct device *dev = pdevice->dev;
+	struct device_node *memnp;
+	int rc;
+
+	/* Get pointer to memory region device node from "memory-region" phandle. */
+	memnp = of_parse_phandle(dev->of_node, "memory-region", 0);
+	if (!memnp) {
+		dev_err(dev, "no memory-region node at index 0\n");
+		return 0;
+	}
+
+	rc = of_address_to_resource(memnp, 0, &pdevice->mem_res[0]);
+	of_node_put(memnp);
+	if (rc) {
+		dev_err(dev,
+			"failed to translate memory-region to a resource\n");
+		return 0;
+	}
+
+	/* Get pointer to memory region device node from "memory-region" phandle. */
+	memnp = of_parse_phandle(dev->of_node, "memory-region", 1);
+	if (!memnp) {
+		dev_err(dev, "no memory-region node at index 0\n");
+		return 0;
+	}
+
+	rc = of_address_to_resource(memnp, 0, &pdevice->mem_res[1]);
+	of_node_put(memnp);
+	if (rc) {
+		dev_err(dev,
+			"failed to translate memory-region to a resource\n");
+		return 0;
+	}
+
+	return 0;
+}
+
+static int hantro_drm_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	int result = 0;
+	struct device_info *pdevinfo = NULL;
+
+	pr_info("%s: dev %s probe\n", __func__, pdev->name);
+	if (!dev->of_node)
+		return 0;
+
+	pdevinfo = devm_kzalloc(dev, sizeof(struct device_info), GFP_KERNEL);
+	if (!pdevinfo)
+		return -ENOMEM;
+
+	init_device_info(dev, pdevinfo);
+	setup_thermal_cooling(pdevinfo);
+	hantro_clock_control(dev, true);
+	if (hantro_drm.device_type != DEVICE_KEEMBAY) {
+		hantro_reset_control(pdev, true);
+		hantro_mmu_control(pdev);
+	}
+
+	result = of_reserved_mem_device_init(dev);
+	if (result)
+		return result;
+
+	dma_set_mask(dev, DMA_BIT_MASK(48));
+	dma_set_coherent_mask(dev, DMA_BIT_MASK(48));
+	hantro_analyze_subnode(pdev, dev->of_node, pdevinfo);
+	result = init_codec_rsvd_mem(pdevinfo->dev, pdevinfo, "codec_reserved", 1);
+	if (result) {
+		dev_err(pdevinfo->dev, "Failed to set up codec reserved memory.\n");
+		return result;
+	}
+
+	get_reserved_mem_size(pdevinfo);
+	create_debugfs(pdevinfo, result == 0);
+	link_device_drm(pdevinfo);
+	result = create_sysfs(pdevinfo);
+	if (result)
+		pr_info("Failed to create sysfs.\n");
+
+	result = class_compat_create_link(hantro_drm.media_class, &pdev->dev,
+					  pdev->dev.parent);
+	if (result)
+		pr_info("Failed to create compatibility class link.\n");
+
+	idr_init(&pdevinfo->clients);
+	idr_init(&pdevinfo->allocations);
+	mutex_init(&pdevinfo->alloc_mutex);
+	mutex_init(&hantro_drm.hantro_mutex);
+
+	return 0;
+}
+
+static int hantro_drm_remove(struct platform_device *pdev)
+{
+	struct device_info *pdevinfo;
+
+	if (!pdev->dev.of_node)
+		return 0;
+
+	pdevinfo = dev_get_drvdata(&pdev->dev);
+	if (!pdevinfo)
+		return 0;
+
+	if (pdevinfo->codec_rsvmem) {
+		of_reserved_mem_device_release(pdevinfo->codec_rsvmem);
+		device_del(pdevinfo->codec_rsvmem);
+		put_device(pdevinfo->codec_rsvmem);
+		pdevinfo->codec_rsvmem = NULL;
+	}
+
+	idr_destroy(&pdevinfo->clients);
+	idr_destroy(&pdevinfo->allocations);
+	mutex_destroy(&pdevinfo->alloc_mutex);
+	mutex_destroy(&hantro_drm.hantro_mutex);
+
+	class_compat_remove_link(hantro_drm.media_class, &pdev->dev,
+				 pdev->dev.parent);
+	remove_sysfs(pdevinfo);
+	hantrodec_remove(pdevinfo);
+	hantroenc_remove(pdevinfo);
+	hantrodec400_remove(pdevinfo);
+	hantrocache_remove(pdevinfo);
+	unlink_device_drm(pdevinfo);
+	return 0;
+}
+
+static const struct platform_device_id hantro_drm_platform_ids[] = {
+	{
+		.name = DRIVER_NAME,
+	},
+	{},
+};
+
+MODULE_DEVICE_TABLE(platform, hantro_drm_platform_ids);
+
+static const struct of_device_id hantro_of_match[] = {
+	/*to match dtb, else reg io will fail*/
+	{ .compatible = "kmb,hantro" },
+	{ .compatible = "thunderbay,hantro" },
+	{ /* sentinel */ }
+};
+
+static int hantro_pm_suspend(struct device *kdev)
+{
+	return 0;
+}
+
+static int hantro_pm_resume(struct device *kdev)
+{
+	return 0;
+}
+
+static const struct dev_pm_ops hantro_pm_ops = {
+	.suspend = hantro_pm_suspend,
+	.resume = hantro_pm_resume,
+};
+
+static struct platform_driver hantro_drm_platform_driver = {
+	.probe = hantro_drm_probe,
+	.remove = hantro_drm_remove,
+	.driver = {
+			.name = DRIVER_NAME,
+			.owner = THIS_MODULE,
+			.of_match_table = hantro_of_match,
+			.pm = &hantro_pm_ops,
+		},
+	.id_table = hantro_drm_platform_ids,
+};
+
+static const struct platform_device_info hantro_platform_info = {
+	.name = DRIVER_NAME,
+	.id = -1,
+	.dma_mask = DMA_BIT_MASK(48),
+};
+
+static void __exit hantro_cleanup(void)
+{
+	debugfs_remove(hantro_drm.debugfs_root);
+	release_fence_data();
+
+	hantrodec_cleanup();
+	hantroenc_cleanup();
+
+	drm_dev_unregister(hantro_drm.drm_dev);
+	platform_device_unregister(hantro_drm.platformdev);
+	platform_driver_unregister(&hantro_drm_platform_driver);
+	drm_dev_put(hantro_drm.drm_dev);
+	class_compat_unregister(hantro_drm.media_class);
+	hantro_drm.media_class = NULL;
+	mutex_destroy(&hantro_drm.hantro_mutex);
+	pr_info("hantro driver removed\n");
+}
+
+static int __init hantro_init(void)
+{
+	int result, i;
+
+	hantro_drm.debugfs_root = debugfs_create_dir("hantro", NULL);
+	if (!hantro_drm.media_class)
+		hantro_drm.media_class = class_compat_register("media");
+
+	if (!hantro_drm.media_class)
+		return -ENOMEM;
+
+	if (IS_ERR_OR_NULL(hantro_drm.media_class)) {
+		result = PTR_ERR(hantro_drm.media_class);
+		hantro_drm.media_class = NULL;
+		pr_err("[%s]: couldn't create driver class, return=%d\n",
+		       DRIVER_NAME, result);
+		result = (result == 0) ? -ENOMEM : result;
+		return result;
+	}
+
+	mutex_init(&hantro_drm.hantro_mutex);
+	hantro_drm.device_type = DEVICE_UNKNOWN;
+	result = platform_driver_register(&hantro_drm_platform_driver);
+	if (result < 0) {
+		pr_info("hantro create platform driver fail");
+		return result;
+	}
+
+	hantro_drm.platformdev =
+		platform_device_register_full(&hantro_platform_info);
+	if (!hantro_drm.platformdev) {
+		platform_driver_unregister(&hantro_drm_platform_driver);
+		pr_info("hantro create platform device fail");
+		return PTR_ERR(hantro_drm.platformdev);
+	}
+
+	hantro_drm.drm_dev = create_hantro_drm(&hantro_drm.platformdev->dev);
+	if (!hantro_drm.drm_dev || IS_ERR(hantro_drm.drm_dev)) {
+		platform_device_unregister(hantro_drm.platformdev);
+		platform_driver_unregister(&hantro_drm_platform_driver);
+		return PTR_ERR(hantro_drm.drm_dev);
+	}
+
+	init_fence_data();
+	for (i = 0; i < get_device_count(); i++) {
+		struct device_info *pdevinfo = get_device_info(i);
+
+		hantro_drm.config |= pdevinfo->config;
+	}
+
+	hantrodec_init();
+	hantroenc_init();
+
+	if (verbose)
+		device_print_debug();
+
+	pr_info("hantro device created\n");
+	return result;
+}
+
+module_init(hantro_init);
+module_exit(hantro_cleanup);
+
+/* module description */
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("VeriSilicon");
+MODULE_AUTHOR("Mehmood, Arshad <arshad.mehmood@intel.com>");
+MODULE_AUTHOR("Murugasen Krishnan, Kuhanh <kuhanh.murugasen.krishnan@intel.com>");
+MODULE_AUTHOR("Hoe, Sheng Yang <sheng.yang.hoe@intel.com>");
+MODULE_DESCRIPTION("Hantro DRM manager");
diff --git a/drivers/gpu/drm/hantro/hantro_dwl_defs.h b/drivers/gpu/drm/hantro/hantro_dwl_defs.h
new file mode 100644
index 000000000000..ee88130b0af7
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_dwl_defs.h
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro driver hardware register definition.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __DWL_DEFS_H__
+#define __DWL_DEFS_H__
+
+#define DWL_MPEG2_E		31 /* 1 bit  */
+#define DWL_VC1_E		29 /* 2 bits */
+#define DWL_JPEG_E		28 /* 1 bit  */
+#define DWL_MPEG4_E		26 /* 2 bits */
+#define DWL_H264_E		24 /* 2 bits */
+#define DWL_H264HIGH10_E	20 /* 1 bits */
+#define DWL_VP6_E		23 /* 1 bit  */
+#define DWL_RV_E		26 /* 2 bits */
+#define DWL_VP8_E		23 /* 1 bit  */
+#define DWL_VP7_E		24 /* 1 bit  */
+#define DWL_WEBP_E		19 /* 1 bit  */
+#define DWL_AVS_E		22 /* 1 bit  */
+#define DWL_G1_PP_E		16 /* 1 bit  */
+#define DWL_G2_PP_E		31 /* 1 bit  */
+#define DWL_PP_E		31 /* 1 bit  */
+#define DWL_HEVC_E		26 /* 3 bits */
+#define DWL_VP9_E		29 /* 3 bits */
+
+#define DWL_H264_PIPELINE_E	31 /* 1 bit */
+#define DWL_JPEG_PIPELINE_E	30 /* 1 bit */
+
+#define DWL_G2_HEVC_E		0 /* 1 bits */
+#define DWL_G2_VP9_E		1 /* 1 bits */
+#define DWL_G2_RFC_E		2 /* 1 bits */
+#define DWL_RFC_E		17 /* 2 bits */
+#define DWL_G2_DS_E		3 /* 1 bits */
+#define DWL_DS_E		28 /* 3 bits */
+#define DWL_HEVC_VER		8 /* 4 bits */
+#define DWL_VP9_PROFILE		12 /* 3 bits */
+#define DWL_RING_E		16 /* 1 bits */
+
+#define HANTRODEC_IRQ_STAT_DEC		1
+#define HANTRODEC_IRQ_STAT_DEC_OFF	(HANTRODEC_IRQ_STAT_DEC * 4)
+
+#define HANTRODECPP_SYNTH_CFG		60
+#define HANTRODECPP_SYNTH_CFG_OFF	(HANTRODECPP_SYNTH_CFG * 4)
+#define HANTRODEC_SYNTH_CFG		50
+#define HANTRODEC_SYNTH_CFG_OFF		(HANTRODEC_SYNTH_CFG * 4)
+#define HANTRODEC_SYNTH_CFG_2		54
+#define HANTRODEC_SYNTH_CFG_2_OFF	(HANTRODEC_SYNTH_CFG_2 * 4)
+#define HANTRODEC_SYNTH_CFG_3		56
+#define HANTRODEC_SYNTH_CFG_3_OFF	(HANTRODEC_SYNTH_CFG_3 * 4)
+#define HANTRODEC_CFG_STAT		23
+#define HANTRODEC_CFG_STAT_OFF		(HANTRODEC_CFG_STAT * 4)
+#define HANTRODECPP_CFG_STAT		260
+#define HANTRODECPP_CFG_STAT_OFF	(HANTRODECPP_CFG_STAT * 4)
+/* VC8000D HW build id */
+#define HANTRODEC_HW_BUILD_ID		309
+#define HANTRODEC_HW_BUILD_ID_OFF	(HANTRODEC_HW_BUILD_ID * 4)
+
+#define HANTRODEC_DEC_E			0x01
+#define HANTRODEC_PP_E			0x01
+#define HANTRODEC_DEC_ABORT		0x20
+#define HANTRODEC_DEC_IRQ_DISABLE	0x10
+#define HANTRODEC_DEC_IRQ		0x100
+
+/* Legacy from G1 */
+#define HANTRO_IRQ_STAT_DEC		1
+#define HANTRO_IRQ_STAT_DEC_OFF		(HANTRO_IRQ_STAT_DEC * 4)
+#define HANTRO_IRQ_STAT_PP		60
+#define HANTRO_IRQ_STAT_PP_OFF		(HANTRO_IRQ_STAT_PP * 4)
+
+#define HANTROPP_SYNTH_CFG		100
+#define HANTROPP_SYNTH_CFG_OFF		(HANTROPP_SYNTH_CFG * 4)
+#define HANTRODEC_SYNTH_CFG		50
+#define HANTRODEC_SYNTH_CFG_OFF		(HANTRODEC_SYNTH_CFG * 4)
+#define HANTRODEC_SYNTH_CFG_2		54
+#define HANTRODEC_SYNTH_CFG_2_OFF	(HANTRODEC_SYNTH_CFG_2 * 4)
+
+#define HANTRO_DEC_E			0x01
+#define HANTRO_PP_E			0x01
+#define HANTRO_DEC_ABORT		0x20
+#define HANTRO_DEC_IRQ_DISABLE		0x10
+#define HANTRO_PP_IRQ_DISABLE		0x10
+#define HANTRO_DEC_IRQ			0x100
+#define HANTRO_PP_IRQ			0x100
+
+#endif /* __DWL_DEFS_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_enc.c b/drivers/gpu/drm/hantro/hantro_enc.c
new file mode 100644
index 000000000000..0dabb00432e5
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_enc.c
@@ -0,0 +1,783 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro encoder hardware driver.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_enc.h"
+
+static u32 resource_shared;
+
+#define KMB_VC8000E_PAGE_LUT           0x20885000
+
+#define HANTRO_VC8KE_REG_BWREAD_KMB	215
+#define HANTRO_VC8KE_REG_BWREAD		216
+#define HANTRO_VC8KE_REG_BWWRITE_KMB	219
+#define HANTRO_VC8KE_REG_BWWRITE	220
+#define VC8KE_BURSTWIDTH 16
+
+static int reserve_io(struct hantroenc_t *pcore);
+static void release_io(struct hantroenc_t *pcore);
+static void reset_asic(struct hantroenc_t *dev);
+static int check_core_occupation(struct hantroenc_t *dev);
+static void release_encoder(struct hantroenc_t *dev, u32 *core_info,
+			    u32 nodenum);
+/* IRQ handler */
+static irqreturn_t hantroenc_isr(int irq, void *dev_id);
+
+static unsigned long long sram_base;
+static unsigned int sram_size;
+static int hantroenc_major;
+
+static int check_enc_irq(struct hantroenc_t *dev, u32 *core_info,
+			 u32 *irq_status, u32 nodenum)
+{
+	unsigned long flags;
+	int rdy = 0;
+	u32 i = 0;
+	u8 core_mapping = 0;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	core_mapping = (u8)(*core_info & 0xFF);
+
+	while (core_mapping) {
+		if (core_mapping & 0x1) {
+			if (i >= nodenum)
+				break;
+
+			spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+
+			if (dev->irq_received) {
+				/* reset the wait condition(s) */
+				PDEBUG("check %d irq ready\n", i);
+				dev->irq_received = 0;
+				rdy = 1;
+				*core_info = i;
+				*irq_status = dev->irq_status;
+			}
+
+			spin_unlock_irqrestore(&pdevinfo->enc_owner_lock,
+					       flags);
+			break;
+		}
+		core_mapping = core_mapping >> 1;
+		i++;
+		dev = dev->next;
+	}
+
+	return rdy;
+}
+
+static unsigned int wait_enc_ready(struct hantroenc_t *dev, u32 *core_info,
+				   u32 *irq_status, u32 nodenum)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	PDEBUG("%s\n", __func__);
+	if (wait_event_interruptible(pdevinfo->enc_wait_queue,
+				     check_enc_irq(dev, core_info, irq_status,
+						   nodenum))) {
+		PDEBUG("ENC wait_event_interruptible interrupted\n");
+		release_encoder(dev, core_info, nodenum);
+		return -ERESTARTSYS;
+	}
+
+	return 0;
+}
+
+u32 hantroenc_read_bandwidth(struct device_info *pdevinfo, int is_read_bw)
+{
+	int i, devcnt = get_device_count();
+	u32 bandwidth = 0;
+	struct hantroenc_t *pcore;
+
+	if (!pdevinfo) {
+		for (i = 0; i < devcnt; i++) {
+			pcore = get_enc_node_by_device_id(i, 0);
+			while (pcore) {
+				if (is_read_bw) {
+					if (hantro_drm.device_type == DEVICE_KEEMBAY)
+						bandwidth +=
+							ioread32((pcore->hwregs +
+									  HANTRO_VC8KE_REG_BWREAD_KMB * 4));
+					else
+						bandwidth +=
+							ioread32((pcore->hwregs +
+									  HANTRO_VC8KE_REG_BWREAD * 4));
+				} else {
+					if (hantro_drm.device_type == DEVICE_KEEMBAY)
+						bandwidth +=
+							ioread32((pcore->hwregs +
+									  HANTRO_VC8KE_REG_BWWRITE_KMB * 4));
+					else
+						bandwidth +=
+							ioread32((pcore->hwregs +
+									  HANTRO_VC8KE_REG_BWWRITE * 4));
+				}
+
+				pcore = pcore->next;
+			}
+		}
+	} else {
+		pcore = get_enc_node(pdevinfo, 0);
+		while (pcore) {
+			if (is_read_bw) {
+				if (hantro_drm.device_type == DEVICE_KEEMBAY)
+					bandwidth += ioread32((pcore->hwregs +
+								       HANTRO_VC8KE_REG_BWREAD_KMB * 4));
+				else
+					bandwidth += ioread32((pcore->hwregs +
+								       HANTRO_VC8KE_REG_BWREAD * 4));
+
+			} else {
+				if (hantro_drm.device_type == DEVICE_KEEMBAY)
+					bandwidth += ioread32((pcore->hwregs +
+								       HANTRO_VC8KE_REG_BWWRITE_KMB * 4));
+				else
+					bandwidth += ioread32((pcore->hwregs +
+								       HANTRO_VC8KE_REG_BWWRITE * 4));
+			}
+
+			pcore = pcore->next;
+		}
+	}
+
+	return bandwidth * VC8KE_BURSTWIDTH;
+}
+
+static int check_core_occupation(struct hantroenc_t *dev)
+{
+	int ret = 0;
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+	if (!dev->is_reserved) {
+		dev->is_reserved = 1;
+		dev->pid = current->pid;
+		ret = 1;
+		PDEBUG("%s pid=%d\n", __func__, dev->pid);
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->enc_owner_lock, flags);
+	return ret;
+}
+
+static int get_workable_core(struct hantroenc_t *dev, u32 *core_info,
+			     u32 *core_info_tmp, u32 nodenum)
+{
+	int ret = 0;
+	u32 i = 0;
+	u32 cores;
+	u32 core_id = 0;
+	u8 core_mapping = 0;
+	u32 required_num = 0;
+
+	cores = *core_info;
+	required_num = ((cores >> CORE_INFO_AMOUNT_OFFSET) & 0x7) + 1;
+	core_mapping = (u8)(cores & 0xFF);
+
+	if (*core_info_tmp == 0)
+		*core_info_tmp = required_num << 8;
+	else
+		required_num = ((*core_info_tmp & 0xF00) >> 8);
+
+	PDEBUG("%s:required_num=%d,core_info=%x\n", __func__, required_num,
+	       *core_info);
+	if (required_num) {
+		/* a valid free Core that has specified core id */
+		while (core_mapping) {
+			if (core_mapping & 0x1) {
+				if (i >= nodenum)
+					break;
+
+				core_id = i;
+				if (check_core_occupation(dev)) {
+					*core_info_tmp =
+						((((*core_info_tmp & 0xF00) >>
+						   8) -
+						  1)
+						 << 8) |
+						(*core_info_tmp & 0x0FF);
+					*core_info_tmp =
+						*core_info_tmp | (1 << core_id);
+
+					if (((*core_info_tmp & 0xF00) >> 8) ==
+					    0) {
+						ret = 1;
+						*core_info =
+							(*core_info &
+							 0xFFFFFF00) |
+							(*core_info_tmp & 0xFF);
+						*core_info_tmp = 0;
+						required_num = 0;
+						break;
+					}
+				}
+			}
+
+			core_mapping = core_mapping >> 1;
+			i++;
+			dev = dev->next;
+		}
+	} else {
+		ret = 1;
+	}
+
+	PDEBUG("*core_info = %x\n", *core_info);
+	return ret;
+}
+
+static long reserve_encoder(struct hantroenc_t *dev, u32 *core_info,
+			    u32 nodenum)
+{
+	struct device_info *pdevinfo = dev->pdevinfo;
+	struct hantroenc_t *reserved_core = NULL;
+	u32 core_info_tmp = 0;
+	int ret = 0;
+
+	START_TIME;
+	PDEBUG("hx280enc: %s\n", __func__);
+	/* If HW resources are shared inter cores, just make sure only one is using the HW */
+	if (resource_shared) {
+		if (down_interruptible(&pdevinfo->enc_core_sem)) {
+			ret = -ERESTARTSYS;
+			nodenum = 0xffffffff;
+			goto out;
+		}
+	}
+
+	/* lock a core that has specified core id */
+	if (wait_event_interruptible(pdevinfo->enc_hw_queue,
+				     get_workable_core(dev, core_info,
+						       &core_info_tmp,
+						       nodenum) != 0)) {
+		nodenum = 0xffffffff;
+		ret = -ERESTARTSYS;
+		goto out;
+	}
+
+	reserved_core = get_enc_node(pdevinfo, KCORE(*core_info) - 1);
+	if (!reserved_core) {
+		pr_debug("Core not found. Possibly Lookahead node");
+		goto out;
+	}
+
+	if (reserved_core->dev_clk &&
+	    pdevinfo->thermal_data.clk_freq != reserved_core->clk_freq) {
+		clk_set_rate(reserved_core->dev_clk,
+			     pdevinfo->thermal_data.clk_freq);
+		reserved_core->clk_freq = pdevinfo->thermal_data.clk_freq;
+	}
+
+	reserved_core->perf_data.last_resv = sched_clock();
+out:
+	trace_enc_reserve(pdevinfo->deviceid, KCORE((*core_info)),
+			  (sched_clock() - start) / 1000);
+	return ret;
+}
+
+static void release_encoder(struct hantroenc_t *dev, u32 *core_info,
+			    u32 nodenum)
+{
+	unsigned long flags;
+	u32 i = 0, core_id;
+	u8 core_mapping = 0;
+	struct device_info *pdevinfo = dev->pdevinfo;
+	struct hantroenc_t *reserved_core;
+
+	core_id = KCORE((*core_info));
+	reserved_core = get_enc_node(pdevinfo, core_id - 1);
+	if (reserved_core) {
+		reserved_core->perf_data.count++;
+		reserved_core->perf_data.totaltime +=
+			(sched_clock() -
+			 (reserved_core->perf_data.last_resv == 0 ?
+				  sched_clock() :
+				  reserved_core->perf_data.last_resv));
+	}
+
+	core_mapping = (u8)(*core_info & 0xFF);
+	/* release specified core id */
+	while (core_mapping) {
+		if (core_mapping & 0x1) {
+			if (i >= nodenum)
+				break;
+
+			core_id = i;
+			spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+			PDEBUG("dev[core_id].pid=%d,current->pid=%d\n",
+			       dev->pid, current->pid);
+			if (dev->is_reserved && dev->pid == current->pid) {
+				dev->pid = -1;
+				dev->is_reserved = 0;
+				dev->irq_received = 0;
+				dev->irq_status = 0;
+			}
+			spin_unlock_irqrestore(&pdevinfo->enc_owner_lock,
+					       flags);
+		}
+
+		core_mapping = core_mapping >> 1;
+		i++;
+		dev = dev->next;
+	}
+
+	wake_up_interruptible_all(&pdevinfo->enc_hw_queue);
+	if (resource_shared)
+		up(&pdevinfo->enc_core_sem);
+
+	trace_enc_release(pdevinfo->deviceid, KCORE((*core_info)));
+}
+
+long hantroenc_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	unsigned int id, tmp, node, deviceid;
+	struct hantroenc_t *pcore;
+	u32 core_info;
+
+	if (enable_encode == 0)
+		return -EFAULT;
+
+	switch (cmd) {
+	case HX280ENC_IOCGHWOFFSET: {
+		__get_user(id, (unsigned long long __user *)arg);
+		node = KCORE(id);
+		deviceid = DEVICE_ID(id);
+		pcore = get_enc_node_by_device_id(deviceid, node);
+		if (!pcore)
+			return -EFAULT;
+
+		__put_user(pcore->core_cfg.base_addr,
+			   (unsigned long long __user *)arg);
+		break;
+	}
+
+	case HX280ENC_IOCGHWIOSIZE: {
+		u32 io_size;
+
+		__get_user(id, (unsigned long __user *)arg);
+		node = KCORE(id);
+		deviceid = DEVICE_ID(id);
+		pcore = get_enc_node_by_device_id(deviceid, node);
+		if (!pcore)
+			return -EFAULT;
+
+		io_size = pcore->core_cfg.iosize;
+		__put_user(io_size, (u32 __user *)arg);
+		return 0;
+	}
+	case HX280ENC_IOCGSRAMOFFSET:
+		__put_user(sram_base, (unsigned long long __user *)arg);
+		break;
+	case HX280ENC_IOCGSRAMEIOSIZE:
+		__put_user(sram_size, (unsigned int __user *)arg);
+		break;
+	case HX280ENC_IOCG_CORE_NUM:
+		__get_user(tmp, (__u32 __user *)arg);
+		tmp = get_device_core_num(tmp, CORE_ENC);
+		__put_user(tmp, (u32 __user *)arg);
+		break;
+	case HX280ENC_IOCH_ENC_RESERVE: {
+		int ret;
+
+		PDEBUG("Reserve ENC Cores\n");
+		__get_user(core_info, (unsigned long __user *)arg);
+		deviceid = (core_info >> 16) & 0xff;
+		pcore = get_enc_node_by_device_id(deviceid, 0); /* from list header */
+		if (!pcore) {
+			pr_err("wrong device num");
+			return -EFAULT;
+		}
+
+		tmp = get_device_core_num(deviceid, CORE_ENC);
+		ret = reserve_encoder(pcore, &core_info, tmp);
+		if (ret == 0)
+			__put_user(core_info, (u32 __user *)arg);
+
+		return ret;
+	}
+	case HX280ENC_IOCH_ENC_RELEASE: {
+		__get_user(core_info, (unsigned long __user *)arg);
+		deviceid = (core_info >> 16) & 0xff;
+		pcore = get_enc_node_by_device_id(deviceid, 0); /* from list header */
+		if (!pcore)
+			return -EFAULT;
+
+		PDEBUG("Release ENC Core\n");
+		tmp = get_device_core_num(deviceid, CORE_ENC);
+		release_encoder(pcore, &core_info, tmp);
+		break;
+	}
+
+	case HX280ENC_IOCG_CORE_WAIT: {
+		u32 irq_status;
+
+		__get_user(core_info, (u32 __user *)arg);
+		deviceid = DEVICE_ID(core_info);
+		pcore = get_enc_node_by_device_id(deviceid, 0);
+		if (!pcore)
+			return -EFAULT;
+
+		tmp = get_device_core_num(deviceid, CORE_ENC);
+		tmp = wait_enc_ready(pcore, &core_info, &irq_status, tmp);
+		if (tmp == 0) {
+			__put_user(irq_status, (unsigned int __user *)arg);
+			return core_info; /* return core_id */
+		}
+
+		__put_user(0, (unsigned int __user *)arg);
+		return -1;
+	}
+	}
+	return 0;
+}
+
+int hantroenc_release(void)
+{
+	struct device_info *pdevinfo;
+	int i, devicecnt = get_device_count();
+	struct hantroenc_t *dev;
+	unsigned long flags;
+
+	if (enable_encode == 0)
+		return 0;
+
+	for (i = 0; i < devicecnt; i++) {
+		dev = get_enc_node_by_device_id(i, 0);
+		if (!dev)
+			continue;
+
+		pdevinfo = dev->pdevinfo;
+		while (dev) {
+			spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+			if (dev->is_reserved == 1 && dev->pid == current->pid) {
+				dev->pid = -1;
+				dev->is_reserved = 0;
+				dev->irq_received = 0;
+				dev->irq_status = 0;
+				PDEBUG("release reserved core\n");
+			}
+			spin_unlock_irqrestore(&pdevinfo->enc_owner_lock,
+					       flags);
+			dev = dev->next;
+		}
+
+		wake_up_interruptible_all(&pdevinfo->enc_hw_queue);
+		if (resource_shared)
+			up(&pdevinfo->enc_core_sem);
+	}
+
+	return 0;
+}
+
+static void setup_enc_lut(void)
+{
+	u8 __iomem *enc_page_lut_regs;
+
+	if (hantro_drm.enc_page_lut_regs) {
+		pr_info("hantroenc: page_lut already reserved\n");
+		return;
+	}
+
+	/* Register and set the page lookup table for read */
+	if (!request_mem_region(KMB_VC8000E_PAGE_LUT, 0x100,
+				"hantroenc_pagelut_read")) {
+		pr_err("hantroenc: failed to reserve page lookup table registers\n");
+		return;
+	}
+
+	enc_page_lut_regs = ioremap(KMB_VC8000E_PAGE_LUT, 0x100);
+	if (!enc_page_lut_regs) {
+		pr_err("hantroenc: failed to ioremap page lookup table registers\n");
+		return;
+	}
+
+	/* Set write page LUT AXI ID 1-8 to 0x4 */
+	iowrite32(0x04040400, enc_page_lut_regs + 0x10);
+	pr_info("hx280enc: Page LUT WR AXI ID 3:0 = %x\n",
+		ioread32(enc_page_lut_regs + 0x10));
+	iowrite32(0x04040404, enc_page_lut_regs + 0x14);
+	pr_info("hx280enc: Page LUT WR AXI ID 7:4 = %x\n",
+		ioread32(enc_page_lut_regs + 0x14));
+	iowrite32(0x00000004, enc_page_lut_regs + 0x18);
+	pr_info("hx280enc: Page LUT WR AXI ID 8 = %x\n",
+		ioread32(enc_page_lut_regs + 0x18));
+	iowrite32(0x04040004, enc_page_lut_regs);
+	pr_info("hx280enc: RD AXI 3:0 = %x\n",
+		ioread32(enc_page_lut_regs));
+	iowrite32(0x04040404, enc_page_lut_regs + 0x4);
+	pr_info("hx280enc: RD AXI 7:4  = %x\n",
+		ioread32(enc_page_lut_regs + 0x4));
+	iowrite32(0x00000004, enc_page_lut_regs + 0x8);
+	pr_info("hx280enc: RD AXI 8 = %x\n",
+		ioread32(enc_page_lut_regs + 0x8));
+	hantro_drm.enc_page_lut_regs = enc_page_lut_regs;
+}
+
+int __init hantroenc_init(void)
+{
+	sram_base = 0;
+	sram_size = 0;
+	hantroenc_major = 0;
+	resource_shared = 0;
+	if (hantro_drm.device_type == DEVICE_KEEMBAY && enable_lut)
+		setup_enc_lut();
+
+	return 0;
+}
+
+int __exit hantroenc_cleanup(void)
+{
+	if (hantro_drm.enc_page_lut_regs) {
+		iounmap(hantro_drm.enc_page_lut_regs);
+		hantro_drm.enc_page_lut_regs = NULL;
+		release_mem_region(KMB_VC8000E_PAGE_LUT, 0x100);
+	}
+
+	return 0;
+}
+
+static void disable_enc_clock(struct hantroenc_t *pcore)
+{
+	if (!pcore->dev_clk)
+		return;
+
+	clk_disable_unprepare(pcore->dev_clk);
+	clk_put(pcore->dev_clk);
+	pcore->dev_clk = NULL;
+}
+
+static int init_enc_clock(struct hantroenc_t *pcore, struct dtbnode *pnode)
+{
+	if (strlen(pnode->clock_name) == 0)
+		return 0;
+
+	pcore->dev_clk = clk_get(pnode->pdevinfo->dev, pnode->clock_name);
+	if (IS_ERR(pcore->dev_clk) || !pcore->dev_clk) {
+		pr_err("%s: clock %s not found. err = %ld", __func__,
+		       pnode->clock_name, PTR_ERR(pcore->dev_clk));
+		pcore->dev_clk = NULL;
+		return -EINVAL;
+	}
+
+	clk_prepare_enable(pcore->dev_clk);
+	pcore->clk_freq = pnode->pdevinfo->thermal_data.clk_freq;
+	clk_set_rate(pcore->dev_clk, pcore->clk_freq);
+	return 0;
+}
+
+int hantroenc_probe(struct dtbnode *pnode)
+{
+	int result = 0;
+	struct hantroenc_t *pcore;
+	int i;
+	int irqn;
+
+	if (enable_encode == 0)
+		return 0;
+
+	pcore = vmalloc(sizeof(*pcore));
+	if (!pcore)
+		return -ENOMEM;
+
+	memset(pcore, 0, sizeof(struct hantroenc_t));
+	pcore->core_cfg.base_addr = pnode->ioaddr;
+	pcore->core_cfg.iosize = pnode->iosize;
+
+	result = reserve_io(pcore);
+	if (result < 0) {
+		pr_err("hx280enc: reserve reg 0x%llx:%lldfail\n", pnode->ioaddr,
+		       pnode->iosize);
+		vfree(pcore);
+		return -ENODEV;
+	}
+
+	reset_asic(pcore); /* reset hardware */
+	irqn = 0;
+	for (i = 0; i < 4; i++)
+		pcore->irqlist[i] = -1;
+
+	if (enable_irqmode == 1) {
+		for (i = 0; i < 4; i++) {
+			if (pnode->irq[i] > 0) {
+				strcpy(pcore->irq_name[i], pnode->irq_name[i]);
+				result = request_irq(pnode->irq[i],
+						     hantroenc_isr, IRQF_SHARED,
+						     pcore->irq_name[i],
+						     (void *)pcore);
+				if (result == 0) {
+					pcore->irqlist[irqn] = pnode->irq[i];
+					irqn++;
+				} else {
+					pr_info("hx280enc: request IRQ <%d> fail\n",
+						pnode->irq[i]);
+					release_io(pcore);
+					vfree(pcore);
+					return -EINVAL;
+				}
+			}
+		}
+	}
+
+	init_enc_clock(pcore, pnode);
+	add_enc_node(pnode->pdevinfo, pcore);
+	pr_info("hx280enc: module inserted. Major <%d>\n", hantroenc_major);
+	return 0;
+}
+
+void hantroenc_remove(struct device_info *pdevinfo)
+{
+	int k;
+	struct hantroenc_t *pcore, *pnext;
+
+	pcore = get_enc_node(pdevinfo, 0);
+	while (pcore) {
+		u32 hwid = pcore->hw_id;
+		u32 major_id = (hwid & 0x0000FF00) >> 8;
+		u32 wclr = (major_id >= 0x61) ? (0x1FD) : (0);
+
+		pnext = pcore->next;
+		disable_enc_clock(pcore);
+		iowrite32(0, (pcore->hwregs + 0x14)); /* disable HW */
+		iowrite32(wclr,
+			  (pcore->hwregs + 0x04)); /* clear enc IRQ */
+
+		/* free the encoder IRQ */
+		for (k = 0; k < 4; k++)
+			if (pcore->irqlist[k] > 0)
+				free_irq(pcore->irqlist[k], (void *)pcore);
+
+		release_io(pcore);
+		vfree(pcore);
+		pcore = pnext;
+	}
+}
+
+static int reserve_io(struct hantroenc_t *pcore)
+{
+	u32 hwid;
+
+	PDEBUG("hx280enc: %s called\n", __func__);
+	if (!request_mem_region(pcore->core_cfg.base_addr,
+				pcore->core_cfg.iosize, pcore->reg_name)) {
+		pr_info("hantroenc: failed to reserve HW regs\n");
+		return -1;
+	}
+
+	pcore->hwregs = ioremap(pcore->core_cfg.base_addr,
+				pcore->core_cfg.iosize);
+	if (!pcore->hwregs) {
+		pr_info("hantroenc: failed to ioremap HW regs\n");
+		release_mem_region(pcore->core_cfg.base_addr,
+				   pcore->core_cfg.iosize);
+		return -1;
+	}
+
+	/* read hwid and check validness and store it */
+	hwid = (u32)ioread32(pcore->hwregs);
+	/* check for encoder HW ID */
+	if (((((hwid >> 16) & 0xFFFF) != ((ENC_HW_ID1 >> 16) & 0xFFFF))) &&
+	    ((((hwid >> 16) & 0xFFFF) != ((ENC_HW_ID2 >> 16) & 0xFFFF)))) {
+		pr_info("hantroenc: HW not found at %llx, HWID = 0x%x\n",
+			pcore->core_cfg.base_addr, (hwid >> 16) & 0xFFFF);
+		release_io(pcore);
+		return -1;
+	}
+
+	pcore->hw_id = hwid;
+	pr_info("hantroenc: HW at base <0x%llx> with ID 0x%x\n",
+		pcore->core_cfg.base_addr, (hwid >> 16) & 0xFFFF);
+	return 0;
+}
+
+static void release_io(struct hantroenc_t *pcore)
+{
+	if (pcore->hwregs)
+		iounmap(pcore->hwregs);
+	release_mem_region(pcore->core_cfg.base_addr, pcore->core_cfg.iosize);
+}
+
+static irqreturn_t hantroenc_isr(int irq, void *dev_id)
+{
+	unsigned int handled = 0;
+	struct hantroenc_t *dev = (struct hantroenc_t *)dev_id;
+	u32 irq_status;
+	unsigned long flags;
+	struct device_info *pdevinfo = dev->pdevinfo;
+
+	/*
+	 * If core is not reserved by any user, but irq is received, just
+	 * ignore it
+	 */
+	spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+	if (!dev->is_reserved) {
+		PDEBUG("%s:received IRQ but core is not reserved!\n", __func__);
+		irq_status = (u32)ioread32((dev->hwregs + 0x04));
+		if (irq_status & 0x01) {
+			/*
+			 * clear all IRQ bits. (hwid >= 0x80006100) means IRQ
+			 * is cleared by writing 1
+			 */
+			u32 hwid = ioread32(dev->hwregs);
+			u32 major_id = (hwid & 0x0000FF00) >> 8;
+			u32 wclr = (major_id >= 0x61) ? irq_status :
+							(irq_status & (~0x1FD));
+
+			/*
+			 * Disable HW when buffer over-flow happen
+			 * HW behavior changed in over-flow
+			 * in-pass, HW cleanup HWIF_ENC_E auto
+			 * new version:  ask SW cleanup HWIF_ENC_E when buffer
+			 * over-flow
+			 */
+			if (irq_status & 0x20)
+				iowrite32(0, (dev->hwregs + 0x14));
+			iowrite32(wclr, (dev->hwregs + 0x04));
+		}
+
+		spin_unlock_irqrestore(&pdevinfo->enc_owner_lock, flags);
+		return IRQ_HANDLED;
+	}
+
+	spin_unlock_irqrestore(&pdevinfo->enc_owner_lock, flags);
+	irq_status = (u32)ioread32((dev->hwregs + 0x04));
+	if (irq_status & 0x01) {
+		/*
+		 * clear all IRQ bits. (hwid >= 0x80006100) means IRQ is
+		 * cleared by writing 1
+		 */
+		u32 hwid = ioread32(dev->hwregs);
+		u32 major_id = (hwid & 0x0000FF00) >> 8;
+		u32 wclr = (major_id >= 0x61) ? irq_status :
+						(irq_status & (~0x1FD));
+		if (irq_status & 0x20)
+			iowrite32(0, (dev->hwregs + 0x14));
+
+		iowrite32(wclr, (dev->hwregs + 0x04));
+		spin_lock_irqsave(&pdevinfo->enc_owner_lock, flags);
+		dev->irq_received = 1;
+		dev->irq_status = irq_status & (~0x01);
+		spin_unlock_irqrestore(&pdevinfo->enc_owner_lock, flags);
+		wake_up_interruptible_all(&pdevinfo->enc_wait_queue);
+		handled++;
+	}
+
+	if (!handled)
+		pr_info("IRQ received, but not hantro enc's!\n");
+
+	return IRQ_HANDLED;
+}
+
+static void reset_asic(struct hantroenc_t *dev)
+{
+	int i;
+
+	PDEBUG("hx280enc: %s\n", __func__);
+	iowrite32(0, (dev->hwregs + 0x14));
+	for (i = 4; i < dev->core_cfg.iosize; i += 4)
+		iowrite32(0, (dev->hwregs + i));
+}
diff --git a/drivers/gpu/drm/hantro/hantro_enc.h b/drivers/gpu/drm/hantro/hantro_enc.h
new file mode 100644
index 000000000000..6d44496e9e14
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_enc.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro encoder hardware driver header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HX280ENC_H__
+#define __HX280ENC_H__
+
+#define ENC_HW_ID1		0x48320100
+#define ENC_HW_ID2		0x80006000
+#define CORE_INFO_AMOUNT_OFFSET	28
+
+long hantroenc_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+int hantroenc_init(void);
+int hantroenc_cleanup(void);
+int hantroenc_probe(struct dtbnode *pnode);
+void hantroenc_remove(struct device_info *pdevinfo);
+u32 hantroenc_read_bandwidth(struct device_info *pdevinfo, int is_read_bw);
+int hantroenc_release(void);
+
+#endif /* __HX280ENC_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_fence.c b/drivers/gpu/drm/hantro/hantro_fence.c
new file mode 100644
index 000000000000..4da53923405b
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_fence.c
@@ -0,0 +1,277 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver DMA_BUF fence operation.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+
+static unsigned long seqno;
+static DEFINE_IDR(fence_idr);
+/* fence mutex struct */
+static struct mutex fence_mutex;
+
+static const char *hantro_fence_get_driver_name(struct dma_fence *fence)
+{
+	return "hantro";
+}
+
+static const char *hantro_fence_get_timeline_name(struct dma_fence *fence)
+{
+	return " ";
+}
+
+static bool hantro_fence_enable_signaling(struct dma_fence *fence)
+{
+	if (test_bit(HANTRO_FENCE_FLAG_ENABLE_SIGNAL_BIT, &fence->flags))
+		return true;
+	else
+		return false;
+}
+
+static bool hantro_fence_signaled(struct dma_fence *fobj)
+{
+	unsigned long irqflags;
+	bool ret;
+
+	spin_lock_irqsave(fobj->lock, irqflags);
+	ret = (test_bit(HANTRO_FENCE_FLAG_SIGNAL_BIT, &fobj->flags) != 0);
+	spin_unlock_irqrestore(fobj->lock, irqflags);
+	return ret;
+}
+
+static void hantro_fence_free(struct dma_fence *fence)
+{
+	kfree(fence->lock);
+	fence->lock = NULL;
+	dma_fence_free(fence);
+}
+
+static const struct dma_fence_ops hantro_fenceops = {
+	.get_driver_name = hantro_fence_get_driver_name,
+	.get_timeline_name = hantro_fence_get_timeline_name,
+	.enable_signaling = hantro_fence_enable_signaling,
+	.signaled = hantro_fence_signaled,
+	.wait = hantro_fence_default_wait,
+	.release = hantro_fence_free,
+};
+
+static struct dma_fence *alloc_fence(unsigned int ctxno)
+{
+	struct dma_fence *fobj;
+	/* spinlock for fence */
+	spinlock_t *lock;
+
+	fobj = kzalloc(sizeof(*fobj), GFP_KERNEL);
+	if (!fobj)
+		return NULL;
+
+	lock = kzalloc(sizeof(*lock), GFP_KERNEL);
+	if (!lock) {
+		kfree(fobj);
+		return NULL;
+	}
+
+	spin_lock_init(lock);
+	hantro_fence_init(fobj, &hantro_fenceops, lock, ctxno, seqno++);
+	clear_bit(HANTRO_FENCE_FLAG_SIGNAL_BIT, &fobj->flags);
+	set_bit(HANTRO_FENCE_FLAG_ENABLE_SIGNAL_BIT, &fobj->flags);
+	return fobj;
+}
+
+int init_hantro_resv(struct dma_resv *presv,
+		     struct drm_gem_hantro_object *cma_obj)
+{
+	dma_resv_init(presv);
+	cma_obj->ctxno = hantro_fence_context_alloc(1);
+
+	return 0;
+}
+
+int hantro_setdomain(struct drm_device *dev, void *data,
+		     struct drm_file *file_priv)
+{
+	return 0;
+}
+
+void init_fence_data(void)
+{
+	seqno = 0;
+	mutex_init(&fence_mutex);
+	idr_init(&fence_idr);
+}
+
+static int fence_idr_fini(int id, void *p, void *data)
+{
+	hantro_fence_signal(p);
+	hantro_fence_put(p);
+	return 0;
+}
+
+void release_fence_data(void)
+{
+	mutex_lock(&fence_mutex);
+	idr_for_each(&fence_idr, fence_idr_fini, NULL);
+	idr_destroy(&fence_idr);
+	mutex_unlock(&fence_mutex);
+}
+
+int hantro_acquirebuf(struct drm_device *dev, void *data,
+		      struct drm_file *file_priv)
+{
+	struct hantro_acquirebuf *arg = data;
+	struct dma_resv *resv;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj;
+	struct dma_fence *fence = NULL;
+	unsigned long timeout = arg->timeout;
+	long fenceid = -1;
+	int ret = 0;
+
+	START_TIME;
+	obj = hantro_gem_object_lookup(dev, file_priv, arg->handle);
+	if (!obj) {
+		ret = -ENOENT;
+		trace_fence_acquirebuf(NULL, arg->handle, -1, 0, ret);
+		return ret;
+	}
+
+	if (!obj->dma_buf) {
+		if (hantro_drm.drm_dev == obj->dev) {
+			struct drm_gem_hantro_object *hobj =
+				to_drm_gem_hantro_obj(obj);
+
+			resv = &hobj->kresv;
+		} else {
+			ret = -ENOENT;
+			goto err;
+		}
+
+	} else {
+		resv = obj->dma_buf->resv;
+	}
+
+	/* Check for a stalled fence */
+	if (!dma_resv_wait_timeout(resv, arg->flags & HANTRO_FENCE_WRITE, 1,
+				       timeout)) {
+		ret = -EBUSY;
+		goto err;
+	}
+
+	/* Expose the fence via the dma-buf */
+	ret = -ENOMEM;
+	fence = alloc_fence(hantro_fence_context_alloc(1));
+	if (!fence)
+		goto err;
+
+	mutex_lock(&fence_mutex);
+	ret = idr_alloc(&fence_idr, fence, 1, 0, GFP_KERNEL);
+	mutex_unlock(&fence_mutex);
+	if (ret >= 0)
+		fenceid = ret;
+	else
+		goto err;
+
+	dma_resv_lock(resv, NULL);
+	ret = 0;
+	if (arg->flags & HANTRO_FENCE_WRITE) {
+		dma_resv_add_excl_fence(resv, fence);
+	} else {
+		ret = hantro_reserve_obj_shared(resv, 1);
+		if (ret == 0)
+			dma_resv_add_shared_fence(resv, fence);
+	}
+
+	dma_resv_unlock(resv);
+
+	/* Record the fence in our idr for later signaling */
+	if (ret == 0) {
+		arg->fence_handle = fenceid;
+		goto out;
+	}
+
+err:
+	if (fenceid >= 0) {
+		mutex_lock(&fence_mutex);
+		idr_remove(&fence_idr, fenceid);
+		mutex_unlock(&fence_mutex);
+	}
+
+	if (fence) {
+		hantro_fence_signal(fence);
+		hantro_fence_put(fence);
+	}
+
+out:
+	cma_obj = (struct drm_gem_hantro_object *)obj;
+	trace_fence_acquirebuf((void *)cma_obj->paddr, arg->handle,
+			       arg->fence_handle,
+			       (sched_clock() - start) / 1000, ret);
+	hantro_unref_drmobj(obj);
+	return ret;
+}
+
+int hantro_testbufvalid(struct drm_device *dev, void *data,
+			struct drm_file *file_priv)
+{
+	struct hantro_fencecheck *arg = data;
+	struct dma_resv *resv;
+	struct drm_gem_object *obj;
+
+	arg->ready = 0;
+	obj = hantro_gem_object_lookup(dev, file_priv, arg->handle);
+	if (!obj)
+		return -ENOENT;
+
+	if (!obj->dma_buf) {
+		if (hantro_drm.drm_dev == obj->dev) {
+			struct drm_gem_hantro_object *hobj =
+				to_drm_gem_hantro_obj(obj);
+
+			resv = &hobj->kresv;
+		} else {
+			hantro_unref_drmobj(obj);
+			return -ENOENT;
+		}
+
+	} else {
+		resv = obj->dma_buf->resv;
+	}
+
+	/* Check for a stalled fence */
+	if (dma_resv_wait_timeout(resv, 1, 1, 0) <= 0)
+		arg->ready = 0;
+	else
+		arg->ready = 1;
+
+	hantro_unref_drmobj(obj);
+	return 0;
+}
+
+int hantro_releasebuf(struct drm_device *dev, void *data,
+		      struct drm_file *file_priv)
+{
+	struct hantro_releasebuf *arg = data;
+	struct dma_fence *fence;
+	int ret = 0;
+
+	mutex_lock(&fence_mutex);
+	fence = idr_replace(&fence_idr, NULL, arg->fence_handle);
+	mutex_unlock(&fence_mutex);
+
+	if (!fence || IS_ERR(fence))
+		return -ENOENT;
+
+	if (hantro_fence_is_signaled(fence))
+		ret = -ETIMEDOUT;
+
+	trace_fence_releasebuf(arg->fence_handle, ret);
+	hantro_fence_signal(fence);
+	hantro_fence_put(fence);
+	mutex_lock(&fence_mutex);
+	idr_remove(&fence_idr, arg->fence_handle);
+	mutex_unlock(&fence_mutex);
+	return ret;
+}
diff --git a/drivers/gpu/drm/hantro/hantro_fs.c b/drivers/gpu/drm/hantro/hantro_fs.c
new file mode 100644
index 000000000000..dad4ced1aa35
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_fs.c
@@ -0,0 +1,476 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver fs operation.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_priv.h"
+#include "hantro_dec.h"
+#include "hantro_enc.h"
+
+static ssize_t bandwidth_dec_read_show(struct device *kdev,
+				       struct device_attribute *attr, char *buf)
+{
+	/*
+	 * sys/bus/platform/drivers/hantro/xxxxx.vpu/bandwidth_dec_read
+	 * used to show bandwidth info to user space
+	 * all core' bandwidth might be exported in same string.
+	 * data is just an example. Real data should be read from HW registers
+	 * this file is read only.
+	 */
+	u32 bandwidth;
+	struct device_info *pdevinfo;
+
+	pdevinfo = dev_get_drvdata(kdev);
+	if (!pdevinfo)
+		return 0;
+
+	bandwidth = hantrodec_read_bandwidth(pdevinfo, 1);
+	return snprintf(buf, PAGE_SIZE, "%u\n", bandwidth);
+}
+
+static ssize_t bandwidth_dec_write_show(struct device *kdev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	u32 bandwidth;
+	struct device_info *pdevinfo;
+
+	pdevinfo = dev_get_drvdata(kdev);
+	if (!pdevinfo)
+		return 0;
+
+	bandwidth = hantrodec_read_bandwidth(pdevinfo, 0);
+	return snprintf(buf, PAGE_SIZE, "%u\n", bandwidth);
+}
+
+static ssize_t bandwidth_enc_read_show(struct device *kdev,
+				       struct device_attribute *attr, char *buf)
+{
+	u32 bandwidth;
+	struct device_info *pdevinfo;
+
+	pdevinfo = dev_get_drvdata(kdev);
+	if (!pdevinfo)
+		return 0;
+
+	bandwidth = hantroenc_read_bandwidth(pdevinfo, 1);
+	return snprintf(buf, PAGE_SIZE, "%u\n", bandwidth);
+}
+
+static ssize_t bandwidth_enc_write_show(struct device *kdev,
+					struct device_attribute *attr,
+					char *buf)
+{
+	u32 bandwidth;
+	struct device_info *pdevinfo;
+
+	pdevinfo = dev_get_drvdata(kdev);
+	if (!pdevinfo)
+		return 0;
+
+	bandwidth = hantroenc_read_bandwidth(pdevinfo, 0);
+	return snprintf(buf, PAGE_SIZE, "%u\n", bandwidth);
+}
+
+static ssize_t clients_show(struct device *kdev, struct device_attribute *attr,
+			    char *buf)
+{
+	struct device_info *pdevinfo = dev_get_drvdata(kdev);
+	int handle, buf_used = 0, client_count = 0;
+	bool noprint = false;
+	struct hantro_client *client;
+	int deviceidx = -1;
+	static char optype[64];
+	static char profile[64];
+	static const char *const unknown = "Unknown";
+	static const char *const optypes[] = { "Decode", "Encode" };
+	static const char *const profiles[] = {
+		/* brief Profile ID used for video processing. */
+		"VAProfileMPEG2Simple",
+		"VAProfileMPEG2Main",
+		"VAProfileMPEG4Simple",
+		"VAProfileMPEG4AdvancedSimple",
+		"VAProfileMPEG4Main",
+		"VAProfileH264Baseline",
+		"VAProfileH264Main",
+		"VAProfileH264High",
+		"VAProfileVC1Simple",
+		"VAProfileVC1Main",
+		"VAProfileVC1Advanced",
+		"VAProfileH263Baseline",
+		"VAProfileJPEGBaseline",
+		"VAProfileH264ConstrainedBaseline",
+		"VAProfileVP8Version0_3",
+		"VAProfileH264MultiviewHigh",
+		"VAProfileH264StereoHigh",
+		"VAProfileHEVCMain",
+		"VAProfileHEVCMain10",
+		"VAProfileVP9Profile0",
+		"VAProfileVP9Profile1",
+		"VAProfileVP9Profile2",
+		"VAProfileVP9Profile3"
+	};
+	/* brief Profile ID used for video processing. */
+	static const char * const hantro_profiles[] = {
+		"HANTROProfileHEVCMSt",
+		"HANTROProfileH264H10"
+	};
+
+	if (!pdevinfo)
+		return 0;
+
+	deviceidx = pdevinfo->deviceid;
+	buf_used += snprintf(buf + buf_used, PAGE_SIZE,
+			     "File Id : ContextId :  Operation   :                Codec                    :  Resolution\n");
+	/* Go through all open drm files */
+	mutex_lock(&pdevinfo->alloc_mutex);
+	idr_for_each_entry(&pdevinfo->clients, client, handle) {
+		if (client && client->deviceid == deviceidx) {
+			if (buf_used < (PAGE_SIZE - 200)) {
+				if (client->profile >= 0 &&
+				    client->profile <= 22) {
+					strncpy(profile,
+						profiles[client->profile],
+						strlen(profiles[client->profile]) +
+							1);
+
+				} else {
+					if (client->profile >= 100 &&
+					    client->profile <= 101)
+						strncpy(profile,
+							hantro_profiles
+								[client->profile -
+								 100],
+							strlen(hantro_profiles
+								       [client->profile -
+									100]) +
+								1);
+					else
+						strncpy(profile, unknown,
+							strlen(unknown) + 1);
+				}
+
+				if (client->codec == 0 || client->codec == 1)
+					strncpy(optype, optypes[client->codec],
+						strlen(optypes[client->codec]) + 1);
+				else
+					strncpy(optype, unknown,
+						strlen(unknown) + 1);
+
+				buf_used += snprintf(buf + buf_used, PAGE_SIZE,
+					"%8p %9x     %s (%d)     %-32s (%2d)     %lux%lu\n",
+					client->file, client->clientid, optype,
+					client->codec, profile, client->profile,
+					client->width, client->height);
+			} else {
+				/*  optimization to save buf space due to a PAGE_SIZE mem only */
+				if (!noprint) {
+					buf_used +=
+						snprintf(buf + buf_used,
+							 PAGE_SIZE, " ....\n");
+					noprint =
+						true; /* print ... only one time */
+				}
+			}
+
+			client_count++;
+		}
+	}
+	mutex_unlock(&pdevinfo->alloc_mutex);
+	buf_used += snprintf(buf + buf_used, PAGE_SIZE, "\n%d clients\n\n",
+			     client_count);
+	return buf_used;
+}
+
+int mem_usage_internal(unsigned int deviceidx, struct device *memdev,
+		       u32 *pused_mem, u32 *pallocations, struct seq_file *s)
+{
+	struct drm_gem_hantro_object *cma_obj;
+	struct device_info *pdevinfo;
+	int alloc_count = 0, handle;
+	ssize_t mem_used = 0;
+
+	pdevinfo = get_device_info(deviceidx);
+	if (!pdevinfo)
+		return -EINVAL;
+
+	if (s)
+		seq_puts(s,
+			 "Physical Addr    :  Virtual Addr       :   Size  : GEM handle : DMABuf fd\n");
+
+	mutex_lock(&pdevinfo->alloc_mutex);
+	/* Iterate through cma objects added to file's driver_priv */
+	/* checkout hantro_record_mem */
+	idr_for_each_entry(&pdevinfo->allocations, cma_obj, handle) {
+		if (cma_obj && cma_obj->memdev == memdev) {
+			if (s) {
+				seq_printf(s,
+					   " 0x%-13llx :  0x%-15p  : %6ldK : %4d  [%2d] :   ",
+					   cma_obj->paddr, cma_obj->vaddr,
+					   cma_obj->base.size / 1024,
+					   cma_obj->handle,
+					   kref_read(&cma_obj->base.refcount));
+				if (cma_obj->fd >= 0)
+					seq_printf(s, "%d\n", cma_obj->fd);
+				else
+					seq_puts(s, "\n");
+			}
+
+			mem_used += cma_obj->base.size;
+			alloc_count++;
+		}
+	}
+
+	mutex_unlock(&pdevinfo->alloc_mutex);
+	if (s)
+		seq_printf(s, "\n%ldK in %d allocations\n\n", mem_used / 1024,
+			   alloc_count);
+
+	if (pused_mem)
+		*pused_mem = mem_used;
+
+	if (pallocations)
+		*pallocations = alloc_count;
+
+	return 0;
+}
+
+/* print mem usage summary through sysfs */
+static ssize_t mem_usage_show(struct device *kdev, struct device_attribute *attr,
+			      char *buf)
+{
+	struct device_info *pdevinfo = dev_get_drvdata(kdev);
+	int deviceidx, bufsize = 0;
+
+	int used_mem0 = 0, alloc_count0 = 0;
+	int used_mem1 = 0, alloc_count1 = 0;
+
+	if (!pdevinfo)
+		return 0;
+
+	deviceidx = pdevinfo->deviceid;
+	mem_usage_internal(deviceidx, pdevinfo->dev, &used_mem0, &alloc_count0,
+			   NULL);
+	if (pdevinfo->codec_rsvmem)
+		mem_usage_internal(deviceidx, pdevinfo->codec_rsvmem, &used_mem1,
+				   &alloc_count1, NULL);
+
+	bufsize += snprintf(buf + bufsize, PAGE_SIZE, "Device %d mem usage:\n",
+			    deviceidx);
+	bufsize += snprintf(buf + bufsize, PAGE_SIZE,
+		"\tCMA 0 : %lld MB - [0x%llx - 0x%llx]\n\t\t %dK in %d allocations\n\n",
+		resource_size(&pdevinfo->mem_res[0]) / (1024 * 1024),
+		pdevinfo->mem_res[0].start, pdevinfo->mem_res[0].end,
+		used_mem0 / 1024, alloc_count0);
+	bufsize += snprintf(buf + bufsize, PAGE_SIZE,
+		"\tCMA 1 : %lld MB - [0x%llx - 0x%llx]\n\t\t %dK in %d allocations\n",
+		resource_size(&pdevinfo->mem_res[1]) / (1024 * 1024),
+		pdevinfo->mem_res[1].start, pdevinfo->mem_res[1].end,
+		used_mem1 / 1024, alloc_count1);
+	return bufsize;
+}
+
+static void reset_perf_data(void)
+{
+	struct device_info *pdevinfo;
+	struct hantrodec_t *dechdr;
+	struct hantroenc_t *enchdr;
+	int i, devcnt;
+
+	devcnt = get_device_count();
+	for (i = 0; i < devcnt; i++) {
+		pdevinfo = get_device_info(i);
+		dechdr = pdevinfo->dechdr;
+		while (dechdr) {
+			dechdr->perf_data.count = 0;
+			dechdr->perf_data.totaltime = 0;
+			dechdr->perf_data.hwcycles = 0;
+			dechdr = dechdr->next;
+		}
+
+		enchdr = pdevinfo->enchdr;
+		while (enchdr) {
+			enchdr->perf_data.count = 0;
+			enchdr->perf_data.totaltime = 0;
+			enchdr->perf_data.hwcycles = 0;
+			enchdr = enchdr->next;
+		}
+	}
+}
+
+static ssize_t fps_show(struct device *kdev, struct device_attribute *attr,
+			char *buf)
+{
+	struct device_info *pdevinfo;
+	struct hantrodec_t *dechdr;
+	struct hantroenc_t *enchdr;
+	int buf_size = 0, i, diff, devcnt, core;
+	u64 start, averagecycles, totaltime_hw, totaltime_sw;
+	u32 fps, totalfps;
+	long clk_freq;
+
+	devcnt = get_device_count();
+	reset_perf_data();
+	start = sched_clock();
+	/* wait for 1 sec */
+	msleep(1000);
+	diff = (sched_clock() - start) / 1000000; /* diff in ms */
+	for (i = 0; i < devcnt; i++) {
+		pdevinfo = get_device_info(i);
+		buf_size +=
+			snprintf(buf + buf_size, PAGE_SIZE, "Device %d:\n", i);
+		dechdr = pdevinfo->dechdr;
+		core = 0;
+		totalfps = 0;
+		buf_size += snprintf(buf + buf_size, PAGE_SIZE, "\tDecode\n");
+		while (dechdr) {
+			fps = 0;
+			clk_freq = clk_get_rate(dechdr->dev_clk);
+			if (dechdr->perf_data.count == 0) {
+				buf_size += snprintf(buf + buf_size, PAGE_SIZE,
+					"\t\tCore [%d]    0 fps, %ld Mhz\n",
+					core, clk_freq / 1000000);
+			} else {
+				averagecycles = dechdr->perf_data.hwcycles /
+						dechdr->perf_data.count;
+				totaltime_hw = dechdr->perf_data.hwcycles /
+					       (dechdr->clk_freq / 100000);
+				totaltime_sw =
+					dechdr->perf_data.totaltime / 10000;
+				fps = (dechdr->perf_data.count * 1000) /
+				      (diff == 0 ? 1 : diff);
+				buf_size += snprintf(buf + buf_size, PAGE_SIZE,
+					"\t\tCore [%d] %4d fps (%lld%%:%lld%%), %-8lld avg cycles, %ld Mhz\n",
+					core, fps, totaltime_hw / diff,
+					totaltime_sw / diff, averagecycles,
+					clk_freq / 1000000);
+				totalfps += fps;
+			}
+
+			dechdr = dechdr->next;
+			core++;
+		}
+		buf_size += snprintf(buf + buf_size, PAGE_SIZE, "\t%d fps\n\n",
+				     totalfps);
+
+		core = 0;
+		totalfps = 0;
+		buf_size += snprintf(buf + buf_size, PAGE_SIZE, "\tEncode\n");
+		enchdr = pdevinfo->enchdr;
+		while (enchdr) {
+			fps = 0;
+			clk_freq = clk_get_rate(enchdr->dev_clk);
+			if (clk_freq != 0) {
+				if (enchdr->perf_data.count == 0) {
+					buf_size += snprintf(buf + buf_size, PAGE_SIZE,
+						"\t\tCore [%d]    0 fps, %ld Mhz\n",
+						core, clk_freq / 1000000);
+				} else {
+					totaltime_sw =
+						enchdr->perf_data.totaltime /
+						10000;
+					fps = (enchdr->perf_data.count * 1000) /
+					      (diff == 0 ? 1 : diff);
+					buf_size += snprintf(buf + buf_size, PAGE_SIZE,
+						"\t\tCore [%d] %4d fps (%lld%%), %lu Mhz\n",
+						core, fps, totaltime_sw / diff,
+						enchdr->clk_freq / 1000000);
+					totalfps += fps;
+				}
+			}
+
+			enchdr = enchdr->next;
+			core++;
+		}
+
+		buf_size += snprintf(buf + buf_size, PAGE_SIZE, "\t%d fps\n\n",
+				     totalfps);
+	}
+	return buf_size;
+}
+
+static ssize_t version_show(struct device *kdev, struct device_attribute *attr,
+			    char *buf)
+{
+	return snprintf(buf, PAGE_SIZE,
+		"Hantro driver info:\n\tversion: %d.%d\n\t   Date: %s\n",
+		DRIVER_MAJOR, DRIVER_MINOR, DRIVER_DATE);
+}
+
+static DEVICE_ATTR_RO(bandwidth_dec_read);
+static DEVICE_ATTR_RO(bandwidth_dec_write);
+static DEVICE_ATTR_RO(bandwidth_enc_read);
+static DEVICE_ATTR_RO(bandwidth_enc_write);
+static DEVICE_ATTR_RO(clients);
+static DEVICE_ATTR_RO(mem_usage);
+static DEVICE_ATTR_RO(fps);
+static DEVICE_ATTR_RO(version);
+
+static struct attribute *hantro_attrs[] = {
+	&dev_attr_bandwidth_dec_read.attr,
+	&dev_attr_bandwidth_dec_write.attr,
+	&dev_attr_bandwidth_enc_read.attr,
+	&dev_attr_bandwidth_enc_write.attr,
+	&dev_attr_clients.attr,
+	&dev_attr_mem_usage.attr,
+	&dev_attr_fps.attr,
+	&dev_attr_version.attr,
+	NULL,
+};
+
+static const struct attribute_group hantro_attr_group = {
+	.attrs = hantro_attrs,
+};
+
+/* print mem_usage details through debugfs */
+static int mem_usage_debugfs_show(struct seq_file *s, void *v)
+{
+	struct device_info *pdevinfo = s->private;
+	int deviceidx;
+
+	if (!pdevinfo)
+		return 0;
+
+	deviceidx = pdevinfo->deviceid;
+	seq_printf(s, "Memory usage for device %d:\n", deviceidx);
+	seq_printf(s, "Pixel CMA: %lld MB - [0x%llx - 0x%llx]\n",
+		   resource_size(&pdevinfo->mem_res[0]) / (1024 * 1024),
+		   pdevinfo->mem_res[0].start, pdevinfo->mem_res[0].end);
+	mem_usage_internal(deviceidx, pdevinfo->dev, NULL, NULL, s);
+	if (pdevinfo->codec_rsvmem) {
+		seq_printf(s, "Codec CMA: %lld MB - [0x%llx - 0x%llx]\n",
+			   resource_size(&pdevinfo->mem_res[1]) / (1024 * 1024),
+			   pdevinfo->mem_res[1].start, pdevinfo->mem_res[1].end);
+		mem_usage_internal(deviceidx, pdevinfo->codec_rsvmem, NULL, NULL,
+				   s);
+	}
+
+	return 0;
+}
+
+DEFINE_SHOW_ATTRIBUTE(mem_usage_debugfs);
+
+int create_sysfs(struct device_info *pdevinfo)
+{
+	return devm_device_add_group(pdevinfo->dev, &hantro_attr_group);
+}
+
+void remove_sysfs(struct device_info *pdevinfo)
+{
+	devm_device_remove_group(pdevinfo->dev, &hantro_attr_group);
+}
+
+void create_debugfs(struct device_info *pdevinfo, bool has_codecmem)
+{
+	char filename[64];
+
+	if (!hantro_drm.debugfs_root)
+		return;
+
+	sprintf(filename, "mem_usage%d", pdevinfo->deviceid);
+	debugfs_create_file(filename, S_IFREG | 0444, hantro_drm.debugfs_root,
+			    pdevinfo, &mem_usage_debugfs_fops);
+}
diff --git a/drivers/gpu/drm/hantro/hantro_metadata.c b/drivers/gpu/drm/hantro/hantro_metadata.c
new file mode 100644
index 000000000000..b6106bb29031
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_metadata.c
@@ -0,0 +1,94 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro Metadata file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#include "hantro_metadata.h"
+#include "hantro_priv.h"
+
+static int hantro_query_metadata(struct drm_device *dev, void *data,
+				 struct drm_file *file_priv)
+{
+	struct hantro_metainfo_params *metadata_info_p =
+		(struct hantro_metainfo_params *)data;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj = NULL;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, metadata_info_p->handle);
+	if (!obj)
+		return -ENOENT;
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	memcpy(&metadata_info_p->info, &cma_obj->dmapriv.meta_data_info,
+	       sizeof(const struct viv_vidmem_metadata));
+	hantro_unref_drmobj(obj);
+	return 0;
+}
+
+static int hantro_update_metadata(struct drm_device *dev, void *data,
+				  struct drm_file *file_priv)
+{
+	struct hantro_metainfo_params *metadata_info_p =
+		(struct hantro_metainfo_params *)data;
+	struct drm_gem_object *obj;
+	struct drm_gem_hantro_object *cma_obj = NULL;
+
+	obj = hantro_gem_object_lookup(dev, file_priv, metadata_info_p->handle);
+	if (!obj)
+		return -ENOENT;
+
+	cma_obj = to_drm_gem_hantro_obj(obj);
+	memcpy(&cma_obj->dmapriv.meta_data_info, &metadata_info_p->info,
+	       sizeof(const struct viv_vidmem_metadata));
+	hantro_unref_drmobj(obj);
+	return 0;
+}
+
+long hantrometadata_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct drm_file *file_priv = file->private_data;
+	struct drm_device *dev = hantro_drm.drm_dev;
+	char stack_kdata[256];
+	char *kdata = stack_kdata;
+	int ret;
+
+	switch (cmd) {
+	case DRM_IOCTL_HANTRO_QUERY_METADATA:
+		if (copy_from_user(kdata, (void __user *)arg,
+				   sizeof(struct hantro_metainfo_params)) != 0)
+			return -EFAULT;
+
+		ret = hantro_query_metadata(dev, kdata, file_priv);
+
+		if (ret < 0)
+			return -EFAULT;
+
+		if (copy_to_user((void __user *)arg, kdata,
+				 sizeof(struct hantro_metainfo_params)) != 0)
+			return -EFAULT;
+
+		break;
+	case DRM_IOCTL_HANTRO_UPDATE_METADATA:
+		if (copy_from_user(kdata, (void __user *)arg,
+				   sizeof(struct hantro_metainfo_params)) != 0)
+			return -EFAULT;
+
+		ret = hantro_update_metadata(dev, kdata, file_priv);
+
+		if (ret < 0)
+			return -EFAULT;
+
+		if (copy_to_user((void __user *)arg, kdata,
+				 sizeof(struct hantro_metainfo_params)) != 0)
+			return -EFAULT;
+
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	return 0;
+}
diff --git a/drivers/gpu/drm/hantro/hantro_metadata.h b/drivers/gpu/drm/hantro/hantro_metadata.h
new file mode 100644
index 000000000000..d4fc35a3a4be
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_metadata.h
@@ -0,0 +1,153 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro Metadata header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef HANTRO_METADATA_H
+#define HANTRO_METADATA_H
+
+#include <linux/platform_device.h>
+
+#define HANTRO_MAGIC(ch0, ch1, ch2, ch3)                                       \
+	((unsigned long)(unsigned char)(ch0) |                                 \
+	 ((unsigned long)(unsigned char)(ch1) << 8) |                          \
+	 ((unsigned long)(unsigned char)(ch2) << 16) |                         \
+	 ((unsigned long)(unsigned char)(ch3) << 24))
+
+#define HANTRO_IMAGE_VIV_META_DATA_MAGIC HANTRO_MAGIC('V', 'I', 'V', 'M')
+
+/*
+ * name of image format for exchange.
+ */
+enum viv_image_format {
+	IMAGE_BYTE = 0, // pure data, not image
+	IMAGE_YUV420, // 3 plane YUV420 8 bits
+	IMAGE_NV12, // 2 plane NV12 8 bits
+	IMAGE_UYVY, // packed YUV422 8 bits
+	IMAGE_YUY2, // packed YUV422 8 bits
+	IMAGE_Y8, // single plane 8 bits
+	IMAGE_UV8, // single UV interleave plan 8 bits
+	IMAGE_MS_P010, // 2 plane Simiplanar 10 bits, MSB valid
+	IMAGE_P010, // 3 plane YUV420 10 bits, MSB valid
+	IMAGE_Y210, // packed YUV422 10 bits, LSB valid
+	IMAGE_Y10, // single plane 10 bits
+	IMAGE_UV10, // single UV interleave plan 10 bits
+	IMAGE_XRGB8888, // packed XRGB 8 bits
+	IMAGE_ARGB8888,
+	IMAGE_A2R10G10B10,
+	IMAGE_X2R10G10B10,
+	IMAGE_BAYER10, // packed
+	IMAGE_BAYER12,
+	IMAGE_BAYER14,
+	IMAGE_BAYER16,
+	IMAGE_FORMAT_MAX
+};
+
+/*
+ * dec400 tile format, defines how one tile is scanned.
+ */
+enum viv_tile_format {
+	TILE_NONE = 0, // raster scane linear
+	TILE_Y_4x4, // luma 4x4
+	TILE_Y_8x8, // luma 8x8
+	TILE_4x4_INTERLEAVE, // U_4x4 tile followed with V4x4,
+	TILE_8x8_X_MAJOR, // RGB super tile X majory
+	TILE_8x8_Y_MAJOR, // RGB super tile y major
+	TILE_FMT_MAX
+};
+
+/*
+ * dec400 tile mode, defines the tile size for each compression block
+ */
+enum viv_dec_tile_mode {
+	CMVDEC_TILE_MODE_8x8_X_MAJOR = 0,
+	CMVDEC_TILE_MODE_8x8_Y_MAJOR = 1,
+	CMVDEC_TILE_MODE_16x4 = 2,
+	CMVDEC_TILE_MODE_8x4 = 3,
+	CMVDEC_TILE_MODE_4x8 = 4,
+	CMVDEC_TILE_MODE_4x4 = 5,
+	CMVDEC_TILE_MODE_16x4_RASTER = 6,
+	CMVDEC_TILE_MODE_64x4 = 7,
+	CMVDEC_TILE_MODE_32x4 = 8,
+	CMVDEC_TILE_MODE_256x1_RASTER = 9,
+	CMVDEC_TILE_MODE_128x1_RASTER = 10,
+	CMVDEC_TILE_MODE_64x4_RASTER = 11,
+	CMVDEC_TILE_MODE_256x2_RASTER = 12,
+	CMVDEC_TILE_MODE_128x2_RASTER = 13,
+	CMVDEC_TILE_MODE_128x4_RASTER = 14,
+	CMVDEC_TILE_MODE_64x1_RASTER = 15,
+	CMVDEC_TILE_MODE_16x8_RASTER = 16,
+	CMVDEC_TILE_MODE_8x16_RASTER = 17,
+	CMVDEC_TILE_MODE_512x1_RASTER = 18,
+	CMVDEC_TILE_MODE_32x4_RASTER = 19,
+	CMVDEC_TILE_MODE_64x2_RASTER = 20,
+	CMVDEC_TILE_MODE_32x2_RASTER = 21,
+	CMVDEC_TILE_MODE_32x1_RASTER = 22,
+	CMVDEC_TILE_MODE_16x1_RASTER = 23,
+	CMVDEC_TILE_MODE_128x4 = 24,
+	CMVDEC_TILE_MODE_256x4 = 25,
+	CMVDEC_TILE_MODE_512x4 = 26,
+	CMVDEC_TILE_MODE_16x16 = 27,
+	CMVDEC_TILE_MODE_32x16 = 28,
+};
+
+/*
+ * The surface meta data saved in meta data buffer
+ */
+struct viv_vidmem_metadata {
+	u32 magic; // __FOURCC('V', 'I', 'V', 'M')
+	u32 dmabuf_size; // DMABUF buffer size in byte (Maximum 4GB)
+	u32 time_stamp; // time stamp for the DMABUF buffer
+
+	u32 image_format; // ImageFormat, determined plane number.
+	u32 compressed; // if DMABUF buffer is compressed by DEC400
+	struct {
+		u32 offset; // plane buffer address offset from DMABUF address
+		u32 stride; // pitch in bytes
+		u32 width; // width in pixels
+		u32 height; // height in pixels
+
+		u32 tile_format; // uncompressed tile format
+		u32 compress_format; // tile mode for DEC400
+
+		/*
+		 * tile status buffer offset within this plane buffer. when it is 0
+		 * indicates using separate tile status buffer
+		 */
+		u32 ts_offset;
+		/* fd of separate tile status buffer of the plane buffer */
+		s32 ts_fd;
+		/* valid fd of the ts buffer in consumer side. */
+		s32 ts_fd2;
+		/* the vpu virtual address for this ts data buffer */
+		s32 ts_vaddr;
+
+		/* gpu fastclear enabled for the plane buffer */
+		u32 fc_enabled;
+		/* gpu fastclear color value (lower 32 bits) for the plane buffer */
+		u32 fc_value_lower;
+		/* gpu fastclear color value (upper 32 bits) for the plane buffer */
+		u32 fc_value_upper;
+	} plane[3];
+};
+
+/*
+ * The meta data location information exchange IOCTL parameters.
+ */
+struct hantro_metainfo_params {
+	int handle;				/* the handle of current bo */
+	struct viv_vidmem_metadata info;	/* the meta data */
+};
+
+struct dmapriv {
+	struct viv_vidmem_metadata meta_data_info;
+	u32 magic_num;	/* HANTRO_IMAGE_VIV_META_DATA_MAGIC */
+	void *self;	/* ptr of parent cma obj */
+};
+
+long hantrometadata_ioctl(struct file *file, unsigned int cmd, unsigned long arg);
+
+#endif /* HANTRO_METADATA_H */
diff --git a/drivers/gpu/drm/hantro/hantro_priv.h b/drivers/gpu/drm/hantro/hantro_priv.h
new file mode 100644
index 000000000000..b8c89f8ac419
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_priv.h
@@ -0,0 +1,250 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro driver private header file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#ifndef __HANTRO_PRIV_H__
+#define __HANTRO_PRIV_H__
+#include <linux/io.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/shmem_fs.h>
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/interrupt.h>
+#include <linux/semaphore.h>
+#include <linux/atomic.h>
+#include <linux/spinlock.h>
+#include <linux/ioctl.h>
+#include <linux/pci.h>
+#include <linux/ioport.h>
+#include <linux/version.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/delay.h>
+#include <linux/timer.h>
+#include <linux/ioctl.h>
+#include <linux/types.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/of_irq.h>
+#include <linux/reset.h>
+#include <linux/clk.h>
+#include <linux/debugfs.h>
+#include <linux/thermal.h>
+
+#include <linux/ioctl.h>
+#include <linux/dma-resv.h>
+#include <linux/dma-mapping.h>
+#include <drm/drm_vma_manager.h>
+#include <drm/drm_gem_cma_helper.h>
+#include <drm/drm_gem.h>
+#include <linux/dma-buf.h>
+#include <drm/drm_auth.h>
+#include <linux/version.h>
+#include <linux/dma-fence.h>
+#include <linux/sched/clock.h>
+#include <linux/dma-mapping.h>
+#include <drm/drm_fourcc.h>
+#include <drm/drm_framebuffer.h>
+#include <drm/drm_legacy.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/uaccess.h>
+#include <drm/drm_modeset_helper.h>
+
+#include "hantro_drm.h"
+#include "hantro_devicemgr.h"
+#include "hantro_metadata.h"
+
+#include "trace.h"
+
+#define HANTRO_GEM_FLAG_IMPORT		BIT(0)
+#define HANTRO_GEM_FLAG_EXPORT		BIT(1)
+#define HANTRO_GEM_FLAG_EXPORTUSED	BIT(2)
+#define HANTRO_GEM_FLAG_FOREIGN_IMPORTED    BIT(3)
+
+#define DRIVER_NAME	"hantro"
+#define DRIVER_DESC	"hantro DRM"
+#define DRIVER_DATE	"20210304"
+#define DRIVER_MAJOR	1
+#define DRIVER_MINOR	0
+
+#define hantro_access_ok(a, b, c) access_ok(b, c)
+#define hantro_reserve_obj_shared(a, b) dma_resv_reserve_shared(a, b)
+#define hantro_ref_drmobj drm_gem_object_get
+#define hantro_unref_drmobj drm_gem_object_put
+
+#define NODENAME_DECODER	"decoder"
+#define NODENAME_ENCODER	"encoder"
+#define NODENAME_CACHE		"cache"
+#define NODENAME_DEC400		"dec400"
+
+#define PIXEL_CMA 0
+#define CODEC_RESERVED 1
+
+#undef PDEBUG /* undef it, just in case */
+#ifdef __KERNEL__
+/* This one if debugging is on, and kernel space */
+#define PDEBUG(fmt, arg...)                                                    \
+	do {                                                                   \
+		if (verbose)                                                   \
+			pr_info(fmt, ##arg);                                   \
+	} while (0)
+
+#else
+/* This one for user space */
+#define PDEBUG(fmt, args...) printf(__FILE__ ":%d: " fmt, __LINE__, ##args)
+#endif
+
+extern bool verbose;
+extern bool enable_lut;
+extern bool enable_encode;
+extern bool enable_decode;
+extern bool enable_dec400;
+extern bool enable_irqmode;
+
+extern long kmb_freq_table[3];
+extern long tbh_freq_table[3];
+
+extern const struct dma_buf_ops hantro_dmabuf_ops;
+
+struct dtbnode {
+	struct device_node *ofnode;
+	int type;
+	phys_addr_t ioaddr;
+	phys_addr_t iosize;
+	char reg_name[32];
+	int irq[4];
+	char irq_name[4][32];
+	char clock_name[32];
+	int parenttype;
+	phys_addr_t parentaddr;
+	int deviceidx;
+	struct device_info *pdevinfo;
+	struct dtbnode *next;
+	struct device *dev;
+};
+
+struct hantro_drm_handle {
+	struct platform_device *platformdev; /* parent device */
+	struct device *dev;
+	struct drm_device *drm_dev;
+	struct device_info *pdevice_list;
+	struct dentry *debugfs_root;
+	struct class_compat *media_class;
+	enum hantro_device_type device_type;
+	u8 __iomem *dec_page_lut_regs;
+	u8 __iomem *enc_page_lut_regs;
+	atomic_t devicecount;
+	/* hantro mutex struct */
+	struct mutex hantro_mutex;
+	u32 config;
+};
+
+#define HANTRO_FENCE_FLAG_ENABLE_SIGNAL_BIT	DMA_FENCE_FLAG_ENABLE_SIGNAL_BIT
+#define HANTRO_FENCE_FLAG_SIGNAL_BIT		DMA_FENCE_FLAG_SIGNALED_BIT
+
+extern struct hantro_drm_handle hantro_drm;
+
+static inline struct drm_gem_object *
+hantro_get_gem_from_dmabuf(struct dma_buf *dma_buf)
+{
+	struct drm_gem_hantro_object *cma_obj =
+		(struct drm_gem_hantro_object
+			 *)(((struct dmapriv *)dma_buf->priv)->self);
+
+	if (cma_obj && cma_obj->dmapriv.magic_num == HANTRO_IMAGE_VIV_META_DATA_MAGIC)
+		return &cma_obj->base;
+
+	return NULL;
+}
+
+static inline signed long
+hantro_fence_default_wait(struct dma_fence *fence, bool intr, signed long timeout)
+{
+	return dma_fence_default_wait(fence, intr, timeout);
+}
+
+static inline void hantro_fence_init(struct dma_fence *fence,
+				     const struct dma_fence_ops *ops,
+				     spinlock_t *lock, unsigned int context,
+				     unsigned int seqno)
+{
+	return dma_fence_init(fence, ops, lock, context, seqno);
+}
+
+static inline unsigned int hantro_fence_context_alloc(unsigned int num)
+{
+	return dma_fence_context_alloc(num);
+}
+
+static inline struct drm_gem_object *
+hantro_gem_object_lookup(struct drm_device *dev, struct drm_file *file,
+			 u32 handle)
+{
+	return drm_gem_object_lookup(file, handle);
+}
+
+static inline void hantro_fence_put(struct dma_fence *fence)
+{
+	return dma_fence_put(fence);
+}
+
+static inline int hantro_fence_signal(struct dma_fence *fence)
+{
+	return dma_fence_signal(fence);
+}
+
+static inline void unref_page(struct page *pp)
+{
+	atomic_dec(&pp->_refcount);
+	atomic_dec(&pp->_mapcount);
+}
+
+static inline bool hantro_fence_is_signaled(struct dma_fence *fence)
+{
+	return dma_fence_is_signaled(fence);
+}
+
+static inline struct drm_gem_hantro_object *
+to_drm_gem_hantro_obj(struct drm_gem_object *gem_obj)
+{
+	return container_of(gem_obj, struct drm_gem_hantro_object, base);
+}
+
+int hantro_setdomain(struct drm_device *dev, void *data,
+		     struct drm_file *file_priv);
+int hantro_acquirebuf(struct drm_device *dev, void *data,
+		      struct drm_file *file_priv);
+int hantro_testbufvalid(struct drm_device *dev, void *data,
+			struct drm_file *file_priv);
+int hantro_releasebuf(struct drm_device *dev, void *data,
+		      struct drm_file *file_priv);
+int init_hantro_resv(struct dma_resv *presv,
+		     struct drm_gem_hantro_object *cma_obj);
+void create_debugfs(struct device_info *pdevinfo, bool has_codecmem);
+int mem_usage_internal(unsigned int deviceidx, struct device *memdev,
+		       u32 *pused_mem, u32 *pallocations, struct seq_file *s);
+
+struct drm_device *create_hantro_drm(struct device *dev);
+int create_sysfs(struct device_info *pdevinfo);
+void remove_sysfs(struct device_info *pdevinfo);
+struct device_info *get_device_info(int deviceid);
+void init_fence_data(void);
+void release_fence_data(void);
+
+#endif /* __HANTRO_PRIV_H__ */
diff --git a/drivers/gpu/drm/hantro/hantro_slice.h b/drivers/gpu/drm/hantro/hantro_slice.h
new file mode 100644
index 000000000000..ee480a3f42c0
--- /dev/null
+++ b/drivers/gpu/drm/hantro/hantro_slice.h
@@ -0,0 +1,255 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro decoder hardware driver.
+ *
+ *    Copyright (c) 2017, VeriSilicon Inc.
+ *
+ *    This program is free software; you can redistribute it and/or
+ *    modify it under the terms of the GNU General Public License
+ *    as published by the Free Software Foundation; either version 2
+ *    of the License, or (at your option) any later version.
+ *
+ *    This program is distributed in the hope that it will be useful,
+ *    but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *    GNU General Public License for more details.
+ *
+ *    You may obtain a copy of the GNU General Public License
+ *    Version 2 or later at the following locations:
+ *    http://www.opensource.org/licenses/gpl-license.html
+ *    http://www.gnu.org/copyleft/gpl.html
+ */
+
+#ifndef __HANTRO_SLICE_H__
+#define __HANTRO_SLICE_H__
+
+#include "hantro.h"
+
+#define MAX(a, b) (((a) > (b)) ? (a) : (b))
+#define MIN(a, b) (((a) < (b)) ? (a) : (b))
+
+/* supported core type */
+typedef enum {
+	CORE_UNKNOWN = -1,
+	CORE_SLICE = 0,
+	CORE_DEC,
+	CORE_ENC,
+	CORE_CACHE,
+	CORE_DEC400,
+} slice_coretype;
+
+struct cache_core_config {
+	cache_client_type client;
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	driver_cache_dir dir;
+	u32 sliceidx;
+	unsigned long long parentaddr;
+};
+
+struct cache_dev_t {
+	struct cache_core_config core_cfg; /* config of each core,such as base addr, irq,etc */
+	unsigned long hw_id; /* hw id to indicate project */
+	u32 core_id; /* core id for driver and sw internal use */
+	u32 is_valid; /* indicate this core is hantro's core or not */
+	u32 is_reserved; /* indicate this core is occupied by user or not */
+	struct file *cacheowner; /* indicate which process is occupying the core */
+	u32 irq_received; /* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 *hwregs;
+	char reg_name[32];
+	unsigned long long com_base_addr; /* common base addr of each L2 */
+	int irqlist[4];
+	char irq_name[4][32];
+	slice_coretype parenttype;
+	u32 parentid; /* parent codec core's core_id */
+	void *parentcore; /* either struct hantroenc_t or struct hantrodec_t, or slice itself */
+	void *parentslice;
+	int sliceidx;
+	struct cache_dev_t *next;
+};
+
+struct dec400_core_cfg {
+	unsigned long long dec400corebase;
+	volatile unsigned int iosize;
+	u32 sliceidx;
+	unsigned long long parentaddr;
+};
+
+struct dec400_t {
+	struct dec400_core_cfg core_cfg;
+
+	u32 core_id;
+	volatile u8 *hwregs;
+
+	char reg_name[32];
+	slice_coretype parenttype;
+	u32 parentid; /* parent codec core's core_id */
+	void *parentcore; /* either struct hantroenc_t or struct hantrodec_t, or slice itself */
+	void *parentslice;
+	struct dec400_t *next;
+};
+
+typedef struct {
+	unsigned long long base_addr;
+	u32 iosize;
+	int irq;
+	/*
+	 * resouce_shared indicate core share resources with other cores.
+	 * If 1, cores can not work at same time.
+	 */
+	u32 resouce_shared;
+	u32 sliceidx;
+} CORE_CONFIG;
+
+struct hantroenc_t {
+	CORE_CONFIG core_cfg; /* config of each core,such as base addr, irq,etc */
+	u32 hw_id; /* hw id to indicate project */
+	u32 core_id; /* core id for driver and sw internal use */
+	u32 is_reserved; /* indicate this core is occupied by user or not */
+	int pid; /* indicate which process is occupying the core */
+	u32 irq_received; /* indicate this core receives irq */
+	u32 irq_status;
+	char *buffer;
+	unsigned int buffsize;
+	u8 *hwregs;
+	char reg_name[32];
+	struct fasync_struct *async_queue;
+	int irqlist[4];
+	char irq_name[4][32];
+	void *parentslice;
+	int sliceidx;
+	struct hantroenc_t *next;
+};
+
+#define HANTRO_G1_DEC_REGS	155 /* G1 total regs */
+#define HANTRO_G2_DEC_REGS	337 /* G2 total regs */
+#define HANTRO_VC8000D_REGS	393 /* VC8000D total regs */
+#define DEC_IO_SIZE_MAX \
+	(MAX(MAX(HANTRO_G2_DEC_REGS, HANTRO_G1_DEC_REGS), \
+	     HANTRO_VC8000D_REGS) * 4)
+
+struct hantrodec_t {
+	u32 cfg;
+	int core_id;
+	unsigned int iosize;
+	u32 cfg_backup;
+	/* indicate if main core exist */
+	struct hantrodec_t *its_main_core_id;
+	/* indicate if aux core exist */
+	struct hantrodec_t *its_aux_core_id;
+	/*
+	 * all access to hwregs are through readl/writel
+	 * so volatile is removed according to doc "volatile is evil"
+	 */
+	u8 *hwregs;
+	int hw_id;
+	char reg_name[32];
+	unsigned long long multicorebase;
+	/*
+	 * Because one core may contain multi-pipeline,
+	 * so multicore base may be changed
+	 */
+	unsigned long long multicorebase_actual;
+
+	u32 dec_regs[DEC_IO_SIZE_MAX / 4];
+	int irqlist[4];
+	char irq_name[4][32];
+
+	struct file *dec_owner;
+	struct file *pp_owner;
+	void *parentslice;
+	u32 sliceidx;
+	struct hantrodec_t *next;
+};
+
+/*
+current internal slice data structure will look like this:
+	slicehdr->	slice 0		-> slice 1 -> ...
+				|
+		________________________________________
+		|				|			|			|
+	dec core 0		enc core 0	dec400 0		cache core 0
+		|				|			|			|
+	dec core 1		enc core 1	dec400 1		cache core 1
+		|				|			|			|
+	.......				......		   .....		     .....
+
+Each core node contains its own info: io region, irq, hwid, direction, etc., depending on its type.
+Each dec400 core has pointer to a dec or enc core. Each dec/enc core has a pointer to dec400 core.
+Each cache core has pointer to a dec or enc core. Each dec/enc core has pointers to dec400 core.
+We do it this way since we don't know which one will be probed first, dec/enc or dec400/cache.
+And only ID to connect dec/enc to dec400/cache core is their HW address.
+*/
+
+struct slice_info {
+	struct device *dev; /* related dev, for drm usage */
+	struct device *codec_rsvmem;
+	phys_addr_t rsvmem_addr;
+	phys_addr_t memsize;
+	u32 config;
+	int deccore_num;
+	int enccore_num;
+	int dec400core_num;
+	int cachecore_num;
+
+	struct hantrodec_t *dechdr;
+	struct hantroenc_t *enchdr;
+	struct cache_dev_t *cachehdr;
+	struct dec400_t *dec400hdr;
+
+	/* orig cache global vars */
+	wait_queue_head_t cache_hw_queue;
+	wait_queue_head_t cache_wait_queue;
+	spinlock_t cache_owner_lock;
+
+	/* orig enc global vars */
+	struct semaphore enc_core_sem;
+	wait_queue_head_t enc_hw_queue;
+	spinlock_t enc_owner_lock;
+	wait_queue_head_t enc_wait_queue;
+
+	/* orig dec global vars */
+	int dec_irq;
+	int pp_irq;
+	spinlock_t owner_lock;
+	wait_queue_head_t dec_wait_queue;
+	wait_queue_head_t pp_wait_queue;
+	wait_queue_head_t hw_queue;
+	struct semaphore dec_core_sem;
+	struct semaphore pp_core_sem;
+
+	struct slice_info *next;
+};
+
+int findslice_bydev(struct device *dev);
+int addslice(struct device *dev, phys_addr_t sliceaddr, phys_addr_t slicesize);
+struct slice_info *getslicenode(u32 sliceindex);
+struct slice_info *getslicenode_inInit(u32 sliceindex);
+int get_slicecorenum(u32 sliceindex, slice_coretype type);
+struct hantrodec_t *get_decnodes(u32 sliceindex, u32 nodeidx);
+struct hantrodec_t *getfirst_decnodes(struct slice_info *pslice);
+struct hantroenc_t *get_encnodes(u32 sliceindex, u32 nodeidx);
+struct cache_dev_t *get_cachenodes(u32 sliceindex, u32 nodeidx);
+struct cache_dev_t *get_cachenodebytype(u32 sliceindex, u32 parenttype,
+					u32 parentnodeidx);
+struct dec400_t *get_dec400nodes(u32 sliceindex, u32 nodeidx);
+struct dec400_t *get_dec400nodebytype(u32 sliceindex, u32 parenttype,
+				      u32 parentnodeidx);
+int add_decnode(u32 sliceindex, struct hantrodec_t *deccore);
+int add_encnode(u32 sliceindex, struct hantroenc_t *enccore);
+int add_dec400node(u32 sliceindex, struct dec400_t *dec400core);
+int add_cachenode(u32 sliceindex, struct cache_dev_t *cachecore);
+int get_slicenumber(void);
+struct slice_info *getparentslice(void *node, int type);
+int slice_remove(void);
+int slice_init(void);
+void slice_init_finish(void);
+long hantroslice_ioctl(struct file *filp, unsigned int cmd, unsigned long arg);
+
+void slice_printdebug(void);
+
+#endif /* __HANTRO_SLICE_H__ */
diff --git a/drivers/gpu/drm/hantro/trace.c b/drivers/gpu/drm/hantro/trace.c
new file mode 100644
index 000000000000..8d37ce7d21c7
--- /dev/null
+++ b/drivers/gpu/drm/hantro/trace.c
@@ -0,0 +1,41 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ *    Hantro driver trace file.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+/* bug in tracepoint.h, it should include this */
+#include <linux/module.h>
+
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
+void __trace_hantro_msg(const char *fmt, ...)
+{
+	struct va_format vaf = {
+		.fmt = fmt,
+	};
+	va_list args;
+
+	va_start(args, fmt);
+	vaf.va = &args;
+
+	trace_hantro_msg(&vaf);
+	va_end(args);
+}
+
+void __trace_hantro_err(const char *fmt, ...)
+{
+	struct va_format vaf = {
+		.fmt = fmt,
+	};
+	va_list args;
+
+	va_start(args, fmt);
+	vaf.va = &args;
+
+	trace_hantro_err(&vaf);
+	va_end(args);
+}
diff --git a/drivers/gpu/drm/hantro/trace.h b/drivers/gpu/drm/hantro/trace.h
new file mode 100644
index 000000000000..cbb121a303eb
--- /dev/null
+++ b/drivers/gpu/drm/hantro/trace.h
@@ -0,0 +1,333 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ *    Hantro driver trace header.
+ *
+ *    Copyright (c) 2017 - 2020, VeriSilicon Inc.
+ *    Copyright (c) 2020 - 2021, Intel Corporation
+ */
+
+#if defined(CONFIG_DRM_HANTRO_TRACEPOINTS)
+#if !defined(__HANTRO_DRIVER_TRACE) || defined(TRACE_HEADER_MULTI_READ)
+#define __HANTRO_DRIVER_TRACE
+#include <linux/tracepoint.h>
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM hantro
+#define START_TIME u64 start = sched_clock()
+#define MAX_MSG_LEN 120
+
+TRACE_EVENT(drm_file_open,
+	    TP_PROTO(void *drmdev, void *file),
+	    TP_ARGS(drmdev, file),
+	    TP_STRUCT__entry(__field(void *, drmdev)
+			     __field(void *, file)),
+	    TP_fast_assign(__entry->drmdev = drmdev;
+			   __entry->file = file;),
+	    TP_printk("drmdev = %p, file = %p", __entry->drmdev,
+		      __entry->file));
+
+TRACE_EVENT(drm_file_close,
+	    TP_PROTO(void *drmdev, void *file),
+	    TP_ARGS(drmdev, file),
+	    TP_STRUCT__entry(__field(void *, drmdev)
+			     __field(void *, file)),
+	    TP_fast_assign(__entry->drmdev = drmdev;
+			   __entry->file = file;),
+	    TP_printk("drmdev = %p, file = %p", __entry->drmdev,
+		      __entry->file));
+
+TRACE_EVENT(client_add,
+	    TP_PROTO(void *file_attr, int deviceidx, int clientid, int codec,
+		     int profile, unsigned long width, unsigned long height),
+	    TP_ARGS(file_attr, deviceidx, clientid, codec, profile, width,
+		    height),
+	    TP_STRUCT__entry(__field(void *, file_attr)
+			     __field(int, deviceidx)
+			     __field(int, clientid)
+			     __field(int, codec)
+			     __field(int, profile)
+			     __field(unsigned long, width)
+			     __field(unsigned long, height)),
+	    TP_fast_assign(__entry->file_attr = file_attr;
+			   __entry->deviceidx = deviceidx;
+			   __entry->clientid = clientid;
+			   __entry->codec = codec;
+			   __entry->profile = profile;
+			   __entry->width = width;
+			   __entry->height = height;),
+	    TP_printk("file_attr %p, deviceidx = %d, clientid = %x, type = %d, profile = %d, width = %ld, height = %ld",
+		      __entry->file_attr, __entry->deviceidx,
+		      __entry->clientid, __entry->codec, __entry->profile,
+		      __entry->width, __entry->height));
+
+TRACE_EVENT(client_remove,
+	    TP_PROTO(void *file_attr, int clientid),
+	    TP_ARGS(file_attr, clientid),
+	    TP_STRUCT__entry(__field(void *, file_attr)
+			     __field(int, clientid)),
+	    TP_fast_assign(__entry->file_attr = file_attr;
+			   __entry->clientid = clientid;),
+	    TP_printk("file_attr %p, clientid = %x", __entry->file_attr,
+		      __entry->clientid));
+
+TRACE_EVENT(hantro_cma_alloc,
+	    TP_PROTO(int deviceidx, int region, void *paddr, int handle,
+		     int size),
+	    TP_ARGS(deviceidx, region, paddr, handle, size),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, region)
+			     __field(void *, paddr)
+			     __field(int, handle)
+			     __field(int, size)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->region = region;
+			   __entry->paddr = paddr;
+			   __entry->handle = handle;
+			   __entry->size = size;),
+	    TP_printk("deviceidx = %d, region = %d, paddr = %p, handle = %-3d, size = %d",
+		      __entry->deviceidx, __entry->region, __entry->paddr,
+		      __entry->handle, __entry->size));
+
+TRACE_EVENT(hantro_cma_free,
+	    TP_PROTO(int deviceidx, void *paddr, int handle),
+	    TP_ARGS(deviceidx, paddr, handle),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(void *, paddr)
+			     __field(int, handle)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->paddr = paddr;
+			   __entry->handle = handle;),
+	    TP_printk("deviceidx = %d, paddr = %p, handle = %-3d,",
+		      __entry->deviceidx,
+		      __entry->paddr,
+		      __entry->handle));
+
+TRACE_EVENT(dec_reserve,
+	    TP_PROTO(int deviceidx, int core, int waittime),
+	    TP_ARGS(deviceidx, core, waittime),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, core)
+			     __field(int, waittime)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->core = core;
+			   __entry->waittime = waittime;),
+	    TP_printk("deviceidx = %d, core = %d, waittime = %d us",
+		      __entry->deviceidx, __entry->core, __entry->waittime));
+
+TRACE_EVENT(dec_release,
+	    TP_PROTO(int deviceidx, int core),
+	    TP_ARGS(deviceidx, core),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, core)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->core = core;),
+	    TP_printk("deviceidx = %d, core = %d", __entry->deviceidx,
+		      __entry->core));
+
+TRACE_EVENT(enc_reserve,
+	    TP_PROTO(int deviceidx, int core, int waittime),
+	    TP_ARGS(deviceidx, core, waittime),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, core)
+			     __field(int, waittime)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->core = core;
+			   __entry->waittime = waittime;),
+	    TP_printk("deviceidx = %d, core = %d, waittime = %d us",
+		      __entry->deviceidx, __entry->core, __entry->waittime));
+
+TRACE_EVENT(enc_release,
+	    TP_PROTO(int deviceidx, int core),
+	    TP_ARGS(deviceidx, core),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, core)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->core = core;),
+	    TP_printk("deviceidx = %d, core = %d", __entry->deviceidx,
+		      __entry->core));
+
+TRACE_EVENT(cache_reserve,
+	    TP_PROTO(int deviceidx, int waittime),
+	    TP_ARGS(deviceidx, waittime),
+	    TP_STRUCT__entry(__field(int, deviceidx)
+			     __field(int, waittime)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;
+			   __entry->waittime = waittime;),
+	    TP_printk("deviceidx = %d, waittime = %d us", __entry->deviceidx,
+		      __entry->waittime));
+
+TRACE_EVENT(cache_release,
+	    TP_PROTO(int deviceidx),
+	    TP_ARGS(deviceidx),
+	    TP_STRUCT__entry(__field(int, deviceidx)),
+	    TP_fast_assign(__entry->deviceidx = deviceidx;),
+	    TP_printk("deviceidx = %d", __entry->deviceidx));
+
+TRACE_EVENT(fence_acquirebuf,
+	    TP_PROTO(void *obj, int handle, int fence_handle, int waittime,
+		     int ret),
+	    TP_ARGS(obj, handle, fence_handle, waittime, ret),
+	    TP_STRUCT__entry(__field(void *, obj)
+			     __field(int, handle)
+			     __field(int, fence_handle)
+			     __field(int, waittime)
+			     __field(int, ret)),
+	    TP_fast_assign(__entry->obj = obj;
+			   __entry->handle = handle;
+			   __entry->fence_handle = fence_handle;
+			   __entry->waittime = waittime;
+			   __entry->ret = ret;),
+	    TP_printk("obj = %p, handle = %-3d, fence = %-3d, waittime = %-4d us, ret = %d",
+		      __entry->obj, __entry->handle, __entry->fence_handle,
+		      __entry->waittime, __entry->ret));
+
+TRACE_EVENT(fence_releasebuf,
+	    TP_PROTO(int fence_handle, int ret),
+	    TP_ARGS(fence_handle, ret),
+	    TP_STRUCT__entry(__field(int, fence_handle)
+			     __field(int, ret)),
+	    TP_fast_assign(__entry->fence_handle = fence_handle;
+			   __entry->ret = ret;),
+	    TP_printk("fence = %-3d, ret = %d", __entry->fence_handle,
+		      __entry->ret));
+
+TRACE_EVENT(prime_handle_to_fd,
+	    TP_PROTO(void *obj, int handle, int fd, int ret),
+	    TP_ARGS(obj, handle, fd, ret),
+	    TP_STRUCT__entry(__field(void *, obj)
+			     __field(int, handle)
+			     __field(int, fd)
+			     __field(int, ret)),
+	    TP_fast_assign(__entry->obj = obj;
+			   __entry->handle = handle;
+			   __entry->fd = fd;
+			   __entry->ret = ret;),
+	    TP_printk("obj = %p, handle = %-3d, fd = %-3d, ret = %d",
+		      __entry->obj, __entry->handle, __entry->fd,
+		      __entry->ret));
+
+TRACE_EVENT(prime_fd_to_handle,
+	    TP_PROTO(int fd, int handle, int ret),
+	    TP_ARGS(fd, handle, ret),
+	    TP_STRUCT__entry(__field(int, fd)
+			     __field(int, handle)
+			     __field(int, ret)),
+	    TP_fast_assign(__entry->fd = fd;
+			   __entry->handle = handle;
+			   __entry->ret = ret;),
+	    TP_printk("fd = %-3d, handle = %-3d, ret = %d", __entry->fd,
+		      __entry->handle, __entry->ret));
+
+TRACE_EVENT(prime_dmabuf_export,
+	    TP_PROTO(void *paddr, int handle, void *dmabuf),
+	    TP_ARGS(paddr, handle, dmabuf),
+	    TP_STRUCT__entry(__field(void *, paddr)
+			     __field(int, handle)
+			     __field(void *, dmabuf)),
+	    TP_fast_assign(__entry->paddr = paddr;
+			   __entry->handle = handle;
+			   __entry->dmabuf = dmabuf;),
+	    TP_printk("paddr = %p, handle = %-3d, dmabuf = %p", __entry->paddr,
+		      __entry->handle, __entry->dmabuf));
+
+TRACE_EVENT(prime_dmabuf_import,
+	    TP_PROTO(void *dmabuf, void *gem_obj),
+	    TP_ARGS(dmabuf, gem_obj),
+	    TP_STRUCT__entry(__field(void *, dmabuf)
+			     __field(void *, gem_obj)),
+	    TP_fast_assign(__entry->dmabuf = dmabuf;
+			   __entry->gem_obj = gem_obj;),
+	    TP_printk("dmabuf = %p, gemobj = %p", __entry->dmabuf,
+		      __entry->gem_obj));
+
+TRACE_EVENT(prime_dmabuf_put,
+	    TP_PROTO(void *paddr, void *dmabuf, int fd),
+	    TP_ARGS(paddr, dmabuf, fd),
+	    TP_STRUCT__entry(__field(void *, paddr)
+			     __field(void *, dmabuf)
+			     __field(int, fd)),
+	    TP_fast_assign(__entry->paddr = paddr;
+			   __entry->dmabuf = dmabuf;
+			   __entry->fd = fd;),
+	    TP_printk("paddr = %p, dmabuf = %p", __entry->paddr,
+		      __entry->dmabuf));
+
+TRACE_EVENT(prime_drm_dmabuf_release,
+	    TP_PROTO(void *dmabuf),
+	    TP_ARGS(dmabuf),
+	    TP_STRUCT__entry(__field(void *, dmabuf)),
+	    TP_fast_assign(__entry->dmabuf = dmabuf;),
+	    TP_printk("dmabuf = %p", __entry->dmabuf));
+
+TRACE_EVENT(gem_handle_create,
+	    TP_PROTO(int handle),
+	    TP_ARGS(handle),
+	    TP_STRUCT__entry(__field(int, handle)),
+	    TP_fast_assign(__entry->handle = handle;),
+	    TP_printk("handle = %-3d,", __entry->handle));
+
+TRACE_EVENT(gem_handle_delete,
+	    TP_PROTO(int handle),
+	    TP_ARGS(handle),
+	    TP_STRUCT__entry(__field(int, handle)),
+	    TP_fast_assign(__entry->handle = handle;),
+	    TP_printk("handle = %-3d,", __entry->handle));
+
+TRACE_EVENT(hantro_msg,
+	    TP_PROTO(struct va_format *vaf),
+	    TP_ARGS(vaf),
+	    TP_STRUCT__entry(__dynamic_array(char, msg, MAX_MSG_LEN)),
+	    TP_fast_assign(WARN_ON_ONCE(vsnprintf(__get_dynamic_array(msg),
+						  MAX_MSG_LEN, vaf->fmt,
+						  *vaf->va) >= MAX_MSG_LEN);),
+	    TP_printk("%s", __get_str(msg)));
+
+TRACE_EVENT(hantro_err,
+	    TP_PROTO(struct va_format *vaf),
+	    TP_ARGS(vaf),
+	    TP_STRUCT__entry(__dynamic_array(char, msg, MAX_MSG_LEN)),
+	    TP_fast_assign(WARN_ON_ONCE(vsnprintf(__get_dynamic_array(msg),
+						  MAX_MSG_LEN, vaf->fmt,
+						  *vaf->va) >= MAX_MSG_LEN);),
+	    TP_printk("%s", __get_str(msg)));
+
+#endif /* !__HANTRO_DRIVER_TRACE || TRACE_HEADER_MULTI_READ */
+
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+#undef TRACE_INCLUDE_FILE
+#define TRACE_INCLUDE_FILE trace
+#include <trace/define_trace.h>
+
+void __trace_hantro_msg(const char *fmt, ...);
+void __trace_hantro_err(const char *fmt, ...);
+#else
+
+#define START_TIME
+#define trace_hantro_msg(vaf)
+#define trace_hantro_err(vaf)
+#define trace_drm_file_open(drmdev, file)
+#define trace_drm_file_close(drmdev, file)
+#define trace_client_add(file_attr, deviceidx, clientid, codec, profile,       \
+			 width, height)
+#define trace_client_remove(file_attr, clientid)
+#define trace_hantro_cma_alloc(deviceidx, region, paddr, handle, size)
+#define trace_hantro_cma_free(deviceidx, paddr, handle)
+#define trace_dec_reserve(deviceidx, core, waittime)
+#define trace_dec_release(deviceidx, core)
+#define trace_enc_reserve(deviceidx, core, waittime)
+#define trace_enc_release(deviceidx, core)
+#define trace_cache_reserve(deviceidx, waittime)
+#define trace_cache_release(deviceidx)
+#define trace_fence_acquirebuf(obj, handle, fence_handle, waittime, ret)
+#define trace_fence_releasebuf(fence_handle, ret)
+#define trace_prime_handle_to_fd(obj, handle, fd, ret)
+#define trace_prime_fd_to_handle(fd, handle, ret)
+#define trace_gem_handle_create(handle)
+#define trace_gem_handle_delete(handle)
+#define trace_prime_dmabuf_import(dev, file)
+#define trace_prime_dmabuf_export(obj, dev, file)
+#define trace_prime_dmabuf_put(paddr, dmabuf, fd)
+#define trace_prime_drm_dmabuf_release(dmabuf)
+#define __trace_hantro_msg(fmt, args...)
+#define __trace_hantro_err(fmt, args...)
+#endif
-- 
2.25.1

