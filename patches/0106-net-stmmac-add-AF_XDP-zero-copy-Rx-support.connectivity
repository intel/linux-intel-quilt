From ed41bbb524b42246a5ed340ae39272ce66840d36 Mon Sep 17 00:00:00 2001
From: Ong Boon Leong <boon.leong.ong@intel.com>
Date: Sat, 28 Sep 2019 17:49:02 +0800
Subject: [PATCH 106/113] net: stmmac: add AF_XDP zero-copy Rx support

This add the support for zero-copy RX support for AF_XDP socket.
The Rx buffers are added from MEM_TYPE_ZERO_COPY when AF_XDP is
enabled for certain queue.

All AF_XDP specific functions are implemented inside stmmac_xsk.c.

When AF_XDP zero-copy is enabled, the XDP action XDP_PASS will allocate
a new buffer, make a copy from the zero-copy frame to the new buffer
prior passing it to the kernel stack.

Signed-off-by: Ong Boon Leong <boon.leong.ong@intel.com>
Signed-off-by: Voon Weifeng <weifeng.voon@intel.com>
---
 drivers/net/ethernet/stmicro/stmmac/Makefile  |   2 +-
 drivers/net/ethernet/stmicro/stmmac/stmmac.h  |  37 +-
 .../net/ethernet/stmicro/stmmac/stmmac_main.c | 333 ++++++++-
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.c  | 679 ++++++++++++++++++
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.h  |  16 +
 5 files changed, 1048 insertions(+), 19 deletions(-)
 create mode 100644 drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
 create mode 100644 drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h

diff --git a/drivers/net/ethernet/stmicro/stmmac/Makefile b/drivers/net/ethernet/stmicro/stmmac/Makefile
index cd663017ea20..d34b4fd48b9b 100644
--- a/drivers/net/ethernet/stmicro/stmmac/Makefile
+++ b/drivers/net/ethernet/stmicro/stmmac/Makefile
@@ -6,7 +6,7 @@ stmmac-objs:= stmmac_main.o stmmac_ethtool.o stmmac_mdio.o ring_mode.o	\
 	      mmc_core.o stmmac_hwtstamp.o stmmac_ptp.o dwmac4_descs.o	\
 	      dwmac4_dma.o dwmac4_lib.o dwmac4_core.o dwmac5.o hwif.o \
 	      stmmac_tc.o dwxgmac2_core.o dwxgmac2_dma.o dwxgmac2_descs.o \
-	      intel_serdes.o stmmac_tsn.o dwmac5_tsn.o $(stmmac-y)
+	      intel_serdes.o stmmac_tsn.o dwmac5_tsn.o stmmac_xsk.o $(stmmac-y)
 
 stmmac-$(CONFIG_STMMAC_SELFTESTS) += stmmac_selftests.o
 
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac.h b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
index b288cdf8b0bb..aae4806e510d 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
@@ -65,13 +65,23 @@ struct stmmac_tx_queue {
 	dma_addr_t dma_tx_phy;
 	u32 tx_tail_addr;
 	u32 mss;
+	struct xdp_umem *xsk_umem;
+	struct zero_copy_allocator zca; /* ZC allocator */
 };
 
 struct stmmac_rx_buffer {
-	struct page *page;
-	struct page *sec_page;
 	dma_addr_t addr;
 	dma_addr_t sec_addr;
+	union {
+		struct {
+			struct page *page;
+			struct page *sec_page;
+		};
+		struct {
+			void *umem_addr;
+			u64 umem_handle;
+		};
+	};
 };
 
 struct stmmac_rx_queue {
@@ -84,6 +94,7 @@ struct stmmac_rx_queue {
 	struct dma_desc *dma_rx ____cacheline_aligned_in_smp;
 	unsigned int cur_rx;
 	unsigned int dirty_rx;
+	unsigned int next_to_alloc;
 	u32 rx_zeroc_thresh;
 	dma_addr_t dma_rx_phy;
 	u32 rx_tail_addr;
@@ -96,6 +107,8 @@ struct stmmac_rx_queue {
 	struct bpf_prog *xdp_prog;
 	struct xdp_rxq_info xdp_rxq;
 	unsigned int dma_buf_sz;
+	struct xdp_umem *xsk_umem;
+	struct zero_copy_allocator zca; /* ZC allocator */
 };
 
 struct stmmac_channel {
@@ -287,6 +300,10 @@ struct stmmac_priv {
 
 	/* XDP BPF Program */
 	struct bpf_prog *xdp_prog;
+
+	/* AF_XDP zero-copy */
+	unsigned long af_xdp_zc_qps; /* tracks AF_XDP ZC enabled qps */
+	struct xdp_umem **xsk_umems;
 };
 
 enum stmmac_state {
@@ -326,6 +343,11 @@ int stmmac_resume_main(struct stmmac_priv *priv, struct net_device *ndev);
 #define STMMAC_XDP_TX		BIT(1)
 #define STMMAC_XDP_REDIR	BIT(2)
 
+#define STMMAC_RX_BUFFER_WRITE	32	/* Must be power of 2 */
+
+#define STMMAC_RX_DMA_ATTR \
+	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
+
 static inline bool stmmac_enabled_xdp(struct stmmac_priv *priv)
 {
 	return !!priv->xdp_prog;
@@ -358,10 +380,21 @@ static inline struct stmmac_tx_queue *get_tx_queue(struct stmmac_priv *priv,
 	((((x)->dirty_tx > (x)->cur_tx) ? 0 : priv->dma_tx_size) + \
 	(x)->dirty_tx - (x)->cur_tx - 1)
 
+#define STMMAC_RX_DESC_UNUSED(x)	\
+	((((x)->cur_rx > (x)->dirty_rx) ? 0 : priv->dma_rx_size) + \
+	(x)->cur_rx - (x)->dirty_rx - 1)
+
 int stmmac_xmit_xdp_tx_queue(struct xdp_buff *xdp,
 			     struct stmmac_tx_queue *xdp_q);
 void stmmac_xdp_queue_update_tail(struct stmmac_tx_queue *xdp_q);
 
+void stmmac_finalize_xdp_rx(struct stmmac_rx_queue *rx_q, unsigned int xdp_res);
+int stmmac_queue_pair_enable(struct stmmac_priv *priv, u16 qid);
+int stmmac_queue_pair_disable(struct stmmac_priv *priv, u16 qid);
+void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb);
+void stmmac_get_rx_hwtstamp(struct stmmac_priv *priv, struct dma_desc *p,
+			    struct dma_desc *np, struct sk_buff *skb);
+
 #if IS_ENABLED(CONFIG_STMMAC_SELFTESTS)
 void stmmac_selftest_run(struct net_device *dev,
 			 struct ethtool_test *etest, u64 *buf);
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index f9a90dc28330..02399221e2cc 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -38,6 +38,7 @@
 #include <linux/phylink.h>
 #include <linux/pci.h>
 #include <net/pkt_cls.h>
+#include <net/xdp_sock.h>
 #include "stmmac_ptp.h"
 #include "stmmac.h"
 #include <linux/reset.h>
@@ -50,6 +51,7 @@
 #include "hwif.h"
 #include "intel_serdes.h"
 #include "stmmac_tsn.h"
+#include "stmmac_xsk.h"
 #ifdef CONFIG_STMMAC_NETWORK_PROXY
 #include "stmmac_netproxy.h"
 #endif
@@ -477,8 +479,8 @@ static void stmmac_get_tx_hwtstamp(struct stmmac_priv *priv,
  * This function will read received packet's timestamp from the descriptor
  * and pass it to stack. It also perform some sanity checks.
  */
-static void stmmac_get_rx_hwtstamp(struct stmmac_priv *priv, struct dma_desc *p,
-				   struct dma_desc *np, struct sk_buff *skb)
+void stmmac_get_rx_hwtstamp(struct stmmac_priv *priv, struct dma_desc *p,
+			    struct dma_desc *np, struct sk_buff *skb)
 {
 	struct skb_shared_hwtstamps *shhwtstamp = NULL;
 	struct dma_desc *desc = p;
@@ -1340,6 +1342,22 @@ static void stmmac_free_tx_buffer(struct stmmac_priv *priv, u32 queue, int i)
 	tx_q->tx_skbuff_dma[i].map_as_page = false;
 }
 
+/**
+ * stmmac_xsk_rx_umem - Retrieve the AF_XDP ZC if XDP and ZC is enabled
+ * @priv: private structure
+ * @queue: RX queue index
+ * Returns the UMEM or NULL.
+ **/
+static struct xdp_umem *stmmac_xsk_rx_umem(struct stmmac_priv *priv, u32 queue)
+{
+	bool xdp_on = stmmac_enabled_xdp(priv);
+
+	if (!xdp_on || !test_bit(queue, &priv->af_xdp_zc_qps))
+		return NULL;
+
+	return xdp_get_umem_from_qid(priv->dev, queue);
+}
+
 bool stmmac_alloc_rx_buffers(struct stmmac_rx_queue *rx_q, u32 count)
 {
 	struct stmmac_priv *priv = rx_q->priv_data;
@@ -1387,20 +1405,47 @@ static int init_dma_rx_desc_ring(struct stmmac_priv *priv, u32 queue,
 
 	xdp_rxq_info_unreg_mem_model(&rx_q->xdp_rxq);
 
-	rx_q->dma_buf_sz = priv->dma_buf_sz;
-
-	ret = xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
-					 MEM_TYPE_PAGE_POOL,
-					 rx_q->page_pool);
-	if (ret)
-		return ret;
+	rx_q->xsk_umem = stmmac_xsk_rx_umem(priv, queue);
+	if (rx_q->xsk_umem) {
+		rx_q->dma_buf_sz = rx_q->xsk_umem->chunk_size_nohr -
+				   XDP_PACKET_HEADROOM;
+		/* For AF_XDP ZC, we disallow packets to span on
+		* multiple buffers, thus letting us skip that
+		* handling in the fast-path.
+		*/
+		rx_q->zca.free = stmmac_zca_free;
+		ret = xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
+						 MEM_TYPE_ZERO_COPY,
+						 &rx_q->zca);
+		if (ret)
+			return ret;
 
-	netdev_info(priv->dev, "Register XDP MEM_TYPE_PAGE_SHARED RxQ-%d\n",
-		    rx_q->queue_index);
+		netdev_info(priv->dev,
+			    "Register XDP MEM_TYPE_ZERO_COPY RxQ-%d\n",
+			    rx_q->queue_index);
+	} else {
+		ret = xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
+						 MEM_TYPE_PAGE_POOL,
+						 rx_q->page_pool);
+		if (ret)
+			return ret;
 
-	ok = stmmac_alloc_rx_buffers(rx_q, priv->dma_rx_size);
-	if (!ok)
-		return -ENOMEM;
+		netdev_info(priv->dev,
+			    "Register XDP MEM_TYPE_PAGE_POOL RxQ-%d\n",
+			    rx_q->queue_index);
+	}
+
+	ok = rx_q->xsk_umem ?
+	     stmmac_alloc_rx_buffers_zc(rx_q, STMMAC_RX_DESC_UNUSED(rx_q)) :
+	     stmmac_alloc_rx_buffers(rx_q, priv->dma_rx_size);
+	if (!ok) {
+		if (rx_q->xsk_umem)
+			netdev_info(priv->dev,
+				    "Failed to alloc Rx UMEM at Q-%d\n",
+				    rx_q->queue_index);
+		else
+			return -ENOMEM;
+	}
 
 	/* Setup the chained descriptor addresses */
 	if (priv->mode == STMMAC_CHAIN_MODE) {
@@ -4072,7 +4117,7 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 	return NETDEV_TX_OK;
 }
 
-static void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb)
+void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb)
 {
 	struct vlan_ethhdr *veth;
 	__be16 vlan_proto;
@@ -4668,11 +4713,15 @@ static int stmmac_napi_poll_rx(struct napi_struct *napi, int budget)
 		container_of(napi, struct stmmac_channel, rx_napi);
 	struct stmmac_priv *priv = ch->priv_data;
 	u32 chan = ch->index;
+	struct stmmac_rx_queue *rx_q;
 	int work_done;
 
 	priv->xstats.napi_poll++;
+	rx_q = &priv->rx_queue[chan];
+
+	work_done = rx_q->xsk_umem ? stmmac_rx_zc(priv, budget, chan) :
+		    stmmac_rx(priv, budget, chan);
 
-	work_done = stmmac_rx(priv, budget, chan);
 	if (work_done < budget && napi_complete_done(napi, work_done))
 		stmmac_enable_dma_irq(priv, priv->ioaddr, chan);
 	return work_done;
@@ -5192,6 +5241,254 @@ static int stmmac_set_mac_address(struct net_device *ndev, void *addr)
 	return ret;
 }
 
+static void stmmac_napi_control(struct stmmac_priv *priv, u16 qid, bool en)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+	struct stmmac_channel *xdp_ch;
+	struct stmmac_channel *ch;
+
+	xdp_ch = &priv->channel[qid + qp_num];
+	ch = &priv->channel[qid];
+
+	if (en) {
+		napi_enable(&ch->rx_napi);
+		napi_enable(&ch->tx_napi);
+		if (queue_is_xdp(priv, qid + qp_num))
+			napi_enable(&xdp_ch->tx_napi);
+	} else {
+		napi_disable(&ch->rx_napi);
+		napi_disable(&ch->tx_napi);
+		if (queue_is_xdp(priv, qid + qp_num))
+			napi_disable(&xdp_ch->tx_napi);
+	}
+}
+
+static int stmmac_txrx_irq_control(struct stmmac_priv *priv, u16 qid, bool en)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+	int ret;
+
+	if (en) {
+		char *int_name;
+
+		if (priv->rx_irq[qid] == 0)
+			goto irq_err;
+
+		int_name = priv->int_name_rx_irq[qid];
+		sprintf(int_name, "%s:%s-%d", priv->dev->name, "rx", qid);
+		ret = request_irq(priv->rx_irq[qid],
+				 stmmac_msi_intr_rx,
+				 0, int_name, &priv->rx_queue[qid]);
+		if (unlikely(ret < 0)) {
+			netdev_err(priv->dev,
+				   "%s: alloc rx-%d  MSI %d (error: %d)\n",
+				   __func__, qid, priv->rx_irq[qid], ret);
+			goto irq_err;
+		}
+
+		if (priv->tx_irq[qid] == 0)
+			goto irq_err;
+
+		int_name = priv->int_name_tx_irq[qid];
+		sprintf(int_name, "%s:%s-%d", priv->dev->name,
+			"tx", qid);
+		ret = request_irq(priv->tx_irq[qid],
+				  stmmac_msi_intr_tx,
+				  0, int_name, get_tx_queue(priv, qid));
+		if (unlikely(ret < 0)) {
+			netdev_err(priv->dev,
+				   "%s: alloc tx-%d  MSI %d (error: %d)\n",
+				   __func__, qid, priv->tx_irq[qid], ret);
+			goto irq_err;
+		}
+
+		if (!queue_is_xdp(priv, qid + qp_num))
+			goto irq_done;
+
+		if (priv->tx_irq[qid + qp_num] == 0)
+			goto irq_err;
+
+		int_name = priv->int_name_tx_irq[qid + qp_num];
+		sprintf(int_name, "%s:%s-%d", priv->dev->name,
+			"tx-xdp", qid + qp_num);
+		ret = request_irq(priv->tx_irq[qid + qp_num],
+				  stmmac_msi_intr_tx,
+				  0, int_name,
+				  get_tx_queue(priv, qid + qp_num));
+		if (unlikely(ret < 0)) {
+			netdev_err(priv->dev,
+				   "%s: alloc tx-%d  MSI %d (error: %d)\n",
+				   __func__, qid + qp_num,
+				   priv->tx_irq[qid + qp_num], ret);
+			goto irq_err;
+		}
+	} else {
+		if (priv->rx_irq[qid] > 0)
+			free_irq(priv->rx_irq[qid], &priv->rx_queue[qid]);
+
+		if (priv->tx_irq[qid] > 0)
+			free_irq(priv->tx_irq[qid],
+					get_tx_queue(priv, qid));
+
+		if (!queue_is_xdp(priv, qid + qp_num))
+			goto irq_done;
+
+		if (priv->tx_irq[qid + qp_num] > 0)
+			free_irq(priv->tx_irq[qid + qp_num],
+					get_tx_queue(priv, qid + qp_num));
+	}
+
+irq_done:
+	return 0;
+
+irq_err:
+	return -EINVAL;
+}
+
+static void stmmac_txrx_dma_control(struct stmmac_priv *priv, u16 qid, bool en)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+
+	if (en) {
+		stmmac_start_rx_dma(priv, qid);
+		stmmac_start_tx_dma(priv, qid);
+		if (queue_is_xdp(priv, qid + qp_num))
+			stmmac_start_tx_dma(priv, qid + qp_num);
+	} else {
+		stmmac_stop_rx_dma(priv, qid);
+		stmmac_stop_tx_dma(priv, qid);
+		if (queue_is_xdp(priv, qid + qp_num))
+			stmmac_stop_tx_dma(priv, qid + qp_num);
+	}
+}
+
+static void stmmac_txrx_desc_control(struct stmmac_priv *priv, u16 qid, bool en)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+
+	if (en) {
+		if (stmmac_enabled_xdp(priv)) {
+			clear_queue_xdp(priv, qid);
+			set_queue_xdp(priv, qid + qp_num);
+		}
+
+		alloc_dma_rx_desc_resources_q(priv, qid);
+		alloc_dma_tx_desc_resources_q(priv, qid);
+		if (queue_is_xdp(priv, qid + qp_num))
+			alloc_dma_tx_desc_resources_q(priv, qid + qp_num);
+
+		init_dma_rx_desc_ring(priv, qid, 0);
+		init_dma_tx_desc_ring(priv, qid);
+		if (queue_is_xdp(priv, qid + qp_num))
+			init_dma_tx_desc_ring(priv, qid + qp_num);
+	} else {
+		free_dma_rx_desc_resources_q(priv, qid);
+		free_dma_tx_desc_resources_q(priv, qid);
+		if (queue_is_xdp(priv, qid + qp_num))
+			free_dma_tx_desc_resources_q(priv, qid + qp_num);
+
+		if (!stmmac_enabled_xdp(priv)) {
+			clear_queue_xdp(priv, qid);
+			clear_queue_xdp(priv, qid + qp_num);
+		}
+	}
+}
+
+static void stmmac_txrx_ch_init(struct stmmac_priv *priv, u16 qid)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[qid];
+	struct stmmac_tx_queue *tx_q = get_tx_queue(priv, qid);
+	struct stmmac_tx_queue *xdp_q = get_tx_queue(priv, qid + qp_num);
+
+	stmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
+			    rx_q->dma_rx_phy, rx_q->queue_index);
+
+	rx_q->rx_tail_addr = rx_q->dma_rx_phy + (priv->dma_rx_size *
+			     sizeof(struct dma_desc));
+	stmmac_set_rx_tail_ptr(priv, priv->ioaddr,
+			       rx_q->rx_tail_addr, rx_q->queue_index);
+
+	stmmac_set_dma_bfsize(priv, priv->ioaddr, rx_q->dma_buf_sz,
+			      rx_q->queue_index);
+
+	stmmac_init_tx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
+			    tx_q->dma_tx_phy, tx_q->queue_index);
+
+	tx_q->tx_tail_addr = tx_q->dma_tx_phy;
+	stmmac_set_tx_tail_ptr(priv, priv->ioaddr,
+			       tx_q->tx_tail_addr, tx_q->queue_index);
+
+	if (queue_is_xdp(priv, qid + qp_num)) {
+		xdp_q->tx_tail_addr = xdp_q->dma_tx_phy;
+		stmmac_set_tx_tail_ptr(priv, priv->ioaddr,
+				       xdp_q->tx_tail_addr,
+				       xdp_q->queue_index);
+	}
+}
+
+/**
+ * stmmac_queue_pair_enable - Enables a queue pair
+ * @priv: driver private structure
+ * @queue_pair: queue pair
+ *
+ * Returns 0 on success, <0 on failure.
+ **/
+int stmmac_queue_pair_enable(struct stmmac_priv *priv, u16 qid)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+	int ret;
+
+	if (qid >= qp_num) {
+		netdev_err(priv->dev,
+			   "%s: qid (%d) > number of queue pairs (%d)\n",
+			   __func__, qid, qp_num);
+
+		return -EINVAL;
+	}
+
+	stmmac_txrx_desc_control(priv, qid, true);
+	stmmac_txrx_dma_control(priv, qid, true);
+	stmmac_txrx_ch_init(priv, qid);
+
+	ret = stmmac_txrx_irq_control(priv, qid, true);
+	if (ret)
+		return ret;
+	stmmac_napi_control(priv, qid, true);
+
+	return 0;
+}
+
+/**
+ * stmmac_queue_pair_disable - Disables a queue pair
+ * @priv: driver private structure
+ * @queue_pair: queue pair
+ *
+ * Returns 0 on success, <0 on failure.
+ **/
+int stmmac_queue_pair_disable(struct stmmac_priv *priv, u16 qid)
+{
+	u16 qp_num = priv->plat->num_queue_pairs;
+	int ret;
+
+	if (qid >= qp_num) {
+		netdev_err(priv->dev,
+			   "%s: qid (%d) > number of queue pairs (%d)\n",
+			   __func__, qid, qp_num);
+
+		return -EINVAL;
+	}
+
+	stmmac_napi_control(priv, qid, false);
+	ret = stmmac_txrx_irq_control(priv, qid, false);
+	if (ret)
+		return ret;
+	stmmac_txrx_dma_control(priv, qid, false);
+	stmmac_txrx_desc_control(priv, qid, false);
+
+	return ret;
+}
+
 /**
  * stmmac_xdp_xmit - Implements ndo_xdp_xmit
  * @dev: netdev
@@ -5299,6 +5596,9 @@ static int stmmac_xdp(struct net_device *dev,
 	case XDP_QUERY_PROG:
 		xdp->prog_id = priv->xdp_prog ? priv->xdp_prog->aux->id : 0;
 		return 0;
+	case XDP_SETUP_XSK_UMEM:
+		return stmmac_xsk_umem_setup(priv, xdp->xsk.umem,
+					     xdp->xsk.queue_id);
 	default:
 		return -EINVAL;
 	}
@@ -6276,6 +6576,7 @@ static void stmmac_reset_queues_param(struct stmmac_priv *priv)
 
 		rx_q->cur_rx = 0;
 		rx_q->dirty_rx = 0;
+		rx_q->next_to_alloc = 0;
 	}
 
 	for (queue = 0; queue < tx_cnt; queue++) {
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
new file mode 100644
index 000000000000..3969fd40a094
--- /dev/null
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
@@ -0,0 +1,679 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2019 Intel Corporation. */
+
+#include <linux/bpf_trace.h>
+#include <net/xdp_sock.h>
+#include <net/xdp.h>
+
+#include "stmmac.h"
+
+/**
+ * stmmac_xsk_umem_dma_map - DMA maps all UMEM memory for the netdev
+ * @priv: driver private structure
+ * @umem: UMEM to DMA map
+ *
+ * Returns 0 on success, <0 on failure
+ **/
+static int stmmac_xsk_umem_dma_map(struct stmmac_priv *priv,
+				   struct xdp_umem *umem)
+{
+	struct device *dev;
+	unsigned int i, j;
+	dma_addr_t dma;
+
+	dev = priv->device;
+	for (i = 0; i < umem->npgs; i++) {
+		dma = dma_map_page_attrs(dev, umem->pgs[i], 0, PAGE_SIZE,
+					 DMA_BIDIRECTIONAL,
+					 STMMAC_RX_DMA_ATTR);
+		if (dma_mapping_error(dev, dma))
+			goto out_unmap;
+
+		umem->pages[i].dma = dma;
+	}
+
+	return 0;
+
+out_unmap:
+	for (j = 0; j < i; j++) {
+		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
+				     DMA_BIDIRECTIONAL, STMMAC_RX_DMA_ATTR);
+		umem->pages[i].dma = 0;
+	}
+
+	return -1;
+}
+
+/**
+ * stmmac_xsk_umem_dma_unmap - DMA unmaps all UMEM memory for the netdev
+ * @priv: driver private structure
+ * @umem: UMEM to DMA map
+ **/
+static void stmmac_xsk_umem_dma_unmap(struct stmmac_priv *priv,
+				      struct xdp_umem *umem)
+{
+	struct device *dev;
+	unsigned int i;
+
+	dev = priv->device;
+
+	for (i = 0; i < umem->npgs; i++) {
+		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
+				     DMA_BIDIRECTIONAL, STMMAC_RX_DMA_ATTR);
+
+		umem->pages[i].dma = 0;
+	}
+}
+
+/**
+ * stmmac_xsk_umem_enable - Enable/associate a UMEM to a certain ring/qid
+ * @priv: driver private structure
+ * @umem: UMEM
+ * @qid: Rx queue to associate UMEM to
+ *
+ * Returns 0 on success, <0 on failure
+ **/
+static int stmmac_xsk_umem_enable(struct stmmac_priv *priv,
+				  struct xdp_umem *umem, u16 qid)
+{
+	struct net_device *netdev = priv->dev;
+	struct xdp_umem_fq_reuse *reuseq;
+	bool if_running;
+	int err;
+
+	if (qid >= priv->plat->num_queue_pairs)
+		return -EINVAL;
+
+	if (qid >= netdev->real_num_rx_queues ||
+	    qid >= netdev->real_num_tx_queues)
+		return -EINVAL;
+
+	reuseq = xsk_reuseq_prepare(priv->dma_rx_size);
+	if (!reuseq)
+		return -ENOMEM;
+
+	xsk_reuseq_free(xsk_reuseq_swap(umem, reuseq));
+
+	err = stmmac_xsk_umem_dma_map(priv, umem);
+	if (err)
+		return err;
+
+	set_bit(qid, &priv->af_xdp_zc_qps);
+
+	if_running = netif_running(priv->dev) && stmmac_enabled_xdp(priv);
+
+	if (if_running) {
+		err = stmmac_queue_pair_disable(priv, qid);
+		if (err)
+			return err;
+
+		err = stmmac_queue_pair_enable(priv, qid);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+/**
+ * stmmac_xsk_umem_disable - Disassociate a UMEM from a certain ring/qid
+ * @priv: driver private structure
+ * @qid: Rx queue to associate UMEM to
+ *
+ * Returns 0 on success, <0 on failure
+ **/
+static int stmmac_xsk_umem_disable(struct stmmac_priv *priv, u16 qid)
+{
+	struct net_device *netdev = priv->dev;
+	struct xdp_umem *umem;
+	bool if_running;
+	int err;
+
+	umem = xdp_get_umem_from_qid(netdev, qid);
+	if (!umem)
+		return -EINVAL;
+
+	if_running = netif_running(priv->dev) && stmmac_enabled_xdp(priv);
+
+	if (if_running) {
+		err = stmmac_queue_pair_disable(priv, qid);
+		if (err)
+			return err;
+	}
+
+	clear_bit(qid, &priv->af_xdp_zc_qps);
+	stmmac_xsk_umem_dma_unmap(priv, umem);
+
+	if (if_running) {
+		err = stmmac_queue_pair_enable(priv, qid);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+/**
+ * stmmac_xsk_umem_setup - Enable/disassociate a UMEM to/from a ring/qid
+ * @priv: driver private structure
+ * @umem: UMEM to enable/associate to a ring, or NULL to disable
+ * @qid: Rx queue to (dis)associate UMEM (from)to
+ *
+ * This function enables or disables a UMEM to a certain queue.
+ *
+ * Returns 0 on success, <0 on failure
+ **/
+int stmmac_xsk_umem_setup(struct stmmac_priv *priv, struct xdp_umem *umem,
+			  u16 qid)
+{
+	return umem ? stmmac_xsk_umem_enable(priv, umem, qid) :
+	       stmmac_xsk_umem_disable(priv, qid);
+}
+
+/**
+ * stmmac_run_xdp_zc - Executes an XDP program on an xdp_buff
+ * @rx_q: Rx queue structure
+ * @xdp: xdp_buff used as input to the XDP program
+ *
+ * This function enables or disables a UMEM to a certain ring.
+ *
+ * Returns any of I_XDP_{PASS, CONSUMED, TX, REDIR}
+ **/
+static int stmmac_run_xdp_zc(struct stmmac_rx_queue *rx_q, struct xdp_buff *xdp)
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	int err, result = STMMAC_XDP_PASS;
+	struct stmmac_tx_queue *xdp_q;
+	struct bpf_prog *xdp_prog;
+	u32 act;
+
+	rcu_read_lock();
+	/* NB! xdp_prog will always be !NULL, due to the fact that
+	 * this path is enabled by setting an XDP program.
+	 */
+	xdp_prog = READ_ONCE(rx_q->xdp_prog);
+	act = bpf_prog_run_xdp(xdp_prog, xdp);
+	xdp->handle += xdp->data - xdp->data_hard_start;
+
+	switch (act) {
+	case XDP_PASS:
+		break;
+	case XDP_TX:
+		xdp_q = &priv->xdp_queue[rx_q->queue_index];
+		result = stmmac_xmit_xdp_tx_queue(xdp, xdp_q);
+		break;
+	case XDP_REDIRECT:
+		err = xdp_do_redirect(priv->dev, xdp, xdp_prog);
+		result = !err ? STMMAC_XDP_REDIR : STMMAC_XDP_CONSUMED;
+		break;
+	default:
+		bpf_warn_invalid_xdp_action(act);
+		/* fall through */
+	case XDP_ABORTED:
+		trace_xdp_exception(priv->dev, xdp_prog, act);
+		/* fall through -- handle aborts by dropping packet */
+	case XDP_DROP:
+		result = STMMAC_XDP_CONSUMED;
+		break;
+	}
+	rcu_read_unlock();
+	return result;
+}
+
+/**
+ * stmmac_alloc_buffer_zc - Allocates an Rx Buffer from XDP ZC
+ * @rx_q: RX queue structure
+ * @bi: Rx buffer to populate
+ *
+ * This function allocates an Rx buffer. The buffer can come from fill
+ * queue, or via the recycle queue (next_to_alloc).
+ *
+ * Returns true for a successful allocation, false otherwise
+ **/
+static bool stmmac_alloc_buffer_zc(struct stmmac_rx_queue *rx_q,
+				   struct stmmac_rx_buffer *buf)
+{
+	struct xdp_umem *umem = rx_q->xsk_umem;
+	void *addr = buf->umem_addr;
+	u64 handle, hr;
+
+	if (addr)
+		return true;
+
+	if (!xsk_umem_peek_addr(umem, &handle))
+		return false;
+
+	hr = umem->headroom + XDP_PACKET_HEADROOM;
+
+	buf->addr = xdp_umem_get_dma(umem, handle);
+	buf->addr += hr;
+
+	buf->umem_addr = xdp_umem_get_data(umem, handle);
+	buf->umem_addr += hr;
+
+	buf->umem_handle = handle + umem->headroom;
+
+	xsk_umem_discard_addr(umem);
+
+	return true;
+}
+
+/**
+ * stmmac_alloc_buffer_slow_zc - Allocates an stmmac_rx_buffer
+ * @rx_q: Rx queue
+ * @buf: Rx buffer to populate
+ *
+ * This function allocates an Rx buffer. The buffer can come from fill
+ * queue, or via the reuse queue.
+ *
+ * Returns true for a successful allocation, false otherwise
+ **/
+static bool stmmac_alloc_buffer_slow_zc(struct stmmac_rx_queue *rx_q,
+					struct stmmac_rx_buffer *buf)
+{
+	struct xdp_umem *umem = rx_q->xsk_umem;
+	u64 handle, hr;
+
+	if (!xsk_umem_peek_addr_rq(umem, &handle))
+		return false;
+
+	handle &= rx_q->xsk_umem->chunk_mask;
+
+	hr = umem->headroom + XDP_PACKET_HEADROOM;
+
+	buf->addr = xdp_umem_get_dma(umem, handle);
+	buf->addr += hr;
+
+	buf->umem_addr = xdp_umem_get_data(umem, handle);
+	buf->umem_addr += hr;
+
+	buf->umem_handle = handle + umem->headroom;
+
+	xsk_umem_discard_addr_rq(umem);
+
+	return true;
+}
+
+static __always_inline bool
+__stmmac_alloc_rx_buffers_zc(struct stmmac_rx_queue *rx_q, u16 count,
+			     bool alloc(struct stmmac_rx_queue *rx_q,
+					struct stmmac_rx_buffer *buf))
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	u16 entry = rx_q->dirty_rx;
+	bool ok = true;
+	struct stmmac_rx_buffer *buf;
+	struct dma_desc *rx_desc;
+	unsigned int last_refill = entry;
+
+	do {
+		bool use_rx_wd;
+
+		if (priv->extend_desc)
+			rx_desc = (struct dma_desc *)(rx_q->dma_erx + entry);
+		else
+			rx_desc = rx_q->dma_rx + entry;
+
+		buf = &rx_q->buf_pool[entry];
+		if (!alloc(rx_q, buf)) {
+			ok = false;
+			goto no_buffers;
+		}
+
+		dma_sync_single_range_for_device(priv->device, buf->addr, 0,
+						 rx_q->dma_buf_sz,
+						 DMA_BIDIRECTIONAL);
+
+		stmmac_set_desc_addr(priv, rx_desc, buf->addr);
+		stmmac_refill_desc3(priv, rx_q, rx_desc);
+		use_rx_wd = priv->use_riwt && rx_q->rx_count_frames;
+
+		stmmac_set_rx_owner(priv, rx_desc, use_rx_wd);
+		last_refill = entry;
+		entry = STMMAC_GET_ENTRY(entry, priv->dma_rx_size);
+
+		count--;
+	} while (count);
+
+no_buffers:
+	if (rx_q->dirty_rx != entry) {
+		rx_q->dirty_rx = entry;
+		rx_q->next_to_alloc = entry;
+
+		wmb();
+		rx_q->rx_tail_addr = rx_q->dma_rx_phy + (last_refill *
+				     sizeof(struct dma_desc));
+		stmmac_set_rx_tail_ptr(priv, priv->ioaddr,
+				       rx_q->rx_tail_addr, rx_q->queue_index);
+	}
+
+	return ok;
+}
+
+/**
+ * stmmac_alloc_rx_buffers_zc - Allocates a number of Rx buffers
+ * @rx_q: Rx queue structure
+ * @count: The number of buffers to allocate
+ *
+ * This function allocates a number of Rx buffers from the reuse queue
+ * or fill ring and places them on the Rx queue.
+ *
+ * Returns true for a successful allocation, false otherwise
+ **/
+bool stmmac_alloc_rx_buffers_zc(struct stmmac_rx_queue *rx_q, u16 count)
+{
+	rx_q->cur_rx = 0;
+	rx_q->dirty_rx = 0;
+	rx_q->next_to_alloc = 0;
+
+	return __stmmac_alloc_rx_buffers_zc(rx_q, count,
+					    stmmac_alloc_buffer_slow_zc);
+}
+
+/**
+ * stmmac_alloc_rx_buffers_fast_zc - Allocates a number of Rx buffers
+ * @rx_q: Rx queue
+ * @count: The number of buffers to allocate
+ *
+ * This function allocates a number of Rx buffers from the fill ring
+ * or the internal recycle mechanism and places them on the Rx ring.
+ *
+ * Returns true for a successful allocation, false otherwise
+ **/
+static bool stmmac_alloc_rx_buffers_fast_zc(struct stmmac_rx_queue *rx_q,
+					    u16 count)
+{
+	return __stmmac_alloc_rx_buffers_zc(rx_q, count,
+					   stmmac_alloc_buffer_zc);
+}
+
+/**
+ * stmmac_get_rx_buffer_zc - Return the current Rx buffer
+ * @rx_q: Rx queue structure
+ * @size: The size of the rx buffer (read from descriptor)
+ *
+ * This function returns the current, received Rx buffer, and also
+ * does DMA synchronization for the Rx queue.
+ *
+ * Returns the received Rx buffer
+ **/
+static struct stmmac_rx_buffer *stmmac_get_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+							const unsigned int size)
+{
+	struct stmmac_rx_buffer *buf;
+	struct stmmac_priv *priv;
+
+	buf = &rx_q->buf_pool[rx_q->cur_rx];
+	priv = rx_q->priv_data;
+
+	/* we are reusing so sync this buffer for CPU use */
+	dma_sync_single_range_for_cpu(priv->device,
+				      buf->addr, 0,
+				      size,
+				      DMA_BIDIRECTIONAL);
+
+	return buf;
+}
+
+/**
+ * stmmac_reuse_rx_buffer_zc - Recycle an Rx buffer
+ * @rx_q: Rx queue
+ * @old_buf: The Rx buffer to recycle
+ *
+ * This function recycles a finished Rx buffer, and places it on the
+ * recycle queue (next_to_alloc).
+ **/
+static void stmmac_reuse_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+				      struct stmmac_rx_buffer *old_buf)
+{
+	struct stmmac_rx_buffer *new_buf = &rx_q->buf_pool[rx_q->next_to_alloc];
+	unsigned long mask = (unsigned long)rx_q->xsk_umem->chunk_mask;
+	u64 hr = rx_q->xsk_umem->headroom + XDP_PACKET_HEADROOM;
+	struct stmmac_priv *priv = rx_q->priv_data;
+	u16 nta = rx_q->next_to_alloc;
+
+	/* update, and store next to alloc */
+	nta++;
+	rx_q->next_to_alloc = (nta < priv->dma_rx_size) ? nta : 0;
+
+	/* transfer page from old buffer to new buffer */
+	new_buf->addr = old_buf->addr & mask;
+	new_buf->addr += hr;
+
+	new_buf->umem_addr = (void *)((unsigned long)old_buf->umem_addr & mask);
+	new_buf->umem_addr += hr;
+
+	new_buf->umem_handle = old_buf->umem_handle & mask;
+	new_buf->umem_handle += rx_q->xsk_umem->headroom;
+
+	old_buf->umem_addr = NULL;
+}
+
+/**
+ * stmmac_zca_free - Free callback for MEM_TYPE_ZERO_COPY allocations
+ * @alloc: Zero-copy allocator
+ * @handle: Buffer handle
+ **/
+void stmmac_zca_free(struct zero_copy_allocator *alloc, unsigned long handle)
+{
+	struct stmmac_rx_buffer *buf;
+	struct stmmac_rx_queue *rx_q;
+	struct stmmac_priv *priv;
+	u64 hr, mask;
+	u16 nta;
+
+	rx_q = container_of(alloc, struct stmmac_rx_queue, zca);
+	hr = rx_q->xsk_umem->headroom + XDP_PACKET_HEADROOM;
+	mask = rx_q->xsk_umem->chunk_mask;
+
+	nta = rx_q->next_to_alloc;
+	buf = &rx_q->buf_pool[nta];
+	priv = rx_q->priv_data;
+
+	nta++;
+	rx_q->next_to_alloc = (nta < priv->dma_rx_size) ? nta : 0;
+
+	handle &= mask;
+
+	buf->addr = xdp_umem_get_dma(rx_q->xsk_umem, handle);
+	buf->addr += hr;
+
+	buf->umem_addr = xdp_umem_get_data(rx_q->xsk_umem, handle);
+	buf->umem_addr += hr;
+
+	buf->umem_handle = (u64)handle + rx_q->xsk_umem->headroom;
+}
+
+/**
+ * stmmac_construct_skb_zc - Create skbufff from zero-copy Rx buffer
+ * @rx_q: Rx queue structure
+ * @bi: Rx buffer
+ * @xdp: xdp_buff
+ *
+ * This functions allocates a new skb from a zero-copy Rx buffer.
+ *
+ * Returns the skb, or NULL on failure.
+ **/
+static struct sk_buff *stmmac_construct_skb_zc(struct stmmac_rx_queue *rx_q,
+					       struct stmmac_rx_buffer *buf,
+					       struct xdp_buff *xdp)
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	unsigned int metasize = xdp->data - xdp->data_meta;
+	unsigned int datasize = xdp->data_end - xdp->data;
+	struct stmmac_channel *ch;
+	struct sk_buff *skb;
+
+	ch = &priv->channel[rx_q->queue_index];
+
+	/* allocate a skb to store the frags */
+	skb = __napi_alloc_skb(&ch->rx_napi,
+			       xdp->data_end - xdp->data_hard_start,
+			       GFP_ATOMIC | __GFP_NOWARN);
+	if (unlikely(!skb))
+		return NULL;
+
+	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+	memcpy(__skb_put(skb, datasize), xdp->data, datasize);
+	if (metasize)
+		skb_metadata_set(skb, metasize);
+
+	stmmac_reuse_rx_buffer_zc(rx_q, buf);
+
+	return skb;
+}
+
+/**
+ * stmmac_inc_ntc: Advance the next_to_clean index
+ * @rx_q: Rx queue
+ **/
+static void stmmac_inc_ntc(struct stmmac_rx_queue *rx_q)
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	struct dma_desc *rx_desc;
+	u32 ntc;
+
+	ntc = rx_q->cur_rx + 1;
+	ntc = (ntc < priv->dma_rx_size) ? ntc : 0;
+	rx_q->cur_rx = ntc;
+
+	if (priv->extend_desc)
+		rx_desc = (struct dma_desc *)(rx_q->dma_erx + ntc);
+	else
+		rx_desc = rx_q->dma_rx + ntc;
+
+	prefetch(rx_desc);
+}
+
+/**
+ * stmmac_rx_zc - Consumes Rx packets from the hardware queue
+ * @rx_q: Rx queue structure
+ * @budget: NAPI budget
+ *
+ * Returns amount of work completed
+ **/
+int stmmac_rx_zc(struct stmmac_priv *priv, int budget, u32 queue)
+{
+	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
+	struct stmmac_channel *ch = &priv->channel[queue];
+	u16 fill_count = STMMAC_RX_DESC_UNUSED(rx_q);
+	unsigned int xdp_res, xdp_xmit = 0;
+	int coe = priv->hw->rx_csum;
+	bool failure = false;
+	struct sk_buff *skb;
+	struct xdp_buff xdp;
+
+	xdp.rxq = &rx_q->xdp_rxq;
+
+	while (likely(total_rx_packets < (unsigned int)budget)) {
+		struct stmmac_rx_buffer *buf;
+		struct dma_desc *rx_desc;
+		unsigned int size;
+		int status;
+
+		if (fill_count >= STMMAC_RX_BUFFER_WRITE) {
+			failure = failure ||
+				  !stmmac_alloc_rx_buffers_fast_zc(rx_q,
+								   fill_count);
+			fill_count = 0;
+		}
+
+		if (priv->extend_desc)
+			rx_desc = (struct dma_desc *)(rx_q->dma_erx +
+						      rx_q->cur_rx);
+		else
+			rx_desc = rx_q->dma_rx + rx_q->cur_rx;
+
+		/* This memory barrier is needed to keep us from reading
+		 * any other fields out of the rx_desc until we have
+		 * verified the descriptor has been written back.
+		 */
+		dma_rmb();
+
+		/* read the status of the incoming frame */
+		status = stmmac_rx_status(priv, &priv->dev->stats,
+					  &priv->xstats, rx_desc);
+
+		if (unlikely(status & dma_own))
+			break;
+
+		size = stmmac_get_rx_frame_len(priv, rx_desc,
+					       coe);
+		if (!size)
+			break;
+
+		buf = stmmac_get_rx_buffer_zc(rx_q, size);
+
+		if (unlikely(status == discard_frame)) {
+			stmmac_reuse_rx_buffer_zc(rx_q, buf);
+			priv->dev->stats.rx_errors++;
+			fill_count++;
+			continue;
+		}
+
+		xdp.data = buf->umem_addr;
+		xdp.data_meta = xdp.data;
+		xdp.data_hard_start = xdp.data - XDP_PACKET_HEADROOM;
+		xdp.data_end = xdp.data + size;
+		xdp.handle = buf->umem_handle;
+
+		xdp_res = stmmac_run_xdp_zc(rx_q, &xdp);
+		if (xdp_res) {
+			if (xdp_res & (STMMAC_XDP_TX | STMMAC_XDP_REDIR)) {
+				xdp_xmit |= xdp_res;
+				buf->umem_addr = NULL;
+			} else {
+				stmmac_reuse_rx_buffer_zc(rx_q, buf);
+			}
+
+			total_rx_bytes += size;
+			total_rx_packets++;
+
+			fill_count++;
+			stmmac_inc_ntc(rx_q);
+			continue;
+		}
+
+		/* XDP_PASS path */
+		skb = stmmac_construct_skb_zc(rx_q, buf, &xdp);
+		if (unlikely(!skb)) {
+			priv->dev->stats.rx_dropped++;
+			break;
+		}
+
+		fill_count++;
+
+		stmmac_inc_ntc(rx_q);
+
+		if (eth_skb_pad(skb))
+			continue;
+
+		total_rx_bytes += skb->len;
+		total_rx_packets++;
+
+		/* Use HW to strip VLAN header before fallback
+		 * to SW.
+		 */
+		status = stmmac_rx_hw_vlan(priv, priv->dev,
+					   priv->hw, rx_desc, skb);
+		if (status == -EINVAL)
+			stmmac_rx_vlan(priv->dev, skb);
+
+		skb->protocol = eth_type_trans(skb, priv->dev);
+
+		if (unlikely(!coe))
+			skb_checksum_none_assert(skb);
+		else
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		napi_gro_receive(&ch->rx_napi, skb);
+	}
+
+	stmmac_finalize_xdp_rx(rx_q, xdp_xmit);
+
+	priv->dev->stats.rx_packets += total_rx_packets;
+	priv->dev->stats.rx_bytes += total_rx_bytes;
+
+	return failure ? budget : (int)total_rx_packets;
+}
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
new file mode 100644
index 000000000000..fbf1e70c7a5a
--- /dev/null
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
@@ -0,0 +1,16 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2019 Intel Corporation. */
+
+#ifndef _STMMAC_XSK_H_
+#define _STMMAC_XSK_H_
+
+struct xdp_umem;
+struct zero_copy_allocator;
+
+int stmmac_xsk_umem_setup(struct stmmac_priv *priv, struct xdp_umem *umem,
+			  u16 qid);
+void stmmac_zca_free(struct zero_copy_allocator *alloc, unsigned long handle);
+bool stmmac_alloc_rx_buffers_zc(struct stmmac_rx_queue *rx_q, u16 count);
+int stmmac_rx_zc(struct stmmac_priv *priv, int limit, u32 queue);
+
+#endif /* _STMMAC_XSK_H_ */
-- 
2.17.1

