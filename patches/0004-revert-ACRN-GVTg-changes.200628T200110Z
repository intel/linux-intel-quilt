From 255b6c2e8939d5531abff9e478c74030d28f59c7 Mon Sep 17 00:00:00 2001
From: Junxiao Chang <junxiao.chang@intel.com>
Date: Mon, 25 May 2020 18:06:58 +0800
Subject: [PATCH 04/24] revert ACRN GVTg changes

Squashed commit of the following:

Revert "drm/i915/gvt: some changes to support xengt/acrngt"
This reverts eb974e9696d4.

Revert "drm/i915/gvt: Refactored BXT plane registers"

This reverts 99bac57409c6.

Revert "drm/i915/gvt: passthru PIPE_DSL regiser to guest"

This reverts 621d903aba76.

Revert "drm/i915/gvt: local display support"

This reverts 7bfb76de4a62.

Revert "drm/i915/gvt: local display support in GVT-g guest"

This reverts ea5719e899db.

Revert "drm/i915/gvt: Change DomU to support 2 HDMI displays."

This reverts d079b3e1e810.

Revert "drm/i915: Ignore the lspcon check on VGPU"

This reverts 9592f2bc89b7.

Revert "drm/i915/gvt: add some MMIO value initialization"

This reverts 68846ea37520.

Revert "drm/i915/gvt: enable ppgtt oos sync by default"

This reverts 32be508cf768.

Revert "drm/i915/gvt: emit shadow ppgtt root in LRI"

This reverts fcd9c7357a60.

Revert "drm/i915/gvt: Don't load CSR for Dom U"

This reverts 51fa1a60a5dc.

Revert "drm/i915/gvt: add acrngt support"

This reverts 77f467a3e129.

Revert "drm/i915/gvt: remove some initialization of ggtt in GVTg guest"

This reverts 8bbf692e197e.

Revert "drm/i915/gvt: add param disable_gvt_fw_loading to disable gvt fw loading"

This reverts 1f3f0ff2e359.

Revert "drm/i915/gvt: inject error interrupt to DomU when GPU hang"

This reverts c9de4a8559ff.

Revert "drm/i915/gvt: Add the support of HUC_STATUS2 reg emulation for Guest VGPU"

This reverts 6cd166cd434c.

Revert "drm/i915/gvt: Support vGPU guest framebuffer GEM object"

This reverts 3a5dbee49dfa.

Revert "drm/i915/gvt: unset DDI_BUF_CTL_ENABLE during port emulation reset"

This reverts 80d6062679bb.

Revert "drm/i915/gvt: add scaler owner to support guest plane scaling"

This reverts dc587eb2c807.

Revert "drm/i915/gvt: support guest plane scaling"

This reverts 11d5a574eaf6.

Revert "drm/i915/gvt: add module parameter enable_pvmmio"

This reverts f31725ecd37d.

Revert "drm/i915/gvt: get ready of memory for pvmmio"

This reverts c255f01098dc.

Revert "drm/i915: implement pvmmio in guest i915"

This reverts 7f783261940e.

Revert "drm/i915/gvt: implement pvmmio in GVTg"

This reverts b48fa2d0de62.

Revert "drm/i915/gvt: enable elsp_submitt pvmmio through enable_pvmmio param"

This reverts 0694f9e4e338.

Revert "drm/i915: Use 64-bit write to optimize writing fence_reg"

This reverts d1e45a409f79.

Revert "drm/i915/gvt: don't treat EINVAL if trap pci_command and pci_status together"

This reverts d510889daab0.

Revert "drm/i915/gvt: pvmmio optimization for plane update"

This reverts 4b199a399383.

Revert "drm/i915/gvt: implement gfn_to_mfn with identical 1:1 mapping check"

This reverts 8b6bd8045621.

Revert "drm/i915/gvt: cached read_gpa optimization in shadow ppgtt update"

This reverts 38aec94f80e2.

Revert "drm/i915/gvt: notify ppgtt update through g2v"

This reverts bce2386a5207.

Revert "drm/i915/gvt: handle ppgtt update from g2v"

This reverts c409e7cfb0bf.

Revert "drm/i915/gvt: enable pv ppgtt update by default"

This reverts 2cf03c0e137f.

Revert "drm/i915/gvt: pvmmio optimization for plane wm register update"

This reverts b9b46660017e.

Revert "drm/i915/gvt: use plane size for fb decoder"

This reverts b276a8da2617.

Revert "drm/i915/gvt: Support vgpu workload priority config"

This reverts c29d7e136428.

Revert "drm/i915/gvt: rebase gvtbuffer to use upstream functions."

This reverts c9820dc3bfa2.

Revert "drm/i915/gvt: check msg length before use it"

This reverts 579341e616bc.

Revert "drm/i915/gvt: emulate correct state of SKL_FUSE_STATUS on BXT"

This reverts 1d8f0397c8b0.

Revert "drm/i915/gvt: force to active the high-performance mode during vGPU busy"

This reverts 2491b2923de8.

Revert "drm/i915/gvt: allocate memory for vreg mmio during boot time"

This reverts 93a8edb97e3a.

Revert "drm/i915/gvt: optimize the oos memory setup"

This reverts 588a5d280fa7.

Revert "drm/i915/gvt: add opregion emulation for AcrnGT"

This reverts 23e833f4aa9d.

Revert "drm/i915/gvt: enable local direct display for WaaG"

This reverts be307b5bec53.

Revert "drm/i915/gvt: enable cursor plane emulation"

This reverts 8c5aa19fab06.

Revert "drm/i915/gvt: replace magic for cursor ddb size with GVT_CURSOR_BLOCKS"

This reverts e4f877ce7f19.

Revert "drm/i915: add GEN9 cache sharing control in debugfs"

This reverts de9c2b25247a.

Revert "drm/i915/gvt: create edid from drm_display_mode instance for fixed-mode panel"

This reverts 2cd4f8ee4bfc.

Revert "drm/i915: to limit the supported modifiers for plane restriction"

This reverts c6cf5c51cbef.

Revert "drm/i915: diable huge page ppgtt when using PVMMIO ppgtt update"

This reverts ec3d11f07f6a.

Revert "drm/i915/gvt: Use the gen2 uncore mmio callback for vgpu"

This reverts 77dde628f1bf.

Revert "drm/i915/gvt: workload scan and prepare workload in one thread to avoid incorrect mutex usage"

This reverts a4ce47da0ad5.

Revert "INTERNAL [IOTG] drm/i915: Decouple pipe and crtc index dependencies"

This reverts 49949896d6c4.

Revert "INTERNAL [IOTG] drm/i915: Introduce the Plane Restriction feature"

This reverts 8274bae8d5b9.

Revert "drm/i915/gvt: make KBL also support plane restriction feature"

This reverts 5abe09b81ca4.

Revert "drm/i915/gvt: ensure each pipe has a plane in Host OS"

This reverts b43ab48cafd2.

Revert "drm/i915/gvt: enable gvt gop support"

This reverts 2ae21d5c4823.

Revert "drm/i915/gvt: enable GOP driver for different display modes"

This reverts 380785108156.

Revert "drm/i915/gvt: set vgpu request status is done"

This reverts 6bccdf6d93ff.

Revert "drm/i915/gvt: Add parameter to disable non-context reg save/restore"

This reverts 6b34f07ad657.

Revert "drm/i915/gvt: Add a module parameter to disable GPU frequency adjustment for VGPU workload"

This reverts 87c4f76b1766.

Revert "drm/i915/gvt: don't update invalid surface address"

This reverts 83e7ee9cc861.

Revert "drm/i915/gvt:Expose vgpu bar0 and bar2 information"

This reverts 854d5065762e.

Revert "drm/i915/gvt: disable pipe gamma enable bit"

This reverts de90da0273c5.

Revert "drm/i915/gvt: set correct virtual monitor for WHL"

This reverts 4726da64050d.

Revert "Add PCH slow clock wa for EHL GPU flr"

This reverts d0153c6341e0.

Revert "drm/i915/gvt: Init DPLL/DDI vreg for virtual display
 instead of inheritance."

This reverts 5e171483e947.

Signed-off-by: Junxiao Chang <junxiao.chang@intel.com>
---
 drivers/gpu/drm/i915/Kconfig                  |   10 -
 drivers/gpu/drm/i915/Makefile                 |    2 +-
 drivers/gpu/drm/i915/display/intel_atomic.c   |    8 +-
 drivers/gpu/drm/i915/display/intel_bios.c     |    7 -
 drivers/gpu/drm/i915/display/intel_ddi.c      |    7 +-
 drivers/gpu/drm/i915/display/intel_display.c  |  207 +---
 drivers/gpu/drm/i915/display/intel_display.h  |   14 -
 .../drm/i915/display/intel_display_types.h    |    8 +-
 drivers/gpu/drm/i915/display/intel_dp.c       |    9 +-
 drivers/gpu/drm/i915/display/intel_dpll_mgr.c |    5 +-
 drivers/gpu/drm/i915/display/intel_sprite.c   |   38 +-
 drivers/gpu/drm/i915/gt/intel_lrc.c           |   28 +-
 drivers/gpu/drm/i915/gvt/Makefile             |    1 -
 drivers/gpu/drm/i915/gvt/acrngt.c             | 1079 -----------------
 drivers/gpu/drm/i915/gvt/acrngt.h             |   81 --
 drivers/gpu/drm/i915/gvt/cfg_space.c          |   27 +-
 drivers/gpu/drm/i915/gvt/cmd_parser.c         |   28 -
 drivers/gpu/drm/i915/gvt/cmd_parser.h         |    1 -
 drivers/gpu/drm/i915/gvt/display.c            |  479 +-------
 drivers/gpu/drm/i915/gvt/display.h            |    1 -
 drivers/gpu/drm/i915/gvt/edid.c               |  106 +-
 drivers/gpu/drm/i915/gvt/edid.h               |    4 +-
 drivers/gpu/drm/i915/gvt/fb_decoder.c         |   10 +-
 drivers/gpu/drm/i915/gvt/fb_decoder.h         |    5 +-
 drivers/gpu/drm/i915/gvt/firmware.c           |   17 +-
 drivers/gpu/drm/i915/gvt/gtt.c                |  397 +-----
 drivers/gpu/drm/i915/gvt/gtt.h                |   11 +-
 drivers/gpu/drm/i915/gvt/gvt.c                |  141 +--
 drivers/gpu/drm/i915/gvt/gvt.h                |   51 -
 drivers/gpu/drm/i915/gvt/handlers.c           |  655 ++++------
 drivers/gpu/drm/i915/gvt/hypercall.h          |    4 -
 drivers/gpu/drm/i915/gvt/interrupt.c          |   11 -
 drivers/gpu/drm/i915/gvt/interrupt.h          |    5 -
 drivers/gpu/drm/i915/gvt/mmio.c               |   33 +-
 drivers/gpu/drm/i915/gvt/mpt.h                |   21 -
 drivers/gpu/drm/i915/gvt/opregion.c           |   24 +-
 drivers/gpu/drm/i915/gvt/reg.h                |    9 +-
 drivers/gpu/drm/i915/gvt/sched_policy.c       |    5 -
 drivers/gpu/drm/i915/gvt/scheduler.c          |  104 +-
 drivers/gpu/drm/i915/gvt/scheduler.h          |    1 -
 drivers/gpu/drm/i915/gvt/vgpu.c               |   27 -
 drivers/gpu/drm/i915/i915_debugfs.c           |   20 +-
 drivers/gpu/drm/i915/i915_drv.c               |   53 +-
 drivers/gpu/drm/i915/i915_drv.h               |   17 -
 drivers/gpu/drm/i915/i915_gem.c               |    3 +-
 drivers/gpu/drm/i915/i915_gem_fence_reg.c     |   15 +-
 drivers/gpu/drm/i915/i915_gem_gtt.c           |   75 +-
 drivers/gpu/drm/i915/i915_gem_gvtbuffer.c     |  298 -----
 drivers/gpu/drm/i915/i915_irq.c               |   31 +-
 drivers/gpu/drm/i915/i915_params.c            |   85 --
 drivers/gpu/drm/i915/i915_params.h            |   11 +-
 drivers/gpu/drm/i915/i915_pvinfo.h            |   81 +-
 drivers/gpu/drm/i915/i915_reg.h               |   38 -
 drivers/gpu/drm/i915/i915_vgpu.c              |    8 -
 drivers/gpu/drm/i915/intel_csr.c              |    8 +-
 drivers/gpu/drm/i915/intel_pm.c               |  172 +--
 drivers/gpu/drm/i915/intel_uncore.c           |   54 -
 drivers/gpu/drm/i915/intel_uncore.h           |    7 +-
 drivers/pci/quirks.c                          |   46 -
 include/uapi/drm/drm_fourcc.h                 |    9 -
 include/uapi/drm/i915_drm.h                   |   40 -
 61 files changed, 395 insertions(+), 4357 deletions(-)
 delete mode 100644 drivers/gpu/drm/i915/gvt/acrngt.c
 delete mode 100644 drivers/gpu/drm/i915/gvt/acrngt.h
 delete mode 100644 drivers/gpu/drm/i915/i915_gem_gvtbuffer.c

Index: kernel-lts-staging/drivers/gpu/drm/i915/Kconfig
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/Kconfig
+++ kernel-lts-staging/drivers/gpu/drm/i915/Kconfig
@@ -136,16 +136,6 @@ config DRM_I915_GVT_KVMGT
 	  Choose this option if you want to enable KVMGT support for
 	  Intel GVT-g.
 
-config DRM_I915_GVT_ACRN_GVT
-	tristate "Enable ACRN support for Intel GVT-g"
-	depends on DRM_I915_GVT
-	depends on ACRN_GUEST
-	depends on ACRN_VHM
-	default n
-	help
-          Choose this option if you want to enable ACRN_GVT support for
-          Intel GVT-g under ACRN hypervisor environment.
-
 menu "drm/i915 Debugging"
 depends on DRM_I915
 depends on EXPERT
Index: kernel-lts-staging/drivers/gpu/drm/i915/Makefile
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/Makefile
+++ kernel-lts-staging/drivers/gpu/drm/i915/Makefile
@@ -266,7 +266,7 @@ i915-$(CONFIG_DRM_I915_SELFTEST) += \
 i915-y += i915_vgpu.o
 
 ifeq ($(CONFIG_DRM_I915_GVT),y)
-i915-y += intel_gvt.o i915_gem_gvtbuffer.o
+i915-y += intel_gvt.o
 include $(src)/gvt/Makefile
 endif
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_atomic.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_atomic.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_atomic.c
@@ -274,11 +274,9 @@ static void intel_atomic_setup_scaler(st
 			if (scaler_state->scalers[j].in_use)
 				continue;
 
-			else if (scaler_state->scalers[j].owned == 1) {
-				*scaler_id = j;
-				scaler_state->scalers[*scaler_id].in_use = 1;
-				break;
-			}
+			*scaler_id = j;
+			scaler_state->scalers[*scaler_id].in_use = 1;
+			break;
 		}
 	}
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_bios.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_bios.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_bios.c
@@ -1951,13 +1951,6 @@ void intel_bios_init(struct drm_i915_pri
 		return;
 	}
 
-	if (HAS_PCH_NOP(dev_priv) && !intel_vgpu_active(dev_priv)) {
-		DRM_DEBUG_KMS("Skipping VBT init due to disabled display.\n");
-		return;
-	} else if (HAS_PCH_NOP(dev_priv)) {
-		dev_priv->pch_type = PCH_NONE;
-	}
-
 	init_vbt_defaults(dev_priv);
 
 	/* If the OpRegion does not have VBT, look in PCI ROM. */
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_ddi.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_ddi.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_ddi.c
@@ -2104,7 +2104,7 @@ bool intel_ddi_connector_get_hw_state(st
 		goto out;
 	}
 
-	if (HAS_TRANSCODER_EDP(dev_priv) && port == PORT_A && !intel_vgpu_active(dev_priv))
+	if (HAS_TRANSCODER_EDP(dev_priv) && port == PORT_A)
 		cpu_transcoder = TRANSCODER_EDP;
 	else
 		cpu_transcoder = (enum transcoder) pipe;
@@ -4493,7 +4493,7 @@ static int intel_ddi_compute_config(stru
 	enum port port = encoder->port;
 	int ret;
 
-	if (HAS_TRANSCODER_EDP(dev_priv) && port == PORT_A && !intel_vgpu_active(dev_priv))
+	if (HAS_TRANSCODER_EDP(dev_priv) && port == PORT_A)
 		pipe_config->cpu_transcoder = TRANSCODER_EDP;
 
 	if (intel_crtc_has_type(pipe_config, INTEL_OUTPUT_HDMI)) {
@@ -4800,8 +4800,7 @@ void intel_ddi_init(struct drm_i915_priv
 	init_hdmi = port_info->supports_dvi || port_info->supports_hdmi;
 	init_dp = port_info->supports_dp;
 
-	if (!intel_vgpu_active(dev_priv) &&
-			intel_bios_is_lspcon_present(dev_priv, port)) {
+	if (intel_bios_is_lspcon_present(dev_priv, port)) {
 		/*
 		 * Lspcon device needs to be driven with DP connector
 		 * with special detection sequence. So make sure DP
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_display.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display.c
@@ -82,30 +82,6 @@
 #include "intel_tc.h"
 #include "intel_vga.h"
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-#include "gvt.h"
-#endif
-
-int get_pipe_from_crtc_index(struct drm_device *dev, unsigned int index, enum pipe *pipe)
-{
-	struct drm_crtc *c = drm_crtc_from_index(dev, index);
-
-	if (WARN_ON(!c))
-		return -ENOENT;
-
-	*pipe =  (to_intel_crtc(c)->pipe);
-	return 0;
-}
-
-struct intel_crtc *get_intel_crtc_from_index(struct drm_device *dev,
-		unsigned int index)
-{
-	struct drm_crtc *c = drm_crtc_from_index(dev, index);
-
-	WARN_ON(!c);
-	return to_intel_crtc(c);
-}
-
 /* Primary plane formats for gen <= 3 */
 static const u32 i8xx_primary_formats[] = {
 	DRM_FORMAT_C8,
@@ -2739,7 +2715,7 @@ u32 intel_plane_fb_max_stride(struct drm
 	 * We assume the primary plane for pipe A has
 	 * the highest stride limits of them all.
 	 */
-	crtc = get_intel_crtc_from_index(&(dev_priv->drm), 0);
+	crtc = intel_get_crtc_for_pipe(dev_priv, PIPE_A);
 	if (!crtc)
 		return 0;
 
@@ -4457,12 +4433,6 @@ static void skl_detach_scaler(struct int
 	struct drm_device *dev = intel_crtc->base.dev;
 	struct drm_i915_private *dev_priv = to_i915(dev);
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-	if (intel_gvt_active(dev_priv) &&
-		dev_priv->gvt->pipe_info[intel_crtc->pipe].scaler_owner[id] != 0)
-		return;
-#endif
-
 	I915_WRITE(SKL_PS_CTRL(intel_crtc->pipe, id), 0);
 	I915_WRITE(SKL_PS_WIN_POS(intel_crtc->pipe, id), 0);
 	I915_WRITE(SKL_PS_WIN_SZ(intel_crtc->pipe, id), 0);
@@ -10387,8 +10357,7 @@ static void skylake_get_pfit_config(stru
 	/* find scaler attached to this pipe */
 	for (i = 0; i < crtc->num_scalers; i++) {
 		ps_ctrl = I915_READ(SKL_PS_CTRL(crtc->pipe, i));
-		if (ps_ctrl & PS_SCALER_EN && !(ps_ctrl & PS_PLANE_SEL_MASK) &&
-			scaler_state->scalers[i].owned) {
+		if (ps_ctrl & PS_SCALER_EN && !(ps_ctrl & PS_PLANE_SEL_MASK)) {
 			id = i;
 			pipe_config->pch_pfit.enabled = true;
 			pipe_config->pch_pfit.pos = I915_READ(SKL_PS_WIN_POS(crtc->pipe, i));
@@ -13813,12 +13782,6 @@ static void verify_wm_state(struct intel
 	if (INTEL_GEN(dev_priv) < 9 || !new_crtc_state->hw.active)
 		return;
 
-	/* For the VGPU scenario based on Linux this is skipped as Dom0
-	 * ignores the WM setting from Guest.
-	 */
-	if (intel_vgpu_active(dev_priv))
-		return;
-
 	hw = kzalloc(sizeof(*hw), GFP_KERNEL);
 	if (!hw)
 		return;
@@ -14076,15 +14039,8 @@ verify_crtc_state(struct intel_crtc *crt
 
 	intel_pipe_config_sanity_check(dev_priv, pipe_config);
 
-	/*
-	 * Only check for pipe config if we are not in a GVT guest environment,
-	 * because such a check in a GVT guest environment doesn't make any sense
-	 * as we don't allow the guest to do a mode set, so there can very well
-	 * be a difference between what it has programmed vs. what the host
-	 * truly configured the HW pipe to be in.
-	 */
-	if (!intel_vgpu_active(dev_priv) &&
-		!intel_pipe_config_compare(new_crtc_state, pipe_config, false)) {
+	if (!intel_pipe_config_compare(new_crtc_state,
+				       pipe_config, false)) {
 		I915_STATE_WARN(1, "pipe state doesn't match!\n");
 		intel_dump_pipe_config(pipe_config, NULL, "[hw state]");
 		intel_dump_pipe_config(new_crtc_state, NULL, "[sw state]");
@@ -14143,11 +14099,11 @@ verify_single_dpll_state(struct drm_i915
 	if (new_crtc_state->hw.active)
 		I915_STATE_WARN(!(pll->active_mask & crtc_mask),
 				"pll active mismatch (expected pipe %c in active mask 0x%02x)\n",
-				pipe_name(crtc->pipe), pll->active_mask);
+				pipe_name(drm_crtc_index(&crtc->base)), pll->active_mask);
 	else
 		I915_STATE_WARN(pll->active_mask & crtc_mask,
 				"pll active mismatch (didn't expect pipe %c in active mask 0x%02x)\n",
-				pipe_name(crtc->pipe), pll->active_mask);
+				pipe_name(drm_crtc_index(&crtc->base)), pll->active_mask);
 
 	I915_STATE_WARN(!(pll->state.crtc_mask & crtc_mask),
 			"pll enabled crtcs mismatch (expected 0x%x in 0x%02x)\n",
@@ -14176,10 +14132,10 @@ verify_shared_dpll_state(struct intel_cr
 
 		I915_STATE_WARN(pll->active_mask & crtc_mask,
 				"pll active mismatch (didn't expect pipe %c in active mask)\n",
-				pipe_name(crtc->pipe));
+				pipe_name(drm_crtc_index(&crtc->base)));
 		I915_STATE_WARN(pll->state.crtc_mask & crtc_mask,
 				"pll enabled crtcs mismatch (found %x in enabled mask)\n",
-				pipe_name(crtc->pipe));
+				pipe_name(drm_crtc_index(&crtc->base)));
 	}
 }
 
@@ -16213,16 +16169,6 @@ static void intel_crtc_init_scalers(stru
 
 		scaler->in_use = 0;
 		scaler->mode = 0;
-		scaler->owned = 1;
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-		if (intel_gvt_active(dev_priv) &&
-			dev_priv->gvt->pipe_info[crtc->pipe].scaler_owner[i] != 0)
-			scaler->owned = 0;
-#endif
-		if (intel_vgpu_active(dev_priv) &&
-			!(1 << (crtc->pipe * SKL_NUM_SCALERS + i) &
-			dev_priv->vgpu.scaler_owned))
-			scaler->owned = 0;
 	}
 
 	scaler_state->scaler_id = -1;
@@ -16295,77 +16241,6 @@ static const struct drm_crtc_funcs i8xx_
 	.disable_vblank = i8xx_disable_vblank,
 };
 
-static int intel_crtc_init_restrict_planes(struct drm_i915_private *dev_priv,
-					   enum pipe pipe, int planes_mask)
-{
-	const struct drm_crtc_funcs *funcs;
-	struct intel_crtc *intel_crtc;
-	struct intel_crtc_state *crtc_state = NULL;
-	struct intel_plane *primary = NULL;
-	struct intel_plane *intel_plane = NULL;
-	int plane, ret;
-
-	intel_crtc = kzalloc(sizeof(*intel_crtc), GFP_KERNEL);
-	if (!intel_crtc)
-		return -ENOMEM;
-
-	crtc_state = kzalloc(sizeof(*crtc_state), GFP_KERNEL);
-	if (!crtc_state) {
-		ret = -ENOMEM;
-		goto fail;
-	}
-	__drm_atomic_helper_crtc_reset(&intel_crtc->base, &crtc_state->uapi);
-	intel_crtc->config = crtc_state;
-
-	for_each_universal_plane(dev_priv, pipe, plane) {
-		if (planes_mask & BIT(plane)) {
-			intel_plane = skl_universal_plane_create(dev_priv,
-					pipe, plane);
-			if (IS_ERR(intel_plane)) {
-				DRM_DEBUG_KMS(" plane %d failed for pipe %d\n", plane, pipe);
-				ret = PTR_ERR(intel_plane);
-				goto fail;
-			}
-			if (!primary) {
-				primary = intel_plane;
-			}
-			DRM_DEBUG_KMS(" plane %d created for pipe %d\n", plane, pipe);
-			intel_crtc->plane_ids_mask |= BIT(intel_plane->id);
-		}
-	}
-
-	funcs = &bdw_crtc_funcs;
-
-	ret = drm_crtc_init_with_planes(&dev_priv->drm, &intel_crtc->base,
-					&primary->base, NULL,
-					funcs, "pipe %c", pipe_name(pipe));
-	if (ret)
-		goto fail;
-
-	intel_crtc->pipe = pipe;
-
-	/* initialize shared scalers */
-	intel_crtc_init_scalers(intel_crtc, crtc_state);
-
-	BUG_ON(pipe >= ARRAY_SIZE(dev_priv->pipe_to_crtc_mapping) ||
-	       dev_priv->pipe_to_crtc_mapping[pipe] != NULL);
-	dev_priv->pipe_to_crtc_mapping[pipe] = intel_crtc;
-
-	intel_color_init(intel_crtc);
-
-	return 0;
-
-fail:
-	/*
-	 * drm_mode_config_cleanup() will free up any
-	 * crtcs/planes already initialized.
-	 */
-	kfree(crtc_state);
-	kfree(intel_crtc);
-
-	return ret;
-}
-
 static int intel_crtc_init(struct drm_i915_private *dev_priv, enum pipe pipe)
 {
 	const struct drm_crtc_funcs *funcs;
@@ -17480,43 +17355,12 @@ static void intel_mode_config_init(struc
 	}
 }
 
-static int intel_sanitize_plane_restriction(struct drm_i915_private *dev_priv)
-{
-	unsigned long mask;
-	/*plane restriction feature is only for APL and KBL for now*/
-	if (!(IS_BROXTON(dev_priv) || IS_KABYLAKE(dev_priv) ||
-	      IS_COFFEELAKE(dev_priv))) {
-		i915_modparams.avail_planes_per_pipe = 0;
-		DRM_INFO("Turning off Plane Restrictions feature\n");
-	}
-
-	mask = i915_modparams.avail_planes_per_pipe;
-
-	/* make sure SOS has a (dummy) plane per pipe. */
-	if ((IS_BROXTON(dev_priv) ||
-	     IS_KABYLAKE(dev_priv) ||
-	     IS_COFFEELAKE(dev_priv)) &&
-	    intel_gvt_active(dev_priv) && mask) {
-		enum pipe pipe;
-
-		for_each_pipe(dev_priv, pipe) {
-			if (!AVAIL_PLANE_PER_PIPE(dev_priv, mask, pipe))
-				mask |=  (1 << pipe * BITS_PER_PIPE);
-		}
-		DRM_INFO("Fix internal plane mask: 0x%lx --> 0x%lx",
-				i915_modparams.avail_planes_per_pipe, mask);
-	}
-	return mask;
-}
-
 int intel_modeset_init(struct drm_i915_private *i915)
 {
 	struct drm_device *dev = &i915->drm;
 	enum pipe pipe;
 	struct intel_crtc *crtc;
 	int ret;
-	unsigned int  planes_mask[I915_MAX_PIPES];
-	unsigned int avail_plane_per_pipe_mask = 0;
 
 	i915->modeset_wq = alloc_ordered_workqueue("i915_modeset", 0);
 	i915->flip_wq = alloc_workqueue("i915_flip", WQ_HIGHPRI |
@@ -17546,29 +17390,9 @@ int intel_modeset_init(struct drm_i915_p
 		      INTEL_NUM_PIPES(i915),
 		      INTEL_NUM_PIPES(i915) > 1 ? "s" : "");
 
-	avail_plane_per_pipe_mask = intel_sanitize_plane_restriction(i915);
-	DRM_DEBUG_KMS("avail_planes_per_pipe = 0x%lx \n", i915_modparams.avail_planes_per_pipe);
-	DRM_DEBUG_KMS("domain_plane_owners = 0x%lx \n", i915_modparams.domain_plane_owners);
-
 	if (HAS_DISPLAY(i915) && INTEL_DISPLAY_ENABLED(i915)) {
 		for_each_pipe(i915, pipe) {
-			planes_mask[pipe] = AVAIL_PLANE_PER_PIPE(i915,
-						 avail_plane_per_pipe_mask, pipe);
-			DRM_DEBUG_KMS("for pipe %d plane_mask = %d \n",
-						pipe,
-						planes_mask[pipe]);
-		}
-		for_each_pipe(i915, pipe) {
-			if (!i915_modparams.avail_planes_per_pipe) {
-				ret = intel_crtc_init(i915, pipe);
-			} else {
-				if (!intel_vgpu_active(i915) || (intel_vgpu_active(i915)
-							 && planes_mask[pipe])) {
-				ret = intel_crtc_init_restrict_planes(i915,
-								      pipe,
-								      planes_mask[pipe]);
-				}
-			}
+			ret = intel_crtc_init(i915, pipe);
 			if (ret) {
 				drm_mode_config_cleanup(dev);
 				return ret;
@@ -18036,8 +17860,7 @@ static void intel_modeset_readout_hw_sta
 
 			if (crtc_state->hw.active &&
 			    crtc_state->shared_dpll == pll)
-				pll->state.crtc_mask |=
-					1 << drm_crtc_index(&crtc->base);
+				pll->state.crtc_mask |= 1 << crtc->pipe;
 		}
 		pll->active_mask = pll->state.crtc_mask;
 
@@ -18052,14 +17875,10 @@ static void intel_modeset_readout_hw_sta
 			struct intel_crtc_state *crtc_state;
 
 			crtc = intel_get_crtc_for_pipe(dev_priv, pipe);
-			if (!crtc) {
-				encoder->base.crtc = NULL;
-			} else {
-				crtc_state = to_intel_crtc_state(crtc->base.state);
+			crtc_state = to_intel_crtc_state(crtc->base.state);
 
-				encoder->base.crtc = &crtc->base;
-				encoder->get_config(encoder, crtc_state);
-			}
+			encoder->base.crtc = &crtc->base;
+			encoder->get_config(encoder, crtc_state);
 		} else {
 			encoder->base.crtc = NULL;
 		}
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_display.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display.h
@@ -463,11 +463,6 @@ enum phy_fia {
 			     ((connector) = to_intel_connector((__state)->base.connectors[__i].ptr), \
 			     (new_connector_state) = to_intel_digital_connector_state((__state)->base.connectors[__i].new_state), 1))
 
-#define BITS_PER_PIPE 8
-#define AVAIL_PLANE_PER_PIPE(dev_priv, mask, pipe)  \
-	(((mask) >> (pipe) * BITS_PER_PIPE) & \
-	   ((1 << ((RUNTIME_INFO(dev_priv)->num_sprites[pipe]) + 1)) - 1))
-
 void intel_link_compute_m_n(u16 bpp, int nlanes,
 			    int pixel_clock, int link_clock,
 			    struct intel_link_m_n *m_n,
@@ -595,9 +590,6 @@ unsigned int i9xx_plane_max_stride(struc
 				   unsigned int rotation);
 int bdw_get_pipemisc_bpp(struct intel_crtc *crtc);
 
-/* intel_dp.c */
-void intel_dp_unpack_aux(uint32_t src, uint8_t *dst, int dst_bytes);
-
 struct intel_display_error_state *
 intel_display_capture_error_state(struct drm_i915_private *dev_priv);
 void intel_display_print_error_state(struct drm_i915_error_state_buf *e,
@@ -650,10 +642,4 @@ void assert_pipe(struct drm_i915_private
 #define I915_STATE_WARN_ON(x)						\
 	I915_STATE_WARN((x), "%s", "WARN_ON(" __stringify(x) ")")
 
-int get_pipe_from_crtc_index(struct drm_device *dev,
-			     unsigned int index,
-			     enum pipe *pipe);
-struct intel_crtc *get_intel_crtc_from_index(struct drm_device *dev,
-					     unsigned int index);
-
 #endif
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display_types.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_display_types.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_display_types.h
@@ -620,7 +620,6 @@ struct intel_initial_plane_config {
 struct intel_scaler {
 	int in_use;
 	u32 mode;
-	int owned;
 };
 
 struct intel_crtc_scaler_state {
@@ -1608,12 +1607,7 @@ intel_crtc_has_dp_encoder(const struct i
 static inline void
 intel_wait_for_vblank(struct drm_i915_private *dev_priv, enum pipe pipe)
 {
-	struct intel_crtc *crtc;
-
-	crtc = intel_get_crtc_for_pipe(dev_priv, pipe);
-	if (crtc)
-		drm_wait_one_vblank(&dev_priv->drm,
-				    drm_crtc_index(&crtc->base));
+	drm_wait_one_vblank(&dev_priv->drm, pipe);
 }
 static inline void
 intel_wait_for_vblank_if_active(struct drm_i915_private *dev_priv, enum pipe pipe)
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_dp.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_dp.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_dp.c
@@ -699,7 +699,7 @@ u32 intel_dp_pack_aux(const u8 *src, int
 	return v;
 }
 
-void intel_dp_unpack_aux(u32 src, u8 *dst, int dst_bytes)
+static void intel_dp_unpack_aux(u32 src, u8 *dst, int dst_bytes)
 {
 	int i;
 	if (dst_bytes > 4)
@@ -2559,12 +2559,7 @@ static void wait_panel_status(struct int
 			I915_READ(pp_stat_reg),
 			I915_READ(pp_ctrl_reg));
 
-	/*
-	 * Only wait for panel status if we are not in a GVT guest environment,
-	 * because such a wait in a GVT guest environment doesn't make any sense
-	 * as we are exposing virtual DP monitors to the guest.
-	 */
-	if (!intel_vgpu_active(dev_priv) && intel_de_wait_for_register(dev_priv, pp_stat_reg,
+	if (intel_de_wait_for_register(dev_priv, pp_stat_reg,
 				       mask, value, 5000))
 		DRM_ERROR("Panel status timeout: status %08x control %08x\n",
 				I915_READ(pp_stat_reg),
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_dpll_mgr.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_dpll_mgr.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_dpll_mgr.c
@@ -308,7 +308,7 @@ intel_reference_shared_dpll(struct intel
 	DRM_DEBUG_DRIVER("using %s for pipe %c\n", pll->info->name,
 			 pipe_name(crtc->pipe));
 
-	shared_dpll[id].crtc_mask |= 1 << (drm_crtc_index(&crtc->base));
+	shared_dpll[id].crtc_mask |= 1 << crtc->pipe;
 }
 
 static void intel_unreference_shared_dpll(struct intel_atomic_state *state,
@@ -318,8 +318,7 @@ static void intel_unreference_shared_dpl
 	struct intel_shared_dpll_state *shared_dpll;
 
 	shared_dpll = intel_atomic_get_shared_dpll_state(&state->base);
-	shared_dpll[pll->info->id].crtc_mask &=
-			~(1 << drm_crtc_index(&crtc->base));
+	shared_dpll[pll->info->id].crtc_mask &= ~(1 << crtc->pipe);
 }
 
 static void intel_put_dpll(struct intel_atomic_state *state,
Index: kernel-lts-staging/drivers/gpu/drm/i915/display/intel_sprite.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/display/intel_sprite.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/display/intel_sprite.c
@@ -48,10 +48,6 @@
 #include "intel_psr.h"
 #include "intel_sprite.h"
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-#include "gvt.h"
-#endif
-
 int intel_usecs_to_scanlines(const struct drm_display_mode *adjusted_mode,
 			     int usecs)
 {
@@ -609,12 +605,6 @@ skl_program_plane(struct intel_plane *pl
 		plane_color_ctl = plane_state->color_ctl |
 			glk_plane_color_ctl_crtc(crtc_state);
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-	if (dev_priv->gvt &&
-			dev_priv->gvt->pipe_info[pipe].plane_owner[plane_id])
-		return;
-#endif
-
 	/* Sizes are 0 based */
 	src_w--;
 	src_h--;
@@ -650,9 +640,7 @@ skl_program_plane(struct intel_plane *pl
 	if (fb->format->is_yuv && icl_is_hdr_plane(dev_priv, plane_id))
 		icl_program_input_csc(plane, crtc_state, plane_state);
 
-	/* In VGPU or gvt-g mode, skip plane DDB/WM */
-	if (!(intel_gvt_active(dev_priv) || intel_vgpu_active(dev_priv)))
-		skl_write_plane_wm(plane, crtc_state);
+	skl_write_plane_wm(plane, crtc_state);
 
 	I915_WRITE_FW(PLANE_KEYVAL(pipe, plane_id), key->min_value);
 	I915_WRITE_FW(PLANE_KEYMSK(pipe, plane_id), keymsk);
@@ -702,20 +690,12 @@ skl_disable_plane(struct intel_plane *pl
 	enum pipe pipe = plane->pipe;
 	unsigned long irqflags;
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-	if (dev_priv->gvt &&
-			dev_priv->gvt->pipe_info[pipe].plane_owner[plane_id])
-		return;
-#endif
-
 	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
 
 	if (icl_is_hdr_plane(dev_priv, plane_id))
 		I915_WRITE_FW(PLANE_CUS_CTL(pipe, plane_id), 0);
 
-	/* In VGPU ornative mode, skip plane DDB/WM */
-	if (!(intel_gvt_active(dev_priv) || intel_vgpu_active(dev_priv)))
-		skl_write_plane_wm(plane, crtc_state);
+	skl_write_plane_wm(plane, crtc_state);
 
 	I915_WRITE_FW(PLANE_CTL(pipe, plane_id), 0);
 	I915_WRITE_FW(PLANE_SURF(pipe, plane_id), 0);
@@ -2925,11 +2905,6 @@ static const u32 *skl_get_plane_formats(
 					enum pipe pipe, enum plane_id plane_id,
 					int *num_formats)
 {
-	if (intel_gvt_active(dev_priv) || intel_vgpu_active(dev_priv)) {
-		*num_formats = ARRAY_SIZE(skl_plane_formats);
-		return skl_plane_formats;
-	}
-
 	if (skl_plane_has_planar(dev_priv, pipe, plane_id)) {
 		*num_formats = ARRAY_SIZE(skl_planar_formats);
 		return skl_planar_formats;
@@ -3044,17 +3019,10 @@ skl_universal_plane_create(struct drm_i9
 		modifiers = gen12_get_plane_modifiers(plane_id);
 		plane_funcs = &gen12_plane_funcs;
 	} else {
-		if (intel_gvt_active(dev_priv) || intel_vgpu_active(dev_priv))
-			plane->has_ccs = false;
-
 		if (plane->has_ccs)
 			modifiers = skl_plane_format_modifiers_ccs;
 		else
 			modifiers = skl_plane_format_modifiers_noccs;
-
-		if (intel_gvt_active(dev_priv) || intel_vgpu_active(dev_priv))
-			modifiers = i9xx_plane_format_modifiers;
-
 		plane_funcs = &skl_plane_funcs;
 	}
 
@@ -3063,7 +3031,7 @@ skl_universal_plane_create(struct drm_i9
 	else
 		plane_type = DRM_PLANE_TYPE_OVERLAY;
 
-	possible_crtcs = 1 << (dev_priv->drm.mode_config.num_crtc);
+	possible_crtcs = BIT(pipe);
 
 	ret = drm_universal_plane_init(&dev_priv->drm, &plane->base,
 				       possible_crtcs, plane_funcs,
Index: kernel-lts-staging/drivers/gpu/drm/i915/gt/intel_lrc.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gt/intel_lrc.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gt/intel_lrc.c
@@ -1350,8 +1350,6 @@ static void execlists_submit_ports(struc
 {
 	struct intel_engine_execlists *execlists = &engine->execlists;
 	unsigned int n;
-	u32 descs[4];
-	int i = 0;
 
 	GEM_BUG_ON(!assert_pending_valid(execlists, "submit"));
 
@@ -1373,32 +1371,12 @@ static void execlists_submit_ports(struc
 	 */
 	for (n = execlists_num_ports(execlists); n--; ) {
 		struct i915_request *rq = execlists->pending[n];
-		u64 desc;
 
-		desc = rq ? execlists_update_context(rq) : 0;
-
-		if (intel_vgpu_active(engine->i915) &&
-				PVMMIO_LEVEL(engine->i915, PVMMIO_ELSP_SUBMIT)) {
-			BUG_ON(i >= 4);
-			descs[i] = upper_32_bits(desc);
-			descs[i + 1] = lower_32_bits(desc);
-			i += 2;
-			continue;
-		}
-
-		write_desc(execlists, desc, n);
+		write_desc(execlists,
+			   rq ? execlists_update_context(rq) : 0,
+			   n);
 	}
 
-	if (intel_vgpu_active(engine->i915) &&
-			PVMMIO_LEVEL(engine->i915, PVMMIO_ELSP_SUBMIT)) {
-		u32 __iomem *elsp_data = engine->i915->shared_page->elsp_data;
-		spin_lock(&engine->i915->shared_page_lock);
-		writel(descs[0], elsp_data);
-		writel(descs[1], elsp_data + 1);
-		writel(descs[2], elsp_data + 2);
-		writel(descs[3], execlists->submit_reg);
-		spin_unlock(&engine->i915->shared_page_lock);
-	}
 	/* we need to manually load the submit queue */
 	if (execlists->ctrl_reg)
 		writel(EL_CTRL_LOAD, execlists->ctrl_reg);
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/Makefile
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/Makefile
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/Makefile
@@ -7,4 +7,3 @@ GVT_SOURCE := gvt.o aperture_gm.o handle
 
 ccflags-y				+= -I $(srctree)/$(src) -I $(srctree)/$(src)/$(GVT_DIR)/
 i915-y					+= $(addprefix $(GVT_DIR)/, $(GVT_SOURCE))
-obj-$(CONFIG_DRM_I915_GVT_ACRN_GVT)    	+= $(GVT_DIR)/acrngt.o
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/acrngt.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/acrngt.c
+++ /dev/null
@@ -1,1079 +0,0 @@
-/*
- * Interfaces coupled to ACRN
- *
- * Copyright(c) 2018 Intel Corporation. All rights reserved.
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of Version 2 of the GNU General Public License as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful, but
- * WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program.
- *
- */
-
-/*
- * NOTE:
- * This file contains hypervisor specific interactions to
- * implement the concept of mediated pass-through framework.
- * What this file provides is actually a general abstraction
- * of in-kernel device model, which is not gvt specific.
- *
- */
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/types.h>
-#include <linux/kthread.h>
-#include <linux/time.h>
-#include <linux/freezer.h>
-#include <linux/wait.h>
-#include <linux/sched.h>
-
-#include <linux/vhm/acrn_hv_defs.h>
-#include <linux/vhm/acrn_common.h>
-#include <linux/vhm/acrn_vhm_ioreq.h>
-#include <linux/vhm/acrn_vhm_mm.h>
-#include <linux/vhm/vhm_vm_mngt.h>
-
-#include <i915_drv.h>
-#include <i915_pvinfo.h>
-#include <gvt/gvt.h>
-#include "acrngt.h"
-
-MODULE_AUTHOR("Intel Corporation");
-MODULE_DESCRIPTION("ACRNGT mediated passthrough driver");
-MODULE_LICENSE("GPL");
-MODULE_VERSION("0.1");
-
-#define ASSERT(x)                                                           \
-do {	if (x) break;                                                       \
-	printk(KERN_EMERG "### ASSERTION FAILED %s: %s: %d: %s\n",          \
-		__FILE__, __func__, __LINE__, #x); dump_stack(); BUG();     \
-} while (0)
-
-
-struct kobject *acrn_gvt_ctrl_kobj;
-static struct kset *acrn_gvt_kset;
-static DEFINE_MUTEX(acrn_gvt_sysfs_lock);
-
-struct gvt_acrngt acrngt_priv;
-const struct intel_gvt_ops *intel_gvt_ops;
-
-static void disable_domu_plane(int pipe, int plane)
-{
-	struct drm_i915_private *dev_priv = acrngt_priv.gvt->dev_priv;
-
-	I915_WRITE(PLANE_CTL(pipe, plane), 0);
-
-	I915_WRITE(PLANE_SURF(pipe, plane), 0);
-	POSTING_READ(PLANE_SURF(pipe, plane));
-}
-
-void acrngt_instance_destroy(struct intel_vgpu *vgpu)
-{
-	int pipe, plane;
-	struct acrngt_hvm_dev *info = NULL;
-	struct intel_gvt *gvt = acrngt_priv.gvt;
-
-	if (vgpu) {
-		info = (struct acrngt_hvm_dev *)vgpu->handle;
-
-		if (info && info->emulation_thread != NULL)
-			kthread_stop(info->emulation_thread);
-
-		for_each_pipe(gvt->dev_priv, pipe) {
-			for_each_universal_plane(gvt->dev_priv, pipe, plane) {
-				if (gvt->pipe_info[pipe].plane_owner[plane] ==
-						vgpu->id) {
-					disable_domu_plane(pipe, plane);
-				}
-			}
-		}
-
-		intel_gvt_ops->vgpu_release(vgpu);
-		intel_gvt_ops->vgpu_destroy(vgpu);
-	}
-
-	if (info) {
-		gvt_dbg_core("destroy vgpu instance, vm id: %d, client %d",
-				info->vm_id, info->client);
-
-		if (info->client != 0)
-			acrn_ioreq_destroy_client(info->client);
-
-		if (info->vm)
-			put_vm(info->vm);
-
-		kfree(info);
-	}
-}
-
-static bool acrngt_write_cfg_space(struct intel_vgpu *vgpu,
-	unsigned int port, unsigned int bytes, unsigned long val)
-{
-	if (intel_gvt_ops->emulate_cfg_write(vgpu, port, &val, bytes)) {
-		gvt_err("failed to write config space port 0x%x\n", port);
-		return false;
-	}
-	return true;
-}
-
-static bool acrngt_read_cfg_space(struct intel_vgpu *vgpu,
-	unsigned int port, unsigned int bytes, unsigned long *val)
-{
-	unsigned long data;
-
-	if (intel_gvt_ops->emulate_cfg_read(vgpu, port, &data, bytes)) {
-		gvt_err("failed to read config space port 0x%x\n", port);
-		return false;
-	}
-	memcpy(val, &data, bytes);
-	return true;
-}
-
-static int acrngt_hvm_pio_emulation(struct intel_vgpu *vgpu,
-						struct vhm_request *req)
-{
-	if (req->reqs.pci_request.direction == REQUEST_READ) {
-		/* PIO READ */
-		gvt_dbg_core("handle pio read emulation at port 0x%x\n",
-			req->reqs.pci_request.reg);
-		if (!acrngt_read_cfg_space(vgpu,
-			req->reqs.pci_request.reg,
-			req->reqs.pci_request.size,
-			(unsigned long *)&req->reqs.pci_request.value)) {
-			gvt_err("failed to read pio at addr 0x%x\n",
-				req->reqs.pci_request.reg);
-			return -EINVAL;
-		}
-	} else if (req->reqs.pci_request.direction == REQUEST_WRITE) {
-		/* PIO WRITE */
-		gvt_dbg_core("handle pio write emulation at address 0x%x, "
-			"value 0x%x\n",
-			req->reqs.pci_request.reg, req->reqs.pci_request.value);
-		if (!acrngt_write_cfg_space(vgpu,
-			req->reqs.pci_request.reg,
-			req->reqs.pci_request.size,
-			(unsigned long)req->reqs.pci_request.value)) {
-			gvt_err("failed to write pio at addr 0x%x\n",
-				req->reqs.pci_request.reg);
-			return -EINVAL;
-		}
-	}
-	return 0;
-}
-
-static int acrngt_hvm_write_handler(struct intel_vgpu *vgpu, uint64_t pa,
-				   void *p_data, unsigned int bytes)
-{
-
-	/* Check whether pa is ppgtt */
-	if (intel_gvt_ops->write_protect_handler(vgpu, pa, p_data, bytes) == 0)
-		return 0;
-
-	/* pa is mmio reg or gtt */
-	return intel_gvt_ops->emulate_mmio_write(vgpu, pa, p_data, bytes);
-}
-
-static int acrngt_hvm_mmio_emulation(struct intel_vgpu *vgpu,
-		struct vhm_request *req)
-{
-	if (req->reqs.mmio_request.direction == REQUEST_READ) {
-		/* MMIO READ */
-		gvt_dbg_core("handle mmio read emulation at address 0x%llx\n",
-			req->reqs.mmio_request.address);
-		if (intel_gvt_ops->emulate_mmio_read(vgpu,
-				req->reqs.mmio_request.address,
-				&req->reqs.mmio_request.value,
-				req->reqs.mmio_request.size)) {
-			gvt_err("failed to read mmio at addr 0x%llx\n",
-				req->reqs.mmio_request.address);
-			return -EINVAL;
-		}
-	} else if (req->reqs.mmio_request.direction == REQUEST_WRITE) {
-		/* MMIO Write */
-		if (acrngt_hvm_write_handler(vgpu,
-				req->reqs.mmio_request.address,
-				&req->reqs.mmio_request.value,
-				req->reqs.mmio_request.size)) {
-			gvt_err("failed to write mmio at addr 0x%llx\n",
-				req->reqs.mmio_request.address);
-			return -EINVAL;
-		}
-		gvt_dbg_core("handle mmio write emulation at address 0x%llx, "
-			"value 0x%llx\n",
-			req->reqs.mmio_request.address, req->reqs.mmio_request.value);
-	}
-
-	return 0;
-}
-
-static void handle_request_error(struct intel_vgpu *vgpu)
-{
-	mutex_lock(&vgpu->gvt->lock);
-	if (vgpu->failsafe == false) {
-		vgpu->failsafe = true;
-		gvt_err("Now vgpu %d will enter failsafe mode.\n", vgpu->id);
-	}
-	mutex_unlock(&vgpu->gvt->lock);
-}
-
-static int acrngt_emulation_thread(void *priv)
-{
-	struct intel_vgpu *vgpu = (struct intel_vgpu *)priv;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)vgpu->handle;
-	struct vhm_request *req;
-
-	int vcpu, ret;
-	int nr_vcpus = info->nr_vcpu;
-
-	gvt_dbg_core("start kthread for VM%d\n", info->vm_id);
-	ASSERT(info->nr_vcpu <= MAX_HVM_VCPUS_SUPPORTED);
-
-	set_freezable();
-	while (1) {
-		ret = acrn_ioreq_attach_client(info->client, 1);
-
-		if (ret) {
-			gvt_err("error while attach ioreq client %d\n", ret);
-			info->client = 0;
-			info->emulation_thread = NULL;
-			return 0;
-		}
-
-		if (kthread_should_stop())
-			return 0;
-
-		for (vcpu = 0; vcpu < nr_vcpus; vcpu++) {
-			req = &info->req_buf[vcpu];
-			if (atomic_read(&req->processed) ==
-				REQ_STATE_PROCESSING &&
-				req->client == info->client) {
-				gvt_dbg_core("handle ioreq type %d\n",
-						req->type);
-				switch (req->type) {
-				case REQ_PCICFG:
-					ret = acrngt_hvm_pio_emulation(vgpu, req);
-					break;
-				case REQ_MMIO:
-				case REQ_WP:
-					ret = acrngt_hvm_mmio_emulation(vgpu, req);
-					break;
-				default:
-					gvt_err("Unknown ioreq type %x\n",
-						req->type);
-					ret = -EINVAL;
-					break;
-				}
-				/* error handling */
-				if (ret)
-					handle_request_error(vgpu);
-
-				smp_mb();
-				atomic_set(&req->processed, REQ_STATE_COMPLETE);
-				/* complete request */
-				if (acrn_ioreq_complete_request(info->client,
-						vcpu, req))
-					gvt_err("failed complete request\n");
-			}
-		}
-	}
-
-	BUG(); /* It's actually impossible to reach here */
-	return 0;
-}
-
-struct intel_vgpu *acrngt_instance_create(domid_t vm_id,
-					struct intel_vgpu_type *vgpu_type)
-{
-	struct acrngt_hvm_dev *info;
-	struct intel_vgpu *vgpu;
-	int ret = 0;
-	struct task_struct *thread;
-	struct vm_info vm_info;
-
-	gvt_dbg_core("acrngt_instance_create enter\n");
-	if (!intel_gvt_ops || !acrngt_priv.gvt)
-		return NULL;
-
-	vgpu = intel_gvt_ops->vgpu_create(acrngt_priv.gvt, vgpu_type);
-	if (IS_ERR(vgpu)) {
-		gvt_err("failed to create vgpu\n");
-		return NULL;
-	}
-
-	info = kzalloc(sizeof(struct acrngt_hvm_dev), GFP_KERNEL);
-	if (info == NULL) {
-		gvt_err("failed to alloc acrngt_hvm_dev\n");
-		goto err;
-	}
-
-	info->vm_id = vm_id;
-	info->vgpu = vgpu;
-	vgpu->handle = (unsigned long)info;
-	info->vm = find_get_vm(vm_id);
-	if (info->vm == NULL) {
-		gvt_err("failed to get vm %d\n", vm_id);
-		acrngt_instance_destroy(vgpu);
-		return NULL;
-	}
-	if (info->vm->req_buf == NULL) {
-		gvt_err("failed to get req buf for vm %d\n", vm_id);
-		goto err;
-	}
-	gvt_dbg_core("get vm req_buf from vm_id %d\n", vm_id);
-
-	/* create client: no handler -> handle request by itself */
-	info->client = acrn_ioreq_create_client(vm_id, NULL, "ioreq gvt-g");
-	if (info->client < 0) {
-		gvt_err("failed to create ioreq client for vm id %d\n", vm_id);
-		goto err;
-	}
-
-	/* get vm info */
-	ret = vhm_get_vm_info(vm_id, &vm_info);
-	if (ret < 0) {
-		gvt_err("failed to get vm info for vm id %d\n", vm_id);
-		goto err;
-	}
-
-	info->nr_vcpu = vm_info.max_vcpu;
-
-	/* get req buf */
-	info->req_buf = acrn_ioreq_get_reqbuf(info->client);
-	if (info->req_buf == NULL) {
-		gvt_err("failed to get req_buf for client %d\n", info->client);
-		goto err;
-	}
-
-	/* trap config space access */
-	acrn_ioreq_intercept_bdf(info->client, 0, 2, 0);
-
-	thread = kthread_run(acrngt_emulation_thread, vgpu,
-			"acrngt_emulation:%d", vm_id);
-	if (IS_ERR(thread)) {
-		gvt_err("failed to run emulation thread for vm %d\n", vm_id);
-		goto err;
-	}
-	info->emulation_thread = thread;
-	gvt_dbg_core("create vgpu instance success, vm_id %d, client %d,"
-		" nr_vcpu %d\n", info->vm_id, info->client, info->nr_vcpu);
-
-	intel_gvt_ops->vgpu_activate(vgpu);
-
-	return vgpu;
-
-err:
-	acrngt_instance_destroy(vgpu);
-	return NULL;
-}
-
-static ssize_t kobj_attr_show(struct kobject *kobj,
-				struct attribute *attr,	char *buf)
-{
-	struct kobj_attribute *kattr;
-	ssize_t ret = -EIO;
-
-	kattr = container_of(attr, struct kobj_attribute, attr);
-	if (kattr->show)
-		ret = kattr->show(kobj, kattr, buf);
-	return ret;
-}
-
-static ssize_t kobj_attr_store(struct kobject *kobj,
-	struct attribute *attr,	const char *buf, size_t count)
-{
-	struct kobj_attribute *kattr;
-	ssize_t ret = -EIO;
-
-	kattr = container_of(attr, struct kobj_attribute, attr);
-	if (kattr->store)
-		ret = kattr->store(kobj, kattr, buf, count);
-	return ret;
-}
-
-const struct sysfs_ops acrngt_kobj_sysfs_ops = {
-	.show   = kobj_attr_show,
-	.store  = kobj_attr_store,
-};
-
-static ssize_t acrngt_sysfs_vgpu_id(struct kobject *kobj,
-		struct kobj_attribute *attr, char *buf)
-{
-	int i;
-
-	for (i = 0; i < GVT_MAX_VGPU_INSTANCE; i++) {
-		if (acrngt_priv.vgpus[i] &&
-			(kobj == &((struct acrngt_hvm_dev *)
-				(acrngt_priv.vgpus[i]->handle))->kobj)) {
-			return sprintf(buf, "%d\n", acrngt_priv.vgpus[i]->id);
-		}
-	}
-	return 0;
-}
-
-static ssize_t acrngt_sysfs_vgpu_bar_info(struct kobject *kobj,
-		struct kobj_attribute *attr, char *buf)
-{
-	u64 bar0_start, bar0_len, bar2_start, bar2_len;
-	u64 mask;
-	struct acrngt_hvm_dev *info;
-	struct intel_vgpu *vgpu;
-
-	info = container_of(kobj, struct acrngt_hvm_dev, kobj);
-	if (!info || !info->vgpu)
-		return 0;
-	vgpu = info->vgpu;
-	mask = ~(0xf);
-	/* clear last four bits according to PCI spec */
-	bar0_start = *(u64 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_0) & mask;
-	bar0_len = vgpu->cfg_space.bar[0].size;
-	bar2_start = *(u64 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_2) & mask;
-	bar2_len = vgpu->cfg_space.bar[1].size;
-	return sprintf(buf, "0x%016llx 0x%016llx\n0x%016llx 0x%016llx\n",
-		bar0_start, bar0_len, bar2_start, bar2_len);
-
-	return 0;
-}
-
-static struct kobj_attribute acrngt_vm_attr =
-__ATTR(vgpu_id, 0440, acrngt_sysfs_vgpu_id, NULL);
-
-static struct kobj_attribute acrngt_vgpu_bar_info =
-__ATTR(vgpu_bar_info, 0440, acrngt_sysfs_vgpu_bar_info, NULL);
-
-
-static struct attribute *acrngt_vm_attrs[] = {
-	&acrngt_vm_attr.attr,
-	&acrngt_vgpu_bar_info.attr,
-	NULL,   /* need to NULL terminate the list of attributes */
-};
-
-static struct kobj_type acrngt_instance_ktype = {
-	.sysfs_ops  = &acrngt_kobj_sysfs_ops,
-	.default_attrs = acrngt_vm_attrs,
-};
-
-static int acrngt_sysfs_add_instance(struct acrngt_hvm_params *vp)
-{
-	int ret = 0;
-	struct intel_vgpu *vgpu;
-	struct acrngt_hvm_dev *info;
-
-	struct intel_vgpu_type type = acrngt_priv.gvt->types[0];
-	type.low_gm_size = vp->aperture_sz * VMEM_1MB;
-	type.high_gm_size = (vp->gm_sz - vp->aperture_sz) * VMEM_1MB;
-	type.fence = vp->fence_sz;
-	mutex_lock(&acrn_gvt_sysfs_lock);
-	vgpu = acrngt_instance_create(vp->vm_id, &type);
-	mutex_unlock(&acrn_gvt_sysfs_lock);
-	if (vgpu == NULL) {
-		gvt_err("acrngt_sysfs_add_instance failed.\n");
-		ret = -EINVAL;
-	} else {
-		info = (struct acrngt_hvm_dev *) vgpu->handle;
-		info->vm_id = vp->vm_id;
-		acrngt_priv.vgpus[vgpu->id - 1] = vgpu;
-		gvt_dbg_core("add acrngt instance for vm-%d with vgpu-%d.\n",
-			vp->vm_id, vgpu->id);
-
-		kobject_init(&info->kobj, &acrngt_instance_ktype);
-		info->kobj.kset = acrn_gvt_kset;
-		/* add kobject, NULL parent indicates using kset as parent */
-		ret = kobject_add(&info->kobj, NULL, "vm%u", info->vm_id);
-		if (ret) {
-			gvt_err("%s: kobject add error: %d\n", __func__, ret);
-			kobject_put(&info->kobj);
-		}
-	}
-
-	return ret;
-}
-
-static struct intel_vgpu *vgpu_from_id(int vm_id)
-{
-	int i;
-	struct acrngt_hvm_dev *hvm_dev = NULL;
-
-	/* vm_id is negtive in del_instance call */
-	if (vm_id < 0)
-		vm_id = -vm_id;
-	for (i = 0; i < GVT_MAX_VGPU_INSTANCE; i++)
-		if (acrngt_priv.vgpus[i]) {
-			hvm_dev = (struct acrngt_hvm_dev *)
-					acrngt_priv.vgpus[i]->handle;
-			if (hvm_dev && (vm_id == hvm_dev->vm_id))
-				return acrngt_priv.vgpus[i];
-		}
-	return NULL;
-}
-
-static int acrngt_sysfs_del_instance(struct acrngt_hvm_params *vp)
-{
-	int ret = 0;
-	struct intel_vgpu *vgpu = vgpu_from_id(vp->vm_id);
-	struct acrngt_hvm_dev *info = NULL;
-
-	if (vgpu) {
-		info = (struct acrngt_hvm_dev *) vgpu->handle;
-		gvt_dbg_core("remove vm-%d sysfs node.\n", vp->vm_id);
-		kobject_put(&info->kobj);
-
-		mutex_lock(&acrn_gvt_sysfs_lock);
-		acrngt_priv.vgpus[vgpu->id - 1] = NULL;
-		acrngt_instance_destroy(vgpu);
-		mutex_unlock(&acrn_gvt_sysfs_lock);
-	}
-
-	return ret;
-}
-
-static ssize_t acrngt_sysfs_instance_manage(struct kobject *kobj,
-		struct kobj_attribute *attr, const char *buf, size_t count)
-{
-	struct acrngt_hvm_params vp;
-	int param_cnt;
-	char param_str[64];
-	int rc;
-	int high_gm_sz;
-	int low_gm_sz;
-
-	/* We expect the param_str should be vmid,a,b,c (where the guest
-	 * wants a MB aperture and b MB gm, and c fence registers) or -vmid
-	 * (where we want to release the gvt instance).
-	 */
-	(void)sscanf(buf, "%63s", param_str);
-	param_cnt = sscanf(param_str, "%d,%d,%d,%d", &vp.vm_id,
-			&low_gm_sz, &high_gm_sz, &vp.fence_sz);
-	gvt_dbg_core("create vm-%d sysfs node, low gm size %d,"
-		" high gm size %d, fence size %d\n",
-		vp.vm_id, low_gm_sz, high_gm_sz, vp.fence_sz);
-	vp.aperture_sz = low_gm_sz;
-	vp.gm_sz = high_gm_sz + low_gm_sz;
-	if (param_cnt == 1) {
-		if (vp.vm_id >= 0)
-			return -EINVAL;
-	} else if (param_cnt == 4) {
-		if (!(vp.vm_id > 0 && vp.aperture_sz > 0 &&
-			vp.aperture_sz <= vp.gm_sz && vp.fence_sz > 0))
-			return -EINVAL;
-	} else {
-		gvt_err("%s: parameter counter incorrect\n", __func__);
-		return -EINVAL;
-	}
-
-	rc = (vp.vm_id > 0) ? acrngt_sysfs_add_instance(&vp) :
-		acrngt_sysfs_del_instance(&vp);
-
-	return rc < 0 ? rc : count;
-}
-
-static ssize_t show_plane_owner(struct kobject *kobj,
-		struct kobj_attribute *attr, char *buf)
-{
-	return sprintf(buf, "Planes:\nPipe A: %d %d %d %d\n"
-				"Pipe B: %d %d %d %d\nPipe C: %d %d %d\n",
-		acrngt_priv.gvt->pipe_info[PIPE_A].plane_owner[PLANE_PRIMARY],
-		acrngt_priv.gvt->pipe_info[PIPE_A].plane_owner[PLANE_SPRITE0],
-		acrngt_priv.gvt->pipe_info[PIPE_A].plane_owner[PLANE_SPRITE1],
-		acrngt_priv.gvt->pipe_info[PIPE_A].plane_owner[PLANE_SPRITE2],
-		acrngt_priv.gvt->pipe_info[PIPE_B].plane_owner[PLANE_PRIMARY],
-		acrngt_priv.gvt->pipe_info[PIPE_B].plane_owner[PLANE_SPRITE0],
-		acrngt_priv.gvt->pipe_info[PIPE_B].plane_owner[PLANE_SPRITE1],
-		acrngt_priv.gvt->pipe_info[PIPE_B].plane_owner[PLANE_SPRITE2],
-		acrngt_priv.gvt->pipe_info[PIPE_C].plane_owner[PLANE_PRIMARY],
-		acrngt_priv.gvt->pipe_info[PIPE_C].plane_owner[PLANE_SPRITE0],
-		acrngt_priv.gvt->pipe_info[PIPE_C].plane_owner[PLANE_SPRITE1]);
-}
-
-static struct kobj_attribute acrngt_instance_attr =
-__ATTR(create_gvt_instance, 0220, NULL, acrngt_sysfs_instance_manage);
-
-static struct kobj_attribute plane_owner_attr =
-__ATTR(plane_owner_show, 0440, show_plane_owner, NULL);
-
-static struct attribute *acrngt_ctrl_attrs[] = {
-	&acrngt_instance_attr.attr,
-	&plane_owner_attr.attr,
-	NULL,   /* need to NULL terminate the list of attributes */
-};
-
-static struct kobj_type acrngt_ctrl_ktype = {
-	.sysfs_ops  = &acrngt_kobj_sysfs_ops,
-	.default_attrs = acrngt_ctrl_attrs,
-};
-
-int acrngt_sysfs_init(struct intel_gvt *gvt)
-{
-	int ret;
-
-	acrn_gvt_kset = kset_create_and_add("gvt", NULL, kernel_kobj);
-	if (!acrn_gvt_kset) {
-		ret = -ENOMEM;
-		goto kset_fail;
-	}
-
-	acrn_gvt_ctrl_kobj = kzalloc(sizeof(struct kobject), GFP_KERNEL);
-	if (!acrn_gvt_ctrl_kobj) {
-		ret = -ENOMEM;
-		goto ctrl_fail;
-	}
-
-	acrn_gvt_ctrl_kobj->kset = acrn_gvt_kset;
-	ret = kobject_init_and_add(acrn_gvt_ctrl_kobj, &acrngt_ctrl_ktype,
-			NULL, "control");
-	if (ret) {
-		ret = -EINVAL;
-		goto kobj_fail;
-	}
-
-	return 0;
-
-kobj_fail:
-	kobject_put(acrn_gvt_ctrl_kobj);
-ctrl_fail:
-	kset_unregister(acrn_gvt_kset);
-kset_fail:
-	return ret;
-}
-
-void acrngt_sysfs_del(void)
-{
-	kobject_put(acrn_gvt_ctrl_kobj);
-	kset_unregister(acrn_gvt_kset);
-}
-
-static int acrngt_host_init(struct device *dev, void *gvt, const void *ops)
-{
-	int ret = -EFAULT;
-
-	if (!gvt || !ops)
-		return -EINVAL;
-
-	acrngt_priv.gvt = (struct intel_gvt *)gvt;
-	intel_gvt_ops = (const struct intel_gvt_ops *)ops;
-
-	ret = acrngt_sysfs_init(acrngt_priv.gvt);
-	if (ret) {
-		gvt_err("failed call acrngt_sysfs_init, error: %d\n", ret);
-		acrngt_priv.gvt = NULL;
-		intel_gvt_ops = NULL;
-	}
-
-	return ret;
-}
-
-static void acrngt_host_exit(struct device *dev)
-{
-	acrngt_sysfs_del();
-	acrngt_priv.gvt = NULL;
-	intel_gvt_ops = NULL;
-}
-
-static int acrngt_attach_vgpu(void *vgpu, unsigned long *handle)
-{
-	return 0;
-}
-
-static void acrngt_detach_vgpu(void *handle)
-{
-	return;
-}
-
-static int acrngt_inject_msi(unsigned long handle, u32 addr_lo, u16 data)
-{
-	int ret;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("inject msi irq, addr 0x%x, data 0x%hx\n", addr_lo, data);
-
-	ret = vhm_inject_msi(info->vm_id, addr_lo, data);
-	if (ret)
-		gvt_err("failed to inject msi for vm %d\n", info->vm_id);
-	return ret;
-}
-
-static unsigned long acrngt_virt_to_mfn(void *addr)
-{
-	uint64_t    gpa;
-	uint64_t    hpa;
-	gvt_dbg_core("virt 0x%lx to mfn\n", (unsigned long)addr);
-
-	gpa = virt_to_phys(addr);
-	/* FIXME: We assume the vmid of SOS is aways 0 here. But in hybird mode,
-	   the vmid of SOS could be 1. Need to identify this case. */
-	hpa = vhm_vm_gpa2hpa(0, gpa);
-
-	return (unsigned long) (hpa >> PAGE_SHIFT);
-}
-
-static int acrngt_page_track_add(unsigned long handle, u64 gfn)
-{
-	int ret;
-	unsigned long hpa;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("set wp page for gfn 0x%llx\n", gfn);
-
-	hpa = vhm_vm_gpa2hpa(info->vm_id, gfn << PAGE_SHIFT);
-	ret = acrn_ioreq_add_iorange(info->client, REQ_WP, gfn << PAGE_SHIFT,
-				((gfn + 1) << PAGE_SHIFT) - 1);
-	if (ret) {
-		gvt_err("failed acrn_ioreq_add_iorange for gfn 0x%llx\n", gfn);
-		return ret;
-	}
-	ret = write_protect_page(info->vm_id, gfn << PAGE_SHIFT, true);
-	if (ret)
-		gvt_err("failed set write protect for gfn 0x%llx\n", gfn);
-	return ret;
-}
-
-static int acrngt_page_track_remove(unsigned long handle, u64 gfn)
-{
-	int ret;
-	unsigned long hpa;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("unset wp page for gfx 0x%llx\n", gfn);
-
-	hpa = vhm_vm_gpa2hpa(info->vm_id, gfn << PAGE_SHIFT);
-	ret = write_protect_page(info->vm_id, gfn << PAGE_SHIFT, false);
-	if (ret) {
-		gvt_err("failed update_memmap_attr unset for gfn 0x%llx\n", gfn);
-		return ret;
-	}
-	ret = acrn_ioreq_del_iorange(info->client, REQ_WP, gfn << PAGE_SHIFT,
-				((gfn + 1) << PAGE_SHIFT) - 1);
-	if (ret)
-		gvt_err("failed acrn_ioreq_del_iorange for gfn 0x%llx\n", gfn);
-	return ret;
-}
-
-static int acrngt_read_gpa(unsigned long handle, unsigned long gpa,
-				void *buf, unsigned long len)
-{
-	void *va = NULL;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("read gpa 0x%lx with len 0x%lx\n", gpa, len);
-
-	va = map_guest_phys(info->vm_id, gpa, len);
-	if (!va) {
-		gvt_err("GVT: can not read gpa = 0x%lx!!!\n", gpa);
-		return -EFAULT;
-	}
-
-	switch (len) {
-	case 1:
-		*((uint8_t *)  buf) = *((uint8_t *)  va);
-		break;
-	case 2:
-		*((uint16_t *) buf) = *((uint16_t *) va);
-		break;
-	case 4:
-		*((uint32_t *) buf) = *((uint32_t *) va);
-		break;
-	case 8:
-		*((uint64_t *) buf) = *((uint64_t *) va);
-		break;
-	default:
-		memcpy(buf, va, len);
-	}
-	return 0;
-}
-
-static int acrngt_write_gpa(unsigned long handle, unsigned long gpa,
-				void *buf, unsigned long len)
-{
-	void *va = NULL;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("write gpa 0x%lx with len 0x%lx\n", gpa, len);
-
-	va = map_guest_phys(info->vm_id, gpa, len);
-	if (!va) {
-		gvt_err("GVT: can not write gpa = 0x%lx!!!\n", gpa);
-		return -EFAULT;
-	}
-
-	switch (len) {
-	case 1:
-		*((uint8_t *) va)  = *((uint8_t *)  buf);
-		break;
-	case 2:
-		*((uint16_t *) va) = *((uint16_t *) buf);
-		break;
-	case 4:
-		*((uint32_t *) va) = *((uint32_t *) buf);
-		break;
-	case 8:
-		*((uint64_t *) va) = *((uint64_t *) buf);
-		break;
-	default:
-		memcpy(va, buf, len);
-	}
-	return 0;
-}
-
-static bool is_identical_mmap(void)
-{
-	/* todo: need add hypercall to get such info from hypervisor */
-	return true;
-}
-
-static unsigned long acrngt_gfn_to_pfn(unsigned long handle, unsigned long gfn)
-{
-	unsigned long hpa;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-
-	gvt_dbg_core("convert gfn 0x%lx to pfn\n", gfn);
-	if (is_identical_mmap()) {
-		void *va = NULL;
-
-		va = map_guest_phys(info->vm_id, gfn << PAGE_SHIFT,
-				    1 << PAGE_SHIFT);
-		if (!va) {
-			gvt_err("GVT: can not map gfn = 0x%lx!!!\n", gfn);
-			hpa = vhm_vm_gpa2hpa(info->vm_id, gfn << PAGE_SHIFT);
-		} else {
-			hpa = virt_to_phys(va);
-		}
-	} else {
-		hpa = vhm_vm_gpa2hpa(info->vm_id, gfn << PAGE_SHIFT);
-	}
-
-	return hpa >> PAGE_SHIFT;
-}
-
-static int acrngt_map_gfn_to_mfn(unsigned long handle, unsigned long gfn,
-				unsigned long mfn, unsigned int nr, bool map)
-{
-	int ret;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("map/unmap gfn 0x%lx to mfn 0x%lx with %u pages, map %d\n",
-		gfn, mfn, nr, map);
-
-	if (map)
-		ret = add_memory_region(info->vm_id, gfn << PAGE_SHIFT,
-					mfn << PAGE_SHIFT, nr << PAGE_SHIFT,
-					MEM_TYPE_UC, MEM_ACCESS_RWX);
-	else
-		ret = del_memory_region(info->vm_id, gfn << PAGE_SHIFT,
-					nr << PAGE_SHIFT);
-	if (ret)
-		gvt_err("failed map/unmap gfn 0x%lx to mfn 0x%lx with %u pages,"
-			" map %d\n", gfn, mfn, nr, map);
-	return ret;
-}
-
-static int acrngt_set_trap_area(unsigned long handle, u64 start,
-							u64 end, bool map)
-{
-	int ret;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-	gvt_dbg_core("set trap area, start 0x%llx, end 0x%llx, map %d\n",
-			start, end, map);
-
-	if (map)
-		ret = acrn_ioreq_add_iorange(info->client, REQ_MMIO,
-					start, end);
-	else
-		ret = acrn_ioreq_del_iorange(info->client, REQ_MMIO,
-					start, end);
-	if (ret)
-		gvt_err("failed set trap, start 0x%llx, end 0x%llx, map %d\n",
-			start, end, map);
-	return ret;
-}
-
-static int acrngt_set_pvmmio(unsigned long handle, u64 start, u64 end, bool map)
-{
-	int rc, i;
-	unsigned long mfn, shared_mfn;
-	unsigned long pfn = start >> PAGE_SHIFT;
-	u32 mmio_size_fn = acrngt_priv.gvt->device_info.mmio_size >> PAGE_SHIFT;
-	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
-
-	if (map) {
-		mfn = acrngt_virt_to_mfn(info->vgpu->mmio.vreg);
-		rc = acrngt_map_gfn_to_mfn(handle, pfn, mfn, mmio_size_fn, map);
-		if (rc) {
-			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
-					pfn, mfn, rc);
-			return rc;
-		}
-
-		/* map the shared page to guest */
-		shared_mfn = acrngt_virt_to_mfn(info->vgpu->mmio.shared_page);
-		rc = acrngt_map_gfn_to_mfn(handle, pfn + mmio_size_fn, shared_mfn, 1, map);
-		if (rc) {
-			gvt_err("acrn-gvt: map shared page fail with ret %d\n", rc);
-			return rc;
-		}
-
-		/* mmio access is trapped like memory write protection */
-		rc = acrn_ioreq_add_iorange(info->client, REQ_WP, pfn << PAGE_SHIFT,
-					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
-		if (rc) {
-			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n", pfn);
-			return rc;
-		}
-
-		for (i = 0; i < mmio_size_fn; i++) {
-			rc = write_protect_page(info->vm_id,
-				(pfn + i) << PAGE_SHIFT, true);
-			if (rc) {
-				gvt_err("failed set wp for pfn 0x%lx\n", pfn + i);
-				return rc;
-			}
-		}
-
-		/* scratch reg access is trapped like mmio access, 1 page */
-		rc = acrngt_map_gfn_to_mfn(handle, pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
-					mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT), 1, 0);
-		if (rc) {
-			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
-					pfn, mfn, rc);
-			return rc;
-		}
-		rc = acrn_ioreq_add_iorange(info->client, REQ_MMIO,
-				(pfn << PAGE_SHIFT) + VGT_PVINFO_PAGE,
-				((pfn + 1) << PAGE_SHIFT) + VGT_PVINFO_PAGE - 1);
-		if (rc) {
-			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n",
-				(pfn << PAGE_SHIFT) + VGT_PVINFO_PAGE);
-			return rc;
-		}
-
-	} else {
-		mfn = acrngt_virt_to_mfn(info->vgpu->mmio.vreg);
-
-		/* scratch reg access is trapped like mmio access, 1 page */
-		rc = acrngt_map_gfn_to_mfn(handle,
-				pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
-				mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT), 1, 1);
-		if (rc) {
-			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
-					pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
-					mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
-					rc);
-			return rc;
-		}
-
-		rc = acrngt_map_gfn_to_mfn(handle, pfn, mfn, mmio_size_fn, map);
-		if (rc) {
-			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
-					pfn, mfn, rc);
-			return rc;
-		}
-		rc = acrn_ioreq_del_iorange(info->client, REQ_WP, pfn << PAGE_SHIFT,
-					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
-		if (rc) {
-			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n", pfn);
-			return rc;
-		}
-		rc = acrn_ioreq_add_iorange(info->client, REQ_MMIO, pfn << PAGE_SHIFT,
-					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
-		if (rc) {
-			gvt_err("failed acrn_ioreq_del_iorange for pfn 0x%lx\n", pfn);
-			return rc;
-		}
-
-		/* unmap the shared page to guest */
-		shared_mfn = acrngt_virt_to_mfn(info->vgpu->mmio.shared_page);
-		rc = acrngt_map_gfn_to_mfn(handle, pfn + mmio_size_fn, shared_mfn, 1, map);
-		if (rc) {
-			gvt_err("acrn-gvt: map shared page fail with ret %d\n", rc);
-			return rc;
-		}
-	}
-	return rc;
-}
-
-static int acrngt_dma_map_guest_page(unsigned long handle, unsigned long gfn,
-				  unsigned long size, dma_addr_t *dma_addr)
-{
-	unsigned long pfn;
-
-	pfn = acrngt_gfn_to_pfn(handle, gfn);
-	*dma_addr = pfn << PAGE_SHIFT;
-
-	return 0;
-}
-
-static void acrngt_dma_unmap_guest_page(unsigned long handle,
-		dma_addr_t dma_addr)
-{
-}
-
-static int acrngt_set_opregion(void *p_vgpu)
-{
-	struct intel_vgpu *vgpu = (struct intel_vgpu *)p_vgpu;
-	void *base;
-	u32 asls;
-	int i;
-
-	gvt_dbg_dpy("acrngt set opregion\n");
-
-	base = vgpu_opregion(vgpu)->va;
-	if (!base)
-		return -ENOMEM;
-
-	/* hard code opregion to [0xDFFFD000, 0xE0000000]
-	 * ToDo:
-	 * 1. reserve the region in dm then use it in OVMF and kernel
-	 * 2. pass the offset to acrngt through kernel parameter
-	 */
-	asls = 0xDFFFD000;
-	*(u32 *)(vgpu_cfg_space(vgpu) + INTEL_GVT_PCI_OPREGION) = asls;
-
-	for (i = 0; i < INTEL_GVT_OPREGION_PAGES; i++) {
-		vgpu_opregion(vgpu)->gfn[i] = (asls >> PAGE_SHIFT) + i;
-	}
-
-	return 0;
-}
-
-struct intel_gvt_mpt acrn_gvt_mpt = {
-	.type = INTEL_GVT_HYPERVISOR_ACRN,
-	.host_init = acrngt_host_init,
-	.host_exit = acrngt_host_exit,
-	.attach_vgpu = acrngt_attach_vgpu,
-	.detach_vgpu = acrngt_detach_vgpu,
-	.inject_msi = acrngt_inject_msi,
-	.from_virt_to_mfn = acrngt_virt_to_mfn,
-	.enable_page_track = acrngt_page_track_add,
-	.disable_page_track = acrngt_page_track_remove,
-	.read_gpa = acrngt_read_gpa,
-	.write_gpa = acrngt_write_gpa,
-	.gfn_to_mfn = acrngt_gfn_to_pfn,
-	.map_gfn_to_mfn = acrngt_map_gfn_to_mfn,
-	.dma_map_guest_page = acrngt_dma_map_guest_page,
-	.dma_unmap_guest_page = acrngt_dma_unmap_guest_page,
-	.set_trap_area = acrngt_set_trap_area,
-	.set_pvmmio = acrngt_set_pvmmio,
-	.set_opregion = acrngt_set_opregion,
-};
-EXPORT_SYMBOL_GPL(acrn_gvt_mpt);
-
-static int __init acrngt_init(void)
-{
-	/* todo: to support need implment check_gfx_iommu_enabled func */
-	if (intel_gvt_register_hypervisor(&acrn_gvt_mpt) < 0)
-		return -ENODEV;
-	gvt_dbg_core("acrngt loaded\n");
-	return 0;
-}
-
-static void __exit acrngt_exit(void)
-{
-	intel_gvt_unregister_hypervisor();
-	gvt_dbg_core("acrngt: unloaded\n");
-}
-
-#ifndef MODULE
-late_initcall(acrngt_init)
-#else
-module_init(acrngt_init);
-#endif
-module_exit(acrngt_exit);
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/acrngt.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/acrngt.h
+++ /dev/null
@@ -1,81 +0,0 @@
-/*
- * Copyright(c) 2011-2016 Intel Corporation. All rights reserved.
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
- * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
- * SOFTWARE.
- *
- */
-
-#ifndef INTEL_GVT_ACRNGT_H
-#define INTEL_GVT_ACRNGT_H
-
-extern struct intel_gvt *gvt_instance;
-extern const struct intel_gvt_ops *acrn_intel_gvt_ops;
-
-#define MAX_HVM_VCPUS_SUPPORTED 127
-
-#define VMEM_1MB		(1ULL << 20)	/* the size of the first 1MB */
-
-typedef uint16_t domid_t;
-
-/*
- * acrngt_hvm_dev is a wrapper of a vGPU instance which is reprensented by the
- * intel_vgpu structure. Under acrn hypervisor, the acrngt_instance stands for a
- * HVM device, which the related resource.
- */
-struct acrngt_hvm_dev {
-	domid_t vm_id;
-	struct kobject kobj;
-	struct intel_vgpu *vgpu;
-
-	int nr_vcpu;
-	struct task_struct *emulation_thread;
-
-	int client;
-	struct vhm_request *req_buf;
-	struct vhm_vm *vm;
-};
-
-struct acrngt_hvm_params {
-	int vm_id;
-	int aperture_sz; /* in MB */
-	int gm_sz;  /* in MB */
-	int fence_sz;
-};
-
-/*
- * struct gvt_acrngt should be a single instance to share global
- * information for ACRNGT module.
- */
-#define GVT_MAX_VGPU_INSTANCE 15
-struct gvt_acrngt {
-	struct intel_gvt *gvt;
-	struct intel_vgpu *vgpus[GVT_MAX_VGPU_INSTANCE];
-};
-
-static ssize_t acrngt_sysfs_instance_manage(struct kobject *kobj,
-	struct kobj_attribute *attr, const char *buf, size_t count);
-static ssize_t acrngt_sysfs_vgpu_id(struct kobject *kobj,
-	struct kobj_attribute *attr, char *buf);
-
-struct intel_vgpu *acrngt_instance_create(domid_t vm_id,
-		struct intel_vgpu_type *type);
-void acrngt_instance_destroy(struct intel_vgpu *vgpu);
-
-#endif
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/cfg_space.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/cfg_space.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/cfg_space.c
@@ -112,13 +112,6 @@ int intel_vgpu_emulate_cfg_read(struct i
 	if (WARN_ON(offset + bytes > vgpu->gvt->device_info.cfg_space_size))
 		return -EINVAL;
 
-	if (rounddown(offset, 4) == INTEL_GVT_PCI_OPREGION) {
-		if (!vgpu_opregion(vgpu)->mapped) {
-			gvt_dbg_dpy("set up virtual opregion mapping\n");
-			map_vgpu_opregion(vgpu, true);
-		}
-	}
-
 	memcpy(p_data, vgpu_cfg_space(vgpu) + offset, bytes);
 	return 0;
 }
@@ -314,22 +307,9 @@ int intel_vgpu_emulate_cfg_write(struct
 
 	/* First check if it's PCI_COMMAND */
 	if (IS_ALIGNED(offset, 2) && offset == PCI_COMMAND) {
-		if (WARN_ON(bytes != 2 && bytes != 4))
+		if (WARN_ON(bytes > 2))
 			return -EINVAL;
-
-		ret = -EINVAL;
-		if (bytes == 2)
-			ret = emulate_pci_command_write(vgpu, offset,
-							p_data, bytes);
-		if (bytes ==  4) {
-			ret = emulate_pci_command_write(vgpu, offset,
-							p_data, 2);
-			if (ret)
-				return ret;
-			vgpu_pci_cfg_mem_write(vgpu, offset + 2,
-					       (u8 *)p_data + 2, 2);
-		}
-		return ret;
+		return emulate_pci_command_write(vgpu, offset, p_data, bytes);
 	}
 
 	switch (rounddown(offset, 4)) {
@@ -354,7 +334,6 @@ int intel_vgpu_emulate_cfg_write(struct
 	case INTEL_GVT_PCI_OPREGION:
 		if (WARN_ON(!IS_ALIGNED(offset, 4)))
 			return -EINVAL;
-
 		ret = intel_vgpu_opregion_base_write_handler(vgpu,
 						   *(u32 *)p_data);
 		if (ret)
@@ -432,8 +411,6 @@ void intel_vgpu_reset_cfg_space(struct i
 				INTEL_GVT_PCI_CLASS_VGA_OTHER;
 
 	if (cmd & PCI_COMMAND_MEMORY) {
-		if (VGPU_PVMMIO(vgpu))
-			set_pvmmio(vgpu, false);
 		trap_gttmmio(vgpu, false);
 		map_aperture(vgpu, false);
 	}
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/cmd_parser.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/cmd_parser.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/cmd_parser.c
@@ -2837,34 +2837,6 @@ out:
 	return ret;
 }
 
-#define GEN8_PDPES    4
-int gvt_emit_pdps(struct intel_vgpu_workload *workload)
-{
-	const int num_cmds = GEN8_PDPES * 2;
-	struct i915_request *req = workload->req;
-	struct intel_engine_cs *engine = req->engine;
-	u32 base = engine->mmio_base;
-	u32 *cs;
-	u32 *pdps = (u32 *)(workload->shadow_mm->ppgtt_mm.shadow_pdps);
-	int i;
-
-	cs = intel_ring_begin(req, num_cmds * 2 + 2);
-	if (IS_ERR(cs))
-		return PTR_ERR(cs);
-
-	*cs++ = MI_LOAD_REGISTER_IMM(num_cmds);
-	for (i = 0; i < GEN8_PDPES; i++) {
-		*cs++ = i915_mmio_reg_offset(GEN8_RING_PDP_LDW(base, i));
-		*cs++ = pdps[i * 2];
-		*cs++ = i915_mmio_reg_offset(GEN8_RING_PDP_UDW(base, i));
-		*cs++ = pdps[i * 2 + 1];
-	}
-	*cs++ = MI_NOOP;
-	intel_ring_advance(req, cs);
-
-	return 0;
-}
-
 static int shadow_workload_ring_buffer(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/cmd_parser.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/cmd_parser.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/cmd_parser.h
@@ -46,5 +46,4 @@ int intel_gvt_scan_and_shadow_ringbuffer
 
 int intel_gvt_scan_and_shadow_wa_ctx(struct intel_shadow_wa_ctx *wa_ctx);
 
-int gvt_emit_pdps(struct intel_vgpu_workload *workload);
 #endif
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/display.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/display.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/display.c
@@ -33,7 +33,6 @@
  */
 
 #include "i915_drv.h"
-#include "display/intel_display_types.h"
 #include "gvt.h"
 
 static int get_edp_pipe(struct intel_vgpu *vgpu)
@@ -192,12 +191,6 @@ static void emulate_monitor_status_chang
 				BXT_DE_PORT_HP_DDIC;
 		}
 
-		vgpu_vreg_t(vgpu, SKL_FUSE_STATUS) |=
-				SKL_FUSE_DOWNLOAD_STATUS |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG0) |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG1) |
-				SKL_FUSE_PG_DIST_STATUS(SKL_PG2);
-
 		return;
 	}
 
@@ -214,41 +207,14 @@ static void emulate_monitor_status_chang
 				SKL_FUSE_PG_DIST_STATUS(SKL_PG0) |
 				SKL_FUSE_PG_DIST_STATUS(SKL_PG1) |
 				SKL_FUSE_PG_DIST_STATUS(SKL_PG2);
-		/*
-		 * Only 1 PIPE enabled in current vGPU display and PIPE_A is
-		 *  tied to TRANSCODER_A in HW, so it's safe to assume PIPE_A,
-		 *   TRANSCODER_A can be enabled. PORT_x depends on the input of
-		 *   setup_virtual_dp_monitor, we can bind DPLL0 to any PORT_x
-		 *   so we fixed to DPLL0 here.
-		 * Setup DPLL0: DP link clk 1620 MHz, non SSC, DP Mode
-		 */
-		vgpu_vreg_t(vgpu, DPLL_CTRL1) =
-			DPLL_CTRL1_OVERRIDE(DPLL_ID_SKL_DPLL0);
-		vgpu_vreg_t(vgpu, DPLL_CTRL1) |=
-			DPLL_CTRL1_LINK_RATE(DPLL_CTRL1_LINK_RATE_1620, DPLL_ID_SKL_DPLL0);
-		vgpu_vreg_t(vgpu, LCPLL1_CTL) =
-			LCPLL_PLL_ENABLE | LCPLL_PLL_LOCK;
-		vgpu_vreg_t(vgpu, DPLL_STATUS) = DPLL_LOCK(DPLL_ID_SKL_DPLL0);
-		/*
-		 * Golden M/N are calculated based on:
-		 *   24 bpp, 4 lanes, 154000 pixel clk (from virtual EDID),
-		 *   DP link clk 1620 MHz and non-constant_n.
-		 * TODO: calculate DP link symbol clk and stream clk m/n.
-		 */
-		vgpu_vreg_t(vgpu, PIPE_DATA_M1(TRANSCODER_A)) = 63 << TU_SIZE_SHIFT;
-		vgpu_vreg_t(vgpu, PIPE_DATA_M1(TRANSCODER_A)) |= 0x5b425e;
-		vgpu_vreg_t(vgpu, PIPE_DATA_N1(TRANSCODER_A)) = 0x800000;
-		vgpu_vreg_t(vgpu, PIPE_LINK_M1(TRANSCODER_A)) = 0x3cd6e;
-		vgpu_vreg_t(vgpu, PIPE_LINK_N1(TRANSCODER_A)) = 0x80000;
+		vgpu_vreg_t(vgpu, LCPLL1_CTL) |=
+				LCPLL_PLL_ENABLE |
+				LCPLL_PLL_LOCK;
+		vgpu_vreg_t(vgpu, LCPLL2_CTL) |= LCPLL_PLL_ENABLE;
+
 	}
 
 	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_B)) {
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) &=
-			~DPLL_CTRL2_DDI_CLK_OFF(PORT_B);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_CLK_SEL(DPLL_ID_SKL_DPLL0, PORT_B);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_SEL_OVERRIDE(PORT_B);
 		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDIB_DETECTED;
 		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
 			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
@@ -263,18 +229,12 @@ static void emulate_monitor_status_chang
 			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_B)) |=
 				PORT_CLK_SEL_LCPLL_810;
 		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_B)) &= ~DDI_BUF_CTL_ENABLE;
+		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_B)) |= DDI_BUF_CTL_ENABLE;
 		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_B)) &= ~DDI_BUF_IS_IDLE;
 		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTB_HOTPLUG_CPT;
 	}
 
 	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_C)) {
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) &=
-			~DPLL_CTRL2_DDI_CLK_OFF(PORT_C);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_CLK_SEL(DPLL_ID_SKL_DPLL0, PORT_C);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_SEL_OVERRIDE(PORT_C);
 		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTC_HOTPLUG_CPT;
 		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
 			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
@@ -289,18 +249,12 @@ static void emulate_monitor_status_chang
 			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_C)) |=
 				PORT_CLK_SEL_LCPLL_810;
 		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_C)) &= ~DDI_BUF_CTL_ENABLE;
+		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_C)) |= DDI_BUF_CTL_ENABLE;
 		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_C)) &= ~DDI_BUF_IS_IDLE;
 		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDIC_DETECTED;
 	}
 
 	if (intel_vgpu_has_monitor_on_port(vgpu, PORT_D)) {
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) &=
-			~DPLL_CTRL2_DDI_CLK_OFF(PORT_D);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_CLK_SEL(DPLL_ID_SKL_DPLL0, PORT_D);
-		vgpu_vreg_t(vgpu, DPLL_CTRL2) |=
-			DPLL_CTRL2_DDI_SEL_OVERRIDE(PORT_D);
 		vgpu_vreg_t(vgpu, SDEISR) |= SDE_PORTD_HOTPLUG_CPT;
 		vgpu_vreg_t(vgpu, TRANS_DDI_FUNC_CTL(TRANSCODER_A)) &=
 			~(TRANS_DDI_BPC_MASK | TRANS_DDI_MODE_SELECT_MASK |
@@ -315,7 +269,7 @@ static void emulate_monitor_status_chang
 			vgpu_vreg_t(vgpu, PORT_CLK_SEL(PORT_D)) |=
 				PORT_CLK_SEL_LCPLL_810;
 		}
-		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_D)) &= ~DDI_BUF_CTL_ENABLE;
+		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_D)) |= DDI_BUF_CTL_ENABLE;
 		vgpu_vreg_t(vgpu, DDI_BUF_CTL(PORT_D)) &= ~DDI_BUF_IS_IDLE;
 		vgpu_vreg_t(vgpu, SFUSE_STRAP) |= SFUSE_STRAP_DDID_DETECTED;
 	}
@@ -362,20 +316,15 @@ static void clean_virtual_dp_monitor(str
 	port->dpcd = NULL;
 }
 
-static int setup_virtual_monitor(struct intel_vgpu *vgpu, int port_num,
-		int type, unsigned int resolution, void *edid, bool is_dp)
+static int setup_virtual_dp_monitor(struct intel_vgpu *vgpu, int port_num,
+				    int type, unsigned int resolution)
 {
 	struct intel_vgpu_port *port = intel_vgpu_port(vgpu, port_num);
-	int valid_extensions = 1;
-	struct edid *tmp_edid = NULL;
 
 	if (WARN_ON(resolution >= GVT_EDID_NUM))
 		return -EINVAL;
 
-	if (edid)
-		valid_extensions += ((struct edid *)edid)->extensions;
-	port->edid = kzalloc(sizeof(*(port->edid))
-			+ valid_extensions * EDID_SIZE, GFP_KERNEL);
+	port->edid = kzalloc(sizeof(*(port->edid)), GFP_KERNEL);
 	if (!port->edid)
 		return -ENOMEM;
 
@@ -385,30 +334,13 @@ static int setup_virtual_monitor(struct
 		return -ENOMEM;
 	}
 
-	if (edid)
-		memcpy(port->edid->edid_block, edid, EDID_SIZE * valid_extensions);
-	else
-		memcpy(port->edid->edid_block, virtual_dp_monitor_edid[resolution],
-				EDID_SIZE);
-
-	/* Sometimes the physical display will report the EDID with no
-	 * digital bit set, which will cause the guest fail to enumerate
-	 * the virtual HDMI monitor. So here we will set the digital
-	 * bit and re-calculate the checksum.
-	 */
-	tmp_edid = ((struct edid *)port->edid->edid_block);
-	if (!(tmp_edid->input & DRM_EDID_INPUT_DIGITAL)) {
-		tmp_edid->input += DRM_EDID_INPUT_DIGITAL;
-		tmp_edid->checksum -= DRM_EDID_INPUT_DIGITAL;
-	}
-
+	memcpy(port->edid->edid_block, virtual_dp_monitor_edid[resolution],
+			EDID_SIZE);
 	port->edid->data_valid = true;
 
-	if (is_dp) {
-		memcpy(port->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
-		port->dpcd->data_valid = true;
-		port->dpcd->data[DPCD_SINK_COUNT] = 0x1;
-	}
+	memcpy(port->dpcd->data, dpcd_fix_data, DPCD_HEADER_SIZE);
+	port->dpcd->data_valid = true;
+	port->dpcd->data[DPCD_SINK_COUNT] = 0x1;
 	port->type = type;
 	port->id = resolution;
 
@@ -543,122 +475,6 @@ void intel_vgpu_emulate_hotplug(struct i
 	}
 }
 
-static void intel_gvt_vblank_work(struct work_struct *w)
-{
-	struct intel_gvt_pipe_info *pipe_info = container_of(w,
-			struct intel_gvt_pipe_info, vblank_work);
-	struct intel_gvt *gvt = pipe_info->gvt;
-	struct intel_vgpu *vgpu;
-	int id;
-
-	mutex_lock(&gvt->lock);
-	for_each_active_vgpu(gvt, vgpu, id)
-		emulate_vblank_on_pipe(vgpu, pipe_info->pipe_num);
-	mutex_unlock(&gvt->lock);
-}
-
-#define BITS_PER_DOMAIN 4
-#define MAX_SCALERS_PER_DOMAIN 2
-
-#define DOMAIN_SCALER_OWNER(owner, pipe, scaler) \
-	((((owner) >> (pipe) * BITS_PER_DOMAIN * MAX_SCALERS_PER_DOMAIN) >>  \
-	BITS_PER_DOMAIN * (scaler)) & 0xf)
-
-int intel_check_planes(struct intel_vgpu *vgpu, int pipe)
-{
-	int plane = 0;
-	bool ret = false;
-
-	for (plane = 0;
-	     plane < ((RUNTIME_INFO(vgpu->gvt->dev_priv)->num_sprites[pipe]) + 1);
-	     plane++) {
-		if (vgpu->gvt->pipe_info[pipe].plane_owner[plane] == vgpu->id) {
-			ret = true;
-			break;
-		}
-	}
-	return ret;
-}
-
-void intel_gvt_init_pipe_info(struct intel_gvt *gvt)
-{
-	enum pipe pipe;
-	unsigned int scaler;
-	unsigned int domain_scaler_owner = i915_modparams.domain_scaler_owner;
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
-
-	for (pipe = PIPE_A; pipe <= PIPE_C; pipe++) {
-		gvt->pipe_info[pipe].pipe_num = pipe;
-		gvt->pipe_info[pipe].gvt = gvt;
-		INIT_WORK(&gvt->pipe_info[pipe].vblank_work,
-				intel_gvt_vblank_work);
-		/* Each nibble represents domain id
-		 * ids can be from 0-F. 0 for Dom0, 1,2,3...0xF for DomUs
-		 * scaler_owner[i] holds the id of the domain that owns it,
-		 * eg:0,1,2 etc
-		 */
-		for_each_universal_scaler(dev_priv, pipe, scaler)
-			gvt->pipe_info[pipe].scaler_owner[scaler] =
-			DOMAIN_SCALER_OWNER(domain_scaler_owner, pipe, scaler);
-	}
-}
-
-bool gvt_emulate_hdmi; /* default value: false */
-
-int setup_virtual_monitors(struct intel_vgpu *vgpu)
-{
-	struct intel_connector *connector = NULL;
-	struct drm_connector_list_iter conn_iter;
-	int pipe = 0;
-	int ret = 0;
-	struct edid *edid;
-	int type = gvt_emulate_hdmi ? GVT_HDMI_B : GVT_DP_B;
-	int port = PORT_B;
-
-
-	drm_connector_list_iter_begin(&vgpu->gvt->dev_priv->drm, &conn_iter);
-	for_each_intel_connector_iter(connector, &conn_iter) {
-		if (connector->encoder->get_hw_state(connector->encoder, &pipe)) {
-			/* if no planes are allocated for this pipe, skip it */
-			if (i915_modparams.avail_planes_per_pipe &&
-			    !intel_check_planes(vgpu, pipe))
-				continue;
-
-			if (connector->panel.fixed_mode) {
-				edid = intel_gvt_create_edid_from_mode(
-						connector->panel.fixed_mode);
-			} else if (connector->detect_edid) {
-				edid = connector->detect_edid;
-			} else {
-				continue;
-			}
-			/* Get (Dom0) port associated with current pipe. */
-			port = enc_to_dig_port(
-					&(connector->encoder->base))->base.port;
-			ret = setup_virtual_monitor(vgpu, port,
-				type, 0, edid, !gvt_emulate_hdmi);
-			if (ret)
-				return ret;
-			type++;
-			port++;
-		}
-	}
-	drm_connector_list_iter_end(&conn_iter);
-	return 0;
-}
-
-void clean_virtual_monitors(struct intel_vgpu *vgpu)
-{
-	int port = 0;
-
-	for (port = PORT_A; port < I915_MAX_PORTS; port++) {
-		struct intel_vgpu_port *p = intel_vgpu_port(vgpu, port);
-
-		if (p->edid)
-			clean_virtual_dp_monitor(vgpu, port);
-	}
-}
-
 /**
  * intel_vgpu_clean_display - clean vGPU virtual display emulation
  * @vgpu: a vGPU
@@ -670,10 +486,8 @@ void intel_vgpu_clean_display(struct int
 {
 	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 
-	if (IS_BROXTON(dev_priv) || IS_KABYLAKE(dev_priv) ||
-		IS_COFFEELAKE(dev_priv))
-		clean_virtual_monitors(vgpu);
-	else if (IS_SKYLAKE(dev_priv))
+	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
+	    IS_COFFEELAKE(dev_priv))
 		clean_virtual_dp_monitor(vgpu, PORT_D);
 	else
 		clean_virtual_dp_monitor(vgpu, PORT_B);
@@ -696,15 +510,13 @@ int intel_vgpu_init_display(struct intel
 
 	intel_vgpu_init_i2c_edid(vgpu);
 
-	if (IS_BROXTON(dev_priv) || IS_KABYLAKE(dev_priv) ||
-		IS_COFFEELAKE(dev_priv))
-		return setup_virtual_monitors(vgpu);
-	else if (IS_SKYLAKE(dev_priv))
-		return setup_virtual_monitor(vgpu, PORT_D, GVT_DP_D,
-						resolution, NULL, true);
+	if (IS_SKYLAKE(dev_priv) || IS_KABYLAKE(dev_priv) ||
+	    IS_COFFEELAKE(dev_priv))
+		return setup_virtual_dp_monitor(vgpu, PORT_D, GVT_DP_D,
+						resolution);
 	else
-		return setup_virtual_monitor(vgpu, PORT_B, GVT_DP_B,
-						resolution, NULL, true);
+		return setup_virtual_dp_monitor(vgpu, PORT_B, GVT_DP_B,
+						resolution);
 }
 
 /**
@@ -718,246 +530,3 @@ void intel_vgpu_reset_display(struct int
 {
 	emulate_monitor_status_change(vgpu);
 }
-
-
-#define GOP_FB_SIZE		0x800000
-#define GOP_DISPLAY_WIDTH	1920u
-#define GOP_DISPLAY_HEIGHT	1080u
-
-/*
- * check_gop_mode to query current mode and pass it to GOP
- *
- * 1. Get current mode from ctrc)
- * 2. use crtc mode as GOP mode if mode <=1920x1080
- * 3. use 1920x1080 as GOP mode if mode > 1080p
- * 4.   enable panel scale (ToDO)
- * 5. pass GOP mode to OVMF
- *
- */
-static int check_gop_mode(struct intel_vgpu *vgpu)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe, plane;
-	struct intel_crtc *crtc;
-	struct intel_crtc_state *crtc_state;
-	struct drm_display_mode mode;
-	bool found = false;
-
-	/* we will get the gop output on the first pipe the vgpu ownes */
-	for_each_pipe(dev_priv, pipe) {
-		for_each_universal_plane(dev_priv, pipe, plane) {
-			if (vgpu->gvt->pipe_info[pipe].plane_owner[plane]
-					== vgpu->id) {
-				found = true;
-				break;
-			}
-		}
-		if (found)
-			break;
-	}
-
-	if (found == false) {
-		gvt_dbg_dpy("Failed to find owned plane for %d", vgpu->id);
-		return -ENODEV;
-	}
-
-	crtc = intel_get_crtc_for_pipe(vgpu->gvt->dev_priv, pipe);
-	crtc_state = to_intel_crtc_state(crtc->base.state);
-	intel_mode_from_pipe_config(&mode, crtc_state);
-
-	drm_mode_debug_printmodeline(&mode);
-
-	if (mode.vdisplay <= 0 || mode.hdisplay <= 0)
-		return -EINVAL;
-
-	vgpu->gm.gop.width = mode.hdisplay;
-	vgpu->gm.gop.height = mode.vdisplay;
-	vgpu->gm.gop.pitch = mode.hdisplay;
-	vgpu->gm.gop.Bpp = 4;
-
-	/* populate mode for OVMF GOP driver */
-	if (mode.hdisplay * mode.vdisplay * 4 > GOP_FB_SIZE) {
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.width)) =
-			min(vgpu->gm.gop.width, GOP_DISPLAY_WIDTH);
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.height)) =
-			min(vgpu->gm.gop.height, GOP_DISPLAY_HEIGHT);
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.pitch)) =
-			min(vgpu->gm.gop.pitch, GOP_DISPLAY_WIDTH);
-	} else {
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.width)) = vgpu->gm.gop.width;
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.height)) = vgpu->gm.gop.height;
-		vgpu_vreg_t(vgpu, vgtif_reg(gop.pitch)) = vgpu->gm.gop.pitch;
-	}
-
-	vgpu->gm.gop.size = 4 * vgpu_vreg_t(vgpu, vgtif_reg(gop.width)) *
-				vgpu_vreg_t(vgpu, vgtif_reg(gop.height));
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.Bpp)) = 4;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.size)) = vgpu->gm.gop.size;
-
-	DRM_INFO("prepare GOP fb: %dKB for %dX%d@%d\n",
-			vgpu_vreg_t(vgpu, vgtif_reg(gop.size))>>10,
-			vgpu_vreg_t(vgpu, vgtif_reg(gop.width)),
-			vgpu_vreg_t(vgpu, vgtif_reg(gop.height)),
-			vgpu_vreg_t(vgpu, vgtif_reg(gop.Bpp))*8);
-	return 0;
-}
-
-/*
- * prepare_gop_fb will allocate a arrange of memory, and then map them
- * into the ggtt table of the guest partition in the aperture.
- */
-static int prepare_gop_fb(struct intel_vgpu *vgpu, u32 size)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	struct page **pages = NULL;
-	u32 count, npages = size >> PAGE_SHIFT;
-	struct i915_ggtt *ggtt = &dev_priv->ggtt;
-	struct i915_vma vma;
-	struct drm_mm_node *node = &vgpu->gm.high_gm_node;
-	struct sg_table st;
-	unsigned int cache_level = HAS_LLC(dev_priv) ?
-				I915_CACHE_LLC : I915_CACHE_NONE;
-	int ret = 0;
-
-	pages = kmalloc_array(npages, sizeof(struct page *), GFP_KERNEL);
-	if (!pages)
-		return -ENOMEM;
-
-	for (count = 0; count < npages; count++) {
-		struct page *page = alloc_page(GFP_KERNEL);
-		if (!page) {
-			ret = -ENOMEM;
-			goto free_pgs;
-		}
-		pages[count] = page;
-
-		intel_gvt_hypervisor_map_gfn_to_mfn (vgpu,
-				(GOP_FB_BASE >> PAGE_SHIFT) + count,
-				page_to_pfn(page), 1, true);
-	}
-
-	ret = sg_alloc_table_from_pages(&st, pages, npages,
-			0, npages << PAGE_SHIFT, GFP_KERNEL);
-	if (ret)
-		goto free_pgs;
-
-	if (!dma_map_sg(&dev_priv->drm.pdev->dev, st.sgl, st.nents,
-				PCI_DMA_BIDIRECTIONAL)) {
-		ret = -ENOMEM;
-		goto free_sg;
-	}
-
-	memset(&vma, 0, sizeof(vma));
-	vma.node.start = node->start;
-	vma.node.size = size;
-	vma.pages = &st;
-	ggtt->vm.insert_entries(&ggtt->vm, &vma, cache_level, 0);
-	sg_free_table(&st);
-
-	vgpu->gm.gop_fb_pages = pages;
-	vgpu->gm.gop_fb_size = count;
-	return 0;
-
-free_sg:
-	sg_free_table(&st);
-
-free_pgs:
-	release_pages(pages, count);
-	kfree(pages);
-	return ret;
-}
-
-static int setup_gop_display(struct intel_vgpu *vgpu)
-{
-	int ret = 0;
-	unsigned int pipe, plane;
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	bool found = false;
-
-	u32 width, height, Bpp;
-	u32 stride, ctl, surf;
-	unsigned long irqflags;
-
-	width = vgpu_vreg_t(vgpu, vgtif_reg(gop.width));
-	height = vgpu_vreg_t(vgpu, vgtif_reg(gop.height));
-	Bpp = vgpu_vreg_t(vgpu, vgtif_reg(gop.Bpp));
-
-	DRM_INFO("Set up display w:%u h:%u for GOP \n", width, height);
-
-	/* we will display the gop output on the first plane the vgpu ownes */
-	for_each_pipe(dev_priv, pipe) {
-		for_each_universal_plane(dev_priv, pipe, plane) {
-			if (vgpu->gvt->pipe_info[pipe].plane_owner[plane]
-					== vgpu->id) {
-				found = true;
-				break;
-			}
-		}
-		if (found)
-			break;
-	}
-
-	if (!found) {
-		gvt_dbg_dpy("Failed to find owned plane for %d", vgpu->id);
-		return -ENODEV;
-	}
-
-	/* Sizes are 0 based */
-	stride = width * Bpp / 64; /* 32bit per pixel */
-	width--;
-	height--;
-	surf = vgpu->gm.high_gm_node.start;
-
-	ctl = PLANE_CTL_ENABLE | PLANE_CTL_FORMAT_XRGB_8888;
-
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-	I915_WRITE_FW(PLANE_OFFSET(pipe, plane), 0);
-	I915_WRITE_FW(PLANE_STRIDE(pipe, plane), stride);
-	I915_WRITE_FW(PLANE_SIZE(pipe, plane), (height << 16) | width);
-	I915_WRITE_FW(PLANE_AUX_DIST(pipe, plane), 0xFFFFF000);
-	I915_WRITE_FW(PLANE_AUX_OFFSET(pipe, plane), 0);
-	I915_WRITE_FW(PLANE_POS(pipe, plane), 0);
-	I915_WRITE_FW(PLANE_CTL(pipe, plane), ctl);
-	I915_WRITE_FW(PLANE_SURF(pipe, plane), surf);
-	I915_READ_FW(PLANE_SURF(pipe, plane));
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
-	return ret;
-}
-
-int intel_vgpu_g2v_setup_gop(struct intel_vgpu *vgpu)
-{
-	int ret = 0;
-
-	gvt_dbg_dpy("intel_vgpu_g2v_setup_gop\n");
-
-	if (vgpu->gm.gop_fb_pages)
-		goto Done;
-
-	ret = check_gop_mode(vgpu);
-	if (ret) {
-		gvt_vgpu_err("gop check pipe faile %d\n", ret);
-		goto Done;
-	}
-
-	ret = prepare_gop_fb(vgpu, vgpu->gm.gop.size);
-	if (ret) {
-		gvt_vgpu_err("gop prepared failed %d\n", ret);
-		goto Done;
-	}
-
-	ret = setup_gop_display(vgpu);
-	if (ret) {
-		gvt_vgpu_err("gop display setup failed %d\n", ret);
-		goto Done;
-	}
-
-	vgpu->gm.gop.fb_base = GOP_FB_BASE;
-
-	vgpu_vreg(vgpu, _vgtif_reg(gop.fb_base)) = vgpu->gm.gop.fb_base;
-
-	gvt_dbg_dpy("set up gop FbBase: %x\n",
-			vgpu_vreg(vgpu, _vgtif_reg(gop.fb_base)));
-
-Done:
-	return 0;
-}
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/display.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/display.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/display.h
@@ -206,5 +206,4 @@ void intel_vgpu_clean_display(struct int
 
 int pipe_is_enabled(struct intel_vgpu *vgpu, int pipe);
 
-int intel_vgpu_g2v_setup_gop(struct intel_vgpu *vgpu);
 #endif
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/edid.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/edid.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/edid.c
@@ -55,6 +55,10 @@ static unsigned char edid_get_byte(struc
 		gvt_vgpu_err("Driver tries to read EDID without proper sequence!\n");
 		return 0;
 	}
+	if (edid->current_edid_read >= EDID_SIZE) {
+		gvt_vgpu_err("edid_get_byte() exceeds the size of EDID!\n");
+		return 0;
+	}
 
 	if (!edid->edid_available) {
 		gvt_vgpu_err("Reading EDID but EDID is not available!\n");
@@ -450,89 +454,6 @@ static inline int get_aux_ch_reg(unsigne
 	return reg;
 }
 
-static u8 edid_checksum(struct edid *edid)
-{
-	u8 *raw = (u8 *)edid, csum = 0;
-	int i;
-
-	for (i = 0; i < EDID_LENGTH; i++)
-		csum += raw[i];
-
-	return csum;
-}
-
-struct edid *intel_gvt_create_edid_from_mode(struct drm_display_mode *mode)
-{
-	struct edid *edid = NULL;
-	struct detailed_pixel_timing *pt;
-
-	edid = kzalloc(sizeof(struct edid), GFP_KERNEL);
-	if (edid) {
-		/* EDID header */
-		memset(&edid->header[1], 0xff, 6);
-		/* Vendor & product info */
-		edid->mfg_id[0] = 0x22, edid->mfg_id[1] = 0xf0;
-		edid->prod_code[0] = 0x54, edid->prod_code[1] = 0x29;
-		edid->serial = 0x00000000;
-		edid->mfg_week = 0x05, edid->mfg_year = 0x19;
-		/* EDID version */
-		edid->version = 0x01, edid->revision = 0x04;
-		/* Display info */
-		edid->input = 0xa5;
-		edid->width_cm = 0x34;
-		edid->height_cm = 0x20;
-		edid->gamma = 0x78;
-		edid->features = 0x23;
-		/* Color characteristics */
-		edid->red_green_lo = 0xfc;
-		edid->black_white_lo = 0x81;
-		edid->red_x = 0xa4, edid->red_y = 0x55;
-		edid->green_x = 0x4d, edid->green_y =  0x9d;
-		edid->blue_x = 0x25, edid->blue_y = 0x12;
-		edid->white_x = 0x50, edid->white_y = 0x54;
-		/* Detailed timings */
-		edid->detailed_timings[0].pixel_clock = cpu_to_le16(mode->clock / 10);
-		pt = &edid->detailed_timings[0].data.pixel_data;
-		pt->hactive_lo = mode->hdisplay & 0xff;
-		pt->hblank_lo = (mode->htotal - mode->hdisplay) & 0xff;
-		pt->hactive_hblank_hi = (mode->hdisplay & 0xf00) >> 4
-			| ((mode->htotal - mode->hdisplay) & 0xf00) >> 8;
-		pt->vactive_lo = mode->vdisplay & 0xff;
-		pt->vblank_lo = (mode->vtotal - mode->vdisplay) & 0xff;
-		pt->vactive_vblank_hi = (mode->vdisplay & 0xf00) >> 4
-			| ((mode->vtotal - mode->vdisplay) & 0xf00) >> 8;
-		pt->hsync_offset_lo = (mode->hsync_start - mode->hdisplay) & 0xff;
-		pt->hsync_pulse_width_lo = (mode->hsync_end - mode->hsync_start) & 0xff;
-		pt->vsync_offset_pulse_width_lo =
-			((mode->vsync_start - mode->vdisplay) & 0x0f) << 4;
-		pt->vsync_offset_pulse_width_lo |=
-			(mode->vsync_end - mode->vsync_start) & 0x0f;
-		pt->hsync_vsync_offset_pulse_width_hi =
-			((mode->hsync_start - mode->hdisplay) & 0x300) >> 2;
-		pt->hsync_vsync_offset_pulse_width_hi |=
-			((mode->hsync_end - mode->hsync_start) & 0x300) >> 4;
-		pt->hsync_vsync_offset_pulse_width_hi |=
-			((mode->vsync_start - mode->vdisplay) & 0x30) >> 2;
-		pt->hsync_vsync_offset_pulse_width_hi |=
-			((mode->vsync_end - mode->vsync_start) & 0x30) >> 4;
-		pt->width_mm_lo = mode->width_mm & 0xff;
-		pt->height_mm_lo = mode->height_mm & 0xff;
-		pt->width_height_mm_hi = (mode->width_mm & 0xf00) >> 4;
-		pt->width_height_mm_hi |= (mode->height_mm & 0xf00) >> 8;
-
-		pt->misc = mode->flags & DRM_MODE_FLAG_PHSYNC ?
-			DRM_EDID_PT_HSYNC_POSITIVE : 0;
-		pt->misc |= mode->flags & DRM_MODE_FLAG_PVSYNC ?
-			DRM_EDID_PT_VSYNC_POSITIVE : 0;
-		pt->misc |= mode->flags & DRM_MODE_FLAG_INTERLACE ?
-			DRM_EDID_PT_INTERLACED : 0;
-
-		edid->checksum = 0 - edid_checksum(edid);
-	}
-
-	return edid;
-}
-
 #define AUX_CTL_MSG_LENGTH(reg) \
 	((reg & DP_AUX_CH_CTL_MESSAGE_SIZE_MASK) >> \
 		DP_AUX_CH_CTL_MESSAGE_SIZE_SHIFT)
@@ -558,8 +479,6 @@ void intel_gvt_i2c_handle_aux_ch_write(s
 	u32 value = *(u32 *)p_data;
 	int aux_data_for_write = 0;
 	int reg = get_aux_ch_reg(offset);
-	uint8_t rxbuf[20] = {0};
-	size_t rxsize;
 
 	if (reg != AUX_CH_CTL) {
 		vgpu_vreg(vgpu, offset) = value;
@@ -567,12 +486,6 @@ void intel_gvt_i2c_handle_aux_ch_write(s
 	}
 
 	msg_length = AUX_CTL_MSG_LENGTH(value);
-	if (WARN_ON(msg_length <= 0 || msg_length > 20))
-		return;
-
-	for (rxsize = 0; rxsize < msg_length; rxsize += 4)
-		intel_dp_unpack_aux(vgpu_vreg(vgpu, offset + 4 + rxsize),
-				rxbuf + rxsize, msg_length - rxsize);
 	// check the msg in DATA register.
 	msg = vgpu_vreg(vgpu, offset + 4);
 	addr = (msg >> 8) & 0xffff;
@@ -612,13 +525,12 @@ void intel_gvt_i2c_handle_aux_ch_write(s
 			}
 		}
 	} else if ((op & 0x1) == GVT_AUX_I2C_WRITE) {
-		/* We only support EDID reading from I2C_over_AUX.
-		 * But if EDID has extension blocks, we use this write
-		 * operation to set block starting address
+		/* TODO
+		 * We only support EDID reading from I2C_over_AUX. And
+		 * we do not expect the index mode to be used. Right now
+		 * the WRITE operation is ignored. It is good enough to
+		 * support the gfx driver to do EDID access.
 		 */
-		if (addr == EDID_ADDR) {
-			i2c_edid->current_edid_read = rxbuf[4];
-		}
 	} else {
 		if (WARN_ON((op & 0x1) != GVT_AUX_I2C_READ))
 			return;
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/edid.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/edid.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/edid.h
@@ -48,7 +48,7 @@
 
 struct intel_vgpu_edid_data {
 	bool data_valid;
-	unsigned char edid_block[0];
+	unsigned char edid_block[EDID_SIZE];
 };
 
 enum gmbus_cycle_type {
@@ -147,6 +147,4 @@ void intel_gvt_i2c_handle_aux_ch_write(s
 		unsigned int offset,
 		void *p_data);
 
-struct edid *intel_gvt_create_edid_from_mode(struct drm_display_mode *mode);
-
 #endif /*_GVT_EDID_H_*/
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/fb_decoder.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/fb_decoder.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/fb_decoder.c
@@ -37,7 +37,6 @@
 #include "i915_drv.h"
 #include "gvt.h"
 #include "i915_pvinfo.h"
-#include "fb_decoder.h"
 
 #define PRIMARY_FORMAT_NUM	16
 struct pixel_format {
@@ -261,12 +260,11 @@ int intel_vgpu_decode_primary_plane(stru
 			(_PRI_PLANE_STRIDE_MASK >> 6) :
 				_PRI_PLANE_STRIDE_MASK, plane->bpp);
 
-	plane->width = vgpu_vreg_t(vgpu, PLANE_SIZE(pipe, PLANE_PRIMARY))&
-					_PLANE_SIZE_WIDTH_MASK;
-
+	plane->width = (vgpu_vreg_t(vgpu, PIPESRC(pipe)) & _PIPE_H_SRCSZ_MASK) >>
+		_PIPE_H_SRCSZ_SHIFT;
 	plane->width += 1;
-	plane->height = (vgpu_vreg_t(vgpu, PLANE_SIZE(pipe, PLANE_PRIMARY)) &
-			 _PLANE_SIZE_HEIGHT_MASK) >> _PLANE_SIZE_HEIGHT_SHIFT;
+	plane->height = (vgpu_vreg_t(vgpu, PIPESRC(pipe)) &
+			_PIPE_V_SRCSZ_MASK) >> _PIPE_V_SRCSZ_SHIFT;
 	plane->height += 1;	/* raw height is one minus the real value */
 
 	val = vgpu_vreg_t(vgpu, DSPTILEOFF(pipe));
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/fb_decoder.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/fb_decoder.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/fb_decoder.h
@@ -50,10 +50,6 @@
 #define _PRI_PLANE_Y_OFF_SHIFT		16
 #define _PRI_PLANE_Y_OFF_MASK		(0xfff << _PRI_PLANE_Y_OFF_SHIFT)
 
-#define _PLANE_SIZE_HEIGHT_SHIFT	16
-#define _PLANE_SIZE_HEIGHT_MASK		(0xfff << _PLANE_SIZE_HEIGHT_SHIFT)
-#define _PLANE_SIZE_WIDTH_MASK		0x1fff
-
 #define _CURSOR_MODE			0x3f
 #define _CURSOR_ALPHA_FORCE_SHIFT	8
 #define _CURSOR_ALPHA_FORCE_MASK	(0x3 << _CURSOR_ALPHA_FORCE_SHIFT)
@@ -169,4 +165,5 @@ int intel_vgpu_decode_cursor_plane(struc
 	struct intel_vgpu_cursor_plane_format *plane);
 int intel_vgpu_decode_sprite_plane(struct intel_vgpu *vgpu,
 	struct intel_vgpu_sprite_plane_format *plane);
+
 #endif
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/firmware.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/firmware.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/firmware.c
@@ -200,7 +200,6 @@ invalid_firmware:
 
 #define GVT_FIRMWARE_PATH "i915/gvt"
 
-bool disable_gvt_fw_loading = true;
 /**
  * intel_gvt_load_firmware - load GVT firmware
  * @gvt: intel gvt device
@@ -218,27 +217,27 @@ int intel_gvt_load_firmware(struct intel
 	void *mem;
 	int ret;
 
+	path = kmalloc(PATH_MAX, GFP_KERNEL);
+	if (!path)
+		return -ENOMEM;
+
 	mem = kmalloc(info->cfg_space_size, GFP_KERNEL);
-	if (!mem)
+	if (!mem) {
+		kfree(path);
 		return -ENOMEM;
+	}
 
 	firmware->cfg_space = mem;
 
 	mem = kmalloc(info->mmio_size, GFP_KERNEL);
 	if (!mem) {
+		kfree(path);
 		kfree(firmware->cfg_space);
 		return -ENOMEM;
 	}
 
 	firmware->mmio = mem;
 
-	if (disable_gvt_fw_loading)
-		goto expose_firmware;
-
-	path = kmalloc(PATH_MAX, GFP_KERNEL);
-	if (!path)
-		return -ENOMEM;
-
 	sprintf(path, "%s/vid_0x%04x_did_0x%04x_rid_0x%02x.golden_hw_state",
 		 GVT_FIRMWARE_PATH, pdev->vendor, pdev->device,
 		 pdev->revision);
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/gtt.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/gtt.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/gtt.c
@@ -44,8 +44,8 @@
 #define gvt_vdbg_mm(fmt, args...)
 #endif
 
-static bool enable_out_of_sync = true;
-static int preallocated_oos_pages = 2048;
+static bool enable_out_of_sync;
+static int preallocated_oos_pages = 8192;
 
 /*
  * validate a gm address and related range size,
@@ -309,18 +309,6 @@ static inline int gtt_get_entry64(void *
 		return -EINVAL;
 
 	if (hypervisor_access) {
-		if (vgpu->ge_cache_enable && vgpu->cached_guest_entry) {
-			if (index == 0) {
-				ret = intel_gvt_hypervisor_read_gpa(vgpu, gpa,
-				      vgpu->cached_guest_entry,
-				      I915_GTT_PAGE_SIZE);
-				if (WARN_ON(ret))
-					return ret;
-			}
-			e->val64 = *(vgpu->cached_guest_entry + index);
-			return 0;
-		}
-
 		ret = intel_gvt_hypervisor_read_gpa(vgpu, gpa +
 				(index << info->gtt_entry_size_shift),
 				&e->val64, 8);
@@ -1320,10 +1308,8 @@ static int ppgtt_populate_spt(struct int
 	trace_spt_change(spt->vgpu->id, "born", spt,
 			 spt->guest_page.gfn, spt->shadow_page.type);
 
-	vgpu->ge_cache_enable = true;
 	for_each_present_guest_entry(spt, &ge, i) {
 		if (gtt_type_is_pt(get_next_pt_type(ge.type))) {
-			vgpu->ge_cache_enable = false;
 			s = ppgtt_populate_spt_by_guest_entry(vgpu, &ge);
 			if (IS_ERR(s)) {
 				ret = PTR_ERR(s);
@@ -1345,7 +1331,6 @@ static int ppgtt_populate_spt(struct int
 				goto fail;
 		}
 	}
-	vgpu->ge_cache_enable = false;
 	return 0;
 fail:
 	gvt_vgpu_err("fail: shadow page %p guest entry 0x%llx type %d\n",
@@ -1734,8 +1719,6 @@ static int ppgtt_handle_guest_write_page
 
 	index = (pa & (PAGE_SIZE - 1)) >> info->gtt_entry_size_shift;
 
-	/* Set guest ppgtt entry. Optional for KVMGT, but MUST for XENGT. */
-	intel_gvt_hypervisor_write_gpa(vgpu, pa, p_data, bytes);
 	ppgtt_get_guest_entry(spt, &we, index);
 
 	/*
@@ -1788,31 +1771,6 @@ static int ppgtt_handle_guest_write_page
 	return 0;
 }
 
-static void invalidate_mm_pv(struct intel_vgpu_mm *mm)
-{
-	struct intel_vgpu *vgpu = mm->vgpu;
-	struct intel_gvt *gvt = vgpu->gvt;
-	struct intel_gvt_gtt *gtt = &gvt->gtt;
-	struct intel_gvt_gtt_pte_ops *ops = gtt->pte_ops;
-	struct intel_gvt_gtt_entry se;
-
-	if (WARN_ON(mm->ppgtt_mm.root_entry_type !=
-			GTT_TYPE_PPGTT_ROOT_L4_ENTRY))
-		return;
-
-	i915_vm_put(&mm->ppgtt_mm.ppgtt->vm);
-
-	ppgtt_get_shadow_root_entry(mm, &se, 0);
-	if (!ops->test_present(&se))
-		return;
-	trace_spt_guest_change(vgpu->id, "destroy root pointer",
-			NULL, se.type, se.val64, 0);
-	se.val64 = 0;
-	ppgtt_set_shadow_root_entry(mm, &se, 0);
-
-	mm->ppgtt_mm.shadowed = false;
-}
-
 static void invalidate_ppgtt_mm(struct intel_vgpu_mm *mm)
 {
 	struct intel_vgpu *vgpu = mm->vgpu;
@@ -1825,11 +1783,6 @@ static void invalidate_ppgtt_mm(struct i
 	if (!mm->ppgtt_mm.shadowed)
 		return;
 
-	if (VGPU_PVMMIO(mm->vgpu) & PVMMIO_PPGTT_UPDATE) {
-		invalidate_mm_pv(mm);
-		return;
-	}
-
 	for (index = 0; index < ARRAY_SIZE(mm->ppgtt_mm.shadow_pdps); index++) {
 		ppgtt_get_shadow_root_entry(mm, &se, index);
 
@@ -1847,33 +1800,6 @@ static void invalidate_ppgtt_mm(struct i
 	mm->ppgtt_mm.shadowed = false;
 }
 
-static int shadow_mm_pv(struct intel_vgpu_mm *mm)
-{
-	struct intel_vgpu *vgpu = mm->vgpu;
-	struct intel_gvt *gvt = vgpu->gvt;
-	struct intel_gvt_gtt_entry se;
-
-	if (WARN_ON(mm->ppgtt_mm.root_entry_type !=
-			GTT_TYPE_PPGTT_ROOT_L4_ENTRY))
-		return -EINVAL;
-
-	mm->ppgtt_mm.ppgtt = i915_ppgtt_create(gvt->dev_priv);
-	if (IS_ERR(mm->ppgtt_mm.ppgtt)) {
-		gvt_vgpu_err("fail to create ppgtt: %ld\n",
-				PTR_ERR(mm->ppgtt_mm.ppgtt));
-		return PTR_ERR(mm->ppgtt_mm.ppgtt);
-	}
-
-	se.type = GTT_TYPE_PPGTT_ROOT_L4_ENTRY;
-	se.val64 = px_dma(mm->ppgtt_mm.ppgtt->pd);
-	ppgtt_set_shadow_root_entry(mm, &se, 0);
-
-	trace_spt_guest_change(vgpu->id, "populate root pointer",
-			NULL, se.type, se.val64, 0);
-	mm->ppgtt_mm.shadowed = true;
-
-	return 0;
-}
 
 static int shadow_ppgtt_mm(struct intel_vgpu_mm *mm)
 {
@@ -1888,9 +1814,6 @@ static int shadow_ppgtt_mm(struct intel_
 	if (mm->ppgtt_mm.shadowed)
 		return 0;
 
-	if (VGPU_PVMMIO(mm->vgpu) & PVMMIO_PPGTT_UPDATE)
-		return shadow_mm_pv(mm);
-
 	mm->ppgtt_mm.shadowed = true;
 
 	for (index = 0; index < ARRAY_SIZE(mm->ppgtt_mm.guest_pdps); index++) {
@@ -2551,13 +2474,6 @@ int intel_vgpu_init_gtt(struct intel_vgp
 
 	INIT_LIST_HEAD(&gtt->ggtt_mm->ggtt_mm.partial_pte_list);
 
-	vgpu->cached_guest_entry = kzalloc(I915_GTT_PAGE_SIZE, GFP_KERNEL);
-	if (!vgpu->cached_guest_entry) {
-		gvt_vgpu_err("fail to allocate cached_guest_entry page\n");
-		return -ENOMEM;
-	}
-	vgpu->ge_cache_enable = false;
-
 	return create_scratch_page_tree(vgpu);
 }
 
@@ -2595,17 +2511,6 @@ static void intel_vgpu_destroy_ggtt_mm(s
 	vgpu->gtt.ggtt_mm = NULL;
 }
 
-static void clean_gvt_gop(struct intel_vgpu *vgpu)
-{
-	int i;
-	for (i = 0; i < vgpu->gm.gop_fb_size; i++)
-		intel_gvt_hypervisor_map_gfn_to_mfn(vgpu,
-			(GOP_FB_BASE >> PAGE_SHIFT) + i,
-			page_to_pfn(vgpu->gm.gop_fb_pages[i]), 1, false);
-
-	release_pages(vgpu->gm.gop_fb_pages, vgpu->gm.gop_fb_size);
-	kfree(vgpu->gm.gop_fb_pages);
-}
 /**
  * intel_vgpu_clean_gtt - clean up per-vGPU graphics memory virulization
  * @vgpu: a vGPU
@@ -2620,9 +2525,6 @@ void intel_vgpu_clean_gtt(struct intel_v
 {
 	intel_vgpu_destroy_all_ppgtt_mm(vgpu);
 	intel_vgpu_destroy_ggtt_mm(vgpu);
-	kfree(vgpu->cached_guest_entry);
-	if (vgpu->gm.gop_fb_pages)
-		clean_gvt_gop(vgpu);
 	release_scratch_page_tree(vgpu);
 }
 
@@ -2654,7 +2556,7 @@ static int setup_spt_oos(struct intel_gv
 	INIT_LIST_HEAD(&gtt->oos_page_use_list_head);
 
 	for (i = 0; i < preallocated_oos_pages; i++) {
-		oos_page = kmalloc(sizeof(*oos_page), GFP_KERNEL);
+		oos_page = kzalloc(sizeof(*oos_page), GFP_KERNEL);
 		if (!oos_page) {
 			ret = -ENOMEM;
 			goto fail;
@@ -2927,296 +2829,3 @@ void intel_vgpu_reset_gtt(struct intel_v
 	intel_vgpu_destroy_all_ppgtt_mm(vgpu);
 	intel_vgpu_reset_ggtt(vgpu, true);
 }
-
-int intel_vgpu_g2v_pv_ppgtt_alloc_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level)
-{
-	struct pv_ppgtt_update *pv_ppgtt = &vgpu->mmio.shared_page->pv_ppgtt;
-	struct intel_vgpu_mm *mm;
-	u64 pdps[4] = {pv_ppgtt->pdp, 0, 0, 0};
-	int ret = 0;
-
-	if (WARN_ON(page_table_level != 4))
-		return -EINVAL;
-
-	gvt_dbg_mm("alloc_4lvl pdp=%llx start=%llx length=%llx\n",
-			pv_ppgtt->pdp, pv_ppgtt->start,
-			pv_ppgtt->length);
-
-	mm = intel_vgpu_find_ppgtt_mm(vgpu, pdps);
-	if (!mm) {
-		gvt_vgpu_err("failed to find mm for pdp 0x%llx\n", pdps[0]);
-		ret = -EINVAL;
-	} else {
-		ret = mm->ppgtt_mm.ppgtt->vm.allocate_va_range(
-				&mm->ppgtt_mm.ppgtt->vm,
-			pv_ppgtt->start, pv_ppgtt->length);
-		if (ret)
-			gvt_vgpu_err("failed to alloc for pdp %llx\n", pdps[0]);
-	}
-
-	return ret;
-}
-
-int intel_vgpu_g2v_pv_ppgtt_clear_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level)
-{
-	struct pv_ppgtt_update *pv_ppgtt = &vgpu->mmio.shared_page->pv_ppgtt;
-	struct intel_vgpu_mm *mm;
-	u64 pdps[4] = {pv_ppgtt->pdp, 0, 0, 0};
-	int ret = 0;
-
-	if (WARN_ON(page_table_level != 4))
-		return -EINVAL;
-
-	gvt_dbg_mm("clear_4lvl pdp=%llx start=%llx length=%llx\n",
-			pv_ppgtt->pdp, pv_ppgtt->start,
-			pv_ppgtt->length);
-
-	mm = intel_vgpu_find_ppgtt_mm(vgpu, pdps);
-	if (!mm) {
-		gvt_vgpu_err("failed to find mm for pdp 0x%llx\n", pdps[0]);
-		ret = -EINVAL;
-	} else {
-		mm->ppgtt_mm.ppgtt->vm.clear_range(
-				&mm->ppgtt_mm.ppgtt->vm,
-			pv_ppgtt->start, pv_ppgtt->length);
-	}
-
-	return ret;
-}
-
-#define GEN8_PML4E_SIZE		(1UL << 39)
-#define GEN8_PML4E_SIZE_MASK	(~(GEN8_PML4E_SIZE - 1))
-#define GEN8_PDPE_SIZE		(1UL << 30)
-#define GEN8_PDPE_SIZE_MASK	(~(GEN8_PDPE_SIZE - 1))
-#define GEN8_PDE_SIZE		(1UL << 21)
-#define GEN8_PDE_SIZE_MASK	(~(GEN8_PDE_SIZE - 1))
-
-#define pml4_addr_end(addr, end)					\
-({	unsigned long __boundary = \
-			((addr) + GEN8_PML4E_SIZE) & GEN8_PML4E_SIZE_MASK; \
-	(__boundary < (end)) ? __boundary : (end);		\
-})
-
-#define pdp_addr_end(addr, end)						\
-({	unsigned long __boundary = \
-			((addr) + GEN8_PDPE_SIZE) & GEN8_PDPE_SIZE_MASK; \
-	(__boundary < (end)) ? __boundary : (end);		\
-})
-
-#define pd_addr_end(addr, end)						\
-({	unsigned long __boundary = \
-			((addr) + GEN8_PDE_SIZE) & GEN8_PDE_SIZE_MASK;	\
-	(__boundary < (end)) ? __boundary : (end);		\
-})
-
-struct ppgtt_walk {
-	unsigned long *mfns;
-	int mfn_index;
-	unsigned long *pt;
-};
-
-static int walk_pt_range(struct intel_vgpu *vgpu, u64 pt,
-				u64 start, u64 end, struct ppgtt_walk *walk)
-{
-	const struct intel_gvt_device_info *info = &vgpu->gvt->device_info;
-	struct intel_gvt_gtt_gma_ops *gma_ops = vgpu->gvt->gtt.gma_ops;
-	unsigned long start_index, end_index;
-	int ret;
-	int i;
-	unsigned long mfn, gfn;
-
-	start_index = gma_ops->gma_to_pte_index(start);
-	end_index = ((end - start) >> PAGE_SHIFT) + start_index;
-
-	gvt_dbg_mm("%s: %llx start=%llx end=%llx start_index=%lx end_index=%lx mfn_index=%x\n",
-			__func__, pt, start, end,
-			start_index, end_index, walk->mfn_index);
-	ret = intel_gvt_hypervisor_read_gpa(vgpu,
-		(pt & PAGE_MASK) + (start_index << info->gtt_entry_size_shift),
-		walk->pt + start_index,
-		(end_index - start_index) << info->gtt_entry_size_shift);
-	if (ret) {
-		gvt_vgpu_err("fail to read gpa %llx\n", pt);
-		return ret;
-	}
-
-	for (i = start_index; i < end_index; i++) {
-		gfn = walk->pt[i] >> PAGE_SHIFT;
-		mfn = intel_gvt_hypervisor_gfn_to_mfn(vgpu, gfn);
-		if (mfn == INTEL_GVT_INVALID_ADDR) {
-			gvt_vgpu_err("fail to translate gfn: 0x%lx\n", gfn);
-			return -ENXIO;
-		}
-		walk->mfns[walk->mfn_index++] = mfn << PAGE_SHIFT;
-	}
-
-	return 0;
-}
-
-
-static int walk_pd_range(struct intel_vgpu *vgpu, u64 pd,
-				u64 start, u64 end, struct ppgtt_walk *walk)
-{
-	const struct intel_gvt_device_info *info = &vgpu->gvt->device_info;
-	struct intel_gvt_gtt_gma_ops *gma_ops = vgpu->gvt->gtt.gma_ops;
-	unsigned long index;
-	u64 pt, next;
-	int ret  = 0;
-
-	do {
-		index = gma_ops->gma_to_pde_index(start);
-
-		ret = intel_gvt_hypervisor_read_gpa(vgpu,
-			(pd & PAGE_MASK) + (index <<
-			info->gtt_entry_size_shift), &pt, 8);
-		if (ret)
-			return ret;
-		next = pd_addr_end(start, end);
-		gvt_dbg_mm("%s: %llx start=%llx end=%llx next=%llx\n",
-			__func__, pd, start, end, next);
-		walk_pt_range(vgpu, pt, start, next, walk);
-
-		start = next;
-	} while (start != end);
-
-	return ret;
-}
-
-
-static int walk_pdp_range(struct intel_vgpu *vgpu, u64 pdp,
-				  u64 start, u64 end, struct ppgtt_walk *walk)
-{
-	const struct intel_gvt_device_info *info = &vgpu->gvt->device_info;
-	struct intel_gvt_gtt_gma_ops *gma_ops = vgpu->gvt->gtt.gma_ops;
-	unsigned long index;
-	u64 pd, next;
-	int ret  = 0;
-
-	do {
-		index = gma_ops->gma_to_l4_pdp_index(start);
-
-		ret = intel_gvt_hypervisor_read_gpa(vgpu,
-			(pdp & PAGE_MASK) + (index <<
-			info->gtt_entry_size_shift), &pd, 8);
-		if (ret)
-			return ret;
-		next = pdp_addr_end(start, end);
-		gvt_dbg_mm("%s: %llx start=%llx end=%llx next=%llx\n",
-			__func__, pdp, start, end, next);
-
-		walk_pd_range(vgpu, pd, start, next, walk);
-		start = next;
-	} while (start != end);
-
-	return ret;
-}
-
-
-static int walk_pml4_range(struct intel_vgpu *vgpu, u64 pml4,
-				u64 start, u64 end, struct ppgtt_walk *walk)
-{
-	const struct intel_gvt_device_info *info = &vgpu->gvt->device_info;
-	struct intel_gvt_gtt_gma_ops *gma_ops = vgpu->gvt->gtt.gma_ops;
-	unsigned long index;
-	u64 pdp, next;
-	int ret  = 0;
-
-	do {
-		index = gma_ops->gma_to_pml4_index(start);
-		ret = intel_gvt_hypervisor_read_gpa(vgpu,
-			(pml4 & PAGE_MASK) + (index <<
-			info->gtt_entry_size_shift), &pdp, 8);
-		if (ret)
-			return ret;
-		next = pml4_addr_end(start, end);
-		gvt_dbg_mm("%s: %llx start=%llx end=%llx next=%llx\n",
-			__func__, pml4, start, end, next);
-
-		walk_pdp_range(vgpu, pdp, start, next, walk);
-		start = next;
-	} while (start != end);
-
-	return ret;
-}
-
-int intel_vgpu_g2v_pv_ppgtt_insert_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level)
-{
-	struct pv_ppgtt_update *pv_ppgtt = &vgpu->mmio.shared_page->pv_ppgtt;
-	struct intel_vgpu_mm *mm;
-	u64 pdps[4] = {pv_ppgtt->pdp, 0, 0, 0};
-	int ret = 0;
-	u64 start = pv_ppgtt->start;
-	u64 length = pv_ppgtt->length;
-	struct sg_table st;
-	struct scatterlist *sg = NULL;
-	int num_pages = length >> PAGE_SHIFT;
-	struct i915_vma vma;
-	struct ppgtt_walk walk;
-	int i;
-
-	if (WARN_ON(page_table_level != 4))
-		return -EINVAL;
-
-	gvt_dbg_mm("insert_4lvl pml4=%llx start=%llx length=%llx cache=%x\n",
-			pv_ppgtt->pdp, start, length, pv_ppgtt->cache_level);
-
-	mm = intel_vgpu_find_ppgtt_mm(vgpu, pdps);
-	if (!mm) {
-		gvt_vgpu_err("fail to find mm for pml4 0x%llx\n", pdps[0]);
-		return -EINVAL;
-	}
-
-	walk.mfn_index = 0;
-	walk.mfns = NULL;
-	walk.pt = NULL;
-
-	walk.mfns = kmalloc_array(num_pages,
-			sizeof(unsigned long), GFP_KERNEL);
-	if (!walk.mfns) {
-		ret = -ENOMEM;
-		goto fail;
-	}
-
-	walk.pt = (unsigned long *)__get_free_pages(GFP_KERNEL, 0);
-	if (!walk.pt) {
-		ret = -ENOMEM;
-		goto fail;
-	}
-
-	if (sg_alloc_table(&st, num_pages, GFP_KERNEL)) {
-		ret = -ENOMEM;
-		goto fail;
-	}
-
-	ret = walk_pml4_range(vgpu, pdps[0], start, start + length, &walk);
-	if (ret)
-		goto fail_free_sg;
-
-	WARN_ON(num_pages != walk.mfn_index);
-
-	for_each_sg(st.sgl, sg, num_pages, i) {
-		sg->offset = 0;
-		sg->length = PAGE_SIZE;
-		sg_dma_address(sg) = walk.mfns[i];
-		sg_dma_len(sg) = PAGE_SIZE;
-	}
-
-	/* fake vma for insert call*/
-	memset(&vma, 0, sizeof(vma));
-	vma.node.start = start;
-	vma.pages = &st;
-	mm->ppgtt_mm.ppgtt->vm.insert_entries(
-			&mm->ppgtt_mm.ppgtt->vm, &vma,
-			pv_ppgtt->cache_level, 0);
-
-fail_free_sg:
-	sg_free_table(&st);
-fail:
-	kfree(walk.mfns);
-	free_page((unsigned long)walk.pt);
-
-	return ret;
-}
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/gtt.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/gtt.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/gtt.h
@@ -131,7 +131,7 @@ enum intel_gvt_mm_type {
 	INTEL_GVT_MM_PPGTT,
 };
 
-#define GVT_RING_CTX_NR_PDPS   GEN8_3LVL_PDPES
+#define GVT_RING_CTX_NR_PDPS	GEN8_3LVL_PDPES
 
 struct intel_gvt_partial_pte {
 	unsigned long offset;
@@ -160,7 +160,6 @@ struct intel_vgpu_mm {
 
 			struct list_head list;
 			struct list_head lru_list;
-			struct i915_ppgtt *ppgtt;
 		} ppgtt_mm;
 		struct {
 			void *virtual_ggtt;
@@ -279,12 +278,4 @@ int intel_vgpu_emulate_ggtt_mmio_read(st
 int intel_vgpu_emulate_ggtt_mmio_write(struct intel_vgpu *vgpu,
 	unsigned int off, void *p_data, unsigned int bytes);
 
-int intel_vgpu_g2v_pv_ppgtt_alloc_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level);
-
-int intel_vgpu_g2v_pv_ppgtt_clear_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level);
-
-int intel_vgpu_g2v_pv_ppgtt_insert_4lvl(struct intel_vgpu *vgpu,
-		int page_table_level);
 #endif /* _GVT_GTT_H_ */
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/gvt.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/gvt.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/gvt.c
@@ -44,7 +44,6 @@ struct intel_gvt_host intel_gvt_host;
 static const char * const supported_hypervisors[] = {
 	[INTEL_GVT_HYPERVISOR_XEN] = "XEN",
 	[INTEL_GVT_HYPERVISOR_KVM] = "KVM",
-	[INTEL_GVT_HYPERVISOR_ACRN] = "ACRN",
 };
 
 static struct intel_vgpu_type *intel_gvt_find_vgpu_type(struct intel_gvt *gvt,
@@ -197,8 +196,6 @@ static void init_device_info(struct inte
 	info->max_support_vgpus = 8;
 	info->cfg_space_size = PCI_CFG_SPACE_EXP_SIZE;
 	info->mmio_size = 2 * 1024 * 1024;
-	/* order of mmio size. assert(2^order == mmio_size) */
-	info->mmio_size_order = 9;
 	info->mmio_bar = 0;
 	info->gtt_start_offset = 8 * 1024 * 1024;
 	info->gtt_entry_size = 8;
@@ -258,105 +255,6 @@ static int init_service_thread(struct in
 	return 0;
 }
 
-void intel_gvt_init_pipe_info(struct intel_gvt *gvt);
-
-/*
- * When enabling multi-plane in DomU, an issue is that the PLANE_BUF_CFG
- * register cannot be updated dynamically, since Dom0 has no idea of the
- * plane information of DomU's planes, so here we statically allocate the
- * ddb entries for all the possible enabled planes.
- */
-void intel_gvt_allocate_ddb(struct intel_gvt *gvt, unsigned int active_crtcs)
-{
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
-	struct intel_gvt_pipe_info *pipe_info = gvt->pipe_info;
-	const struct intel_runtime_info *info = RUNTIME_INFO(dev_priv);
-	unsigned int pipe_size, ddb_size, plane_size, plane_cnt;
-	u16 start, end;
-	enum pipe pipe;
-	enum plane_id plane;
-	int i = 0;
-	int num_active = hweight32(active_crtcs);
-
-	if (!num_active)
-		return;
-
-	ddb_size = INTEL_INFO(dev_priv)->ddb_size;
-	ddb_size -= 4; /* 4 blocks for bypass path allocation */
-	pipe_size = ddb_size / num_active;
-
-	for_each_pipe_masked(dev_priv, pipe, active_crtcs) {
-		start = pipe_size * (i++);
-		end = start + pipe_size;
-		pipe_info[pipe].pipe_ddb.start = start;
-		pipe_info[pipe].pipe_ddb.end = end;
-
-		pipe_info[pipe].plane_ddb_y[PLANE_CURSOR].start = end - GVT_CURSOR_BLOCKS;
-		pipe_info[pipe].plane_ddb_y[PLANE_CURSOR].end = end;
-
-		plane_cnt = (info->num_sprites[pipe] + 1);
-		plane_size = (pipe_size - GVT_CURSOR_BLOCKS) / plane_cnt;
-
-		for_each_universal_plane(dev_priv, pipe, plane) {
-			pipe_info[pipe].plane_ddb_y[plane].start = start +
-				(plane * (pipe_size - GVT_CURSOR_BLOCKS) / plane_cnt);
-			pipe_info[pipe].plane_ddb_y[plane].end =
-				pipe_info[pipe].plane_ddb_y[plane].start + plane_size;
-		}
-	}
-}
-
-static int intel_gvt_init_vreg_pool(struct intel_gvt *gvt)
-{
-	int i = 0;
-	const struct intel_gvt_device_info *info = &gvt->device_info;
-
-	for (i = 0; i < GVT_MAX_VGPU; i++) {
-		gvt->intel_gvt_vreg_pool[i] = (void *)__get_free_pages(
-			GFP_KERNEL, info->mmio_size_order);
-		if (!gvt->intel_gvt_vreg_pool[i])
-			return -ENOMEM;
-	}
-
-	return 0;
-}
-
-static void intel_gvt_clean_vreg_pool(struct intel_gvt *gvt)
-{
-	int i = 0;
-	const struct intel_gvt_device_info *info = &gvt->device_info;
-
-	for (i = 0; i < GVT_MAX_VGPU && gvt->intel_gvt_vreg_pool[i]; i++)
-		free_pages((unsigned long) gvt->intel_gvt_vreg_pool[i],
-				info->mmio_size_order);
-}
-
-void *intel_gvt_allocate_vreg(struct intel_vgpu *vgpu)
-{
-	int id = vgpu->id - 1;
-	struct intel_gvt *gvt = vgpu->gvt;
-
-	if (id < 0 || id >= GVT_MAX_VGPU ||
-		gvt->intel_gvt_vreg_pool[id] == NULL ||
-		gvt->intel_gvt_vreg_allocated[id])
-		return NULL;
-
-	gvt->intel_gvt_vreg_allocated[id] = true;
-	return gvt->intel_gvt_vreg_pool[id];
-}
-
-void intel_gvt_free_vreg(struct intel_vgpu *vgpu)
-{
-	int id = vgpu->id - 1;
-	struct intel_gvt *gvt = vgpu->gvt;
-
-	if (id < 0 || id >= GVT_MAX_VGPU ||
-		gvt->intel_gvt_vreg_pool[id] == NULL ||
-		!gvt->intel_gvt_vreg_allocated[id])
-		return;
-	gvt->intel_gvt_vreg_allocated[id] = false;
-}
-
 /**
  * intel_gvt_clean_device - clean a GVT device
  * @dev_priv: i915 private
@@ -372,7 +270,6 @@ void intel_gvt_clean_device(struct drm_i
 	if (WARN_ON(!gvt))
 		return;
 
-	intel_gvt_clean_vreg_pool(gvt);
 	intel_gvt_destroy_idle_vgpu(gvt->idle_vgpu);
 	intel_gvt_cleanup_vgpu_type_groups(gvt);
 	intel_gvt_clean_vgpu_types(gvt);
@@ -392,12 +289,6 @@ void intel_gvt_clean_device(struct drm_i
 	dev_priv->gvt = NULL;
 }
 
-#define BITS_PER_DOMAIN 4
-#define MAX_PLANES_PER_DOMAIN 4
-#define DOMAIN_PLANE_OWNER(owner, pipe, plane) \
-		((((owner) >> (pipe) * BITS_PER_DOMAIN * MAX_PLANES_PER_DOMAIN) >>  \
-		  BITS_PER_DOMAIN * (plane)) & 0xf)
-
 /**
  * intel_gvt_init_device - initialize a GVT device
  * @dev_priv: drm i915 private data
@@ -476,8 +367,6 @@ int intel_gvt_init_device(struct drm_i91
 		goto out_clean_types;
 	}
 
-	intel_gvt_init_pipe_info(gvt);
-
 	vgpu = intel_gvt_create_idle_vgpu(gvt);
 	if (IS_ERR(vgpu)) {
 		ret = PTR_ERR(vgpu);
@@ -486,41 +375,14 @@ int intel_gvt_init_device(struct drm_i91
 	}
 	gvt->idle_vgpu = vgpu;
 
-	ret = intel_gvt_init_vreg_pool(gvt);
-	if (ret) {
-		gvt_err("failed to init vreg pool\n");
-		goto out_clean_vreg;
-	}
-
 	intel_gvt_debugfs_init(gvt);
 
-	if (i915_modparams.avail_planes_per_pipe) {
-		unsigned long long domain_plane_owners;
-		int plane;
-		enum pipe pipe;
-
-		/*
-		 * Each nibble represents domain id
-		 * ids can be from 0-F. 0 for Dom0, 1,2,3...0xF for DomUs
-		 * plane_owner[i] holds the id of the domain that owns it,eg:0,1,2 etc
-		 */
-		domain_plane_owners = i915_modparams.domain_plane_owners;
-		for_each_pipe(dev_priv, pipe) {
-			for_each_universal_plane(dev_priv, pipe, plane) {
-				gvt->pipe_info[pipe].plane_owner[plane] =
-					DOMAIN_PLANE_OWNER(domain_plane_owners, pipe, plane);
-			}
-		}
-	}
-
 	gvt_dbg_core("gvt device initialization is done\n");
 	dev_priv->gvt = gvt;
 	intel_gvt_host.dev = &dev_priv->drm.pdev->dev;
 	intel_gvt_host.initialized = true;
 	return 0;
 
-out_clean_vreg:
-	intel_gvt_clean_vreg_pool(gvt);
 out_clean_types:
 	intel_gvt_clean_vgpu_types(gvt);
 out_clean_thread:
@@ -555,8 +417,7 @@ intel_gvt_register_hypervisor(struct int
 		return -ENODEV;
 
 	if (m->type != INTEL_GVT_HYPERVISOR_KVM &&
-	    m->type != INTEL_GVT_HYPERVISOR_XEN &&
-	    m->type != INTEL_GVT_HYPERVISOR_ACRN)
+	    m->type != INTEL_GVT_HYPERVISOR_XEN)
 		return -EINVAL;
 
 	/* Get a reference for device model module */
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/gvt.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/gvt.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/gvt.h
@@ -49,11 +49,9 @@
 #include "fb_decoder.h"
 #include "dmabuf.h"
 #include "page_track.h"
-#include "display/intel_display_types.h"
 
 #define GVT_MAX_VGPU 8
 
-#define GVT_CURSOR_BLOCKS 8
 struct intel_gvt_host {
 	struct device *dev;
 	bool initialized;
@@ -68,7 +66,6 @@ struct intel_gvt_device_info {
 	u32 max_support_vgpus;
 	u32 cfg_space_size;
 	u32 mmio_size;
-	u32 mmio_size_order;
 	u32 mmio_bar;
 	unsigned long msi_cap_offset;
 	u32 gtt_start_offset;
@@ -78,24 +75,12 @@ struct intel_gvt_device_info {
 	u32 max_surface_size;
 };
 
-struct gvt_gop_info {
-	unsigned int fb_base;
-	unsigned int width;
-	unsigned int height;
-	unsigned int pitch;
-	unsigned int Bpp;
-	unsigned int size;
-};
-
 /* GM resources owned by a vGPU */
 struct intel_vgpu_gm {
 	u64 aperture_sz;
 	u64 hidden_sz;
 	struct drm_mm_node low_gm_node;
 	struct drm_mm_node high_gm_node;
-	struct page **gop_fb_pages;
-	struct gvt_gop_info gop;
-	u32 gop_fb_size;
 };
 
 #define INTEL_GVT_MAX_NUM_FENCES 32
@@ -109,7 +94,6 @@ struct intel_vgpu_fence {
 
 struct intel_vgpu_mmio {
 	void *vreg;
-	struct gvt_shared_page *shared_page;
 };
 
 #define INTEL_GVT_MAX_BAR_NUM 4
@@ -132,9 +116,6 @@ struct intel_vgpu_irq {
 		       INTEL_GVT_EVENT_MAX);
 };
 
-/* ToDo: GOP_FB_BASE from kernel parameter */
-#define GOP_FB_BASE	0xDF000000
-
 struct intel_vgpu_opregion {
 	bool mapped;
 	void *va;
@@ -248,9 +229,6 @@ struct intel_vgpu {
 	struct completion vblank_done;
 
 	u32 scan_nonprivbb;
-
-	unsigned long long *cached_guest_entry;
-	bool ge_cache_enable;
 };
 
 /* validating GM healthy status*/
@@ -320,18 +298,6 @@ struct intel_vgpu_type {
 	enum intel_vgpu_edid resolution;
 };
 
-struct intel_gvt_pipe_info {
-	enum pipe pipe_num;
-	int owner;
-	struct intel_gvt *gvt;
-	struct work_struct vblank_work;
-	int plane_owner[I915_MAX_PLANES];
-	int scaler_owner[SKL_NUM_SCALERS];
-	struct skl_ddb_entry plane_ddb_y[I915_MAX_PLANES];
-	struct skl_ddb_entry plane_ddb_uv[I915_MAX_PLANES];
-	struct skl_ddb_entry pipe_ddb;
-};
-
 struct intel_gvt {
 	/* GVT scope lock, protect GVT itself, and all resource currently
 	 * not yet protected by special locks(vgpu and scheduler lock).
@@ -365,8 +331,6 @@ struct intel_gvt {
 	 */
 	unsigned long service_request;
 
-	struct intel_gvt_pipe_info pipe_info[I915_MAX_PIPES];
-
 	struct {
 		struct engine_mmio *mmio;
 		int ctx_mmio_count[I915_NUM_ENGINES];
@@ -377,10 +341,6 @@ struct intel_gvt {
 	} engine_mmio_list;
 
 	struct dentry *debugfs_root;
-	struct work_struct active_hp_work;
-
-	void *intel_gvt_vreg_pool[GVT_MAX_VGPU];
-	bool intel_gvt_vreg_allocated[GVT_MAX_VGPU];
 };
 
 static inline struct intel_gvt *to_gvt(struct drm_i915_private *i915)
@@ -495,11 +455,6 @@ void intel_vgpu_write_fence(struct intel
 	idr_for_each_entry((&(gvt)->vgpu_idr), (vgpu), (id)) \
 		for_each_if(vgpu->active)
 
-#define for_each_universal_scaler(__dev_priv, __pipe, __s)		\
-	for ((__s) = 0;							\
-	     (__s) < RUNTIME_INFO(__dev_priv)->num_scalers[(__pipe)] + 1; \
-	     (__s)++)
-
 static inline void intel_vgpu_write_pci_bar(struct intel_vgpu *vgpu,
 					    u32 offset, u32 val, bool low)
 {
@@ -572,8 +527,6 @@ void intel_vgpu_init_cfg_space(struct in
 		bool primary);
 void intel_vgpu_reset_cfg_space(struct intel_vgpu *vgpu);
 
-int set_pvmmio(struct intel_vgpu *vgpu, bool map);
-
 int intel_vgpu_emulate_cfg_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes);
 
@@ -592,7 +545,6 @@ static inline u64 intel_vgpu_get_bar_gpa
 void intel_vgpu_clean_opregion(struct intel_vgpu *vgpu);
 int intel_vgpu_init_opregion(struct intel_vgpu *vgpu);
 int intel_vgpu_opregion_base_write_handler(struct intel_vgpu *vgpu, u32 gpa);
-int map_vgpu_opregion(struct intel_vgpu *vgpu, bool map);
 
 int intel_vgpu_emulate_opregion_request(struct intel_vgpu *vgpu, u32 swsci);
 void populate_pvinfo_page(struct intel_vgpu *vgpu);
@@ -627,7 +579,6 @@ struct intel_gvt_ops {
 	void (*emulate_hotplug)(struct intel_vgpu *vgpu, bool connected);
 };
 
-void intel_gvt_allocate_ddb(struct intel_gvt *gvt, unsigned int active_crtcs);
 
 enum {
 	GVT_FAILSAFE_UNSUPPORTED_GUEST,
@@ -740,8 +691,6 @@ void intel_gvt_debugfs_remove_vgpu(struc
 void intel_gvt_debugfs_init(struct intel_gvt *gvt);
 void intel_gvt_debugfs_clean(struct intel_gvt *gvt);
 
-void *intel_gvt_allocate_vreg(struct intel_vgpu *vgpu);
-void intel_gvt_free_vreg(struct intel_vgpu *vgpu);
 
 #include "trace.h"
 #include "mpt.h"
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/handlers.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/handlers.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/handlers.c
@@ -37,8 +37,6 @@
  */
 
 #include "i915_drv.h"
-#include "display/intel_display.h"
-#include "display/intel_display_types.h"
 #include "gvt.h"
 #include "i915_pvinfo.h"
 
@@ -415,9 +413,27 @@ static int lcpll_ctl_mmio_write(struct i
 	return 0;
 }
 
-static int mmio_write_empty(struct intel_vgpu *vgpu, unsigned int offset,
+static int dpy_reg_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
+	switch (offset) {
+	case 0xe651c:
+	case 0xe661c:
+	case 0xe671c:
+	case 0xe681c:
+		vgpu_vreg(vgpu, offset) = 1 << 17;
+		break;
+	case 0xe6c04:
+		vgpu_vreg(vgpu, offset) = 0x3;
+		break;
+	case 0xe6e1c:
+		vgpu_vreg(vgpu, offset) = 0x2f << 16;
+		break;
+	default:
+		return -EINVAL;
+	}
+
+	read_vreg(vgpu, offset, p_data, bytes);
 	return 0;
 }
 
@@ -425,21 +441,18 @@ static int pipeconf_mmio_write(struct in
 		void *p_data, unsigned int bytes)
 {
 	u32 data;
-	unsigned int pipe = SKL_PLANE_REG_TO_PIPE(offset);
-	struct intel_crtc *crtc = intel_get_crtc_for_pipe(
-		vgpu->gvt->dev_priv, pipe);
 
 	write_vreg(vgpu, offset, p_data, bytes);
 	data = vgpu_vreg(vgpu, offset);
 
-	if (data & PIPECONF_ENABLE) {
+	if (data & PIPECONF_ENABLE)
 		vgpu_vreg(vgpu, offset) |= I965_PIPECONF_ACTIVE;
-		if (crtc)
-			drm_crtc_vblank_get(&crtc->base);
-	} else {
+	else
 		vgpu_vreg(vgpu, offset) &= ~I965_PIPECONF_ACTIVE;
-	}
-
+	/* vgpu_lock already hold by emulate mmio r/w */
+	mutex_unlock(&vgpu->vgpu_lock);
+	intel_gvt_check_vblank_emulation(vgpu->gvt);
+	mutex_lock(&vgpu->vgpu_lock);
 	return 0;
 }
 
@@ -520,15 +533,6 @@ static int force_nonpriv_write(struct in
 	return 0;
 }
 
-static int pipe_dsl_mmio_read(struct intel_vgpu *vgpu,
-		unsigned int offset, void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-
-	vgpu_vreg(vgpu, offset) = I915_READ(_MMIO(offset));
-	return intel_vgpu_default_mmio_read(vgpu, offset, p_data, bytes);
-}
-
 static int ddi_buf_ctl_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
@@ -1170,11 +1174,13 @@ static int sbi_ctl_mmio_write(struct int
 	return 0;
 }
 
+#define _vgtif_reg(x) \
+	(VGT_PVINFO_PAGE + offsetof(struct vgt_if, x))
+
 static int pvinfo_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
 	bool invalid_read = false;
-	int ret = 0;
 
 	read_vreg(vgpu, offset, p_data, bytes);
 
@@ -1189,33 +1195,8 @@ static int pvinfo_mmio_read(struct intel
 			_vgtif_reg(avail_rs.fence_num) + 4)
 			invalid_read = true;
 		break;
-	case _vgtif_reg(pv_mmio):
-	/* a remap happens from guest mmio read operation, the target reg offset
-	 * is in the first DWORD of shared_page.
-	 */
-	{
-		u32 reg = vgpu->mmio.shared_page->reg_addr;
-		struct intel_gvt_mmio_info *mmio;
-
-		mmio = find_mmio_info(vgpu->gvt, rounddown(reg, 4));
-		if (mmio)
-			ret = mmio->read(vgpu, reg, p_data, bytes);
-		else
-			ret = intel_vgpu_default_mmio_read(vgpu, reg, p_data,
-					bytes);
-		break;
-	}
-
 	case 0x78010:	/* vgt_caps */
 	case 0x7881c:
-	case _vgtif_reg(scaler_owned):
-	case _vgtif_reg(enable_pvmmio):
-		break;
-	case _vgtif_reg(gop.fb_base) ... _vgtif_reg(gop.size):
-		gvt_vgpu_err("pvinfo read gop: [%x:%x] = %x\n",
-				offset, bytes, *(u32 *)p_data);
-		if (offset + bytes > _vgtif_reg(gop.size) + 4)
-			invalid_read = true;
 		break;
 	default:
 		invalid_read = true;
@@ -1225,7 +1206,7 @@ static int pvinfo_mmio_read(struct intel
 		gvt_vgpu_err("invalid pvinfo read: [%x:%x] = %x\n",
 				offset, bytes, *(u32 *)p_data);
 	vgpu->pv_notified = true;
-	return ret;
+	return 0;
 }
 
 static int handle_g2v_notification(struct intel_vgpu *vgpu, int notification)
@@ -1246,18 +1227,10 @@ static int handle_g2v_notification(struc
 	case VGT_G2V_PPGTT_L3_PAGE_TABLE_DESTROY:
 	case VGT_G2V_PPGTT_L4_PAGE_TABLE_DESTROY:
 		return intel_vgpu_put_ppgtt_mm(vgpu, pdps);
-	case VGT_G2V_PPGTT_L4_ALLOC:
-		return intel_vgpu_g2v_pv_ppgtt_alloc_4lvl(vgpu, 4);
-	case VGT_G2V_PPGTT_L4_INSERT:
-		return intel_vgpu_g2v_pv_ppgtt_insert_4lvl(vgpu, 4);
-	case VGT_G2V_PPGTT_L4_CLEAR:
-		return intel_vgpu_g2v_pv_ppgtt_clear_4lvl(vgpu, 4);
 	case VGT_G2V_EXECLIST_CONTEXT_CREATE:
 	case VGT_G2V_EXECLIST_CONTEXT_DESTROY:
 	case 1:	/* Remove this in guest driver. */
 		break;
-	case VGT_G2V_GOP_SETUP:
-		return intel_vgpu_g2v_setup_gop(vgpu);
 	default:
 		gvt_vgpu_err("Invalid PV notification %d\n", notification);
 	}
@@ -1281,26 +1254,6 @@ static int send_display_ready_uevent(str
 	return kobject_uevent_env(kobj, KOBJ_ADD, env);
 }
 
-#define INTEL_GVT_PCI_BAR_GTTMMIO 0
-int set_pvmmio(struct intel_vgpu *vgpu, bool map)
-{
-	u64 start, end;
-	u64 val;
-	int ret;
-
-	val = vgpu_cfg_space(vgpu)[PCI_BASE_ADDRESS_0];
-	if (val & PCI_BASE_ADDRESS_MEM_TYPE_64)
-		start = *(u64 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_0);
-	else
-		start = *(u32 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_0);
-
-	start &= ~GENMASK(3, 0);
-	end = start + vgpu->cfg_space.bar[INTEL_GVT_PCI_BAR_GTTMMIO].size - 1;
-
-	ret = intel_gvt_hypervisor_set_pvmmio(vgpu, start, end, map);
-	return ret;
-}
-
 static int pvinfo_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
@@ -1314,18 +1267,6 @@ static int pvinfo_mmio_write(struct inte
 	case _vgtif_reg(g2v_notify):
 		handle_g2v_notification(vgpu, data);
 		break;
-	case _vgtif_reg(enable_pvmmio):
-		if (i915_modparams.enable_pvmmio) {
-			vgpu_vreg(vgpu, offset) = data &
-				i915_modparams.enable_pvmmio;
-			if (set_pvmmio(vgpu, !!vgpu_vreg(vgpu, offset))) {
-				vgpu_vreg(vgpu, offset) = 0;
-				break;
-			}
-		} else {
-			vgpu_vreg(vgpu, offset) = 0;
-		}
-		break;
 	/* add xhot and yhot to handled list to avoid error log */
 	case _vgtif_reg(cursor_x_hot):
 	case _vgtif_reg(cursor_y_hot):
@@ -1357,6 +1298,22 @@ static int pvinfo_mmio_write(struct inte
 	return 0;
 }
 
+static int pf_write(struct intel_vgpu *vgpu,
+		unsigned int offset, void *p_data, unsigned int bytes)
+{
+	u32 val = *(u32 *)p_data;
+
+	if ((offset == _PS_1A_CTRL || offset == _PS_2A_CTRL ||
+	   offset == _PS_1B_CTRL || offset == _PS_2B_CTRL ||
+	   offset == _PS_1C_CTRL) && (val & PS_PLANE_SEL_MASK) != 0) {
+		WARN_ONCE(true, "VM(%d): guest is trying to scaling a plane\n",
+			  vgpu->id);
+		return 0;
+	}
+
+	return intel_vgpu_default_mmio_write(vgpu, offset, p_data, bytes);
+}
+
 static int power_well_ctl_mmio_write(struct intel_vgpu *vgpu,
 		unsigned int offset, void *p_data, unsigned int bytes)
 {
@@ -1713,7 +1670,6 @@ static int elsp_mmio_write(struct intel_
 	int ring_id = intel_gvt_render_mmio_to_ring_id(vgpu->gvt, offset);
 	struct intel_vgpu_execlist *execlist;
 	u32 data = *(u32 *)p_data;
-	u32 *elsp_data = vgpu->mmio.shared_page->elsp_data;
 	int ret = 0;
 
 	if (WARN_ON(ring_id < 0 || ring_id >= I915_NUM_ENGINES))
@@ -1721,23 +1677,16 @@ static int elsp_mmio_write(struct intel_
 
 	execlist = &vgpu->submission.execlist[ring_id];
 
-	if (VGPU_PVMMIO(vgpu) & PVMMIO_ELSP_SUBMIT) {
-		execlist->elsp_dwords.data[3] = elsp_data[0];
-		execlist->elsp_dwords.data[2] = elsp_data[1];
-		execlist->elsp_dwords.data[1] = elsp_data[2];
-		execlist->elsp_dwords.data[0] = data;
+	execlist->elsp_dwords.data[3 - execlist->elsp_dwords.index] = data;
+	if (execlist->elsp_dwords.index == 3) {
 		ret = intel_vgpu_submit_execlist(vgpu, ring_id);
-	} else {
-		execlist->elsp_dwords.data[3 - execlist->elsp_dwords.index] = data;
-		if (execlist->elsp_dwords.index == 3)
-			ret = intel_vgpu_submit_execlist(vgpu, ring_id);
-		++execlist->elsp_dwords.index;
-		execlist->elsp_dwords.index &= 0x3;
+		if (ret)
+			gvt_vgpu_err("fail submit workload on ring %d\n",
+				ring_id);
 	}
 
-	if (ret)
-		gvt_vgpu_err("fail submit workload on ring %d\n", ring_id);
-
+	++execlist->elsp_dwords.index;
+	execlist->elsp_dwords.index &= 0x3;
 	return ret;
 }
 
@@ -2010,9 +1959,9 @@ static int init_generic_mmio_info(struct
 	MMIO_D(_MMIO(0xc4040), D_ALL);
 	MMIO_D(DERRMR, D_ALL);
 
-	MMIO_DH(PIPEDSL(PIPE_A), D_ALL, pipe_dsl_mmio_read, NULL);
-	MMIO_DH(PIPEDSL(PIPE_B), D_ALL, pipe_dsl_mmio_read, NULL);
-	MMIO_DH(PIPEDSL(PIPE_C), D_ALL, pipe_dsl_mmio_read, NULL);
+	MMIO_D(PIPEDSL(PIPE_A), D_ALL);
+	MMIO_D(PIPEDSL(PIPE_B), D_ALL);
+	MMIO_D(PIPEDSL(PIPE_C), D_ALL);
 	MMIO_D(PIPEDSL(_PIPE_EDP), D_ALL);
 
 	MMIO_DH(PIPECONF(PIPE_A), D_ALL, NULL, pipeconf_mmio_write);
@@ -2035,106 +1984,106 @@ static int init_generic_mmio_info(struct
 	MMIO_D(PIPE_FRMCOUNT_G4X(PIPE_C), D_ALL);
 	MMIO_D(PIPE_FRMCOUNT_G4X(_PIPE_EDP), D_ALL);
 
-	MMIO_D(CURCNTR(PIPE_A), D_BDW);
-	MMIO_D(CURCNTR(PIPE_B), D_BDW);
-	MMIO_D(CURCNTR(PIPE_C), D_BDW);
-
-	MMIO_D(CURPOS(PIPE_A), D_BDW);
-	MMIO_D(CURPOS(PIPE_B), D_BDW);
-	MMIO_D(CURPOS(PIPE_C), D_BDW);
-
-	MMIO_D(CURBASE(PIPE_A), D_BDW);
-	MMIO_D(CURBASE(PIPE_B), D_BDW);
-	MMIO_D(CURBASE(PIPE_C), D_BDW);
-
-	MMIO_D(CUR_FBC_CTL(PIPE_A), D_BDW);
-	MMIO_D(CUR_FBC_CTL(PIPE_B), D_BDW);
-	MMIO_D(CUR_FBC_CTL(PIPE_C), D_BDW);
-
-	MMIO_D(CURSURFLIVE(PIPE_A), D_BDW);
-	MMIO_D(CURSURFLIVE(PIPE_B), D_BDW);
-	MMIO_D(CURSURFLIVE(PIPE_C), D_BDW);
+	MMIO_D(CURCNTR(PIPE_A), D_ALL);
+	MMIO_D(CURCNTR(PIPE_B), D_ALL);
+	MMIO_D(CURCNTR(PIPE_C), D_ALL);
+
+	MMIO_D(CURPOS(PIPE_A), D_ALL);
+	MMIO_D(CURPOS(PIPE_B), D_ALL);
+	MMIO_D(CURPOS(PIPE_C), D_ALL);
+
+	MMIO_D(CURBASE(PIPE_A), D_ALL);
+	MMIO_D(CURBASE(PIPE_B), D_ALL);
+	MMIO_D(CURBASE(PIPE_C), D_ALL);
+
+	MMIO_D(CUR_FBC_CTL(PIPE_A), D_ALL);
+	MMIO_D(CUR_FBC_CTL(PIPE_B), D_ALL);
+	MMIO_D(CUR_FBC_CTL(PIPE_C), D_ALL);
+
+	MMIO_D(_MMIO(0x700ac), D_ALL);
+	MMIO_D(_MMIO(0x710ac), D_ALL);
+	MMIO_D(_MMIO(0x720ac), D_ALL);
 
 	MMIO_D(_MMIO(0x70090), D_ALL);
 	MMIO_D(_MMIO(0x70094), D_ALL);
 	MMIO_D(_MMIO(0x70098), D_ALL);
 	MMIO_D(_MMIO(0x7009c), D_ALL);
 
-	MMIO_D(DSPCNTR(PIPE_A), D_BDW);
-	MMIO_D(DSPADDR(PIPE_A), D_BDW);
-	MMIO_D(DSPSTRIDE(PIPE_A), D_BDW);
-	MMIO_D(DSPPOS(PIPE_A), D_BDW);
-	MMIO_D(DSPSIZE(PIPE_A), D_BDW);
-	MMIO_DH(DSPSURF(PIPE_A), D_BDW, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_A), D_BDW);
-	MMIO_D(DSPSURFLIVE(PIPE_A), D_BDW);
+	MMIO_D(DSPCNTR(PIPE_A), D_ALL);
+	MMIO_D(DSPADDR(PIPE_A), D_ALL);
+	MMIO_D(DSPSTRIDE(PIPE_A), D_ALL);
+	MMIO_D(DSPPOS(PIPE_A), D_ALL);
+	MMIO_D(DSPSIZE(PIPE_A), D_ALL);
+	MMIO_DH(DSPSURF(PIPE_A), D_ALL, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_A), D_ALL);
+	MMIO_D(DSPSURFLIVE(PIPE_A), D_ALL);
 	MMIO_DH(REG_50080(PIPE_A, PLANE_PRIMARY), D_ALL, NULL,
 		reg50080_mmio_write);
 
-	MMIO_D(DSPCNTR(PIPE_B), D_BDW);
-	MMIO_D(DSPADDR(PIPE_B), D_BDW);
-	MMIO_D(DSPSTRIDE(PIPE_B), D_BDW);
-	MMIO_D(DSPPOS(PIPE_B), D_BDW);
-	MMIO_D(DSPSIZE(PIPE_B), D_BDW);
-	MMIO_DH(DSPSURF(PIPE_B), D_BDW, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_B), D_BDW);
-	MMIO_D(DSPSURFLIVE(PIPE_B), D_BDW);
+	MMIO_D(DSPCNTR(PIPE_B), D_ALL);
+	MMIO_D(DSPADDR(PIPE_B), D_ALL);
+	MMIO_D(DSPSTRIDE(PIPE_B), D_ALL);
+	MMIO_D(DSPPOS(PIPE_B), D_ALL);
+	MMIO_D(DSPSIZE(PIPE_B), D_ALL);
+	MMIO_DH(DSPSURF(PIPE_B), D_ALL, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_B), D_ALL);
+	MMIO_D(DSPSURFLIVE(PIPE_B), D_ALL);
 	MMIO_DH(REG_50080(PIPE_B, PLANE_PRIMARY), D_ALL, NULL,
 		reg50080_mmio_write);
 
-	MMIO_D(DSPCNTR(PIPE_C), D_BDW);
-	MMIO_D(DSPADDR(PIPE_C), D_BDW);
-	MMIO_D(DSPSTRIDE(PIPE_C), D_BDW);
-	MMIO_D(DSPPOS(PIPE_C), D_BDW);
-	MMIO_D(DSPSIZE(PIPE_C), D_BDW);
-	MMIO_DH(DSPSURF(PIPE_C), D_BDW, NULL, pri_surf_mmio_write);
-	MMIO_D(DSPOFFSET(PIPE_C), D_BDW);
-	MMIO_D(DSPSURFLIVE(PIPE_C), D_BDW);
+	MMIO_D(DSPCNTR(PIPE_C), D_ALL);
+	MMIO_D(DSPADDR(PIPE_C), D_ALL);
+	MMIO_D(DSPSTRIDE(PIPE_C), D_ALL);
+	MMIO_D(DSPPOS(PIPE_C), D_ALL);
+	MMIO_D(DSPSIZE(PIPE_C), D_ALL);
+	MMIO_DH(DSPSURF(PIPE_C), D_ALL, NULL, pri_surf_mmio_write);
+	MMIO_D(DSPOFFSET(PIPE_C), D_ALL);
+	MMIO_D(DSPSURFLIVE(PIPE_C), D_ALL);
 	MMIO_DH(REG_50080(PIPE_C, PLANE_PRIMARY), D_ALL, NULL,
 		reg50080_mmio_write);
 
-	MMIO_D(SPRCTL(PIPE_A), D_BDW);
-	MMIO_D(SPRLINOFF(PIPE_A), D_BDW);
-	MMIO_D(SPRSTRIDE(PIPE_A), D_BDW);
-	MMIO_D(SPRPOS(PIPE_A), D_BDW);
-	MMIO_D(SPRSIZE(PIPE_A), D_BDW);
-	MMIO_D(SPRKEYVAL(PIPE_A), D_BDW);
-	MMIO_D(SPRKEYMSK(PIPE_A), D_BDW);
-	MMIO_DH(SPRSURF(PIPE_A), D_BDW, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_A), D_BDW);
-	MMIO_D(SPROFFSET(PIPE_A), D_BDW);
-	MMIO_D(SPRSCALE(PIPE_A), D_BDW);
-	MMIO_D(SPRSURFLIVE(PIPE_A), D_BDW);
+	MMIO_D(SPRCTL(PIPE_A), D_ALL);
+	MMIO_D(SPRLINOFF(PIPE_A), D_ALL);
+	MMIO_D(SPRSTRIDE(PIPE_A), D_ALL);
+	MMIO_D(SPRPOS(PIPE_A), D_ALL);
+	MMIO_D(SPRSIZE(PIPE_A), D_ALL);
+	MMIO_D(SPRKEYVAL(PIPE_A), D_ALL);
+	MMIO_D(SPRKEYMSK(PIPE_A), D_ALL);
+	MMIO_DH(SPRSURF(PIPE_A), D_ALL, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_A), D_ALL);
+	MMIO_D(SPROFFSET(PIPE_A), D_ALL);
+	MMIO_D(SPRSCALE(PIPE_A), D_ALL);
+	MMIO_D(SPRSURFLIVE(PIPE_A), D_ALL);
 	MMIO_DH(REG_50080(PIPE_A, PLANE_SPRITE0), D_ALL, NULL,
 		reg50080_mmio_write);
 
-	MMIO_D(SPRCTL(PIPE_B), D_BDW);
-	MMIO_D(SPRLINOFF(PIPE_B), D_BDW);
-	MMIO_D(SPRSTRIDE(PIPE_B), D_BDW);
-	MMIO_D(SPRPOS(PIPE_B), D_BDW);
-	MMIO_D(SPRSIZE(PIPE_B), D_BDW);
-	MMIO_D(SPRKEYVAL(PIPE_B), D_BDW);
-	MMIO_D(SPRKEYMSK(PIPE_B), D_BDW);
-	MMIO_DH(SPRSURF(PIPE_B), D_BDW, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_B), D_BDW);
-	MMIO_D(SPROFFSET(PIPE_B), D_BDW);
-	MMIO_D(SPRSCALE(PIPE_B), D_BDW);
-	MMIO_D(SPRSURFLIVE(PIPE_B), D_BDW);
+	MMIO_D(SPRCTL(PIPE_B), D_ALL);
+	MMIO_D(SPRLINOFF(PIPE_B), D_ALL);
+	MMIO_D(SPRSTRIDE(PIPE_B), D_ALL);
+	MMIO_D(SPRPOS(PIPE_B), D_ALL);
+	MMIO_D(SPRSIZE(PIPE_B), D_ALL);
+	MMIO_D(SPRKEYVAL(PIPE_B), D_ALL);
+	MMIO_D(SPRKEYMSK(PIPE_B), D_ALL);
+	MMIO_DH(SPRSURF(PIPE_B), D_ALL, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_B), D_ALL);
+	MMIO_D(SPROFFSET(PIPE_B), D_ALL);
+	MMIO_D(SPRSCALE(PIPE_B), D_ALL);
+	MMIO_D(SPRSURFLIVE(PIPE_B), D_ALL);
 	MMIO_DH(REG_50080(PIPE_B, PLANE_SPRITE0), D_ALL, NULL,
 		reg50080_mmio_write);
 
-	MMIO_D(SPRCTL(PIPE_C), D_BDW);
-	MMIO_D(SPRLINOFF(PIPE_C), D_BDW);
-	MMIO_D(SPRSTRIDE(PIPE_C), D_BDW);
-	MMIO_D(SPRPOS(PIPE_C), D_BDW);
-	MMIO_D(SPRSIZE(PIPE_C), D_BDW);
-	MMIO_D(SPRKEYVAL(PIPE_C), D_BDW);
-	MMIO_D(SPRKEYMSK(PIPE_C), D_BDW);
-	MMIO_DH(SPRSURF(PIPE_C), D_BDW, NULL, spr_surf_mmio_write);
-	MMIO_D(SPRKEYMAX(PIPE_C), D_BDW);
-	MMIO_D(SPROFFSET(PIPE_C), D_BDW);
-	MMIO_D(SPRSCALE(PIPE_C), D_BDW);
-	MMIO_D(SPRSURFLIVE(PIPE_C), D_BDW);
+	MMIO_D(SPRCTL(PIPE_C), D_ALL);
+	MMIO_D(SPRLINOFF(PIPE_C), D_ALL);
+	MMIO_D(SPRSTRIDE(PIPE_C), D_ALL);
+	MMIO_D(SPRPOS(PIPE_C), D_ALL);
+	MMIO_D(SPRSIZE(PIPE_C), D_ALL);
+	MMIO_D(SPRKEYVAL(PIPE_C), D_ALL);
+	MMIO_D(SPRKEYMSK(PIPE_C), D_ALL);
+	MMIO_DH(SPRSURF(PIPE_C), D_ALL, NULL, spr_surf_mmio_write);
+	MMIO_D(SPRKEYMAX(PIPE_C), D_ALL);
+	MMIO_D(SPROFFSET(PIPE_C), D_ALL);
+	MMIO_D(SPRSCALE(PIPE_C), D_ALL);
+	MMIO_D(SPRSURFLIVE(PIPE_C), D_ALL);
 	MMIO_DH(REG_50080(PIPE_C, PLANE_SPRITE0), D_ALL, NULL,
 		reg50080_mmio_write);
 
@@ -2342,12 +2291,12 @@ static int init_generic_mmio_info(struct
 	MMIO_D(PCH_PP_ON_DELAYS, D_ALL);
 	MMIO_D(PCH_PP_OFF_DELAYS, D_ALL);
 
-	MMIO_DH(_MMIO(0xe651c), D_ALL, NULL, mmio_write_empty);
-	MMIO_DH(_MMIO(0xe661c), D_ALL, NULL, mmio_write_empty);
-	MMIO_DH(_MMIO(0xe671c), D_ALL, NULL, mmio_write_empty);
-	MMIO_DH(_MMIO(0xe681c), D_ALL, NULL, mmio_write_empty);
-	MMIO_DH(_MMIO(0xe6c04), D_ALL, NULL, mmio_write_empty);
-	MMIO_DH(_MMIO(0xe6e1c), D_ALL, NULL, mmio_write_empty);
+	MMIO_DH(_MMIO(0xe651c), D_ALL, dpy_reg_mmio_read, NULL);
+	MMIO_DH(_MMIO(0xe661c), D_ALL, dpy_reg_mmio_read, NULL);
+	MMIO_DH(_MMIO(0xe671c), D_ALL, dpy_reg_mmio_read, NULL);
+	MMIO_DH(_MMIO(0xe681c), D_ALL, dpy_reg_mmio_read, NULL);
+	MMIO_DH(_MMIO(0xe6c04), D_ALL, dpy_reg_mmio_read, NULL);
+	MMIO_DH(_MMIO(0xe6e1c), D_ALL, dpy_reg_mmio_read, NULL);
 
 	MMIO_RO(PCH_PORT_HOTPLUG, D_ALL, 0,
 		PORTA_HOTPLUG_STATUS_MASK
@@ -2914,145 +2863,6 @@ static int init_broadwell_mmio_info(stru
 	return 0;
 }
 
-static int skl_plane_surf_write(struct intel_vgpu *vgpu, unsigned int offset,
-		void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe = SKL_PLANE_REG_TO_PIPE(offset);
-	unsigned int plane = SKL_PLANE_REG_TO_PLANE(offset);
-	i915_reg_t reg_1ac = _MMIO(_REG_701AC(pipe, plane));
-	int flip_event = SKL_FLIP_EVENT(pipe, plane);
-
-	write_vreg(vgpu, offset, p_data, bytes);
-	vgpu_vreg_t(vgpu, reg_1ac) = vgpu_vreg(vgpu, offset);
-
-	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
-		(vgpu->gvt->pipe_info[pipe].plane_owner[plane] == vgpu->id) &&
-		(*(u32 *)p_data != 0)) {
-		I915_WRITE(_MMIO(offset), vgpu_vreg(vgpu, offset));
-	}
-
-	set_bit(flip_event, vgpu->irq.flip_done_event[pipe]);
-	return 0;
-}
-
-static int skl_plane_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
-		void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe = SKL_PLANE_REG_TO_PIPE(offset);
-	unsigned int plane = SKL_PLANE_REG_TO_PLANE(offset);
-
-	write_vreg(vgpu, offset, p_data, bytes);
-	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
-			(vgpu->gvt->pipe_info[pipe].plane_owner[plane] == vgpu->id)) {
-		uint32_t reg_value = vgpu_vreg(vgpu, offset);
-
-		if (offset == PLANE_CTL(pipe, plane).reg)
-			reg_value &= ~DISPPLANE_GAMMA_ENABLE;
-
-		I915_WRITE(_MMIO(offset), reg_value);
-	}
-	return 0;
-}
-
-static int skl_cursor_surf_write(struct intel_vgpu *vgpu, unsigned int offset,
-		void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe = SKL_PLANE_REG_TO_PIPE(offset);
-
-	write_vreg(vgpu, offset, p_data, bytes);
-	vgpu_vreg_t(vgpu, CURSURFLIVE(pipe)) = vgpu_vreg(vgpu, offset);
-
-	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
-			(vgpu->gvt->pipe_info[pipe].plane_owner[0] == vgpu->id)) {
-		/* Each pipe has a primary and a cursor plane. Here we use
-		 * primary plane's ownership to decide whether the vm has the
-		 * cursor.
-		 */
-		I915_WRITE(_MMIO(offset), vgpu_vreg(vgpu, offset));
-	}
-
-	return 0;
-}
-
-
-static int skl_cursor_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
-		void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe = SKL_PLANE_REG_TO_PIPE(offset);
-
-	write_vreg(vgpu, offset, p_data, bytes);
-	if ((vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE) &&
-			(vgpu->gvt->pipe_info[pipe].plane_owner[0] == vgpu->id)) {
-		uint32_t reg_value = vgpu_vreg(vgpu, offset);
-
-		if (offset == CURCNTR(pipe).reg)
-			reg_value &= ~MCURSOR_GAMMA_ENABLE;
-
-		I915_WRITE(_MMIO(offset), reg_value);
-	}
-
-	return 0;
-}
-
-#define MMIO_PIPES_SDH(prefix, plane, s, d, r, w) do { \
-	int pipe; \
-	for_each_pipe(dev_priv, pipe) \
-		MMIO_F(prefix(pipe, plane), s, 0, 0, 0, d, r, w); \
-} while (0)
-
-#define MMIO_PLANES_SDH(prefix, s, d, r, w) do { \
-	int pipe, plane; \
-	for_each_pipe(dev_priv, pipe) \
-		for_each_universal_plane(dev_priv, pipe, plane) \
-			MMIO_F(prefix(pipe, plane), s, 0, 0, 0, d, r, w); \
-} while (0)
-
-#define MMIO_PLANES_DH(prefix, d, r, w) \
-	MMIO_PLANES_SDH(prefix, 4, d, r, w)
-
-#define PLANE_WM_BASE(pipe, plane) _MMIO(_PLANE_WM_BASE(pipe, plane))
-
-static int skl_ps_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
-		void *p_data, unsigned int bytes)
-{
-	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
-	unsigned int pipe = SKL_PS_REG_TO_PIPE(offset);
-	unsigned int scaler = SKL_PS_REG_TO_SCALER(offset) - 1;
-
-	if (pipe >=  I915_MAX_PIPES || scaler >= SKL_NUM_SCALERS ||
-	    vgpu->gvt->pipe_info[pipe].scaler_owner[scaler] != vgpu->id) {
-		return 0;
-	}
-
-	if (!(vgpu_vreg_t(vgpu, PIPECONF(pipe)) & I965_PIPECONF_ACTIVE))
-		return 0;
-
-	if ((offset == _PS_1A_CTRL || offset == _PS_2A_CTRL ||
-	   offset == _PS_1B_CTRL || offset == _PS_2B_CTRL ||
-	   offset == _PS_1C_CTRL) && ((*(u32 *)p_data) & PS_SCALER_EN)) {
-		unsigned int plane;
-
-		if (SKL_PS_REG_VALUE_TO_PLANE(*(u32 *)p_data) == 0) {
-			gvt_vgpu_err("Unsupport crtc scaling for UOS\n");
-			return 0;
-		}
-		plane = SKL_PS_REG_VALUE_TO_PLANE(*(u32 *)p_data) - 1;
-		if (plane >= I915_MAX_PLANES ||
-		    vgpu->gvt->pipe_info[pipe].plane_owner[plane] != vgpu->id) {
-			gvt_vgpu_err("Unsupport plane %d scaling\n", plane);
-			return 0;
-		}
-	}
-
-	write_vreg(vgpu, offset, p_data, bytes);
-	I915_WRITE(_MMIO(offset), vgpu_vreg(vgpu, offset));
-	return 0;
-}
-
 static int init_skl_mmio_info(struct intel_gvt *gvt)
 {
 	struct drm_i915_private *dev_priv = gvt->dev_priv;
@@ -3098,82 +2908,127 @@ static int init_skl_mmio_info(struct int
 	MMIO_D(DPLL_CTRL2, D_SKL_PLUS);
 	MMIO_DH(DPLL_STATUS, D_SKL_PLUS, dpll_status_read, NULL);
 
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-
-	MMIO_DH(SKL_PS_CTRL(PIPE_A, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_A, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_B, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_B, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_C, 0), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-	MMIO_DH(SKL_PS_CTRL(PIPE_C, 1), D_SKL_PLUS, NULL, skl_ps_mmio_write);
-
-	MMIO_PLANES_DH(PLANE_CTL, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_STRIDE, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_POS, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_SIZE, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_KEYVAL, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_KEYMSK, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-
-	MMIO_PLANES_DH(PLANE_SURF, D_SKL_PLUS, NULL, skl_plane_surf_write);
-
-	MMIO_PLANES_DH(PLANE_KEYMAX, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_OFFSET, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_AUX_DIST, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_AUX_OFFSET, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-
-	MMIO_PLANES_SDH(PLANE_WM_BASE, 4 * 8, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_WM_TRANS, D_SKL_PLUS, NULL, skl_plane_mmio_write);
-	MMIO_PLANES_DH(PLANE_NV12_BUF_CFG, D_SKL_PLUS, NULL, NULL);
-	MMIO_PLANES_DH(PLANE_BUF_CFG, D_SKL_PLUS, NULL, NULL);
-
-
-	MMIO_DH(CURCNTR(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CURCNTR(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CURCNTR(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-
-	MMIO_DH(CURPOS(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CURPOS(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CURPOS(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-
-	MMIO_DH(CURBASE(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_surf_write);
-	MMIO_DH(CURBASE(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_surf_write);
-	MMIO_DH(CURBASE(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_surf_write);
-
-	MMIO_DH(CUR_FBC_CTL(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CUR_FBC_CTL(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CUR_FBC_CTL(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-
-	MMIO_DH(CURSURFLIVE(PIPE_A), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(CURSURFLIVE(PIPE_B), D_SKL_PLUS, NULL, NULL);
-	MMIO_DH(CURSURFLIVE(PIPE_C), D_SKL_PLUS, NULL, NULL);
-
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_POS(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
+
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_WIN_SZ(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
+
+	MMIO_DH(SKL_PS_CTRL(PIPE_A, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_CTRL(PIPE_A, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_CTRL(PIPE_B, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_CTRL(PIPE_B, 1), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_CTRL(PIPE_C, 0), D_SKL_PLUS, NULL, pf_write);
+	MMIO_DH(SKL_PS_CTRL(PIPE_C, 1), D_SKL_PLUS, NULL, pf_write);
+
+	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_A, 3), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_B, 3), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_BUF_CFG(PIPE_C, 3), D_SKL_PLUS, NULL, NULL);
 
 	MMIO_DH(CUR_BUF_CFG(PIPE_A), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_BUF_CFG(PIPE_B), D_SKL_PLUS, NULL, NULL);
 	MMIO_DH(CUR_BUF_CFG(PIPE_C), D_SKL_PLUS, NULL, NULL);
 
-	MMIO_F(CUR_WM(PIPE_A, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_F(CUR_WM(PIPE_B, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_F(CUR_WM(PIPE_C, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-
-	MMIO_DH(CUR_WM_TRANS(PIPE_A), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CUR_WM_TRANS(PIPE_B), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
-	MMIO_DH(CUR_WM_TRANS(PIPE_C), D_SKL_PLUS, NULL, skl_cursor_mmio_write);
+	MMIO_F(PLANE_WM(PIPE_A, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_A, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_A, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_F(PLANE_WM(PIPE_B, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_B, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_B, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_F(PLANE_WM(PIPE_C, 0, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_C, 1, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(PLANE_WM(PIPE_C, 2, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_F(CUR_WM(PIPE_A, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(CUR_WM(PIPE_B, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+	MMIO_F(CUR_WM(PIPE_C, 0), 4 * 8, 0, 0, 0, D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_WM_TRANS(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(CUR_WM_TRANS(PIPE_A), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(CUR_WM_TRANS(PIPE_B), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(CUR_WM_TRANS(PIPE_C), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_A, 3), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_B, 3), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 0), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 1), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 2), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(PLANE_NV12_BUF_CFG(PIPE_C, 3), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_A, 4)), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_B, 4)), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C0(PIPE_C, 4)), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_A, 4)), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_B, 4)), D_SKL_PLUS, NULL, NULL);
+
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 1)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 2)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 3)), D_SKL_PLUS, NULL, NULL);
+	MMIO_DH(_MMIO(_REG_701C4(PIPE_C, 4)), D_SKL_PLUS, NULL, NULL);
 
 	MMIO_D(_MMIO(_PLANE_CTL_3_A), D_SKL_PLUS);
 	MMIO_D(_MMIO(_PLANE_CTL_3_B), D_SKL_PLUS);
+	MMIO_D(_MMIO(0x72380), D_SKL_PLUS);
+	MMIO_D(_MMIO(0x7239c), D_SKL_PLUS);
 	MMIO_D(_MMIO(_PLANE_SURF_3_A), D_SKL_PLUS);
 
 	MMIO_D(CSR_SSP_BASE, D_SKL_PLUS);
@@ -3231,6 +3086,16 @@ static int init_skl_mmio_info(struct int
 	MMIO_D(_MMIO(0x71034), D_SKL_PLUS);
 	MMIO_D(_MMIO(0x72034), D_SKL_PLUS);
 
+	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_A)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_B)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYVAL_1(PIPE_C)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_A)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_B)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMAX_1(PIPE_C)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_A)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_B)), D_SKL_PLUS);
+	MMIO_D(_MMIO(_PLANE_KEYMSK_1(PIPE_C)), D_SKL_PLUS);
+
 	MMIO_D(_MMIO(0x44500), D_SKL_PLUS);
 #define CSFE_CHICKEN1_REG(base) _MMIO((base) + 0xD4)
 	MMIO_RING_DFH(CSFE_CHICKEN1_REG, D_SKL_PLUS, F_MODE_MASK | F_CMD_ACCESS,
@@ -3244,8 +3109,6 @@ static int init_skl_mmio_info(struct int
 	MMIO_D(GAMT_CHKN_BIT_REG, D_KBL | D_CFL);
 	MMIO_D(GEN9_CTX_PREEMPT_REG, D_SKL_PLUS);
 
-	MMIO_D(HUC_STATUS2, D_SKL_PLUS);
-
 	return 0;
 }
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/hypercall.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/hypercall.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/hypercall.h
@@ -36,7 +36,6 @@
 enum hypervisor_type {
 	INTEL_GVT_HYPERVISOR_XEN = 0,
 	INTEL_GVT_HYPERVISOR_KVM,
-	INTEL_GVT_HYPERVISOR_ACRN,
 };
 
 /*
@@ -67,8 +66,6 @@ struct intel_gvt_mpt {
 			      unsigned long mfn, unsigned int nr, bool map);
 	int (*set_trap_area)(unsigned long handle, u64 start, u64 end,
 			     bool map);
-	int (*set_pvmmio)(unsigned long handle, u64 start, u64 end,
-			     bool map);
 	int (*set_opregion)(void *vgpu);
 	int (*set_edid)(void *vgpu, int port_num);
 	int (*get_vfio_device)(void *vgpu);
@@ -77,6 +74,5 @@ struct intel_gvt_mpt {
 };
 
 extern struct intel_gvt_mpt xengt_mpt;
-extern struct intel_gvt_mpt acrn_gvt_mpt;
 
 #endif /* _GVT_HYPERCALL_H_ */
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/interrupt.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/interrupt.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/interrupt.c
@@ -69,7 +69,6 @@ static const char * const irq_name[INTEL
 	[VCS_PAGE_DIRECTORY_FAULT] = "Video page directory faults",
 	[VCS_AS_CONTEXT_SWITCH] = "Video AS Context Switch Interrupt",
 	[VCS2_MI_USER_INTERRUPT] = "VCS2 Video CS MI USER INTERRUPT",
-	[VCS2_CMD_STREAMER_ERR] = "VCS2 Video CS error interrupt",
 	[VCS2_MI_FLUSH_DW] = "VCS2 Video MI FLUSH DW notify",
 	[VCS2_AS_CONTEXT_SWITCH] = "VCS2 Context Switch Interrupt",
 
@@ -525,26 +524,21 @@ static void gen8_init_irq(
 
 	/* GEN8 interrupt GT0 events */
 	SET_BIT_INFO(irq, 0, RCS_MI_USER_INTERRUPT, INTEL_GVT_IRQ_INFO_GT0);
-	SET_BIT_INFO(irq, 3, RCS_CMD_STREAMER_ERR, INTEL_GVT_IRQ_INFO_GT0);
 	SET_BIT_INFO(irq, 4, RCS_PIPE_CONTROL, INTEL_GVT_IRQ_INFO_GT0);
 	SET_BIT_INFO(irq, 8, RCS_AS_CONTEXT_SWITCH, INTEL_GVT_IRQ_INFO_GT0);
 
 	SET_BIT_INFO(irq, 16, BCS_MI_USER_INTERRUPT, INTEL_GVT_IRQ_INFO_GT0);
-	SET_BIT_INFO(irq, 19, BCS_CMD_STREAMER_ERR, INTEL_GVT_IRQ_INFO_GT0);
 	SET_BIT_INFO(irq, 20, BCS_MI_FLUSH_DW, INTEL_GVT_IRQ_INFO_GT0);
 	SET_BIT_INFO(irq, 24, BCS_AS_CONTEXT_SWITCH, INTEL_GVT_IRQ_INFO_GT0);
 
 	/* GEN8 interrupt GT1 events */
 	SET_BIT_INFO(irq, 0, VCS_MI_USER_INTERRUPT, INTEL_GVT_IRQ_INFO_GT1);
-	SET_BIT_INFO(irq, 3, VCS_CMD_STREAMER_ERR, INTEL_GVT_IRQ_INFO_GT1);
 	SET_BIT_INFO(irq, 4, VCS_MI_FLUSH_DW, INTEL_GVT_IRQ_INFO_GT1);
 	SET_BIT_INFO(irq, 8, VCS_AS_CONTEXT_SWITCH, INTEL_GVT_IRQ_INFO_GT1);
 
 	if (HAS_ENGINE(gvt->dev_priv, VCS1)) {
 		SET_BIT_INFO(irq, 16, VCS2_MI_USER_INTERRUPT,
 			INTEL_GVT_IRQ_INFO_GT1);
-		SET_BIT_INFO(irq, 19, VCS2_CMD_STREAMER_ERR,
-				INTEL_GVT_IRQ_INFO_GT1);
 		SET_BIT_INFO(irq, 20, VCS2_MI_FLUSH_DW,
 			INTEL_GVT_IRQ_INFO_GT1);
 		SET_BIT_INFO(irq, 24, VCS2_AS_CONTEXT_SWITCH,
@@ -553,7 +547,6 @@ static void gen8_init_irq(
 
 	/* GEN8 interrupt GT3 events */
 	SET_BIT_INFO(irq, 0, VECS_MI_USER_INTERRUPT, INTEL_GVT_IRQ_INFO_GT3);
-	SET_BIT_INFO(irq, 3, VECS_CMD_STREAMER_ERR, INTEL_GVT_IRQ_INFO_GT3);
 	SET_BIT_INFO(irq, 4, VECS_MI_FLUSH_DW, INTEL_GVT_IRQ_INFO_GT3);
 	SET_BIT_INFO(irq, 8, VECS_AS_CONTEXT_SWITCH, INTEL_GVT_IRQ_INFO_GT3);
 
@@ -600,10 +593,6 @@ static void gen8_init_irq(
 		SET_BIT_INFO(irq, 4, SPRITE_A_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_A);
 		SET_BIT_INFO(irq, 4, SPRITE_B_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_B);
 		SET_BIT_INFO(irq, 4, SPRITE_C_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_C);
-
-		SET_BIT_INFO(irq, 5, PLANE_3_A_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_A);
-		SET_BIT_INFO(irq, 5, PLANE_3_B_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_B);
-		SET_BIT_INFO(irq, 5, PLANE_3_C_FLIP_DONE, INTEL_GVT_IRQ_INFO_DE_PIPE_C);
 	}
 
 	/* GEN8 interrupt PCU events */
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/interrupt.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/interrupt.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/interrupt.h
@@ -53,7 +53,6 @@ enum intel_gvt_event_type {
 	VCS_AS_CONTEXT_SWITCH,
 
 	VCS2_MI_USER_INTERRUPT,
-	VCS2_CMD_STREAMER_ERR,
 	VCS2_MI_FLUSH_DW,
 	VCS2_AS_CONTEXT_SWITCH,
 
@@ -65,7 +64,6 @@ enum intel_gvt_event_type {
 	BCS_AS_CONTEXT_SWITCH,
 
 	VECS_MI_USER_INTERRUPT,
-	VECS_CMD_STREAMER_ERR,
 	VECS_MI_FLUSH_DW,
 	VECS_AS_CONTEXT_SWITCH,
 
@@ -94,9 +92,6 @@ enum intel_gvt_event_type {
 	SPRITE_A_FLIP_DONE,
 	SPRITE_B_FLIP_DONE,
 	SPRITE_C_FLIP_DONE,
-	PLANE_3_A_FLIP_DONE,
-	PLANE_3_B_FLIP_DONE,
-	PLANE_3_C_FLIP_DONE,
 
 	PCU_THERMAL,
 	PCU_PCODE2DRIVER_MAILBOX,
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/mmio.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/mmio.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/mmio.c
@@ -236,7 +236,6 @@ void intel_vgpu_reset_mmio(struct intel_
 	struct intel_gvt *gvt = vgpu->gvt;
 	const struct intel_gvt_device_info *info = &gvt->device_info;
 	void  *mmio = gvt->firmware.mmio;
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
 
 	if (dmlr) {
 		memcpy(vgpu->mmio.vreg, mmio, info->mmio_size);
@@ -282,21 +281,6 @@ void intel_vgpu_reset_mmio(struct intel_
 		memcpy(vgpu->mmio.vreg, mmio, GVT_GEN8_MMIO_RESET_OFFSET);
 	}
 
-	/* below vreg init value are got from handler.c,
-	 * which won't change during vgpu life cycle
-	 */
-	vgpu_vreg(vgpu, 0xe651c) = 1 << 17;
-	vgpu_vreg(vgpu, 0xe661c) = 1 << 17;
-	vgpu_vreg(vgpu, 0xe671c) = 1 << 17;
-	vgpu_vreg(vgpu, 0xe681c) = 1 << 17;
-	vgpu_vreg(vgpu, 0xe6c04) = 3;
-	vgpu_vreg(vgpu, 0xe6e1c) = 0x2f << 16;
-
-	if (HAS_GT_UC(dev_priv)) {
-		mmio_hw_access_pre(dev_priv);
-		vgpu_vreg_t(vgpu, HUC_STATUS2) = I915_READ(HUC_STATUS2);
-		mmio_hw_access_post(dev_priv);
-	}
 }
 
 /**
@@ -308,20 +292,12 @@ void intel_vgpu_reset_mmio(struct intel_
  */
 int intel_vgpu_init_mmio(struct intel_vgpu *vgpu)
 {
-	BUILD_BUG_ON(sizeof(struct gvt_shared_page) != PAGE_SIZE);
+	const struct intel_gvt_device_info *info = &vgpu->gvt->device_info;
 
-	vgpu->mmio.vreg = intel_gvt_allocate_vreg(vgpu);
+	vgpu->mmio.vreg = vzalloc(info->mmio_size);
 	if (!vgpu->mmio.vreg)
 		return -ENOMEM;
 
-	vgpu->mmio.shared_page = (struct gvt_shared_page *) __get_free_pages(
-			GFP_KERNEL, 0);
-	if (!vgpu->mmio.shared_page) {
-		intel_gvt_free_vreg(vgpu);
-		vgpu->mmio.vreg = NULL;
-		return -ENOMEM;
-	}
-
 	intel_vgpu_reset_mmio(vgpu, true);
 
 	return 0;
@@ -334,7 +310,6 @@ int intel_vgpu_init_mmio(struct intel_vg
  */
 void intel_vgpu_clean_mmio(struct intel_vgpu *vgpu)
 {
-	intel_gvt_free_vreg(vgpu);
-	free_pages((unsigned long) vgpu->mmio.shared_page, 0);
-	vgpu->mmio.vreg = vgpu->mmio.shared_page = NULL;
+	vfree(vgpu->mmio.vreg);
+	vgpu->mmio.vreg = NULL;
 }
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/mpt.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/mpt.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/mpt.h
@@ -306,27 +306,6 @@ static inline int intel_gvt_hypervisor_s
 }
 
 /**
- * intel_gvt_hypervisor_set_pvmmio - Set the pvmmio area
- * @vgpu: a vGPU
- * @start: the beginning of the guest physical address region
- * @end: the end of the guest physical address region
- * @map: map or unmap
- *
- * Returns:
- * Zero on success, negative error code if failed.
- */
-static inline int intel_gvt_hypervisor_set_pvmmio(
-		struct intel_vgpu *vgpu, u64 start, u64 end, bool map)
-{
-	/* a MPT implementation could have MMIO trapped elsewhere */
-	if (!intel_gvt_host.mpt->set_pvmmio)
-		return -ENOENT;
-
-	return intel_gvt_host.mpt->set_pvmmio(vgpu->handle, start, end, map);
-}
-
-
-/**
  * intel_gvt_hypervisor_set_opregion - Set opregion for guest
  * @vgpu: a vGPU
  *
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/opregion.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/opregion.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/opregion.c
@@ -256,7 +256,7 @@ int intel_vgpu_init_opregion(struct inte
 	return 0;
 }
 
-int map_vgpu_opregion(struct intel_vgpu *vgpu, bool map)
+static int map_vgpu_opregion(struct intel_vgpu *vgpu, bool map)
 {
 	u64 mfn;
 	int i, ret;
@@ -268,10 +268,6 @@ int map_vgpu_opregion(struct intel_vgpu
 			gvt_vgpu_err("fail to get MFN from VA\n");
 			return -EINVAL;
 		}
-
-		gvt_dbg_dpy("Round[%d] gfn: %x ==> mfn: %llx \n", i,
-				vgpu_opregion(vgpu)->gfn[i], mfn);
-
 		ret = intel_gvt_hypervisor_map_gfn_to_mfn(vgpu,
 				vgpu_opregion(vgpu)->gfn[i],
 				mfn, 1, map);
@@ -321,19 +317,6 @@ int intel_vgpu_opregion_base_write_handl
 
 		ret = map_vgpu_opregion(vgpu, true);
 		break;
-	case INTEL_GVT_HYPERVISOR_ACRN:
-		if (gpa == 0) {
-			return 0;
-		}
-
-		if (vgpu_opregion(vgpu)->mapped)
-			map_vgpu_opregion(vgpu, false);
-
-		for (i = 0; i < INTEL_GVT_OPREGION_PAGES; i++)
-			vgpu_opregion(vgpu)->gfn[i] = (gpa >> PAGE_SHIFT) + i;
-
-		ret = map_vgpu_opregion(vgpu, true);
-		break;
 	default:
 		ret = -EINVAL;
 		gvt_vgpu_err("not supported hypervisor\n");
@@ -354,8 +337,7 @@ void intel_vgpu_clean_opregion(struct in
 	if (!vgpu_opregion(vgpu)->va)
 		return;
 
-	if (intel_gvt_host.hypervisor_type == INTEL_GVT_HYPERVISOR_XEN ||
-		intel_gvt_host.hypervisor_type == INTEL_GVT_HYPERVISOR_ACRN) {
+	if (intel_gvt_host.hypervisor_type == INTEL_GVT_HYPERVISOR_XEN) {
 		if (vgpu_opregion(vgpu)->mapped)
 			map_vgpu_opregion(vgpu, false);
 	} else if (intel_gvt_host.hypervisor_type == INTEL_GVT_HYPERVISOR_KVM) {
@@ -491,7 +473,6 @@ int intel_vgpu_emulate_opregion_request(
 
 	switch (intel_gvt_host.hypervisor_type) {
 	case INTEL_GVT_HYPERVISOR_XEN:
-	case INTEL_GVT_HYPERVISOR_ACRN:
 		scic = *((u32 *)vgpu_opregion(vgpu)->va +
 					INTEL_GVT_OPREGION_SCIC);
 		parm = *((u32 *)vgpu_opregion(vgpu)->va +
@@ -557,7 +538,6 @@ int intel_vgpu_emulate_opregion_request(
 out:
 	switch (intel_gvt_host.hypervisor_type) {
 	case INTEL_GVT_HYPERVISOR_XEN:
-	case INTEL_GVT_HYPERVISOR_ACRN:
 		*((u32 *)vgpu_opregion(vgpu)->va +
 					INTEL_GVT_OPREGION_SCIC) = scic;
 		*((u32 *)vgpu_opregion(vgpu)->va +
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/reg.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/reg.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/reg.h
@@ -57,14 +57,9 @@
 
 #define VGT_SPRSTRIDE(pipe)	_PIPE(pipe, _SPRA_STRIDE, _PLANE_STRIDE_2_B)
 
-#define _REG_701AC(pipe, plane) (0x701ac + pipe * 0x1000 + plane * 0x100)
+#define _REG_701C0(pipe, plane) (0x701c0 + pipe * 0x1000 + (plane - 1) * 0x100)
+#define _REG_701C4(pipe, plane) (0x701c4 + pipe * 0x1000 + (plane - 1) * 0x100)
 
-#define SKL_PS_REG_TO_PIPE(reg) (((reg) >> 11) & 0x3)
-#define SKL_PS_REG_TO_SCALER(reg) (((reg) >> 8) & 0x3)
-#define SKL_PS_REG_VALUE_TO_PLANE(val) (((val) >> 25) & 0x7)
-
-#define SKL_PLANE_REG_TO_PIPE(reg) (((reg) >> 12) & 0x3)
-#define SKL_PLANE_REG_TO_PLANE(reg) ((((reg) & 0xFFF) - 0x180) >> 8)
 #define SKL_FLIP_EVENT(pipe, plane) (PRIMARY_A_FLIP_DONE + (plane) * 3 + (pipe))
 
 #define PLANE_CTL_ASYNC_FLIP		(1 << 9)
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/sched_policy.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/sched_policy.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/sched_policy.c
@@ -465,11 +465,6 @@ void intel_vgpu_stop_schedule(struct int
 		scheduler->current_vgpu = NULL;
 	}
 
-	if (!i915_modparams.enable_context_restore) {
-		mutex_unlock(&vgpu->gvt->sched_lock);
-		return;
-	}
-
 	intel_runtime_pm_get(&dev_priv->runtime_pm);
 	spin_lock_bh(&scheduler->mmio_context_lock);
 	for (ring_id = 0; ring_id < I915_NUM_ENGINES; ring_id++) {
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/scheduler.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/scheduler.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -40,11 +40,8 @@
 #include "gt/intel_context.h"
 #include "gt/intel_ring.h"
 
-#include "gt/intel_rps.h"
-
 #include "i915_drv.h"
 #include "gvt.h"
-#include "intel_pm.h"
 
 #define RING_CTX_OFF(x) \
 	offsetof(struct execlist_ring_context, x)
@@ -227,22 +224,6 @@ static void save_ring_hw_state(struct in
 	vgpu_vreg(vgpu, i915_mmio_reg_offset(reg)) = I915_READ_FW(reg);
 }
 
-static void active_hp_work(struct work_struct *work)
-{
-	struct intel_gvt *gvt =
-		container_of(work, struct intel_gvt, active_hp_work);
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
-	struct intel_rps *rps = &dev_priv->gt.rps;
-	u8 freq = rps->rp0_freq;
-
-	if (IS_BROXTON(dev_priv))
-		freq = intel_freq_opcode(rps, 600);
-
-	if (READ_ONCE(rps->cur_freq) < freq) {
-		intel_rps_set(rps, freq);
-	}
-}
-
 static int shadow_context_status_change(struct notifier_block *nb,
 		unsigned long action, void *data)
 {
@@ -255,9 +236,6 @@ static int shadow_context_status_change(
 	unsigned long flags;
 
 	if (!is_gvt_request(req)) {
-		if (!i915_modparams.enable_context_restore)
-			return NOTIFY_OK;
-
 		spin_lock_irqsave(&scheduler->mmio_context_lock, flags);
 		if (action == INTEL_CONTEXT_SCHEDULE_IN &&
 		    scheduler->engine_owner[ring_id]) {
@@ -277,13 +255,6 @@ static int shadow_context_status_change(
 
 	switch (action) {
 	case INTEL_CONTEXT_SCHEDULE_IN:
-		if (!i915_modparams.enable_hp_work)
-			schedule_work(&gvt->active_hp_work);
-
-		if (!i915_modparams.enable_context_restore) {
-			atomic_set(&workload->shadow_ctx_active, 1);
-			break;
-		}
 		spin_lock_irqsave(&scheduler->mmio_context_lock, flags);
 		if (workload->vgpu != scheduler->engine_owner[ring_id]) {
 			/* Switch ring from host to vGPU or vGPU to vGPU. */
@@ -297,15 +268,11 @@ static int shadow_context_status_change(
 		atomic_set(&workload->shadow_ctx_active, 1);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_OUT:
-		if (i915_modparams.enable_context_restore) {
-			save_ring_hw_state(workload->vgpu, ring_id);
-		}
+		save_ring_hw_state(workload->vgpu, ring_id);
 		atomic_set(&workload->shadow_ctx_active, 0);
 		break;
 	case INTEL_CONTEXT_SCHEDULE_PREEMPTED:
-		if (i915_modparams.enable_context_restore) {
-			save_ring_hw_state(workload->vgpu, ring_id);
-		}
+		save_ring_hw_state(workload->vgpu, ring_id);
 		break;
 	default:
 		WARN_ON(1);
@@ -481,15 +448,6 @@ err_shadow:
 	return ret;
 }
 
-static int sanitize_priority(int priority)
-{
-	if (priority > I915_CONTEXT_MAX_USER_PRIORITY)
-		return I915_CONTEXT_MAX_USER_PRIORITY;
-	else if (priority < I915_CONTEXT_MIN_USER_PRIORITY)
-		return I915_CONTEXT_MIN_USER_PRIORITY;
-	return priority;
-}
-
 static void release_shadow_batch_buffer(struct intel_vgpu_workload *workload);
 
 static int prepare_shadow_batch_buffer(struct intel_vgpu_workload *workload)
@@ -688,14 +646,6 @@ static int prepare_workload(struct intel
 		goto err_unpin_mm;
 	}
 
-	/* we consider this as an workaround to avoid the situation that
-	 * PDP's not updated, and right now we only limit it to BXT platform
-	 * since it's not reported on the other platforms
-	 */
-	if (IS_BROXTON(vgpu->gvt->dev_priv)) {
-		gvt_emit_pdps(workload);
-	}
-
 	ret = copy_workload_to_ring_buffer(workload);
 	if (ret) {
 		gvt_vgpu_err("fail to generate request\n");
@@ -736,7 +686,6 @@ static int dispatch_workload(struct inte
 	struct i915_request *rq;
 	int ring_id = workload->ring_id;
 	int ret;
-	struct intel_vgpu_submission *s = &vgpu->submission;
 
 	gvt_dbg_sched("ring id %d prepare to dispatch workload %p\n",
 		ring_id, workload);
@@ -758,8 +707,6 @@ static int dispatch_workload(struct inte
 	}
 
 	ret = prepare_workload(workload);
-
-	workload->guilty_count = atomic_read(&workload->req->gem_context->guilty_count);
 out:
 	if (ret) {
 		/* We might still need to add request with
@@ -772,8 +719,6 @@ out:
 	if (!IS_ERR_OR_NULL(workload->req)) {
 		gvt_dbg_sched("ring id %d submit workload to i915 %p\n",
 				ring_id, workload->req);
-		i915_modparams.gvt_workload_priority = sanitize_priority(i915_modparams.gvt_workload_priority);
-		s->shadow[workload->ring_id]->gem_context->sched.priority = i915_modparams.gvt_workload_priority;
 		i915_request_add(workload->req);
 		workload->dispatched = true;
 	}
@@ -999,9 +944,6 @@ static void complete_current_workload(st
 
 	list_del_init(&workload->list);
 
-	if (workload->status == -EIO)
-		intel_vgpu_reset_submission(vgpu, 1 << ring_id);
-
 	if (workload->status || vgpu->resetting_eng & BIT(ring_id)) {
 		/* if workload->status is not successful means HW GPU
 		 * has occurred GPU hang or something wrong with i915/GVT,
@@ -1031,22 +973,6 @@ static void complete_current_workload(st
 	mutex_unlock(&vgpu->vgpu_lock);
 }
 
-static void inject_error_cs_irq(struct intel_vgpu *vgpu, int ring_id)
-{
-	enum intel_gvt_event_type events[] = {
-		[RCS0] = RCS_CMD_STREAMER_ERR,
-		[BCS0] = BCS_CMD_STREAMER_ERR,
-		[VCS0] = VCS_CMD_STREAMER_ERR,
-		[VCS2] = VCS2_CMD_STREAMER_ERR,
-		[VECS0] = VECS_CMD_STREAMER_ERR,
-	};
-
-	if (unlikely(events[ring_id] == 0))
-		return;
-
-	intel_vgpu_trigger_virtual_event(vgpu, events[ring_id]);
-}
-
 struct workload_thread_param {
 	struct intel_gvt *gvt;
 	int ring_id;
@@ -1113,25 +1039,7 @@ static int workload_thread(void *priv)
 
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
-
-		ret = i915_request_wait(workload->req, 0,
-				MAX_SCHEDULE_TIMEOUT);
-
-		gvt_dbg_sched("i915_wait_request %p returns %d\n",
-				workload, ret);
-
-		if (ret >= 0 && workload->status == -EINPROGRESS)
-			workload->status = 0;
-		/*
-		 * increased guilty_count means that this request triggerred
-		 * a GPU reset, so we need to notify the guest about the
-		 * hang.
-		 */
-		if (workload->guilty_count <
-				atomic_read(&workload->req->gem_context->guilty_count)) {
-			workload->status = -EIO;
-			inject_error_cs_irq(workload->vgpu, ring_id);
-		}
+		i915_request_wait(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",
@@ -1217,8 +1125,6 @@ int intel_gvt_init_workload_scheduler(st
 		atomic_notifier_chain_register(&engine->context_status_notifier,
 					&gvt->shadow_ctx_notifier_block[i]);
 	}
-	INIT_WORK(&gvt->active_hp_work, active_hp_work);
-
 	return 0;
 err:
 	intel_gvt_clean_workload_scheduler(gvt);
@@ -1562,6 +1468,7 @@ intel_vgpu_create_workload(struct intel_
 	struct list_head *q = workload_q_head(vgpu, ring_id);
 	struct intel_vgpu_workload *last_workload = NULL;
 	struct intel_vgpu_workload *workload = NULL;
+	struct drm_i915_private *dev_priv = vgpu->gvt->dev_priv;
 	u64 ring_context_gpa;
 	u32 head, tail, start, ctl, ctx_ctl, per_ctx, indirect_ctx;
 	u32 guest_head;
@@ -1680,14 +1587,11 @@ intel_vgpu_create_workload(struct intel_
 	/* Only scan and shadow the first workload in the queue
 	 * as there is only one pre-allocated buf-obj for shadow.
 	 */
-	/* scan_workload and prepare_shadow are executed in one thread to avoid
-	 * the incorrect mutex lock.
 	if (list_empty(workload_q_head(vgpu, ring_id))) {
 		intel_runtime_pm_get(&dev_priv->runtime_pm);
 		ret = intel_gvt_scan_and_shadow_workload(workload);
 		intel_runtime_pm_put_unchecked(&dev_priv->runtime_pm);
 	}
-	*/
 
 	if (ret) {
 		if (vgpu_is_vm_unhealthy(ret))
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/scheduler.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/scheduler.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/scheduler.h
@@ -85,7 +85,6 @@ struct intel_vgpu_workload {
 	bool dispatched;
 	bool shadow;      /* if workload has done shadow of guest request */
 	int status;
-	unsigned int guilty_count;
 
 	struct intel_vgpu_mm *shadow_mm;
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/gvt/vgpu.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/gvt/vgpu.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/gvt/vgpu.c
@@ -37,11 +37,6 @@
 
 void populate_pvinfo_page(struct intel_vgpu *vgpu)
 {
-	enum pipe pipe;
-	int scaler;
-	struct intel_gvt *gvt = vgpu->gvt;
-	struct drm_i915_private *dev_priv = gvt->dev_priv;
-
 	/* setup the ballooning information */
 	vgpu_vreg64_t(vgpu, vgtif_reg(magic)) = VGT_MAGIC;
 	vgpu_vreg_t(vgpu, vgtif_reg(version_major)) = 1;
@@ -52,7 +47,6 @@ void populate_pvinfo_page(struct intel_v
 	vgpu_vreg_t(vgpu, vgtif_reg(vgt_caps)) = VGT_CAPS_FULL_PPGTT;
 	vgpu_vreg_t(vgpu, vgtif_reg(vgt_caps)) |= VGT_CAPS_HWSP_EMULATION;
 	vgpu_vreg_t(vgpu, vgtif_reg(vgt_caps)) |= VGT_CAPS_HUGE_GTT;
-	vgpu_vreg_t(vgpu, vgtif_reg(vgt_caps)) |= VGT_CAPS_GOP_SUPPORT;
 
 	vgpu_vreg_t(vgpu, vgtif_reg(avail_rs.mappable_gmadr.base)) =
 		vgpu_aperture_gmadr_base(vgpu);
@@ -68,23 +62,6 @@ void populate_pvinfo_page(struct intel_v
 	vgpu_vreg_t(vgpu, vgtif_reg(cursor_x_hot)) = UINT_MAX;
 	vgpu_vreg_t(vgpu, vgtif_reg(cursor_y_hot)) = UINT_MAX;
 
-	vgpu_vreg_t(vgpu, vgtif_reg(scaler_owned)) = 0;
-	for_each_pipe(dev_priv, pipe)
-		for_each_universal_scaler(dev_priv, pipe, scaler)
-			if (gvt->pipe_info[pipe].scaler_owner[scaler] ==
-				vgpu->id)
-				vgpu_vreg_t(vgpu, vgtif_reg(scaler_owned)) |=
-					1 << (pipe * SKL_NUM_SCALERS + scaler);
-
-	vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio)) = 0;
-
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.fb_base)) = 0;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.width)) = 0;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.height)) = 0;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.pitch)) = 0;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.Bpp)) = 0;
-	vgpu_vreg_t(vgpu, vgtif_reg(gop.size)) = 0;
-
 	gvt_dbg_core("Populate PVINFO PAGE for vGPU %d\n", vgpu->id);
 	gvt_dbg_core("aperture base [GMADR] 0x%llx size 0x%llx\n",
 		vgpu_aperture_gmadr_base(vgpu), vgpu_aperture_sz(vgpu));
@@ -315,7 +292,6 @@ void intel_gvt_destroy_vgpu(struct intel
 	intel_vgpu_clean_gtt(vgpu);
 	intel_gvt_hypervisor_detach_vgpu(vgpu);
 	intel_vgpu_free_resource(vgpu);
-	intel_vgpu_reset_cfg_space(vgpu);
 	intel_vgpu_clean_mmio(vgpu);
 	intel_vgpu_dmabuf_cleanup(vgpu);
 	mutex_unlock(&vgpu->vgpu_lock);
@@ -559,7 +535,6 @@ void intel_gvt_reset_vgpu_locked(struct
 	struct intel_gvt *gvt = vgpu->gvt;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	intel_engine_mask_t resetting_eng = dmlr ? ALL_ENGINES : engine_mask;
-	u32 enable_pvmmio = vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio));
 
 	gvt_dbg_core("------------------------------------------\n");
 	gvt_dbg_core("resseting vgpu%d, dmlr %d, engine_mask %08x\n",
@@ -591,8 +566,6 @@ void intel_gvt_reset_vgpu_locked(struct
 
 		intel_vgpu_reset_mmio(vgpu, dmlr);
 		populate_pvinfo_page(vgpu);
-		vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio)) = enable_pvmmio;
-		intel_vgpu_reset_display(vgpu);
 
 		if (dmlr) {
 			intel_vgpu_reset_display(vgpu);
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_debugfs.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_debugfs.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_debugfs.c
@@ -2694,17 +2694,13 @@ static int i915_display_info(struct seq_
 
 			intel_crtc_info(m, crtc);
 
-			if (cursor) {
-				seq_printf(m, "\tcursor visible? %s, position (%d, %d), size %dx%d, addr 0x%08x\n",
+			seq_printf(m, "\tcursor visible? %s, position (%d, %d), size %dx%d, addr 0x%08x\n",
 				   yesno(cursor->base.state->visible),
 				   cursor->base.state->crtc_x,
 				   cursor->base.state->crtc_y,
 				   cursor->base.state->crtc_w,
 				   cursor->base.state->crtc_h,
 				   cursor->cursor.base);
-			} else {
-				seq_puts(m, "\tNo cursor plane available on this platform\n");
-			}
 			intel_scaler_info(m, crtc);
 			intel_plane_info(m, crtc);
 		}
@@ -3659,8 +3655,7 @@ i915_cache_sharing_get(void *data, u64 *
 	intel_wakeref_t wakeref;
 	u32 snpcr = 0;
 
-	if (!(IS_GEN(dev_priv, 6) || IS_GEN(dev_priv, 7)
-				|| IS_GEN(dev_priv, 9)))
+	if (!(IS_GEN_RANGE(dev_priv, 6, 7)))
 		return -ENODEV;
 
 	with_intel_runtime_pm(&dev_priv->runtime_pm, wakeref)
@@ -3676,10 +3671,8 @@ i915_cache_sharing_set(void *data, u64 v
 {
 	struct drm_i915_private *dev_priv = data;
 	intel_wakeref_t wakeref;
-	u32 idicr;
 
-	if (!(IS_GEN(dev_priv, 6) || IS_GEN(dev_priv, 7)
-				|| IS_GEN(dev_priv, 9)))
+	if (!(IS_GEN_RANGE(dev_priv, 6, 7)))
 		return -ENODEV;
 
 	if (val > 3)
@@ -3696,13 +3689,6 @@ i915_cache_sharing_set(void *data, u64 v
 		I915_WRITE(GEN6_MBCUNIT_SNPCR, snpcr);
 	}
 
-	if (IS_GEN(dev_priv, 9)) {
-		idicr = I915_READ(HSW_IDICR);
-		idicr &= ~IDI_QOS_MASK;
-		idicr |= (val << IDI_QOS_SHIFT);
-		I915_WRITE(HSW_IDICR, idicr);
-	}
-
 	return 0;
 }
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_drv.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_drv.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_drv.c
@@ -270,24 +270,6 @@ intel_teardown_mchbar(struct drm_i915_pr
 		release_resource(&dev_priv->mch_res);
 }
 
-static inline int get_max_avail_pipes(struct drm_i915_private *dev_priv)
-{
-	enum pipe pipe;
-	int index = 0;
-
-	if (!intel_vgpu_active(dev_priv) ||
-	    !i915_modparams.avail_planes_per_pipe)
-		return INTEL_NUM_PIPES(dev_priv);
-
-	for_each_pipe(dev_priv, pipe) {
-		if (AVAIL_PLANE_PER_PIPE(dev_priv, i915_modparams.avail_planes_per_pipe,
-					pipe))
-			index++;
-	}
-
-	return index;
-}
-
 static int i915_driver_modeset_probe(struct drm_i915_private *i915)
 {
 	int ret;
@@ -296,13 +278,10 @@ static int i915_driver_modeset_probe(str
 		return -ENODEV;
 
 	if (HAS_DISPLAY(i915) && INTEL_DISPLAY_ENABLED(i915)) {
-		int num_crtcs = get_max_avail_pipes(i915);
-
-		if (num_crtcs) {
-			ret = drm_vblank_init(&i915->drm, num_crtcs);
-			if (ret)
-				goto out;
-		}
+		ret = drm_vblank_init(&i915->drm,
+				      INTEL_NUM_PIPES(i915));
+		if (ret)
+			goto out;
 	}
 
 	intel_bios_init(i915);
@@ -511,8 +490,6 @@ static int i915_driver_early_probe(struc
 	intel_uncore_init_early(&dev_priv->uncore, dev_priv);
 
 	spin_lock_init(&dev_priv->irq_lock);
-	spin_lock_init(&dev_priv->shared_page_lock);
-	spin_lock_init(&dev_priv->pvmmio_ppgtt_lock);
 	spin_lock_init(&dev_priv->gpu_error.lock);
 	mutex_init(&dev_priv->backlight_lock);
 
@@ -619,21 +596,6 @@ static int i915_driver_mmio_probe(struct
 
 	intel_uc_init_mmio(&dev_priv->gt.uc);
 
-	if (intel_vgpu_active(dev_priv) && i915_modparams.enable_pvmmio) {
-		u32 bar = 0;
-		u32 mmio_size = 2 * 1024 * 1024;
-
-		/* Map a share page from the end of 2M mmio region in bar0. */
-		dev_priv->shared_page = (struct gvt_shared_page *)
-			pci_iomap_range(dev_priv->drm.pdev, bar,
-			mmio_size, PAGE_SIZE);
-		if (dev_priv->shared_page == NULL) {
-			ret = -EIO;
-			DRM_ERROR("I915: failed to map share page.\n");
-			goto err_uncore;
-		}
-	}
-
 	ret = intel_engines_init_mmio(&dev_priv->gt);
 	if (ret)
 		goto err_uncore;
@@ -641,9 +603,6 @@ static int i915_driver_mmio_probe(struct
 	return 0;
 
 err_uncore:
-	if (intel_vgpu_active(dev_priv) && dev_priv->shared_page)
-		pci_iounmap(dev_priv->drm.pdev, dev_priv->shared_page);
-
 	intel_teardown_mchbar(dev_priv);
 	intel_uncore_fini_mmio(&dev_priv->uncore);
 err_bridge:
@@ -1562,8 +1521,7 @@ int i915_driver_probe(struct pci_dev *pd
 
 	i915_driver_register(dev_priv);
 
-	if (!intel_vgpu_active(dev_priv))
-		enable_rpm_wakeref_asserts(&dev_priv->runtime_pm);
+	enable_rpm_wakeref_asserts(&dev_priv->runtime_pm);
 
 	i915_welcome_messages(dev_priv);
 
@@ -2785,7 +2743,6 @@ static const struct drm_ioctl_desc i915_
 	DRM_IOCTL_DEF_DRV(I915_QUERY, i915_query_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_VM_CREATE, i915_gem_vm_create_ioctl, DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_GEM_VM_DESTROY, i915_gem_vm_destroy_ioctl, DRM_RENDER_ALLOW),
-	DRM_IOCTL_DEF_DRV(I915_GEM_GVTBUFFER, i915_gem_gvtbuffer_ioctl, DRM_RENDER_ALLOW),
 };
 
 static struct drm_driver driver = {
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_drv.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_drv.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_drv.h
@@ -63,7 +63,6 @@
 #include "i915_fixed.h"
 #include "i915_params.h"
 #include "i915_reg.h"
-#include "i915_pvinfo.h"
 #include "i915_utils.h"
 
 #include "display/intel_bios.h"
@@ -890,7 +889,6 @@ struct i915_virtual_gpu {
 	struct mutex lock; /* serialises sending of g2v_notify command pkts */
 	bool active;
 	u32 caps;
-	u32 scaler_owned;
 };
 
 /* used in computing the new watermarks state */
@@ -940,10 +938,6 @@ struct drm_i915_private {
 	 */
 	resource_size_t stolen_usable_size;	/* Total size minus reserved ranges */
 
-	struct gvt_shared_page *shared_page;
-	spinlock_t shared_page_lock;
-	spinlock_t pvmmio_ppgtt_lock;
-
 	struct intel_uncore uncore;
 	struct intel_uncore_mmio_debug mmio_debug;
 
@@ -1810,17 +1804,6 @@ static inline bool intel_vgpu_active(str
 int i915_getparam_ioctl(struct drm_device *dev, void *data,
 			struct drm_file *file_priv);
 
-#ifdef CONFIG_DRM_I915_GVT
-int i915_gem_gvtbuffer_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *file);
-#else
-static inline int i915_gem_gvtbuffer_ioctl(struct drm_device *dev, void *data,
-			     struct drm_file *file)
-{
-	return -EINVAL;
-}
-#endif
-
 /* i915_gem.c */
 int i915_gem_init_userptr(struct drm_i915_private *dev_priv);
 void i915_gem_cleanup_userptr(struct drm_i915_private *dev_priv);
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_gem.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_gem.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_gem.c
@@ -1205,8 +1205,7 @@ int i915_gem_init(struct drm_i915_privat
 	int ret;
 
 	/* We need to fallback to 4K pages if host doesn't support huge gtt. */
-	if ((intel_vgpu_active(dev_priv) && !intel_vgpu_has_huge_gtt(dev_priv))
-			|| PVMMIO_LEVEL(dev_priv, PVMMIO_PPGTT_UPDATE))
+	if (intel_vgpu_active(dev_priv) && !intel_vgpu_has_huge_gtt(dev_priv))
 		mkwrite_device_info(dev_priv)->page_sizes =
 			I915_GTT_PAGE_SIZE_4K;
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_gem_fence_reg.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_gem_fence_reg.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_gem_fence_reg.c
@@ -75,8 +75,6 @@ static void i965_write_fence_reg(struct
 	i915_reg_t fence_reg_lo, fence_reg_hi;
 	int fence_pitch_shift;
 	u64 val;
-	struct drm_i915_private *dev_priv = fence_to_i915(fence);
-	struct intel_uncore *uncore = fence_to_uncore(fence);
 
 	if (INTEL_GEN(fence_to_i915(fence)) >= 6) {
 		fence_reg_lo = FENCE_REG_GEN6_LO(fence->id);
@@ -106,17 +104,8 @@ static void i965_write_fence_reg(struct
 		val |= I965_FENCE_REG_VALID;
 	}
 
-	if (intel_vgpu_active(dev_priv)) {
-		/* Use the 64-bit RW to write fence reg on VGPU mode.
-		 * The GVT-g can trap the written val of VGPU to program the
-		 * fence reg. And the fence write in gvt-g follows the
-		 * sequence of off/read/double-write/read. This assures that
-		 * the fence reg is configured as expected.
-		 * At the same time the 64-bit op can help to reduce the num
-		 * of VGPU trap for the fence reg.
-		 */
-		intel_uncore_write64_fw(uncore, fence_reg_lo, val);
-	} else if (!pipelined) {
+	if (!pipelined) {
+		struct intel_uncore *uncore = fence_to_uncore(fence);
 
 		/*
 		 * To w/a incoherency with non-atomic 64-bit register updates,
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_gem_gtt.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_gem_gtt.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_gem_gtt.c
@@ -1021,10 +1021,6 @@ static u64 __gen8_ppgtt_clear(struct i91
 static void gen8_ppgtt_clear(struct i915_address_space *vm,
 			     u64 start, u64 length)
 {
-	struct i915_page_directory * const pml4 = i915_vm_to_ppgtt(vm)->pd;
-	u64 orig_start = start;
-	u64 orig_length = length;
-
 	GEM_BUG_ON(!IS_ALIGNED(start, BIT_ULL(GEN8_PTE_SHIFT)));
 	GEM_BUG_ON(!IS_ALIGNED(length, BIT_ULL(GEN8_PTE_SHIFT)));
 	GEM_BUG_ON(range_overflows(start, length, vm->total));
@@ -1033,20 +1029,8 @@ static void gen8_ppgtt_clear(struct i915
 	length >>= GEN8_PTE_SHIFT;
 	GEM_BUG_ON(length == 0);
 
-	__gen8_ppgtt_clear(vm, pml4, start, start + length, vm->top);
-
-	if (PVMMIO_LEVEL(vm->i915, PVMMIO_PPGTT_UPDATE)) {
-		struct drm_i915_private *dev_priv = vm->i915;
-		struct pv_ppgtt_update *pv_ppgtt =
-					&dev_priv->shared_page->pv_ppgtt;
-
-		spin_lock(&dev_priv->pvmmio_ppgtt_lock);
-		writeq(px_dma(pml4), &pv_ppgtt->pdp);
-		writeq(orig_start, &pv_ppgtt->start);
-		writeq(orig_length, &pv_ppgtt->length);
-		I915_WRITE(vgtif_reg(g2v_notify), VGT_G2V_PPGTT_L4_CLEAR);
-		spin_unlock(&dev_priv->pvmmio_ppgtt_lock);
-	}
+	__gen8_ppgtt_clear(vm, i915_vm_to_ppgtt(vm)->pd,
+			   start, start + length, vm->top);
 }
 
 static int __gen8_ppgtt_alloc(struct i915_address_space * const vm,
@@ -1150,9 +1134,6 @@ static int gen8_ppgtt_alloc(struct i915_
 {
 	u64 from;
 	int err;
-	struct i915_page_directory * const pml4 = i915_vm_to_ppgtt(vm)->pd;
-	u64 orig_start = start;
-	u64 orig_length = length;
 
 	GEM_BUG_ON(!IS_ALIGNED(start, BIT_ULL(GEN8_PTE_SHIFT)));
 	GEM_BUG_ON(!IS_ALIGNED(length, BIT_ULL(GEN8_PTE_SHIFT)));
@@ -1163,23 +1144,11 @@ static int gen8_ppgtt_alloc(struct i915_
 	GEM_BUG_ON(length == 0);
 	from = start;
 
-	err = __gen8_ppgtt_alloc(vm, pml4, &start, start + length, vm->top);
-
-	if (PVMMIO_LEVEL(vm->i915, PVMMIO_PPGTT_UPDATE)) {
-		struct drm_i915_private *dev_priv = vm->i915;
-		struct pv_ppgtt_update *pv_ppgtt =
-					&dev_priv->shared_page->pv_ppgtt;
-
-		spin_lock(&dev_priv->pvmmio_ppgtt_lock);
-		writeq(px_dma(pml4), &pv_ppgtt->pdp);
-		writeq(orig_start, &pv_ppgtt->start);
-		writeq(orig_length, &pv_ppgtt->length);
-		I915_WRITE(vgtif_reg(g2v_notify), VGT_G2V_PPGTT_L4_ALLOC);
-		spin_unlock(&dev_priv->pvmmio_ppgtt_lock);
-	}
-
+	err = __gen8_ppgtt_alloc(vm, i915_vm_to_ppgtt(vm)->pd,
+				 &start, start + length, vm->top);
 	if (unlikely(err && from != start))
-		__gen8_ppgtt_clear(vm, pml4, from, start, vm->top);
+		__gen8_ppgtt_clear(vm, i915_vm_to_ppgtt(vm)->pd,
+				   from, start, vm->top);
 
 	return err;
 }
@@ -1367,7 +1336,6 @@ static void gen8_ppgtt_insert(struct i91
 			      u32 flags)
 {
 	struct i915_ppgtt * const ppgtt = i915_vm_to_ppgtt(vm);
-	struct i915_page_directory * const pml4 = ppgtt->pd;
 	struct sgt_dma iter = sgt_dma(vma);
 
 	if (vma->page_sizes.sg > I915_GTT_PAGE_SIZE) {
@@ -1383,20 +1351,6 @@ static void gen8_ppgtt_insert(struct i91
 						    cache_level, flags);
 		} while (idx);
 
-		if (PVMMIO_LEVEL(vm->i915, PVMMIO_PPGTT_UPDATE)) {
-			struct drm_i915_private *dev_priv = vm->i915;
-			struct pv_ppgtt_update *pv_ppgtt =
-				&dev_priv->shared_page->pv_ppgtt;
-
-			spin_lock(&dev_priv->pvmmio_ppgtt_lock);
-			writeq(px_dma(pml4), &pv_ppgtt->pdp);
-			writeq(vma->node.start, &pv_ppgtt->start);
-			writeq(vma->node.size, &pv_ppgtt->length);
-			writel(cache_level, &pv_ppgtt->cache_level);
-			I915_WRITE(vgtif_reg(g2v_notify), VGT_G2V_PPGTT_L4_INSERT);
-			spin_unlock(&dev_priv->pvmmio_ppgtt_lock);
-		}
-
 		vma->page_sizes.gtt = I915_GTT_PAGE_SIZE;
 	}
 }
@@ -2769,20 +2723,17 @@ static int init_ggtt(struct i915_ggtt *g
 	if (ret)
 		goto err;
 
-	if (!intel_vgpu_active(ggtt->vm.i915)) {
-		/* Clear any non-preallocated blocks */
-		drm_mm_for_each_hole(entry, &ggtt->vm.mm, hole_start, hole_end) {
-			DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
+	/* Clear any non-preallocated blocks */
+	drm_mm_for_each_hole(entry, &ggtt->vm.mm, hole_start, hole_end) {
+		DRM_DEBUG_KMS("clearing unused GTT space: [%lx, %lx]\n",
 			      hole_start, hole_end);
-			ggtt->vm.clear_range(&ggtt->vm, hole_start,
+		ggtt->vm.clear_range(&ggtt->vm, hole_start,
 				     hole_end - hole_start);
-		}
-
-		/* And finally clear the reserved guard page */
-		ggtt->vm.clear_range(&ggtt->vm, ggtt->vm.total - PAGE_SIZE, PAGE_SIZE);
-
 	}
 
+	/* And finally clear the reserved guard page */
+	ggtt->vm.clear_range(&ggtt->vm, ggtt->vm.total - PAGE_SIZE, PAGE_SIZE);
+
 	return 0;
 
 err:
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_gem_gvtbuffer.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_gem_gvtbuffer.c
+++ /dev/null
@@ -1,298 +0,0 @@
-/*
- * Copyright  2012 - 2015 Intel Corporation
- *
- * Permission is hereby granted, free of charge, to any person obtaining a
- * copy of this software and associated documentation files (the "Software"),
- * to deal in the Software without restriction, including without limitation
- * the rights to use, copy, modify, merge, publish, distribute, sublicense,
- * and/or sell copies of the Software, and to permit persons to whom the
- * Software is furnished to do so, subject to the following conditions:
- *
- * The above copyright notice and this permission notice (including the next
- * paragraph) shall be included in all copies or substantial portions of the
- * Software.
- *
- * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
- * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
- * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
- * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
- * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
- * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
- * IN THE SOFTWARE.
- *
- */
-
-#include "i915_drv.h"
-#include "i915_trace.h"
-#include <linux/swap.h>
-
-#include "gvt/gvt.h"
-#include "gvt/fb_decoder.h"
-
-static int
-i915_gem_gvtbuffer_get_pages(struct drm_i915_gem_object *obj)
-{
-	BUG();
-	return -EINVAL;
-}
-
-static void i915_gem_gvtbuffer_put_pages(struct drm_i915_gem_object *obj,
-					 struct sg_table *pages)
-{
-	/* like stolen memory, this should only be called during free
-	 * after clearing pin count.
-	 */
-	sg_free_table(pages);
-	kfree(pages);
-}
-
-static void
-i915_gem_gvtbuffer_release(struct drm_i915_gem_object *obj)
-{
-	i915_gem_object_unpin_pages(obj);
-}
-
-static const struct drm_i915_gem_object_ops i915_gem_gvtbuffer_ops = {
-	.get_pages = i915_gem_gvtbuffer_get_pages,
-	.put_pages = i915_gem_gvtbuffer_put_pages,
-	.release = i915_gem_gvtbuffer_release,
-};
-
-#define GEN8_DECODE_PTE(pte) \
-	((dma_addr_t)(((((u64)pte) >> 12) & 0x7ffffffULL) << 12))
-
-#define GEN7_DECODE_PTE(pte) \
-	((dma_addr_t)(((((u64)pte) & 0x7f0) << 28) | (u64)(pte & 0xfffff000)))
-
-static struct sg_table *
-i915_create_sg_pages_for_gvtbuffer(struct drm_device *dev,
-			     u32 start, u32 num_pages)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct sg_table *st;
-	struct scatterlist *sg;
-	int i;
-
-	st = kmalloc(sizeof(*st), GFP_KERNEL);
-	if (st == NULL)
-		return NULL;
-
-	if (sg_alloc_table(st, num_pages, GFP_KERNEL)) {
-		kfree(st);
-		return NULL;
-	}
-
-	if (INTEL_INFO(dev_priv)->gen >= 8) {
-		gen8_pte_t __iomem *gtt_entries =
-			(gen8_pte_t __iomem *)dev_priv->ggtt.gsm +
-			(start >> PAGE_SHIFT);
-		for_each_sg(st->sgl, sg, num_pages, i) {
-			sg->offset = 0;
-			sg->length = PAGE_SIZE;
-			sg_dma_address(sg) =
-				GEN8_DECODE_PTE(readq(&gtt_entries[i]));
-			sg_dma_len(sg) = PAGE_SIZE;
-		}
-	} else {
-		gen6_pte_t __iomem *gtt_entries =
-			(gen6_pte_t __iomem *)dev_priv->ggtt.gsm +
-			(start >> PAGE_SHIFT);
-		for_each_sg(st->sgl, sg, num_pages, i) {
-			sg->offset = 0;
-			sg->length = PAGE_SIZE;
-			sg_dma_address(sg) =
-				GEN7_DECODE_PTE(readq(&gtt_entries[i]));
-			sg_dma_len(sg) = PAGE_SIZE;
-		}
-	}
-
-	return st;
-}
-
-struct drm_i915_gem_object *
-i915_gem_object_create_gvtbuffer(struct drm_device *dev,
-				 u32 start, u32 num_pages)
-{
-	static struct lock_class_key lock_class;
-	struct drm_i915_gem_object *obj;
-	obj = i915_gem_object_alloc();
-	if (obj == NULL)
-		return NULL;
-
-	drm_gem_private_object_init(dev, &obj->base, num_pages << PAGE_SHIFT);
-	i915_gem_object_init(obj, &i915_gem_gvtbuffer_ops, &lock_class);
-
-	obj->mm.pages = i915_create_sg_pages_for_gvtbuffer(dev, start, num_pages);
-	if (obj->mm.pages == NULL) {
-		i915_gem_object_free(obj);
-		return NULL;
-	}
-
-	if (i915_gem_object_pin_pages(obj))
-		printk(KERN_ERR "%s:%d> Pin pages failed!\n", __func__, __LINE__);
-	obj->cache_level = I915_CACHE_L3_LLC;
-
-	DRM_DEBUG_DRIVER("GVT_GEM: backing store base = 0x%x pages = 0x%x\n",
-			 start, num_pages);
-	return obj;
-}
-
-static int gvt_decode_information(struct drm_device *dev,
-				  struct drm_i915_gem_gvtbuffer *args)
-{
-	struct drm_i915_private *dev_priv = dev->dev_private;
-	struct intel_gvt *gvt = dev_priv->gvt;
-	struct intel_vgpu_primary_plane_format p;
-	struct intel_vgpu_cursor_plane_format c;
-	struct intel_vgpu *vgpu = NULL;
-	int ret;
-	int i;
-
-	if (!intel_gvt_active(dev_priv))
-		return -EINVAL;
-
-	mutex_lock(&gvt->lock);
-	for_each_active_vgpu(gvt, vgpu, i)
-		if (vgpu->id == args->id)
-			break;
-
-	if (!vgpu) {
-		gvt_err("Invalid vgpu ID (%d)\n", args->id);
-		mutex_unlock(&gvt->lock);
-		return -ENODEV;
-	}
-	mutex_unlock(&gvt->lock);
-
-	if ((args->plane_id) == I915_GVT_PLANE_PRIMARY) {
-		ret = intel_vgpu_decode_primary_plane(vgpu, &p);
-		if (ret)
-			return ret;
-
-		args->enabled = p.enabled;
-		args->x_offset = p.x_offset;
-		args->y_offset = p.y_offset;
-		args->start = p.base;
-		args->width = p.width;
-		args->height = p.height;
-		args->stride = p.stride;
-		args->bpp = p.bpp;
-		args->hw_format = p.hw_format;
-		args->drm_format = p.drm_format;
-		args->tiled = p.tiled;
-	} else if ((args->plane_id) == I915_GVT_PLANE_CURSOR) {
-		ret = intel_vgpu_decode_cursor_plane(vgpu, &c);
-		if (ret)
-			return ret;
-
-		args->enabled = c.enabled;
-		args->x_offset = c.x_hot;
-		args->y_offset = c.y_hot;
-		args->x_pos = c.x_pos;
-		args->y_pos = c.y_pos;
-		args->start = c.base;
-		args->width = c.width;
-		args->height = c.height;
-		args->stride = c.width * (c.bpp / 8);
-		args->bpp = c.bpp;
-		args->tiled = 0;
-	} else {
-		DRM_DEBUG_DRIVER("GVT_GEM: Invalid plaine_id: %d\n",
-				 args->plane_id);
-		return -EINVAL;
-	}
-
-	args->size = ALIGN(args->stride * args->height, PAGE_SIZE) >> PAGE_SHIFT;
-
-	if (args->start & (PAGE_SIZE - 1)) {
-		DRM_DEBUG_DRIVER("GVT_GEM: Not aligned fb start address: "
-				 "0x%x\n", args->start);
-		return -EINVAL;
-	}
-
-	if (((args->start >> PAGE_SHIFT) + args->size) >
-	    ggtt_total_entries(&dev_priv->ggtt)) {
-		DRM_DEBUG_DRIVER("GVT: Invalid GTT offset or size\n");
-		return -EINVAL;
-	}
-	return 0;
-}
-
-/**
- * Creates a new mm object that wraps some user memory.
- */
-int
-i915_gem_gvtbuffer_ioctl(struct drm_device *dev, void *data,
-			 struct drm_file *file)
-{
-	struct drm_i915_private *dev_priv = to_i915(dev);
-	struct drm_i915_gem_gvtbuffer *args = data;
-	struct drm_i915_gem_object *obj;
-	u32 handle;
-	int ret = 0;
-
-	if (INTEL_INFO(dev_priv)->gen < 7)
-		return -EPERM;
-
-	if (args->flags & I915_GVTBUFFER_CHECK_CAPABILITY)
-		return 0;
-#if 0
-	if (!gvt_check_host())
-		return -EPERM;
-#endif
-	/* if args->start != 0 do not decode, but use it as ggtt offset*/
-	if (args->start == 0) {
-		ret = gvt_decode_information(dev, args);
-		if (ret)
-			return ret;
-	}
-
-	if (ret)
-		return ret;
-
-	if (args->flags & I915_GVTBUFFER_QUERY_ONLY)
-		return 0;
-
-	obj = i915_gem_object_create_gvtbuffer(dev, args->start, args->size);
-	if (!obj) {
-		DRM_DEBUG_DRIVER("GVT_GEM: Failed to create gem object"
-					" for VM FB!\n");
-		return -EINVAL;
-	}
-
-	if (IS_SKYLAKE(dev_priv) || IS_BROXTON(dev_priv) ||
-			IS_KABYLAKE(dev_priv)) {
-		unsigned int tiling_mode = I915_TILING_NONE;
-		unsigned int stride = 0;
-
-		switch (args->tiled << 10) {
-		case PLANE_CTL_TILED_LINEAR:
-			/* Default valid value */
-			break;
-		case PLANE_CTL_TILED_X:
-			tiling_mode = I915_TILING_X;
-			stride = args->stride;
-			break;
-		case PLANE_CTL_TILED_Y:
-			tiling_mode = I915_TILING_Y;
-			stride = args->stride;
-			break;
-		default:
-			DRM_ERROR("gvt: tiling mode %d not supported\n", args->tiled);
-		}
-		obj->tiling_and_stride = tiling_mode | stride;
-	} else {
-		obj->tiling_and_stride = (args->tiled ? I915_TILING_X : I915_TILING_NONE) |
-					 (args->tiled ? args->stride : 0);
-	}
-
-	ret = drm_gem_handle_create(file, &obj->base, &handle);
-
-	/* drop reference from allocate - handle holds it now */
-	i915_gem_object_put(obj);
-
-	if (ret)
-		return ret;
-
-	args->handle = handle;
-	return 0;
-}
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_irq.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_irq.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_irq.c
@@ -52,10 +52,6 @@
 #include "i915_trace.h"
 #include "intel_pm.h"
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-#include "gvt.h"
-#endif
-
 /**
  * DOC: interrupt handling
  *
@@ -257,17 +253,6 @@ void gen2_irq_init(struct intel_uncore *
 	intel_uncore_posting_read16(uncore, GEN2_IMR);
 }
 
-
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-static inline void gvt_notify_vblank(struct drm_i915_private *dev_priv,
-				     enum pipe pipe)
-{
-	if (dev_priv->gvt)
-		queue_work(system_highpri_wq,
-				&dev_priv->gvt->pipe_info[pipe].vblank_work);
-}
-#endif
-
 /* For display hotplug interrupt */
 static inline void
 i915_hotplug_interrupt_update_locked(struct drm_i915_private *dev_priv,
@@ -2330,16 +2315,8 @@ gen8_de_irq_handler(struct drm_i915_priv
 		ret = IRQ_HANDLED;
 		I915_WRITE(GEN8_DE_PIPE_IIR(pipe), iir);
 
-		if (iir & GEN8_PIPE_VBLANK) {
-			struct intel_crtc *crtc =
-				intel_get_crtc_for_pipe(dev_priv, pipe);
-
-			drm_handle_vblank(&dev_priv->drm,
-					  drm_crtc_index(&crtc->base));
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-			gvt_notify_vblank(dev_priv, pipe);
-#endif
-		}
+		if (iir & GEN8_PIPE_VBLANK)
+			drm_handle_vblank(&dev_priv->drm, pipe);
 
 		if (iir & GEN8_PIPE_CDCLK_CRC_DONE)
 			hsw_pipe_crc_irq_handler(dev_priv, pipe);
@@ -2669,9 +2646,7 @@ void bdw_disable_vblank(struct drm_crtc
 	unsigned long irqflags;
 
 	spin_lock_irqsave(&dev_priv->irq_lock, irqflags);
-	/*since guest will see all the pipes, we don't want it disable vblank*/
-	if (!dev_priv->gvt)
-		bdw_disable_pipe_irq(dev_priv, pipe, GEN8_PIPE_VBLANK);
+	bdw_disable_pipe_irq(dev_priv, pipe, GEN8_PIPE_VBLANK);
 	spin_unlock_irqrestore(&dev_priv->irq_lock, irqflags);
 }
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_params.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_params.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_params.c
@@ -189,91 +189,6 @@ i915_param_named_unsafe(fake_lmem_start,
 	"Fake LMEM start offset (default: 0)");
 #endif
 
-/* pipeA Scaler = BITS 0-7 pipeB scaler = 8-15, pipeC = 16-19
- *
- * +----------+------------+-------------+------------+
- * |unused    |  Pipe C    |   Pipe B    |   Pipe A   |
- * +----------+------------+-------------+------------+
- * 31       20 19        16 15           8 7           0
- *
- * Each nibble represents domain id. 0 for Dom0, 1,2,3...0xF for DomUs
- * eg: domains_scaler_owners = 0x00030210 // 0x000|3|02|10
- * scaler          domain
- * scaler_owner1A -0
- * scaler_owner2A -1
- * scaler_owner1B -2
- * scaler_owner2B -0
- * scaler_owner1C -3
- * scaler_owner2C -0
- *
- */
-i915_param_named(domain_scaler_owner, int, 0400,
-	"scaler owners for each domain and for each pipe ids can be from 0-F");
-
-i915_param_named(enable_pvmmio, uint, 0400,
-	"Enable pv mmio feature and set pvmmio level, default 1."
-	"This parameter could only set from host, guest value is set through vgt_if");
-
-i915_param_named(gvt_workload_priority, int, 0600,
-		"Set GVT-g workload priority, (range: (-1023, 1023), default: 0, "
-		"more positive value means higher priority).");
-
-/* pipeA = BITS 0-3, pipeB = BITS 8-11, pipeC = BITS 16-18
- * +----------+-------+---------+--------+--------+--------+--------+
- * |unused    |unused |  Pipe C | unused | Pipe B | unused | Pipe A |
- * +----------+-------+---------+--------+--------+--------+--------+
- * 31         23      18        15       11       7        3        0
- *
- *
- * BITS 0,1,2,3 - needs to be set planes assigned for pipes A and B
- * and BITs 0,1,2 - for pipe C
- * eg: avail_planes_per_pipe = 0x3 - pipe A=2(planes 1 and 2) , pipeB=0 and pipeC=0 planes
- * eg: avail_planes_per_pipe = 0x5 - pipe A=2(planes 1 and 3) , pipeB=0 and pipeC=0 planes
- * avail_planes_per_pipe = 0x030701 - pipe A =1(plane 1, pipeB=3(planes 1,2 and 3), pipeC=2( planes 1 and 2)
- *
- */
-i915_param_named_unsafe(avail_planes_per_pipe, ulong, 0400,
-	"plane mask for each	pipe: \
-	set BITS 0-3:pipeA 8-11:pipeB 16-18:pipeC to specify the planes that \
-	are available eg: 0x030701 : planes 1:pipeA 1,2,3:pipeB \
-	1,2:pipeC (0x0 - default value)");
-
-/* pipeA = BITS 0-15 pipeB = 16-31, pipeC = 32-47
- *
- * +----------+------------+-------------+------------+
- * |unused    |  Pipe C    |   Pipe B    |   Pipe A   |
- * +----------+------------+-------------+------------+
- * 63         47           31            15           0
- *
- * Each nibble represents domain id. 0 for Dom0, 1,2,3...0xF for DomUs
- * eg: domain_plane_owners = 0x022111000010 // 0x0221|1100|0010
- * plane		 domain
- * plane_owner1A -0
- * plane_owner2A -1
- * plane_owner3A -0
- * plane_owner4A -0
- * plane_owner1B -0
- * plane_owner2B -0
- * plane_owner3B -1
- * plane_owner4B -1
- * plane_owner1C -1
- * plane_owner2C -2
- * plane_owner3C -2
- *
- *
- */
-i915_param_named_unsafe(domain_plane_owners, ulong, 0400,
-	"plane owners for each domain and for each pipe \
-	ids can be from 0-F,  eg: domain_plane_owners = 0x022111000010 \
-	planes owner: 3C:2 2C:2 1C:1 4B:1 3B:1 2B:1 1B:0 4A:0 3A:0 2A:1 1A:0 \
-	(0x0 - default value)");
-
-i915_param_named_unsafe(enable_context_restore, bool, 0400,
-	"To togger non-context regs save/restore feature(default:false)");
-
-i915_param_named(enable_hp_work, bool, 0400,
-	"To enable active the high-performance mode during vGPU busy(default:false)");
-
 static __always_inline void _print_param(struct drm_printer *p,
 					 const char *name,
 					 const char *type,
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_params.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_params.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_params.h
@@ -79,16 +79,7 @@ struct drm_printer;
 	param(bool, verbose_state_checks, true) \
 	param(bool, nuclear_pageflip, false) \
 	param(bool, enable_dp_mst, true) \
-	param(int, domain_scaler_owner, 0x11100) \
-	param(unsigned int, enable_pvmmio, \
-			PVMMIO_ELSP_SUBMIT | PVMMIO_PPGTT_UPDATE) \
-	param(bool, enable_gvt, false) \
-	param(int, gvt_workload_priority, 0) \
-	param(unsigned long, avail_planes_per_pipe, 0) \
-	param(unsigned long, domain_plane_owners, 0) \
-	param(bool, enable_context_restore, false) \
-	param(bool, enable_hp_work, false)
-
+	param(bool, enable_gvt, false)
 
 #define MEMBER(T, member, ...) T member;
 struct i915_params {
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_pvinfo.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_pvinfo.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_pvinfo.h
@@ -48,80 +48,15 @@ enum vgt_g2v_type {
 	VGT_G2V_PPGTT_L4_PAGE_TABLE_DESTROY,
 	VGT_G2V_EXECLIST_CONTEXT_CREATE,
 	VGT_G2V_EXECLIST_CONTEXT_DESTROY,
-	VGT_G2V_PPGTT_L4_ALLOC,
-	VGT_G2V_PPGTT_L4_CLEAR,
-	VGT_G2V_PPGTT_L4_INSERT,
-	VGT_RESERVED1,
-	VGT_RESERVED2,
-	VGT_G2V_GOP_SETUP,
 	VGT_G2V_MAX,
 };
 
-#define PLANE_COLOR_CTL_BIT	(1 << 0)
-#define PLANE_KEY_BIT		(1 << 1)
-#define PLANE_SCALER_BIT	(1 << 2)
-
-struct pv_plane_update {
-	u32 flags;
-	u32 plane_color_ctl;
-	u32 plane_key_val;
-	u32 plane_key_max;
-	u32 plane_key_msk;
-	u32 plane_offset;
-	u32 plane_stride;
-	u32 plane_size;
-	u32 plane_aux_dist;
-	u32 plane_aux_offset;
-	u32 ps_ctrl;
-	u32 ps_pwr_gate;
-	u32 ps_win_ps;
-	u32 ps_win_sz;
-	u32 plane_pos;
-	u32 plane_ctl;
-};
-
-struct pv_ppgtt_update {
-	u64 pdp;
-	u64 start;
-	u64 length;
-	u32 cache_level;
-};
-
-/* shared page(4KB) between gvt and VM, located at the first page next
- * to MMIO region(2MB size normally).
- */
-struct gvt_shared_page {
-	u32 elsp_data[4];
-	u32 reg_addr;
-	struct pv_plane_update pv_plane;
-	/* This is reserved only for compatibility */
-	u32 plane_wm_rsvd[11];
-	struct pv_ppgtt_update pv_ppgtt;
-	u32 rsvd2[0x400 - 40];
-};
-
-#define VGPU_PVMMIO(vgpu) vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio))
-
-/*
- * define different levels of PVMMIO optimization
- */
-enum pvmmio_levels {
-	PVMMIO_ELSP_SUBMIT = 0x1,
-	PVMMIO_PLANE_UPDATE = 0x2,
-	/* PVMMIO_XX= 0x4. Reseved for compatibility */
-	PVMMIO_PPGTT_UPDATE = 0x10,
-};
-
 /*
  * VGT capabilities type
  */
 #define VGT_CAPS_FULL_PPGTT		BIT(2)
 #define VGT_CAPS_HWSP_EMULATION		BIT(3)
 #define VGT_CAPS_HUGE_GTT		BIT(4)
-#define VGT_CAPS_GOP_SUPPORT		BIT(5)
-
-#define PVMMIO_LEVEL(dev_priv, level) \
-	(intel_vgpu_active(dev_priv) && (i915_modparams.enable_pvmmio & level))
 
 struct vgt_if {
 	u64 magic;		/* VGT_MAGIC */
@@ -173,24 +108,10 @@ struct vgt_if {
 
 	u32 execlist_context_descriptor_lo;
 	u32 execlist_context_descriptor_hi;
-	u32 enable_pvmmio;
-	u32 pv_mmio;
-	u32 scaler_owned;
 
-	struct {
-		u32 fb_base;
-		u32 width;
-		u32 height;
-		u32 pitch;
-		u32 Bpp;
-		u32 size;
-	} gop;
-	u32  rsv8[0x200 - 33];    /* pad to one page */
+	u32  rsv7[0x200 - 24];    /* pad to one page */
 } __packed;
 
-#define _vgtif_reg(x) \
-	(VGT_PVINFO_PAGE + offsetof(struct vgt_if, x))
-
 #define vgtif_offset(x) (offsetof(struct vgt_if, x))
 
 #define vgtif_reg(x) _MMIO(VGT_PVINFO_PAGE + vgtif_offset(x))
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_reg.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_reg.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_reg.h
@@ -5877,9 +5877,6 @@ enum {
 #define PIPE_C_OFFSET		0x72000
 #define PIPE_D_OFFSET		0x73000
 #define CHV_PIPE_C_OFFSET	0x74000
-
-#define __PIPEBDSL		0x71000
-#define __PIPECDSL		0x72000
 /*
  * There's actually no pipe EDP. Some pipe registers have
  * simply shifted from the pipe to the transcoder, while
@@ -8816,9 +8813,6 @@ enum {
 
 #define  HSW_IDICR				_MMIO(0x9008)
 #define    IDIHASHMSK(x)			(((x) & 0x3f) << 16)
-#define    IDI_QOS_MASK                         (3 << 22)
-#define    IDI_QOS_SHIFT			22
-
 #define  HSW_EDRAM_CAP				_MMIO(0x120010)
 #define    EDRAM_ENABLED			0x1
 #define    EDRAM_NUM_BANKS(cap)			(((cap) >> 1) & 0xf)
@@ -12048,36 +12042,4 @@ enum skl_power_gate {
 #define   DSB_ENABLE			(1 << 31)
 #define   DSB_STATUS			(1 << 0)
 
-#include "gvt/reg.h"
-/* GVT has special read process from some MMIO register,
- * which so that should be trapped to GVT to make a
- * complete emulation. Such MMIO is not too much, now using
- * a static list to cover them.
- */
-static inline bool in_mmio_read_trap_list(u32 reg)
-{
-	if (unlikely(reg >= PCH_GMBUS0.reg && reg <= PCH_GMBUS5.reg))
-		return true;
-
-	if (unlikely(reg == RING_TIMESTAMP(RENDER_RING_BASE).reg ||
-		reg == RING_TIMESTAMP(BLT_RING_BASE).reg ||
-		reg == RING_TIMESTAMP(GEN6_BSD_RING_BASE).reg ||
-		reg == RING_TIMESTAMP(VEBOX_RING_BASE).reg ||
-		reg == RING_TIMESTAMP(GEN8_BSD2_RING_BASE).reg ||
-		reg == RING_TIMESTAMP_UDW(RENDER_RING_BASE).reg ||
-		reg == RING_TIMESTAMP_UDW(BLT_RING_BASE).reg ||
-		reg == RING_TIMESTAMP_UDW(GEN6_BSD_RING_BASE).reg ||
-		reg == RING_TIMESTAMP_UDW(VEBOX_RING_BASE).reg))
-		return true;
-
-	if (unlikely(reg == SBI_DATA.reg || reg == 0x6c060 || reg == 0x206c))
-		return true;
-
-	if (unlikely(reg == _PIPEADSL ||
-				reg == __PIPEBDSL ||
-				reg == __PIPECDSL))
-		return true;
-	return false;
-}
-
 #endif /* _I915_REG_H_ */
Index: kernel-lts-staging/drivers/gpu/drm/i915/i915_vgpu.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/i915_vgpu.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/i915_vgpu.c
@@ -92,14 +92,6 @@ void i915_detect_vgpu(struct drm_i915_pr
 	}
 
 	dev_priv->vgpu.caps = readl(shared_area + vgtif_offset(vgt_caps));
-	dev_priv->vgpu.scaler_owned = readl(shared_area + vgtif_offset(scaler_owned));
-
-	/* If guest wants to enable pvmmio, it needs to enable it explicitly
-	 * through vgt_if interface, and then read back the enable state from
-	 * gvt layer.
-	 */
-	writel(i915_modparams.enable_pvmmio, shared_area + vgtif_offset(enable_pvmmio));
-	i915_modparams.enable_pvmmio = readl(shared_area + vgtif_offset(enable_pvmmio));
 
 	dev_priv->vgpu.active = true;
 	mutex_init(&dev_priv->vgpu.lock);
Index: kernel-lts-staging/drivers/gpu/drm/i915/intel_csr.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/intel_csr.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/intel_csr.c
@@ -665,13 +665,7 @@ void intel_csr_ucode_init(struct drm_i91
 
 	INIT_WORK(&dev_priv->csr.work, csr_load_work_fn);
 
-	/*
-	 * In a GVTg enabled environment, loading the CSR firmware for DomU doesn't
-	 * make much sense since we don't allow it to control display power
-	 * management settings. Furthermore, we can save some time for DomU bootup
-	 * by skipping CSR loading.
-	 */
-	if (!HAS_CSR(dev_priv) || intel_vgpu_active(dev_priv))
+	if (!HAS_CSR(dev_priv))
 		return;
 
 	/*
Index: kernel-lts-staging/drivers/gpu/drm/i915/intel_pm.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/intel_pm.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/intel_pm.c
@@ -390,10 +390,6 @@ static bool _intel_set_memory_cxsr(struc
 	return was_enabled;
 }
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-#include "gvt.h"
-#endif
-
 /**
  * intel_set_memory_cxsr - Configure CxSR state
  * @dev_priv: i915 device
@@ -3762,7 +3758,7 @@ bool intel_can_enable_sagv(struct intel_
 	enum pipe pipe;
 	int level, latency;
 
-	if (!intel_has_sagv(dev_priv) || intel_vgpu_active(dev_priv))
+	if (!intel_has_sagv(dev_priv))
 		return false;
 
 	/*
@@ -5089,33 +5085,6 @@ void skl_write_cursor_wm(struct intel_pl
 	skl_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe), ddb);
 }
 
-void skl_write_planeid_wm(struct drm_i915_private *dev_priv,
-			  enum plane_id plane_id,
-			  const struct intel_crtc_state *crtc_state)
-{
-	int level, max_level = ilk_wm_max_level(dev_priv);
-	struct intel_crtc *crtc = to_intel_crtc(crtc_state->uapi.crtc);
-	enum pipe pipe = crtc->pipe;
-	const struct skl_plane_wm *wm =
-		&crtc_state->wm.skl.optimal.planes[plane_id];
-	const struct skl_ddb_entry *ddb_y =
-		&crtc_state->wm.skl.plane_ddb_y[plane_id];
-	const struct skl_ddb_entry *ddb_uv =
-		&crtc_state->wm.skl.plane_ddb_uv[plane_id];
-
-	for (level = 0; level <= max_level; level++) {
-		skl_write_wm_level(dev_priv, PLANE_WM(pipe, plane_id, level),
-				   &wm->wm[level]);
-	}
-	skl_write_wm_level(dev_priv, PLANE_WM_TRANS(pipe, plane_id),
-			   &wm->trans_wm);
-
-	skl_ddb_entry_write(dev_priv,
-			    PLANE_BUF_CFG(pipe, plane_id), ddb_y);
-	skl_ddb_entry_write(dev_priv,
-			    PLANE_NV12_BUF_CFG(pipe, plane_id), ddb_uv);
-}
-
 bool skl_wm_level_equals(const struct skl_wm_level *l1,
 			 const struct skl_wm_level *l2)
 {
@@ -5219,42 +5188,6 @@ skl_compute_ddb(struct intel_atomic_stat
 
 	memcpy(ddb, &dev_priv->wm.skl_hw.ddb, sizeof(*ddb));
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-	/*
-	 * In GVT environemnt, allocate ddb for all planes in active crtc.
-	 * When there is active pipe change, intel_state active_pipes is
-	 * not zero and updated before dev_priv, so use intel_state
-	 * active_crtc when it is not zero.
-	 */
-	if (dev_priv->gvt) {
-		unsigned int active_pipes;
-		struct intel_gvt_pipe_info *pipe_info;
-
-		active_pipes = state->active_pipes ?
-			state->active_pipes : dev_priv->active_pipes;
-		intel_gvt_allocate_ddb(dev_priv->gvt, active_pipes);
-
-		/* update the ddb into intel_crtc_state->wm */
-		for_each_new_intel_crtc_in_state(state, crtc,
-					new_crtc_state, i) {
-			pipe_info = &(dev_priv->gvt->pipe_info[crtc->pipe]);
-
-			memset(new_crtc_state->wm.skl.plane_ddb_y, 0,
-				sizeof(new_crtc_state->wm.skl.plane_ddb_y));
-			memset(new_crtc_state->wm.skl.plane_ddb_uv, 0,
-				sizeof(new_crtc_state->wm.skl.plane_ddb_uv));
-
-			memcpy(new_crtc_state->wm.skl.plane_ddb_y,
-			       pipe_info->plane_ddb_y,
-			       sizeof(pipe_info->plane_ddb_y));
-			memcpy(&new_crtc_state->wm.skl.ddb,
-			       &pipe_info->pipe_ddb,
-			       sizeof(pipe_info->pipe_ddb));
-		}
-		return 0;
-	}
-#endif
-
 	for_each_oldnew_intel_crtc_in_state(state, crtc, old_crtc_state,
 					    new_crtc_state, i) {
 		ret = skl_allocate_pipe_ddb(new_crtc_state, ddb);
@@ -5522,7 +5455,6 @@ static int skl_wm_add_affected_planes(st
 static int
 skl_compute_wm(struct intel_atomic_state *state)
 {
-	struct drm_i915_private *dev_priv = to_i915(state->base.dev);
 	struct intel_crtc *crtc;
 	struct intel_crtc_state *new_crtc_state;
 	struct intel_crtc_state *old_crtc_state;
@@ -5532,23 +5464,10 @@ skl_compute_wm(struct intel_atomic_state
 	/* Clear all dirty flags */
 	results->dirty_pipes = 0;
 
-	/* For the VGPU scenario based on Linux this is skipped as Dom0
-	 * ignores the WM setting from Guest.
-	 */
-	if (intel_vgpu_active(dev_priv))
-		return 0;
-
 	ret = skl_ddb_add_affected_pipes(state);
 	if (ret)
 		return ret;
 
-	/* In GVT-g scenario the ddb is statically allocated in DOM0 */
-	if (intel_gvt_active(dev_priv)) {
-		ret = skl_compute_ddb(state);
-		if (ret)
-			return ret;
-	}
-
 	/*
 	 * Calculate WM's for all pipes that are part of this transaction.
 	 * Note that skl_ddb_add_affected_pipes may have added more CRTC's that
@@ -5571,40 +5490,15 @@ skl_compute_wm(struct intel_atomic_state
 			results->dirty_pipes |= BIT(crtc->pipe);
 	}
 
-	if (!intel_gvt_active(dev_priv)) {
-		/* On native scenario it uses the dynamic mechanism among pipes */
-		ret = skl_compute_ddb(state);
-		if (ret)
-			return ret;
-	}
+	ret = skl_compute_ddb(state);
+	if (ret)
+		return ret;
 
 	skl_print_wm_changes(state);
 
 	return 0;
 }
 
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-/*
- * when SOS updates plane wm registers, we need to refresh the planes owned by
- * GVT-g guests, to avoid some garbage display on the screen
- */
-static void update_gvt_guest_plane(struct drm_i915_private *dev_priv,
-				   int pipe,
-				   int plane_id)
-{
-	unsigned long value, irqflags;
-
-	spin_lock_irqsave(&dev_priv->uncore.lock, irqflags);
-
-	value = I915_READ_FW(PLANE_CTL(pipe, plane_id));
-	I915_WRITE_FW(PLANE_CTL(pipe, plane_id), value);
-	value = I915_READ_FW(PLANE_SURF(pipe, plane_id));
-	I915_WRITE_FW(PLANE_SURF(pipe, plane_id), value);
-
-	spin_unlock_irqrestore(&dev_priv->uncore.lock, irqflags);
-}
-#endif
-
 static void skl_atomic_update_crtc_wm(struct intel_atomic_state *state,
 				      struct intel_crtc_state *crtc_state)
 {
@@ -5613,64 +5507,10 @@ static void skl_atomic_update_crtc_wm(st
 	struct skl_pipe_wm *pipe_wm = &crtc_state->wm.skl.optimal;
 	enum pipe pipe = crtc->pipe;
 
-	/* This is ignored for VGPU */
-	if (intel_vgpu_active(dev_priv))
-		return;
-
 	if ((state->wm_results.dirty_pipes & BIT(crtc->pipe)) == 0)
 		return;
 
 	I915_WRITE(PIPE_WM_LINETIME(pipe), pipe_wm->linetime);
-
-	if (intel_gvt_active(dev_priv)) {
-		struct intel_plane *intel_plane;
-		enum plane_id plane_id;
-
-		/* TBD: 1. Check the plane_res_block with ddb_plane[i].
-		 *      2. update the plane with plane_id
-		 */
-
-		if (i915_modparams.avail_planes_per_pipe) {
-			enum plane_id valid_plane;
-			struct skl_plane_wm *src_wm, *dst_wm;
-			for_each_intel_plane_on_crtc(&dev_priv->drm, crtc, intel_plane) {
-				valid_plane = intel_plane->id;
-				break;
-			}
-			src_wm = &crtc_state->wm.skl.optimal.planes[valid_plane];
-			for_each_universal_plane(dev_priv, crtc->pipe, plane_id) {
-				dst_wm = &crtc_state->wm.skl.optimal.planes[plane_id];
-				if (!dst_wm->trans_wm.plane_res_b) {
-					memcpy(dst_wm, src_wm, sizeof(*src_wm));
-				}
-				skl_write_planeid_wm(dev_priv, plane_id, crtc_state);
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-				if (dev_priv->gvt &&
-					dev_priv->gvt->pipe_info[pipe].plane_owner[plane_id])
-					update_gvt_guest_plane(dev_priv, pipe, plane_id);
-#endif
-			}
-
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-			skl_ddb_entry_write(dev_priv, CUR_BUF_CFG(pipe),
-					&dev_priv->gvt->pipe_info[pipe].plane_ddb_y[PLANE_CURSOR]);
-#endif
-			return;
-		}
-
-		for_each_intel_plane_on_crtc(&dev_priv->drm, crtc, intel_plane) {
-			plane_id = intel_plane->id;
-#if IS_ENABLED(CONFIG_DRM_I915_GVT)
-			if (dev_priv->gvt &&
-				dev_priv->gvt->pipe_info[pipe].plane_owner[plane_id])
-				continue;
-#endif
-			if (plane_id != PLANE_CURSOR)
-				skl_write_plane_wm(intel_plane, crtc_state);
-			else
-				skl_write_cursor_wm(intel_plane, crtc_state);
-		}
-	}
 }
 
 static void skl_initial_wm(struct intel_atomic_state *state,
@@ -5680,10 +5520,6 @@ static void skl_initial_wm(struct intel_
 	struct drm_i915_private *dev_priv = to_i915(crtc->base.dev);
 	struct skl_ddb_values *results = &state->wm_results;
 
-	/* This is ignored on VGPU */
-	if (intel_vgpu_active(dev_priv))
-		return;
-
 	if ((results->dirty_pipes & BIT(crtc->pipe)) == 0)
 		return;
 
Index: kernel-lts-staging/drivers/gpu/drm/i915/intel_uncore.c
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/intel_uncore.c
+++ kernel-lts-staging/drivers/gpu/drm/i915/intel_uncore.c
@@ -1734,12 +1734,6 @@ static void uncore_raw_init(struct intel
 	GEM_BUG_ON(intel_uncore_has_forcewake(uncore));
 
 	if (intel_vgpu_active(uncore->i915)) {
-		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, gen2);
-		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, gen2);
-		return;
-	}
-
-	if (intel_vgpu_active(uncore->i915)) {
 		ASSIGN_RAW_WRITE_MMIO_VFUNCS(uncore, vgpu);
 		ASSIGN_RAW_READ_MMIO_VFUNCS(uncore, vgpu);
 	} else if (IS_GEN(uncore->i915, 5)) {
@@ -2154,54 +2148,6 @@ intel_uncore_forcewake_for_reg(struct in
 	return fw_domains;
 }
 
-u8 __raw_uncore_read8(const struct intel_uncore *uncore, i915_reg_t reg)
-{
-	struct drm_i915_private *dev_priv;
-
-	dev_priv = uncore->i915;
-	if (!intel_vgpu_active(dev_priv) || !i915_modparams.enable_pvmmio ||
-			likely(!in_mmio_read_trap_list(reg.reg)))
-		return readb((uncore)->regs + i915_mmio_reg_offset(reg));
-	dev_priv->shared_page->reg_addr = i915_mmio_reg_offset(reg);
-	return readb(uncore->regs + i915_mmio_reg_offset(vgtif_reg(pv_mmio)));
-}
-
-u16 __raw_uncore_read16(const struct intel_uncore *uncore, i915_reg_t reg)
-{
-	struct drm_i915_private *dev_priv;
-
-	dev_priv = uncore->i915;
-	if (!intel_vgpu_active(dev_priv) || !i915_modparams.enable_pvmmio ||
-			likely(!in_mmio_read_trap_list(reg.reg)))
-		return readw((uncore)->regs + i915_mmio_reg_offset(reg));
-	dev_priv->shared_page->reg_addr = i915_mmio_reg_offset(reg);
-	return readw(uncore->regs + i915_mmio_reg_offset(vgtif_reg(pv_mmio)));
-}
-
-u32 __raw_uncore_read32(const struct intel_uncore *uncore, i915_reg_t reg)
-{
-	struct drm_i915_private *dev_priv;
-
-	dev_priv = uncore->i915;
-	if (!intel_vgpu_active(dev_priv) || !i915_modparams.enable_pvmmio ||
-			likely(!in_mmio_read_trap_list(reg.reg)))
-		return readl((uncore)->regs + i915_mmio_reg_offset(reg));
-	dev_priv->shared_page->reg_addr = i915_mmio_reg_offset(reg);
-	return readl(uncore->regs + i915_mmio_reg_offset(vgtif_reg(pv_mmio)));
-}
-
-u64 __raw_uncore_read64(const struct intel_uncore *uncore, i915_reg_t reg)
-{
-	struct drm_i915_private *dev_priv;
-
-	dev_priv = uncore->i915;
-	if (!intel_vgpu_active(dev_priv) || !i915_modparams.enable_pvmmio ||
-			likely(!in_mmio_read_trap_list(reg.reg)))
-		return readq((uncore)->regs + i915_mmio_reg_offset(reg));
-	dev_priv->shared_page->reg_addr = i915_mmio_reg_offset(reg);
-	return readq(uncore->regs + i915_mmio_reg_offset(vgtif_reg(pv_mmio)));
-}
-
 #if IS_ENABLED(CONFIG_DRM_I915_SELFTEST)
 #include "selftests/mock_uncore.c"
 #include "selftests/intel_uncore.c"
Index: kernel-lts-staging/drivers/gpu/drm/i915/intel_uncore.h
===================================================================
--- kernel-lts-staging.orig/drivers/gpu/drm/i915/intel_uncore.h
+++ kernel-lts-staging/drivers/gpu/drm/i915/intel_uncore.h
@@ -258,8 +258,11 @@ intel_wait_for_register_fw(struct intel_
 
 /* register access functions */
 #define __raw_read(x__, s__) \
-u##x__ __raw_uncore_read##x__(const struct intel_uncore *uncore, \
-					    i915_reg_t reg);
+static inline u##x__ __raw_uncore_read##x__(const struct intel_uncore *uncore, \
+					    i915_reg_t reg) \
+{ \
+	return read##s__(uncore->regs + i915_mmio_reg_offset(reg)); \
+}
 
 #define __raw_write(x__, s__) \
 static inline void __raw_uncore_write##x__(const struct intel_uncore *uncore, \
Index: kernel-lts-staging/drivers/pci/quirks.c
===================================================================
--- kernel-lts-staging.orig/drivers/pci/quirks.c
+++ kernel-lts-staging/drivers/pci/quirks.c
@@ -3969,50 +3969,6 @@ static int delay_250ms_after_flr(struct
 	return 0;
 }
 
-static int set_gmbus_and_flr(struct pci_dev *dev, int probe)
-{
-	void __iomem *bar;
-	const u32 PCH_GMBUS0 = 0xc5100;
-	u32 gmbus_rd;
-	u32 gmbus_wr;
-	u32 gmbus_rd2;
-	u16 cmd;
-
-	if (!pcie_has_flr(dev))
-		return -ENOTTY;
-
-	if (probe)
-		return 0;
-
-	bar = pci_iomap(dev, 0, PCH_GMBUS0 + sizeof(gmbus_rd));
-	if (!bar) {
-		pr_err("%04x:%04x %s unable to iomap bar 0", dev->vendor, dev->device, __func__);
-		return -ENOTTY;
-	}
-
-	pci_read_config_word(dev, PCI_COMMAND, &cmd);
-	pci_write_config_word(dev, PCI_COMMAND, cmd | PCI_COMMAND_MEMORY);
-
-	pr_err("%04x:%04x %s devfn=0x%x offset=0x%x", dev->vendor, dev->device, __func__, dev->devfn, PCH_GMBUS0);
-	gmbus_rd = readl(bar + PCH_GMBUS0);
-
-	pr_err("%04x:%04x %s gmbus(0x%x)=0x%x before wa", dev->vendor, dev->device, __func__, PCH_GMBUS0, gmbus_rd);
-
-	/* wa for pch slow clock */
-	gmbus_wr = (gmbus_rd & ~0x1F);
-	iowrite32(gmbus_wr, bar + PCH_GMBUS0);
-	gmbus_wr |= 0x1;
-	iowrite32(gmbus_wr, bar + PCH_GMBUS0);
-	gmbus_rd2 = readl(bar + PCH_GMBUS0);
-	pr_err("%04x:%04x %s gmbus=0x%x after wa before flr", dev->vendor, dev->device, __func__, gmbus_rd2);
-
-	pci_iounmap(dev, bar);
-
-	pcie_flr(dev);
-
-	return 0;
-}
-
 static const struct pci_dev_reset_methods pci_dev_reset_methods[] = {
 	{ PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82599_SFP_VF,
 		 reset_intel_82599_sfp_virtfn },
@@ -4022,8 +3978,6 @@ static const struct pci_dev_reset_method
 		reset_ivb_igd },
 	{ PCI_VENDOR_ID_SAMSUNG, 0xa804, nvme_disable_and_flr },
 	{ PCI_VENDOR_ID_INTEL, 0x0953, delay_250ms_after_flr },
-	{ PCI_VENDOR_ID_INTEL, 0x4551, set_gmbus_and_flr },
-	{ PCI_VENDOR_ID_INTEL, 0x4571, set_gmbus_and_flr },
 	{ PCI_VENDOR_ID_CHELSIO, PCI_ANY_ID,
 		reset_chelsio_generic_dev },
 	{ 0 }
Index: kernel-lts-staging/include/uapi/drm/drm_fourcc.h
===================================================================
--- kernel-lts-staging.orig/include/uapi/drm/drm_fourcc.h
+++ kernel-lts-staging/include/uapi/drm/drm_fourcc.h
@@ -139,15 +139,6 @@ extern "C" {
 #define DRM_FORMAT_RGBX1010102	fourcc_code('R', 'X', '3', '0') /* [31:0] R:G:B:x 10:10:10:2 little endian */
 #define DRM_FORMAT_BGRX1010102	fourcc_code('B', 'X', '3', '0') /* [31:0] B:G:R:x 10:10:10:2 little endian */
 
-/* 64 bpp RGB, below two items is add by VGT project, the reason as below:
- * 1. Current version DRM code is not contains 64 bpp RGB definations.
- * 2. VGT should support 64 bpp RGB for Windows 10 guest.
- * 3. VGT add the 64 bpp RGB definations temperarily, before the DRM code add these definations.
- */
-#define DRM_FORMAT_XRGB161616_VGT  fourcc_code('X', 'R', '4', '8') /* [63:0] x:R:G:B 16:16:16:16 little endian */
-#define DRM_FORMAT_XBGR161616_VGT  fourcc_code('X', 'B', '4', '8') /* [63:0] x:B:G:R 16:16:16:16 little endian */
-
-
 #define DRM_FORMAT_ARGB2101010	fourcc_code('A', 'R', '3', '0') /* [31:0] A:R:G:B 2:10:10:10 little endian */
 #define DRM_FORMAT_ABGR2101010	fourcc_code('A', 'B', '3', '0') /* [31:0] A:B:G:R 2:10:10:10 little endian */
 #define DRM_FORMAT_RGBA1010102	fourcc_code('R', 'A', '3', '0') /* [31:0] R:G:B:A 10:10:10:2 little endian */
Index: kernel-lts-staging/include/uapi/drm/i915_drm.h
===================================================================
--- kernel-lts-staging.orig/include/uapi/drm/i915_drm.h
+++ kernel-lts-staging/include/uapi/drm/i915_drm.h
@@ -359,7 +359,6 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_QUERY			0x39
 #define DRM_I915_GEM_VM_CREATE		0x3a
 #define DRM_I915_GEM_VM_DESTROY		0x3b
-#define DRM_I915_GEM_GVTBUFFER		0x40
 /* Must be kept compact -- no holes */
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
@@ -423,8 +422,6 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_GEM_VM_CREATE	DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_GEM_VM_CREATE, struct drm_i915_gem_vm_control)
 #define DRM_IOCTL_I915_GEM_VM_DESTROY	DRM_IOW (DRM_COMMAND_BASE + DRM_I915_GEM_VM_DESTROY, struct drm_i915_gem_vm_control)
 
-#define DRM_IOCTL_I915_GEM_GVTBUFFER		DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_GEM_GVTBUFFER, struct drm_i915_gem_gvtbuffer)
-
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
  */
@@ -2248,43 +2245,6 @@ struct drm_i915_query_perf_config {
 	__u8 data[];
 };
 
-struct drm_i915_gem_gvtbuffer {
-	__u32 id;
-	__u32 plane_id;
-#define I915_GVT_PLANE_PRIMARY 1
-#define I915_GVT_PLANE_SPRITE 2
-#define I915_GVT_PLANE_CURSOR 3
-	__u32 pipe_id;
-	__u32 phys_pipe_id;
-	__u8  enabled;
-	__u8  tiled;
-	__u32 bpp;
-	__u32 hw_format;
-	__u32 drm_format;
-	__u32 start;
-	__u32 x_pos;
-	__u32 y_pos;
-	__u32 x_offset;
-	__u32 y_offset;
-	__u32 size;
-	__u32 width;
-	__u32 height;
-	__u32 stride;
-	__u64 user_ptr;
-	__u32 user_size;
-	__u32 flags;
-#define I915_GVTBUFFER_READ_ONLY (1<<0)
-#define I915_GVTBUFFER_QUERY_ONLY (1<<1)
-#define I915_GVTBUFFER_CHECK_CAPABILITY (1<<2)
-#define I915_GVTBUFFER_UNSYNCHRONIZED 0x80000000
-	/**
-	 * Returned handle for the object.
-	 *
-	 * Object handles are nonzero.
-	 */
-	__u32 handle;
-};
-
 #if defined(__cplusplus)
 }
 #endif
