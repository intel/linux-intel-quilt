From 6e7d7202256bb5a4af3cfa92e5bb1a28240a8f60 Mon Sep 17 00:00:00 2001
From: Raghuveer KadarlaX <raghuveerx.kadarla@intel.com>
Date: Wed, 29 Dec 2021 01:45:08 +0530
Subject: [PATCH 104/109] misc: xpcie: Enhancements of Habana issue WA

* Reduce the detection timeout further to 500msec from 2sec
* Split the 500msec logic into:
  - 250msec to detect the issue
  - 250msec to reset the DMA engine
* Avoid any possible contention between detection timer and
  firing doorbell that might occur under heavy CPU load
* Restart detect timer if transfer_size is non-zero in detect callback

Signed-off-by: Raghuveer KadarlaX <raghuveerx.kadarla@intel.com>
---
 drivers/misc/xlink-pcie/local_host/core.c |  21 ++
 drivers/misc/xlink-pcie/local_host/dma.c  | 355 +++++++++++++++++++++-
 drivers/misc/xlink-pcie/local_host/epf.h  |  27 ++
 3 files changed, 401 insertions(+), 2 deletions(-)

diff --git a/drivers/misc/xlink-pcie/local_host/core.c b/drivers/misc/xlink-pcie/local_host/core.c
index 8aa96df5f01f..27ff9c54164f 100644
--- a/drivers/misc/xlink-pcie/local_host/core.c
+++ b/drivers/misc/xlink-pcie/local_host/core.c
@@ -479,6 +479,8 @@ static irqreturn_t intel_xpcie_core_irq_cb(int irq, void *args)
 
 static int intel_xpcie_events_init(struct xpcie *xpcie)
 {
+	struct xpcie_epf *xpcie_epf = container_of(xpcie, struct xpcie_epf, xpcie);
+
 	xpcie->rx_wq = alloc_ordered_workqueue(XPCIE_DRIVER_NAME,
 					       WQ_MEM_RECLAIM | WQ_HIGHPRI);
 	if (!xpcie->rx_wq) {
@@ -501,17 +503,36 @@ static int intel_xpcie_events_init(struct xpcie *xpcie)
 	INIT_DELAYED_WORK(&xpcie->rx_event, intel_xpcie_rx_event_handler);
 	INIT_DELAYED_WORK(&xpcie->tx_event, intel_xpcie_tx_event_handler);
 
+	hrtimer_init(&xpcie_epf->tx_dma_bug_detect, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	xpcie_epf->tx_dma_bug_detect.function = tx_dma_bug_detect_cb;
+
+	hrtimer_init(&xpcie_epf->rx_dma_bug_detect, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	xpcie_epf->rx_dma_bug_detect.function = rx_dma_bug_detect_cb;
+
+	hrtimer_init(&xpcie_epf->tx_dma_bug_reset, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	xpcie_epf->tx_dma_bug_reset.function = tx_dma_bug_reset_cb;
+
+	hrtimer_init(&xpcie_epf->rx_dma_bug_reset, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	xpcie_epf->rx_dma_bug_reset.function = rx_dma_bug_reset_cb;
+
 	return 0;
 }
 
 static void intel_xpcie_events_cleanup(struct xpcie *xpcie)
 {
+	struct xpcie_epf *xpcie_epf = container_of(xpcie, struct xpcie_epf, xpcie);
+
 	cancel_delayed_work_sync(&xpcie->rx_event);
 	cancel_delayed_work_sync(&xpcie->tx_event);
 
 	destroy_workqueue(xpcie->rx_wq);
 	if (!xpcie->legacy_a0)
 		destroy_workqueue(xpcie->tx_wq);
+
+	hrtimer_cancel(&xpcie_epf->tx_dma_bug_detect);
+	hrtimer_cancel(&xpcie_epf->rx_dma_bug_detect);
+	hrtimer_cancel(&xpcie_epf->tx_dma_bug_reset);
+	hrtimer_cancel(&xpcie_epf->rx_dma_bug_reset);
 }
 
 int intel_xpcie_core_init(struct xpcie *xpcie)
diff --git a/drivers/misc/xlink-pcie/local_host/dma.c b/drivers/misc/xlink-pcie/local_host/dma.c
index 58b4605fb965..75c6d76b4c82 100644
--- a/drivers/misc/xlink-pcie/local_host/dma.c
+++ b/drivers/misc/xlink-pcie/local_host/dma.c
@@ -10,6 +10,8 @@
 
 #include "epf.h"
 
+#define DMA_HW_FAIL_DETECT_TO (250 * 1000) //250 msec delay
+
 /* PCIe DMA control 1 register definitions. */
 #define DMA_CH_CONTROL1_CB_SHIFT	(0)
 #define DMA_CH_CONTROL1_TCB_SHIFT	(1)
@@ -119,7 +121,7 @@ struct __packed pcie_dma_reg {
 	u32 dma_read_int_clear;
 	u32 reserved10;
 	u32 dma_read_err_status_low;
-	u32 dma_rd_err_sts_h;
+	u32 dma_read_err_status_high;
 	u32 reserved11[2];
 	u32 dma_read_linked_list_err_en;
 	u32 reserved12;
@@ -152,6 +154,18 @@ static u32 dma_chan_offset[2][DMA_CHAN_NUM] = {
 	{ 0x300, 0x500, 0x700, 0x900, 0xB00, 0xD00, 0xF00, 0x1100 }
 };
 
+static atomic_t dma_wr_reset_trigger;
+static atomic_t dma_rd_reset_trigger;
+
+static atomic_t dma_wr_reset_all_done;
+static atomic_t dma_rd_reset_all_done;
+
+static wait_queue_head_t		dma_rd_reset_wq;
+static wait_queue_head_t		dma_wr_reset_wq;
+static spinlock_t tx_dma_reset_lock, rx_dma_reset_lock;
+
+static int intel_xpcie_ep_dma_alloc_ll_descs_mem(struct xpcie_epf *xpcie_epf);
+
 static int intel_xpcie_ep_dma_disable(void __iomem *dma_base,
 				      enum xpcie_ep_engine_type rw)
 {
@@ -305,6 +319,254 @@ intel_xpcie_ep_dma_setup_ll_descs(struct __iomem pcie_dma_chan * dma_chan,
 		  (void __iomem *)&dma_chan->dma_llp_high);
 }
 
+void intel_xpcie_ep_start_dma(struct pci_epf *epf)
+{
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+
+	intel_xpcie_ep_dma_alloc_ll_descs_mem(xpcie_epf);
+}
+
+bool intel_xpcie_ep_dma_enabled(struct pci_epf *epf)
+{
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)
+					xpcie_epf->dma_base;
+	void __iomem *w_engine_en = &dma_reg->dma_write_engine_en;
+	void __iomem *r_engine_en = &dma_reg->dma_read_engine_en;
+
+	return (ioread32(w_engine_en) & DMA_ENGINE_EN_MASK) &&
+		(ioread32(r_engine_en) & DMA_ENGINE_EN_MASK);
+}
+
+int intel_xpcie_ep_dma_reset_read_engine(struct pci_epf *epf)
+{
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	/* Disable the DMA read engine. */
+	if (intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, READ_ENGINE))
+		return -EBUSY;
+	intel_xpcie_ep_dma_enable(xpcie_epf->dma_base, READ_ENGINE);
+	return 0;
+}
+
+int intel_xpcie_ep_dma_reset_write_engine(struct pci_epf *epf)
+{
+	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
+	/* Disable the DMA write engine. */
+	if (intel_xpcie_ep_dma_disable(xpcie_epf->dma_base, WRITE_ENGINE))
+		return -EBUSY;
+	intel_xpcie_ep_dma_enable(xpcie_epf->dma_base, WRITE_ENGINE);
+	return 0;
+}
+
+enum hrtimer_restart rx_dma_bug_detect_cb(struct hrtimer *rx_dma_bug_detect)
+{
+	struct xpcie_epf *xpcie_epf =
+		container_of(rx_dma_bug_detect, struct xpcie_epf, rx_dma_bug_detect);
+	void __iomem *dma_base = xpcie_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	unsigned long dma_time_period_us = DMA_HW_FAIL_DETECT_TO;
+	ktime_t rx_dma_reset_tp, rx_dma_detect_tp;
+	struct pci_epf *epf = xpcie_epf->epf;
+	struct pci_epc *epc = epf->epc;
+	int max_slices = epc->max_functions / 2;
+	int chan = xpcie_epf->epf->func_no;
+	struct pcie_dma_chan *dma_chan;
+	u32 transfer_size;
+	int i;
+
+	dma_chan = (struct pcie_dma_chan *)
+		(dma_base + dma_chan_offset[READ_ENGINE][chan]);
+	transfer_size = ioread32(&dma_chan->dma_transfer_size);
+
+	if (transfer_size == 0) {
+		for (i = 0; i < max_slices; i++) {
+			if (chan != (i * 2))
+				intel_xpcie_ep_dma_doorbell(xpcie_epf, (i * 2) | BIT(31),
+							    &dma_reg->dma_read_doorbell);
+		}
+
+		if (!hrtimer_active(&xpcie_epf->rx_dma_bug_reset)) {
+			rx_dma_reset_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+			hrtimer_start(&xpcie_epf->rx_dma_bug_reset, rx_dma_reset_tp,
+				      HRTIMER_MODE_REL);
+		}
+	} else {
+		if (atomic_read(&xpcie_epf->dma_rd_reset_retry)) {
+			dev_info(&xpcie_epf->epf->dev,
+				 "[RX_DMA_TIMER_CB]: Failed to recover, size %d\n", transfer_size);
+			for (i = 0; i < max_slices; i++) {
+				if (chan != (i * 2))
+					intel_xpcie_ep_dma_doorbell(xpcie_epf, (i * 2) | BIT(31),
+								    &dma_reg->dma_read_doorbell);
+			}
+		if (!hrtimer_active(&xpcie_epf->rx_dma_bug_reset)) {
+			rx_dma_reset_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+			hrtimer_start(&xpcie_epf->rx_dma_bug_reset, rx_dma_reset_tp,
+				      HRTIMER_MODE_REL);
+		}
+		} else {
+			if (!hrtimer_active(&xpcie_epf->rx_dma_bug_detect)) {
+				rx_dma_detect_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+				hrtimer_start(&xpcie_epf->rx_dma_bug_detect, rx_dma_detect_tp,
+					      HRTIMER_MODE_REL);
+			}
+			atomic_set(&xpcie_epf->dma_rd_reset_retry, true);
+		}
+	}
+
+	return HRTIMER_NORESTART;
+}
+
+enum hrtimer_restart rx_dma_bug_reset_cb(struct hrtimer *rx_dma_bug_reset)
+{
+	struct xpcie_epf *xpcie_epf =
+		container_of(rx_dma_bug_reset, struct xpcie_epf, rx_dma_bug_reset);
+	void __iomem *dma_base = xpcie_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	struct pci_epf *epf = xpcie_epf->epf;
+	struct pci_epc *epc = epf->epc;
+	int max_slices = epc->max_functions / 2;
+	void __iomem *err_status;
+
+	spin_lock(&rx_dma_reset_lock);
+	if (!atomic_read(&dma_rd_reset_trigger)) {
+		atomic_set(&dma_rd_reset_all_done, false);
+
+		err_status = &dma_reg->dma_read_int_status;
+		xpcie_epf->dma_rd_int_status = ioread32(err_status);
+
+		err_status = &dma_reg->dma_read_err_status_low;
+		xpcie_epf->dma_rd_err_status_low = ioread32(err_status);
+		err_status = &dma_reg->dma_read_err_status_high;
+		xpcie_epf->dma_rd_err_status_high = ioread32(err_status);
+
+		/* RESET DMA READ_ENGINE */
+		xpcie_epf->dma_rd_rc = intel_xpcie_ep_dma_reset_read_engine(xpcie_epf->epf);
+	}
+	atomic_inc(&dma_rd_reset_trigger);
+	atomic_set(&xpcie_epf->dma_rd_eng_reset_cnt, true);
+
+	/* RESET DMA READ_ENGINE */
+	xpcie_epf->dma_rd_done = true;
+	wake_up_interruptible(&xpcie_epf->dma_rd_wq);
+
+	if (atomic_read(&dma_rd_reset_trigger) == max_slices) {
+		atomic_set(&dma_rd_reset_trigger, 0);
+		atomic_set(&dma_rd_reset_all_done, true);
+		wake_up_interruptible_all(&dma_rd_reset_wq);
+	}
+	spin_unlock(&rx_dma_reset_lock);
+
+	return HRTIMER_NORESTART;
+}
+
+enum hrtimer_restart tx_dma_bug_detect_cb(struct hrtimer *tx_dma_bug_detect)
+{
+	struct xpcie_epf *xpcie_epf =
+		container_of(tx_dma_bug_detect, struct xpcie_epf, tx_dma_bug_detect);
+	void __iomem *dma_base = xpcie_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	unsigned long dma_time_period_us = DMA_HW_FAIL_DETECT_TO;
+	ktime_t tx_dma_reset_tp, tx_dma_detect_tp;
+	struct pci_epf *epf = xpcie_epf->epf;
+	struct pci_epc *epc = epf->epc;
+	int max_slices = epc->max_functions / 2;
+	int chan = xpcie_epf->epf->func_no;
+	struct pcie_dma_chan *dma_chan;
+	u32 transfer_size;
+	int i;
+
+	dma_chan = (struct pcie_dma_chan *)
+		(dma_base + dma_chan_offset[READ_ENGINE][chan]);
+	transfer_size = ioread32(&dma_chan->dma_transfer_size);
+
+	if (transfer_size == 0) {
+		for (i = 0; i < max_slices; i++) {
+			if (chan != (i * 2))
+				intel_xpcie_ep_dma_doorbell(xpcie_epf,
+							    (i * 2) | BIT(31),
+							    &dma_reg->dma_write_doorbell);
+		}
+
+		if (!hrtimer_active(&xpcie_epf->tx_dma_bug_reset)) {
+			tx_dma_reset_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+			hrtimer_start(&xpcie_epf->tx_dma_bug_reset, tx_dma_reset_tp,
+				      HRTIMER_MODE_REL);
+		}
+	} else {
+		if (atomic_read(&xpcie_epf->dma_wr_reset_retry)) {
+			dev_err(&xpcie_epf->epf->dev,
+				"[TX_DMA_TIMER_CB]: Failed to recover, size %d\n", transfer_size);
+			for (i = 0; i < max_slices; i++) {
+				if (chan != (i * 2))
+					intel_xpcie_ep_dma_doorbell(xpcie_epf,
+								    (i * 2) | BIT(31),
+								    &dma_reg->dma_write_doorbell);
+			}
+			if (!hrtimer_active(&xpcie_epf->tx_dma_bug_reset)) {
+				tx_dma_reset_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+				hrtimer_start(&xpcie_epf->tx_dma_bug_reset, tx_dma_reset_tp,
+					      HRTIMER_MODE_REL);
+			}
+		} else {
+			if (!hrtimer_active(&xpcie_epf->tx_dma_bug_detect)) {
+				tx_dma_detect_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+				hrtimer_start(&xpcie_epf->tx_dma_bug_detect, tx_dma_detect_tp,
+					      HRTIMER_MODE_REL);
+			}
+			atomic_set(&xpcie_epf->dma_wr_reset_retry, true);
+		}
+	}
+
+	return HRTIMER_NORESTART;
+}
+
+enum hrtimer_restart tx_dma_bug_reset_cb(struct hrtimer *tx_dma_bug_reset)
+{
+	struct xpcie_epf *xpcie_epf =
+		container_of(tx_dma_bug_reset, struct xpcie_epf, tx_dma_bug_reset);
+	void __iomem *dma_base = xpcie_epf->dma_base;
+	struct pcie_dma_reg *dma_reg = (struct pcie_dma_reg *)dma_base;
+	int func_no = xpcie_epf->epf->func_no;
+	struct pci_epf *epf = xpcie_epf->epf;
+	struct pci_epc *epc = epf->epc;
+	int max_slices = epc->max_functions / 2;
+	void __iomem *err_status;
+	int ret1 = 0, ret2 = 0;
+
+	spin_lock(&tx_dma_reset_lock);
+	if (!atomic_read(&dma_wr_reset_trigger)) {
+		atomic_set(&dma_wr_reset_all_done, false);
+
+		err_status = &dma_reg->dma_write_int_status;
+		xpcie_epf->dma_wr_int_status = ioread32(err_status);
+
+		ret1 = intel_xpcie_ep_dma_err_status(&dma_reg->dma_write_err_status, func_no);
+
+		err_status = &dma_reg->dma_write_err_status;
+		xpcie_epf->dma_wr_err_status = ioread32(err_status);
+
+		/* RESET DMA WRITE_ENGINE */
+		ret2 = intel_xpcie_ep_dma_reset_write_engine(xpcie_epf->epf);
+
+		xpcie_epf->dma_wr_rc = ret1 | (ret2 << 16);
+	}
+	atomic_set(&xpcie_epf->dma_wr_eng_reset_cnt, true);
+	atomic_inc(&dma_wr_reset_trigger);
+
+	xpcie_epf->dma_wr_done = true;
+	wake_up_interruptible(&xpcie_epf->dma_wr_wq);
+
+	if (atomic_read(&dma_wr_reset_trigger) == max_slices) {
+		atomic_set(&dma_wr_reset_trigger, 0);
+		atomic_set(&dma_wr_reset_all_done, true);
+		wake_up_interruptible_all(&dma_wr_reset_wq);
+	}
+	spin_unlock(&tx_dma_reset_lock);
+
+	return HRTIMER_NORESTART;
+}
+
 int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 {
 	struct xpcie_epf *xpcie_epf = epf_get_drvdata(epf);
@@ -313,6 +575,9 @@ int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 	struct xpcie_dma_ll_desc_buf *desc_buf;
 	struct __iomem pcie_dma_reg * dma_reg =
 				(struct __iomem pcie_dma_reg *)(dma_base);
+	unsigned long dma_time_period_us = DMA_HW_FAIL_DETECT_TO;
+	ktime_t tx_dma_detect_tp;
+	unsigned long flags;
 	int i, rc;
 
 	if (descs_num <= 0 || descs_num > XPCIE_NUM_TX_DESCS)
@@ -328,12 +593,24 @@ int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 
 	desc_buf = &xpcie_epf->tx_desc_buf;
 
+	if (atomic_read(&dma_wr_reset_trigger)) {
+		rc = wait_event_interruptible(dma_wr_reset_wq,
+					      atomic_read(&dma_wr_reset_all_done) == true);
+	}
+
 	intel_xpcie_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
 
+	spin_lock_irqsave(&xpcie_epf->dma_tx_lock, flags);
+	if (!hrtimer_active(&xpcie_epf->tx_dma_bug_detect)) {
+		tx_dma_detect_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+		hrtimer_start(&xpcie_epf->tx_dma_bug_detect, tx_dma_detect_tp, HRTIMER_MODE_REL);
+	}
+
 	/* Start DMA transfer. */
 	rc = intel_xpcie_ep_dma_doorbell(xpcie_epf, chan,
 					 (void __iomem *)
 					 &dma_reg->dma_write_doorbell);
+	spin_unlock_irqrestore(&xpcie_epf->dma_tx_lock, flags);
 	if (rc)
 		return rc;
 
@@ -359,6 +636,25 @@ int intel_xpcie_ep_dma_write_ll(struct pci_epf *epf, int chan, int descs_num)
 		}
 	}
 
+	if (atomic_read(&xpcie_epf->dma_wr_eng_reset_cnt)) {
+		atomic_set(&xpcie_epf->dma_wr_eng_reset_cnt, false);
+		dev_info(&xpcie_epf->epf->dev,
+			 "[TX_DMA_TIMER_CB] dma write engine reset, retries %d\n",
+			 atomic_read(&xpcie_epf->dma_wr_reset_retry));
+		atomic_set(&xpcie_epf->dma_wr_reset_retry, false);
+		if (xpcie_epf->dma_wr_rc & 0xffff)
+			dev_info(&xpcie_epf->epf->dev,
+				 "[TX_DMA_TIMER_CB]: dma err -EIO\n");
+		if (xpcie_epf->dma_wr_rc & 0xffff0000)
+			dev_info(&xpcie_epf->epf->dev,
+				 "[TX_DMA_TIMER_CB]: dma reset -EBUSY\n");
+		dev_info(&xpcie_epf->epf->dev,
+			 "[TX_DMA_TIMER_CB] dma_write_int_status: 0x%x\n",
+			 xpcie_epf->dma_wr_int_status);
+		dev_info(&xpcie_epf->epf->dev, "dma_write_err_status: 0x%x\n",
+			 xpcie_epf->dma_wr_err_status);
+	}
+
 	rc = intel_xpcie_ep_dma_err_status((void __iomem *)
 					   &dma_reg->dma_write_err_status,
 					   chan);
@@ -389,6 +685,9 @@ int intel_xpcie_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 	struct __iomem pcie_dma_reg * dma_reg =
 				(struct __iomem pcie_dma_reg *)(dma_base);
 	struct __iomem pcie_dma_chan * dma_chan;
+	ktime_t rx_dma_detect_tp;
+	unsigned long dma_time_period_us = DMA_HW_FAIL_DETECT_TO;
+	unsigned long flags;
 	int i, rc;
 
 	if (descs_num <= 0 || descs_num > XPCIE_NUM_RX_DESCS)
@@ -404,12 +703,23 @@ int intel_xpcie_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 
 	desc_buf = &xpcie_epf->rx_desc_buf;
 
+	if (atomic_read(&dma_rd_reset_trigger)) {
+		rc = wait_event_interruptible(dma_rd_reset_wq,
+					      atomic_read(&dma_rd_reset_all_done) == true);
+	}
+
 	intel_xpcie_ep_dma_setup_ll_descs(dma_chan, desc_buf, descs_num);
 
+	spin_lock_irqsave(&xpcie_epf->dma_rx_lock, flags);
+	if (!hrtimer_active(&xpcie_epf->rx_dma_bug_detect)) {
+		rx_dma_detect_tp = ktime_set(0, dma_time_period_us * 1000ULL);
+		hrtimer_start(&xpcie_epf->rx_dma_bug_detect, rx_dma_detect_tp, HRTIMER_MODE_REL);
+	}
 	/* Start DMA transfer. */
 	rc = intel_xpcie_ep_dma_doorbell(xpcie_epf, chan,
 					 (void __iomem *)
 					 &dma_reg->dma_read_doorbell);
+	spin_unlock_irqrestore(&xpcie_epf->dma_rx_lock, flags);
 	if (rc)
 		return rc;
 
@@ -435,13 +745,33 @@ int intel_xpcie_ep_dma_read_ll(struct pci_epf *epf, int chan, int descs_num)
 		}
 	}
 
+	if (atomic_read(&xpcie_epf->dma_rd_eng_reset_cnt)) {
+		atomic_set(&xpcie_epf->dma_rd_eng_reset_cnt, 0);
+		dev_info(&xpcie_epf->epf->dev,
+			 "[RX_DMA_TIMER_CB] dma read engine reset, retries %d\n",
+			 atomic_read(&xpcie_epf->dma_rd_reset_retry));
+		atomic_set(&xpcie_epf->dma_rd_reset_retry, false);
+		if (xpcie_epf->dma_rd_rc)
+			dev_info(&xpcie_epf->epf->dev,
+				 "[RX_DMA_TIMER_CB]: dma reset -EBUSY\n");
+		dev_info(&xpcie_epf->epf->dev,
+			 "[RX_DMA_TIMER_CB] dma_read_int_status: 0x%x\n",
+			 xpcie_epf->dma_rd_int_status);
+		dev_info(&xpcie_epf->epf->dev,
+			 "[RX_DMA_TIMER_CB] dma_read_err_status_low: 0x%x\n",
+			 xpcie_epf->dma_rd_err_status_low);
+		dev_info(&xpcie_epf->epf->dev,
+			 "[RX_DMA_TIMER_CB] dma_read_err_status_high: 0x%x\n",
+			 xpcie_epf->dma_rd_err_status_high);
+	}
+
 	rc = intel_xpcie_ep_dma_err_status((void __iomem *)
 					   &dma_reg->dma_read_err_status_low,
 					   chan);
 	if (!rc) {
 		rc =
 		intel_xpcie_ep_dma_rd_err_sts_h((void __iomem *)
-						&dma_reg->dma_rd_err_sts_h,
+						&dma_reg->dma_read_err_status_high,
 						chan);
 	}
 cleanup:
@@ -547,6 +877,12 @@ static irqreturn_t intel_xpcie_ep_dma_wr_interrupt(int irq, void *args)
 		iowrite32(DMA_DONE_INTERRUPT_CH_MASK(chan),
 			  &dma_reg->dma_write_int_clear);
 
+		if (hrtimer_active(&xpcie_epf->tx_dma_bug_detect))
+			hrtimer_cancel(&xpcie_epf->tx_dma_bug_detect);
+
+		if (hrtimer_active(&xpcie_epf->tx_dma_bug_reset))
+			hrtimer_cancel(&xpcie_epf->tx_dma_bug_reset);
+
 		xpcie_epf->dma_wr_done = true;
 		wake_up_interruptible(&xpcie_epf->dma_wr_wq);
 	}
@@ -568,6 +904,12 @@ static irqreturn_t intel_xpcie_ep_dma_rd_interrupt(int irq, void *args)
 		iowrite32(DMA_DONE_INTERRUPT_CH_MASK(chan),
 			  &dma_reg->dma_read_int_clear);
 
+		if (hrtimer_active(&xpcie_epf->rx_dma_bug_reset))
+			hrtimer_cancel(&xpcie_epf->rx_dma_bug_reset);
+
+		if (hrtimer_active(&xpcie_epf->rx_dma_bug_reset))
+			hrtimer_cancel(&xpcie_epf->rx_dma_bug_reset);
+
 		xpcie_epf->dma_rd_done = true;
 		wake_up_interruptible(&xpcie_epf->dma_rd_wq);
 	}
@@ -600,6 +942,9 @@ int intel_xpcie_ep_dma_init(struct pci_epf *epf)
 		init_waitqueue_head(&xpcie_epf->dma_rd_wq);
 		init_waitqueue_head(&xpcie_epf->dma_wr_wq);
 
+		init_waitqueue_head(&dma_rd_reset_wq);
+		init_waitqueue_head(&dma_wr_reset_wq);
+
 		rc = devm_request_irq(&epf->dev, xpcie_epf->irq_rdma,
 				      &intel_xpcie_ep_dma_rd_interrupt,
 				      0, "xpcie_epf_rd_dma", xpcie_epf);
@@ -621,5 +966,11 @@ int intel_xpcie_ep_dma_init(struct pci_epf *epf)
 	if (rc)
 		return rc;
 
+	spin_lock_init(&tx_dma_reset_lock);
+	spin_lock_init(&rx_dma_reset_lock);
+
+	spin_lock_init(&xpcie_epf->dma_rx_lock);
+	spin_lock_init(&xpcie_epf->dma_tx_lock);
+
 	return intel_xpcie_ep_dma_reset(epf);
 }
diff --git a/drivers/misc/xlink-pcie/local_host/epf.h b/drivers/misc/xlink-pcie/local_host/epf.h
index 868ad79aba13..b36fdb80a9bc 100644
--- a/drivers/misc/xlink-pcie/local_host/epf.h
+++ b/drivers/misc/xlink-pcie/local_host/epf.h
@@ -87,6 +87,27 @@ struct xpcie_epf {
 	struct xpcie_dma_ll_desc_buf	tx_desc_buf;
 	struct xpcie_dma_ll_desc_buf	rx_desc_buf;
 
+	struct hrtimer			tx_dma_bug_detect;
+	struct hrtimer			rx_dma_bug_detect;
+	struct hrtimer			tx_dma_bug_reset;
+	struct hrtimer			rx_dma_bug_reset;
+	/* Tx DMA Spinlock*/
+	spinlock_t			dma_tx_lock;
+	/* Rx DMA Spinlock*/
+	spinlock_t			dma_rx_lock;
+
+	atomic_t			dma_wr_eng_reset_cnt;
+	u32				dma_wr_int_status;
+	u32				dma_wr_err_status;
+	int				dma_wr_rc;
+	atomic_t			dma_rd_eng_reset_cnt;
+	u32				dma_rd_int_status;
+	u32				dma_rd_err_status_low;
+	u32				dma_rd_err_status_high;
+	int				dma_rd_rc;
+	atomic_t			dma_wr_reset_retry;
+	atomic_t			dma_rd_reset_retry;
+
 	wait_queue_head_t		dma_rd_wq;
 	bool				dma_rd_done;
 	wait_queue_head_t		dma_wr_wq;
@@ -115,4 +136,10 @@ int intel_xpcie_copy_from_host_ll(struct xpcie *xpcie,
 				  int chan, int descs_num);
 int intel_xpcie_copy_to_host_ll(struct xpcie *xpcie,
 				int chan, int descs_num);
+
+enum hrtimer_restart tx_dma_bug_detect_cb(struct hrtimer *tx_dma_bug_detect);
+enum hrtimer_restart rx_dma_bug_detect_cb(struct hrtimer *rx_dma_bug_detect);
+enum hrtimer_restart tx_dma_bug_reset_cb(struct hrtimer *tx_dma_bug_reset);
+enum hrtimer_restart rx_dma_bug_reset_cb(struct hrtimer *rx_dma_bug_reset);
+
 #endif /* XPCIE_EPF_HEADER_ */
-- 
2.25.1

