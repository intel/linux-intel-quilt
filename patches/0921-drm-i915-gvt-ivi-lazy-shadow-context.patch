From 738fe5bfb49950620c4e3db01bd3c8b2fc65b6f4 Mon Sep 17 00:00:00 2001
From: Ping Gao <ping.a.gao@intel.com>
Date: Fri, 2 Jun 2017 09:03:19 +0800
Subject: [PATCH 0921/2313] drm/i915/gvt: ivi: lazy shadow context

It's a significant overhead to shadow guest context by copying all
the pages, for performance consideration this patch introduce context
lazy shadow, it copy the first page of the context only and let the
GGTT entries of other shadow context pages point to the corresponding
pages of the guest context.

Change-Id: I6b806da29fa75eff73122d0328b1a277780eabe1
Signed-off-by: Ping Gao <ping.a.gao@intel.com>
Acknowledged-by: Singh, Satyeshwar <satyeshwar.singh@intel.com>
Reviewed-by: He, Min <min.he@intel.com>
Reviewed-by: Jiang, Fei <fei.jiang@intel.com>
Reviewed-by: Dong, Eddie <eddie.dong@intel.com>
Tested-by: Dong, Eddie <eddie.dong@intel.com>
---
 drivers/gpu/drm/i915/gvt/scheduler.c | 90 +++++++++++++++++++++++++++---------
 1 file changed, 67 insertions(+), 23 deletions(-)

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 64dafbd..88108bdb 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -52,6 +52,7 @@ static void set_context_pdp_root_pointer(
 		pdp_pair[i].val = pdp[7 - i];
 }
 
+static bool enable_lazy_shadow_ctx = true;
 static int populate_shadow_context(struct intel_vgpu_workload *workload)
 {
 	struct intel_vgpu *vgpu = workload->vgpu;
@@ -64,6 +65,10 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	struct page *page;
 	void *dst;
 	unsigned long context_gpa, context_page_num;
+	struct drm_i915_private *dev_priv = gvt->dev_priv;
+	struct i915_ggtt *ggtt = &gvt->dev_priv->ggtt;
+	dma_addr_t addr;
+	gen8_pte_t __iomem *pte;
 	int i;
 
 	gvt_dbg_sched("ring id %d workload lrca %x", ring_id,
@@ -77,6 +82,18 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 		context_page_num = 19;
 
 	i = 2;
+#ifdef CONFIG_INTEL_IOMMU
+	/*
+	 * In case IOMMU for graphics is turned on, we don't want to
+	 * turn on lazy shadow context feature because it will touch
+	 * GGTT entries which require a BKL and since this is a
+	 * performance enhancement feature, we will end up negating
+	 * the performance.
+	 */
+	if(intel_iommu_gfx_mapped) {
+		enable_lazy_shadow_ctx = false;
+	}
+#endif
 
 	while (i < context_page_num) {
 		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
@@ -87,14 +104,40 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 			return -EINVAL;
 		}
 
-		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
-		dst = kmap(page);
-		intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
+		if (!enable_lazy_shadow_ctx) {
+			page = i915_gem_object_get_page(ctx_obj,
+					LRC_PPHWSP_PN + i);
+			dst = kmap(page);
+			intel_gvt_hypervisor_read_gpa(vgpu, context_gpa, dst,
 				GTT_PAGE_SIZE);
-		kunmap(page);
+			kunmap(page);
+		} else {
+			unsigned long mfn;
+
+			addr = i915_ggtt_offset(
+					shadow_ctx->engine[ring_id].state) +
+					(LRC_PPHWSP_PN + i) * PAGE_SIZE;
+			pte = (gen8_pte_t __iomem *)ggtt->gsm +
+					(addr >> PAGE_SHIFT);
+
+			mfn = intel_gvt_hypervisor_gfn_to_mfn(vgpu,
+					context_gpa >> 12);
+			if (mfn == INTEL_GVT_INVALID_ADDR) {
+				gvt_vgpu_err("fail to translate gfn during context shadow\n");
+				return -ENXIO;
+			}
+
+			mfn <<= 12;
+			mfn |= _PAGE_PRESENT | _PAGE_RW | PPAT_CACHED_INDEX;
+			writeq(mfn, pte);
+		}
+
 		i++;
 	}
 
+	I915_WRITE(GFX_FLSH_CNTL_GEN6, GFX_FLSH_CNTL_EN);
+	POSTING_READ(GFX_FLSH_CNTL_GEN6);
+
 	page = i915_gem_object_get_page(ctx_obj, LRC_STATE_PN);
 	shadow_ring_context = kmap(page);
 
@@ -332,32 +375,33 @@ static void update_guest_context(struct intel_vgpu_workload *workload)
 	gvt_dbg_sched("ring id %d workload lrca %x\n", ring_id,
 			workload->ctx_desc.lrca);
 
-	context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
+	if (!enable_lazy_shadow_ctx) {
+		context_page_num = gvt->dev_priv->engine[ring_id]->context_size;
+		context_page_num = context_page_num >> PAGE_SHIFT;
 
-	context_page_num = context_page_num >> PAGE_SHIFT;
+		if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
+			context_page_num = 19;
 
-	if (IS_BROADWELL(gvt->dev_priv) && ring_id == RCS)
-		context_page_num = 19;
+		i = 2;
 
-	i = 2;
-
-	while (i < context_page_num) {
-		context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
+		while (i < context_page_num) {
+			context_gpa = intel_vgpu_gma_to_gpa(vgpu->gtt.ggtt_mm,
 				(u32)((workload->ctx_desc.lrca + i) <<
 					GTT_PAGE_SHIFT));
-		if (context_gpa == INTEL_GVT_INVALID_ADDR) {
-			gvt_vgpu_err("invalid guest context descriptor\n");
-			return;
+			if (context_gpa == INTEL_GVT_INVALID_ADDR) {
+				gvt_vgpu_err("invalid guest context descriptor\n");
+				return;
+			}
+
+			page = i915_gem_object_get_page(ctx_obj,
+					LRC_PPHWSP_PN + i);
+			src = kmap(page);
+			intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
+					GTT_PAGE_SIZE);
+			kunmap(page);
+			i++;
 		}
-
-		page = i915_gem_object_get_page(ctx_obj, LRC_PPHWSP_PN + i);
-		src = kmap(page);
-		intel_gvt_hypervisor_write_gpa(vgpu, context_gpa, src,
-				GTT_PAGE_SIZE);
-		kunmap(page);
-		i++;
 	}
-
 	intel_gvt_hypervisor_write_gpa(vgpu, workload->ring_context_gpa +
 		RING_CTX_OFF(ring_header.val), &workload->rb_tail, 4);
 
-- 
2.7.4

