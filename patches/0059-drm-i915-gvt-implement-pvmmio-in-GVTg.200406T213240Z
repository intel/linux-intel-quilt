From b48fa2d0de62c85ff632c7a60d89c738f703eb5a Mon Sep 17 00:00:00 2001
From: Pei Zhang <pei.zhang@intel.com>
Date: Fri, 14 Sep 2018 16:10:19 +0800
Subject: [PATCH 059/100] drm/i915/gvt: implement pvmmio in GVTg

If pvmmio is enabled in i915 host driver, guest i915 will read most MMIO
register directly, which won't be trapped to host GVT. A small range
MMIOs still need trap. They are filtered in a static function, and this
patch is to implement the handler of these registers in GVTg.
Also, when pvmmio is enabled, we will optimize ELSP port writing, to
reduce the mmio trap numbers from 4 to 1, which can improve the guest GPU
performance.

Signed-off-by: Pei Zhang <pei.zhang@intel.com>
Acknowledged-by: Singh, Satyeshwar <satyeshwar.singh@intel.com>
Reviewed-by: He, Min <min.he@intel.com>
Reviewed-by: Jiang, Fei <fei.jiang@intel.com>
Reviewed-by: Dong, Eddie <eddie.dong@intel.com>
Tested-by: Dong, Eddie <eddie.dong@intel.com>
---
 drivers/gpu/drm/i915/gvt/acrngt.c    | 105 +++++++++++++++++++++++++++
 drivers/gpu/drm/i915/gvt/cfg_space.c |   2 +
 drivers/gpu/drm/i915/gvt/gvt.h       |   2 +
 drivers/gpu/drm/i915/gvt/handlers.c  |  75 +++++++++++++++++--
 drivers/gpu/drm/i915/gvt/hypercall.h |   2 +
 drivers/gpu/drm/i915/gvt/mpt.h       |  21 ++++++
 drivers/gpu/drm/i915/gvt/vgpu.c      |   6 ++
 7 files changed, 205 insertions(+), 8 deletions(-)

diff --git a/drivers/gpu/drm/i915/gvt/acrngt.c b/drivers/gpu/drm/i915/gvt/acrngt.c
index d419915e0840..5d1b979a8542 100644
--- a/drivers/gpu/drm/i915/gvt/acrngt.c
+++ b/drivers/gpu/drm/i915/gvt/acrngt.c
@@ -838,6 +838,110 @@ static int acrngt_set_trap_area(unsigned long handle, u64 start,
 	return ret;
 }
 
+static int acrngt_set_pvmmio(unsigned long handle, u64 start, u64 end, bool map)
+{
+	int rc, i;
+	unsigned long mfn, shared_mfn;
+	unsigned long pfn = start >> PAGE_SHIFT;
+	u32 mmio_size_fn = acrngt_priv.gvt->device_info.mmio_size >> PAGE_SHIFT;
+	struct acrngt_hvm_dev *info = (struct acrngt_hvm_dev *)handle;
+
+	if (map) {
+		mfn = acrngt_virt_to_mfn(info->vgpu->mmio.vreg);
+		rc = acrngt_map_gfn_to_mfn(handle, pfn, mfn, mmio_size_fn, map);
+		if (rc) {
+			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
+					pfn, mfn, rc);
+			return rc;
+		}
+
+		/* map the shared page to guest */
+		shared_mfn = acrngt_virt_to_mfn(info->vgpu->mmio.shared_page);
+		rc = acrngt_map_gfn_to_mfn(handle, pfn + mmio_size_fn, shared_mfn, 1, map);
+		if (rc) {
+			gvt_err("acrn-gvt: map shared page fail with ret %d\n", rc);
+			return rc;
+		}
+
+		/* mmio access is trapped like memory write protection */
+		rc = acrn_ioreq_add_iorange(info->client, REQ_WP, pfn << PAGE_SHIFT,
+					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
+		if (rc) {
+			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n", pfn);
+			return rc;
+		}
+
+		for (i = 0; i < mmio_size_fn; i++) {
+			rc = write_protect_page(info->vm_id,
+				(pfn + i) << PAGE_SHIFT, true);
+			if (rc) {
+				gvt_err("failed set wp for pfn 0x%lx\n", pfn + i);
+				return rc;
+			}
+		}
+
+		/* scratch reg access is trapped like mmio access, 1 page */
+		rc = acrngt_map_gfn_to_mfn(handle, pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
+					mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT), 1, 0);
+		if (rc) {
+			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
+					pfn, mfn, rc);
+			return rc;
+		}
+		rc = acrn_ioreq_add_iorange(info->client, REQ_MMIO,
+				(pfn << PAGE_SHIFT) + VGT_PVINFO_PAGE,
+				((pfn + 1) << PAGE_SHIFT) + VGT_PVINFO_PAGE - 1);
+		if (rc) {
+			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n",
+				(pfn << PAGE_SHIFT) + VGT_PVINFO_PAGE);
+			return rc;
+		}
+
+	} else {
+		mfn = acrngt_virt_to_mfn(info->vgpu->mmio.vreg);
+
+		/* scratch reg access is trapped like mmio access, 1 page */
+		rc = acrngt_map_gfn_to_mfn(handle,
+				pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
+				mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT), 1, 1);
+		if (rc) {
+			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
+					pfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
+					mfn + (VGT_PVINFO_PAGE >> PAGE_SHIFT),
+					rc);
+			return rc;
+		}
+
+		rc = acrngt_map_gfn_to_mfn(handle, pfn, mfn, mmio_size_fn, map);
+		if (rc) {
+			gvt_err("acrn-gvt: map pfn %lx to mfn %lx fail with ret %d\n",
+					pfn, mfn, rc);
+			return rc;
+		}
+		rc = acrn_ioreq_del_iorange(info->client, REQ_WP, pfn << PAGE_SHIFT,
+					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
+		if (rc) {
+			gvt_err("failed acrn_ioreq_add_iorange for pfn 0x%lx\n", pfn);
+			return rc;
+		}
+		rc = acrn_ioreq_add_iorange(info->client, REQ_MMIO, pfn << PAGE_SHIFT,
+					((pfn + mmio_size_fn) << PAGE_SHIFT) - 1);
+		if (rc) {
+			gvt_err("failed acrn_ioreq_del_iorange for pfn 0x%lx\n", pfn);
+			return rc;
+		}
+
+		/* unmap the shared page to guest */
+		shared_mfn = acrngt_virt_to_mfn(info->vgpu->mmio.shared_page);
+		rc = acrngt_map_gfn_to_mfn(handle, pfn + mmio_size_fn, shared_mfn, 1, map);
+		if (rc) {
+			gvt_err("acrn-gvt: map shared page fail with ret %d\n", rc);
+			return rc;
+		}
+	}
+	return rc;
+}
+
 static int acrngt_dma_map_guest_page(unsigned long handle, unsigned long gfn,
 				  unsigned long size, dma_addr_t *dma_addr)
 {
@@ -871,6 +975,7 @@ struct intel_gvt_mpt acrn_gvt_mpt = {
 	.dma_map_guest_page = acrngt_dma_map_guest_page,
 	.dma_unmap_guest_page = acrngt_dma_unmap_guest_page,
 	.set_trap_area = acrngt_set_trap_area,
+	.set_pvmmio = acrngt_set_pvmmio,
 };
 EXPORT_SYMBOL_GPL(acrn_gvt_mpt);
 
diff --git a/drivers/gpu/drm/i915/gvt/cfg_space.c b/drivers/gpu/drm/i915/gvt/cfg_space.c
index b4c404ef7b06..f01c31639aba 100644
--- a/drivers/gpu/drm/i915/gvt/cfg_space.c
+++ b/drivers/gpu/drm/i915/gvt/cfg_space.c
@@ -420,6 +420,8 @@ void intel_vgpu_reset_cfg_space(struct intel_vgpu *vgpu)
 				INTEL_GVT_PCI_CLASS_VGA_OTHER;
 
 	if (cmd & PCI_COMMAND_MEMORY) {
+		if (VGPU_PVMMIO(vgpu))
+			set_pvmmio(vgpu, false);
 		trap_gttmmio(vgpu, false);
 		map_aperture(vgpu, false);
 	}
diff --git a/drivers/gpu/drm/i915/gvt/gvt.h b/drivers/gpu/drm/i915/gvt/gvt.h
index 95c04a46e927..5e20d35a4a6d 100644
--- a/drivers/gpu/drm/i915/gvt/gvt.h
+++ b/drivers/gpu/drm/i915/gvt/gvt.h
@@ -549,6 +549,8 @@ void intel_vgpu_init_cfg_space(struct intel_vgpu *vgpu,
 		bool primary);
 void intel_vgpu_reset_cfg_space(struct intel_vgpu *vgpu);
 
+int set_pvmmio(struct intel_vgpu *vgpu, bool map);
+
 int intel_vgpu_emulate_cfg_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes);
 
diff --git a/drivers/gpu/drm/i915/gvt/handlers.c b/drivers/gpu/drm/i915/gvt/handlers.c
index 058723cb69ea..8214be7966f5 100644
--- a/drivers/gpu/drm/i915/gvt/handlers.c
+++ b/drivers/gpu/drm/i915/gvt/handlers.c
@@ -1177,6 +1177,7 @@ static int pvinfo_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
 	bool invalid_read = false;
+	int ret = 0;
 
 	read_vreg(vgpu, offset, p_data, bytes);
 
@@ -1191,9 +1192,27 @@ static int pvinfo_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 			_vgtif_reg(avail_rs.fence_num) + 4)
 			invalid_read = true;
 		break;
+	case _vgtif_reg(pv_mmio):
+	/* a remap happens from guest mmio read operation, the target reg offset
+	 * is in the first DWORD of shared_page.
+	 */
+	{
+		u32 reg = vgpu->mmio.shared_page->reg_addr;
+		struct intel_gvt_mmio_info *mmio;
+
+		mmio = find_mmio_info(vgpu->gvt, rounddown(reg, 4));
+		if (mmio)
+			ret = mmio->read(vgpu, reg, p_data, bytes);
+		else
+			ret = intel_vgpu_default_mmio_read(vgpu, reg, p_data,
+					bytes);
+		break;
+	}
+
 	case 0x78010:	/* vgt_caps */
 	case 0x7881c:
 	case _vgtif_reg(scaler_owned):
+	case _vgtif_reg(enable_pvmmio):
 		break;
 	default:
 		invalid_read = true;
@@ -1203,7 +1222,7 @@ static int pvinfo_mmio_read(struct intel_vgpu *vgpu, unsigned int offset,
 		gvt_vgpu_err("invalid pvinfo read: [%x:%x] = %x\n",
 				offset, bytes, *(u32 *)p_data);
 	vgpu->pv_notified = true;
-	return 0;
+	return ret;
 }
 
 static int handle_g2v_notification(struct intel_vgpu *vgpu, int notification)
@@ -1251,6 +1270,26 @@ static int send_display_ready_uevent(struct intel_vgpu *vgpu, int ready)
 	return kobject_uevent_env(kobj, KOBJ_ADD, env);
 }
 
+#define INTEL_GVT_PCI_BAR_GTTMMIO 0
+int set_pvmmio(struct intel_vgpu *vgpu, bool map)
+{
+	u64 start, end;
+	u64 val;
+	int ret;
+
+	val = vgpu_cfg_space(vgpu)[PCI_BASE_ADDRESS_0];
+	if (val & PCI_BASE_ADDRESS_MEM_TYPE_64)
+		start = *(u64 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_0);
+	else
+		start = *(u32 *)(vgpu_cfg_space(vgpu) + PCI_BASE_ADDRESS_0);
+
+	start &= ~GENMASK(3, 0);
+	end = start + vgpu->cfg_space.bar[INTEL_GVT_PCI_BAR_GTTMMIO].size - 1;
+
+	ret = intel_gvt_hypervisor_set_pvmmio(vgpu, start, end, map);
+	return ret;
+}
+
 static int pvinfo_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 		void *p_data, unsigned int bytes)
 {
@@ -1264,6 +1303,18 @@ static int pvinfo_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 	case _vgtif_reg(g2v_notify):
 		handle_g2v_notification(vgpu, data);
 		break;
+	case _vgtif_reg(enable_pvmmio):
+		if (i915_modparams.enable_pvmmio) {
+			vgpu_vreg(vgpu, offset) = data &
+				i915_modparams.enable_pvmmio;
+			if (set_pvmmio(vgpu, !!vgpu_vreg(vgpu, offset))) {
+				vgpu_vreg(vgpu, offset) = 0;
+				break;
+			}
+		} else {
+			vgpu_vreg(vgpu, offset) = 0;
+		}
+		break;
 	/* add xhot and yhot to handled list to avoid error log */
 	case _vgtif_reg(cursor_x_hot):
 	case _vgtif_reg(cursor_y_hot):
@@ -1651,6 +1702,7 @@ static int elsp_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 	int ring_id = intel_gvt_render_mmio_to_ring_id(vgpu->gvt, offset);
 	struct intel_vgpu_execlist *execlist;
 	u32 data = *(u32 *)p_data;
+	u32 *elsp_data = vgpu->mmio.shared_page->elsp_data;
 	int ret = 0;
 
 	if (WARN_ON(ring_id < 0 || ring_id >= I915_NUM_ENGINES))
@@ -1658,16 +1710,23 @@ static int elsp_mmio_write(struct intel_vgpu *vgpu, unsigned int offset,
 
 	execlist = &vgpu->submission.execlist[ring_id];
 
-	execlist->elsp_dwords.data[3 - execlist->elsp_dwords.index] = data;
-	if (execlist->elsp_dwords.index == 3) {
+	if (VGPU_PVMMIO(vgpu) & PVMMIO_ELSP_SUBMIT) {
+		execlist->elsp_dwords.data[3] = elsp_data[0];
+		execlist->elsp_dwords.data[2] = elsp_data[1];
+		execlist->elsp_dwords.data[1] = elsp_data[2];
+		execlist->elsp_dwords.data[0] = data;
 		ret = intel_vgpu_submit_execlist(vgpu, ring_id);
-		if(ret)
-			gvt_vgpu_err("fail submit workload on ring %d\n",
-				ring_id);
+	} else {
+		execlist->elsp_dwords.data[3 - execlist->elsp_dwords.index] = data;
+		if (execlist->elsp_dwords.index == 3)
+			ret = intel_vgpu_submit_execlist(vgpu, ring_id);
+		++execlist->elsp_dwords.index;
+		execlist->elsp_dwords.index &= 0x3;
 	}
 
-	++execlist->elsp_dwords.index;
-	execlist->elsp_dwords.index &= 0x3;
+	if (ret)
+		gvt_vgpu_err("fail submit workload on ring %d\n", ring_id);
+
 	return ret;
 }
 
diff --git a/drivers/gpu/drm/i915/gvt/hypercall.h b/drivers/gpu/drm/i915/gvt/hypercall.h
index b1848281ffea..bc9368c07836 100644
--- a/drivers/gpu/drm/i915/gvt/hypercall.h
+++ b/drivers/gpu/drm/i915/gvt/hypercall.h
@@ -67,6 +67,8 @@ struct intel_gvt_mpt {
 			      unsigned long mfn, unsigned int nr, bool map);
 	int (*set_trap_area)(unsigned long handle, u64 start, u64 end,
 			     bool map);
+	int (*set_pvmmio)(unsigned long handle, u64 start, u64 end,
+			     bool map);
 	int (*set_opregion)(void *vgpu);
 	int (*set_edid)(void *vgpu, int port_num);
 	int (*get_vfio_device)(void *vgpu);
diff --git a/drivers/gpu/drm/i915/gvt/mpt.h b/drivers/gpu/drm/i915/gvt/mpt.h
index 0f9440128123..86540d0cb1e9 100644
--- a/drivers/gpu/drm/i915/gvt/mpt.h
+++ b/drivers/gpu/drm/i915/gvt/mpt.h
@@ -298,6 +298,27 @@ static inline int intel_gvt_hypervisor_set_trap_area(
 	return intel_gvt_host.mpt->set_trap_area(vgpu->handle, start, end, map);
 }
 
+/**
+ * intel_gvt_hypervisor_set_pvmmio - Set the pvmmio area
+ * @vgpu: a vGPU
+ * @start: the beginning of the guest physical address region
+ * @end: the end of the guest physical address region
+ * @map: map or unmap
+ *
+ * Returns:
+ * Zero on success, negative error code if failed.
+ */
+static inline int intel_gvt_hypervisor_set_pvmmio(
+		struct intel_vgpu *vgpu, u64 start, u64 end, bool map)
+{
+	/* a MPT implementation could have MMIO trapped elsewhere */
+	if (!intel_gvt_host.mpt->set_pvmmio)
+		return -ENOENT;
+
+	return intel_gvt_host.mpt->set_pvmmio(vgpu->handle, start, end, map);
+}
+
+
 /**
  * intel_gvt_hypervisor_set_opregion - Set opregion for guest
  * @vgpu: a vGPU
diff --git a/drivers/gpu/drm/i915/gvt/vgpu.c b/drivers/gpu/drm/i915/gvt/vgpu.c
index bc5cd38e9266..eeb83bb24769 100644
--- a/drivers/gpu/drm/i915/gvt/vgpu.c
+++ b/drivers/gpu/drm/i915/gvt/vgpu.c
@@ -75,6 +75,8 @@ void populate_pvinfo_page(struct intel_vgpu *vgpu)
 				vgpu_vreg_t(vgpu, vgtif_reg(scaler_owned)) |=
 					1 << (pipe * SKL_NUM_SCALERS + scaler);
 
+	vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio)) = 0;
+
 	gvt_dbg_core("Populate PVINFO PAGE for vGPU %d\n", vgpu->id);
 	gvt_dbg_core("aperture base [GMADR] 0x%llx size 0x%llx\n",
 		vgpu_aperture_gmadr_base(vgpu), vgpu_aperture_sz(vgpu));
@@ -305,6 +307,7 @@ void intel_gvt_destroy_vgpu(struct intel_vgpu *vgpu)
 	intel_vgpu_clean_gtt(vgpu);
 	intel_gvt_hypervisor_detach_vgpu(vgpu);
 	intel_vgpu_free_resource(vgpu);
+	intel_vgpu_reset_cfg_space(vgpu);
 	intel_vgpu_clean_mmio(vgpu);
 	intel_vgpu_dmabuf_cleanup(vgpu);
 	mutex_unlock(&vgpu->vgpu_lock);
@@ -548,6 +551,7 @@ void intel_gvt_reset_vgpu_locked(struct intel_vgpu *vgpu, bool dmlr,
 	struct intel_gvt *gvt = vgpu->gvt;
 	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
 	intel_engine_mask_t resetting_eng = dmlr ? ALL_ENGINES : engine_mask;
+	u32 enable_pvmmio = vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio));
 
 	gvt_dbg_core("------------------------------------------\n");
 	gvt_dbg_core("resseting vgpu%d, dmlr %d, engine_mask %08x\n",
@@ -579,6 +583,8 @@ void intel_gvt_reset_vgpu_locked(struct intel_vgpu *vgpu, bool dmlr,
 
 		intel_vgpu_reset_mmio(vgpu, dmlr);
 		populate_pvinfo_page(vgpu);
+		vgpu_vreg_t(vgpu, vgtif_reg(enable_pvmmio)) = enable_pvmmio;
+		intel_vgpu_reset_display(vgpu);
 
 		if (dmlr) {
 			intel_vgpu_reset_display(vgpu);
-- 
2.17.1

