From b0bfcc32010765df5b8dbd7ba2bd70b468f10e15 Mon Sep 17 00:00:00 2001
From: Min He <min.he@intel.com>
Date: Sat, 15 Apr 2017 04:20:20 +0800
Subject: [PATCH 0459/1240] drm/i915/gvt: removed save/store registers

This patch removed save/restore register of gvt context. And dom0 guc
submission can be enabled after applying this patch.

Signed-off-by: Zhao Yan <yan.y.zhao@intel.com>
Signed-off-by: Min He <min.he@intel.com>

Change-Id: I509310bf1febb4cd9918440521c29eacad8c06eb
Acknowledged-by: Vivek Kasireddy <vivek.kasireddy@intel.com>
Reviewed-by: He, Min <min.he@intel.com>
Reviewed-by: Jiang, Fei <fei.jiang@intel.com>
Reviewed-by: Dong, Eddie <eddie.dong@intel.com>
Tested-by: Dong, Eddie <eddie.dong@intel.com>
---
 drivers/gpu/drm/i915/gvt/scheduler.c | 74 ++++--------------------------------
 1 file changed, 8 insertions(+), 66 deletions(-)

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 400aaaa..64dafbd 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -126,64 +126,6 @@ static int populate_shadow_context(struct intel_vgpu_workload *workload)
 	return 0;
 }
 
-static inline bool is_gvt_request(struct drm_i915_gem_request *req)
-{
-	return i915_gem_context_force_single_submission(req->ctx);
-}
-
-static int shadow_context_status_change(struct notifier_block *nb,
-		unsigned long action, void *data)
-{
-	struct drm_i915_gem_request *req = (struct drm_i915_gem_request *)data;
-	struct intel_gvt *gvt = container_of(nb, struct intel_gvt,
-				shadow_ctx_notifier_block[req->engine->id]);
-	struct intel_gvt_workload_scheduler *scheduler = &gvt->scheduler;
-	enum intel_engine_id ring_id = req->engine->id;
-	struct intel_vgpu_workload *workload;
-
-	if (!is_gvt_request(req)) {
-		spin_lock_bh(&scheduler->mmio_context_lock);
-		if (action == INTEL_CONTEXT_SCHEDULE_IN &&
-		    scheduler->engine_owner[ring_id]) {
-			/* Switch ring from vGPU to host. */
-			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
-					      NULL, ring_id);
-			scheduler->engine_owner[ring_id] = NULL;
-		}
-		spin_unlock_bh(&scheduler->mmio_context_lock);
-
-		return NOTIFY_OK;
-	}
-
-	workload = scheduler->current_workload[ring_id];
-	if (unlikely(!workload))
-		return NOTIFY_OK;
-
-	switch (action) {
-	case INTEL_CONTEXT_SCHEDULE_IN:
-		spin_lock_bh(&scheduler->mmio_context_lock);
-		if (workload->vgpu != scheduler->engine_owner[ring_id]) {
-			/* Switch ring from host to vGPU or vGPU to vGPU. */
-			intel_gvt_switch_mmio(scheduler->engine_owner[ring_id],
-					      workload->vgpu, ring_id);
-			scheduler->engine_owner[ring_id] = workload->vgpu;
-		} else
-			gvt_dbg_sched("skip ring %d mmio switch for vgpu%d\n",
-				      ring_id, workload->vgpu->id);
-		spin_unlock_bh(&scheduler->mmio_context_lock);
-		atomic_set(&workload->shadow_ctx_active, 1);
-		break;
-	case INTEL_CONTEXT_SCHEDULE_OUT:
-		atomic_set(&workload->shadow_ctx_active, 0);
-		break;
-	default:
-		WARN_ON(1);
-		return NOTIFY_OK;
-	}
-	wake_up(&workload->shadow_ctx_status_wq);
-	return NOTIFY_OK;
-}
-
 static void shadow_context_descriptor_update(struct i915_gem_context *ctx,
 		struct intel_engine_cs *engine)
 {
@@ -524,6 +466,7 @@ static int workload_thread(void *priv)
 	struct intel_vgpu_workload *workload = NULL;
 	struct intel_vgpu *vgpu = NULL;
 	int ret;
+	long lret;
 	bool need_force_wake = IS_SKYLAKE(gvt->dev_priv)
 			|| IS_BROXTON(gvt->dev_priv)
 			|| IS_KABYLAKE(gvt->dev_priv);
@@ -573,7 +516,13 @@ static int workload_thread(void *priv)
 
 		gvt_dbg_sched("ring id %d wait workload %p\n",
 				workload->ring_id, workload);
-		i915_wait_request(workload->req, 0, MAX_SCHEDULE_TIMEOUT);
+		lret = i915_wait_request(workload->req, 0,
+				MAX_SCHEDULE_TIMEOUT);
+
+		gvt_dbg_sched("i915_wait_request %p returns %ld\n",
+				workload, lret);
+		if (lret >= 0 && workload->status == -EINPROGRESS)
+			workload->status = 0;
 
 complete:
 		gvt_dbg_sched("will complete workload %p, status: %d\n",
@@ -612,9 +561,6 @@ void intel_gvt_clean_workload_scheduler(struct intel_gvt *gvt)
 	gvt_dbg_core("clean workload scheduler\n");
 
 	for_each_engine(engine, gvt->dev_priv, i) {
-		atomic_notifier_chain_unregister(
-					&engine->context_status_notifier,
-					&gvt->shadow_ctx_notifier_block[i]);
 		kthread_stop(scheduler->thread[i]);
 	}
 }
@@ -651,10 +597,6 @@ int intel_gvt_init_workload_scheduler(struct intel_gvt *gvt)
 			goto err;
 		}
 
-		gvt->shadow_ctx_notifier_block[i].notifier_call =
-					shadow_context_status_change;
-		atomic_notifier_chain_register(&engine->context_status_notifier,
-					&gvt->shadow_ctx_notifier_block[i]);
 	}
 	return 0;
 err:
-- 
2.7.4

