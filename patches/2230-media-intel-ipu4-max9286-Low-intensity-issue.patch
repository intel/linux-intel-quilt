From 107860d1b7d2f0055808c0ad1f5ea3ef2afd0e47 Mon Sep 17 00:00:00 2001
From: Yongsheng Chen <yongsheng.chen@intel.com>
Date: Mon, 2 Jul 2018 14:34:08 -0700
Subject: [PATCH 2230/2302] media: intel-ipu4: max9286: Low intensity issue.

Message for Open Source:
Make sure that DIN bit#12 is enabled in CROSSBAR.

Message for Internal:
Make sure that DIN bit#12 is enabled in CROSSBAR.

[Issue/Feature]
Low intensity issue

[Root Cause/Changes]
DIN bit#12 is not enabled in CROSSBAR.

[Test Platform]
 LeafHill

Change-Id: Iea470894a0be703c9cef7a0b471902f2f97b91c5
Signed-off-by: Chen, Yongsheng <yongsheng.chen@intel.com>
Signed-off-by: Meng Wei <wei.meng@intel.com>

Conflicts:
	drivers/media/pci/intel/ipu-psys.c
---
 drivers/media/pci/intel/ipu-psys.c       | 248 +++++++++----------------------
 drivers/media/pci/intel/ipu4/ipu4-psys.c | 175 ++++++++++++++++++----
 2 files changed, 221 insertions(+), 202 deletions(-)

diff --git a/drivers/media/pci/intel/ipu-psys.c b/drivers/media/pci/intel/ipu-psys.c
index 714a790..7118f47 100644
--- a/drivers/media/pci/intel/ipu-psys.c
+++ b/drivers/media/pci/intel/ipu-psys.c
@@ -37,6 +37,7 @@
 #include "ipu-cpd.h"
 #include "ipu-fw-psys.h"
 #include "ipu-psys.h"
+#include "ipu-platform-psys.h"
 #include "ipu-platform-regs.h"
 #include "ipu-fw-isys.h"
 #include "ipu-fw-com.h"
@@ -372,10 +373,11 @@ static void ipu_dma_buf_release(struct dma_buf *buf)
 	if (!kbuf)
 		return;
 
-	dev_dbg(&kbuf->psys->adev->dev, "releasing buffer %d\n", kbuf->fd);
-
-	if (kbuf->db_attach)
+	if (kbuf->db_attach) {
+		dev_dbg(kbuf->db_attach->dev,
+			"releasing buffer %d\n", kbuf->fd);
 		ipu_psys_put_userpages(kbuf->db_attach->priv);
+	}
 	kfree(kbuf);
 }
 
@@ -449,8 +451,7 @@ static int ipu_psys_open(struct inode *inode, struct file *file)
 	struct ipu_psys *psys = inode_to_ipu_psys(inode);
 	struct ipu_device *isp = psys->adev->isp;
 	struct ipu_psys_fh *fh;
-	struct ipu_psys_buffer_set *kbuf_set, *kbuf_set_tmp;
-	int i, rval;
+	int rval;
 
 	if (isp->flr_done)
 		return -EIO;
@@ -461,61 +462,53 @@ static int ipu_psys_open(struct inode *inode, struct file *file)
 		return rval;
 	}
 
-	pm_runtime_use_autosuspend(&psys->adev->dev);
-
 	fh = kzalloc(sizeof(*fh), GFP_KERNEL);
 	if (!fh)
 		return -ENOMEM;
 
+	fh->psys = psys;
+	file->private_data = fh;
+
 	mutex_init(&fh->mutex);
 	INIT_LIST_HEAD(&fh->bufmap);
-	for (i = 0; i < IPU_PSYS_CMD_PRIORITY_NUM; i++)
-		INIT_LIST_HEAD(&fh->kcmds[i]);
-
 	init_waitqueue_head(&fh->wait);
 
-	mutex_init(&fh->bs_mutex);
-	INIT_LIST_HEAD(&fh->buf_sets);
-
-	/* allocate and map memory for buf_sets */
-	for (i = 0; i < IPU_PSYS_BUF_SET_POOL_SIZE; i++) {
-		kbuf_set = kzalloc(sizeof(*kbuf_set), GFP_KERNEL);
-		if (!kbuf_set)
-			goto out_free_buf_sets;
-		kbuf_set->kaddr = dma_alloc_attrs(&psys->adev->dev,
-						  IPU_PSYS_BUF_SET_MAX_SIZE,
-						  &kbuf_set->dma_addr,
-						  GFP_KERNEL,
-						  DMA_ATTR_NON_CONSISTENT);
-		if (!kbuf_set->kaddr) {
-			kfree(kbuf_set);
-			goto out_free_buf_sets;
-		}
-		kbuf_set->size = IPU_PSYS_BUF_SET_MAX_SIZE;
-		list_add(&kbuf_set->list, &fh->buf_sets);
-	}
-
-	fh->psys = psys;
-	file->private_data = fh;
+	rval = ipu_psys_fh_init(fh);
+	if (rval)
+		goto open_failed;
 
 	mutex_lock(&psys->mutex);
+#ifdef IPU_IRQ_POLL
+	if (list_empty(&psys->fhs)) {
+		static const struct sched_param param = {
+			.sched_priority = MAX_USER_RT_PRIO / 2,
+		};
+		psys->isr_thread = kthread_run(ipu_psys_isr_run, psys,
+					       IPU_PSYS_NAME);
+
+		if (IS_ERR(psys->isr_thread)) {
+			mutex_unlock(&psys->mutex);
+			goto open_failed;
+		}
+
+		sched_setscheduler(psys->isr_thread, SCHED_FIFO, &param);
+	}
+#endif
 	list_add_tail(&fh->list, &psys->fhs);
 	mutex_unlock(&psys->mutex);
 
 	return 0;
 
-out_free_buf_sets:
-	list_for_each_entry_safe(kbuf_set, kbuf_set_tmp, &fh->buf_sets, list) {
-		dma_free_attrs(&psys->adev->dev,
-			       kbuf_set->size, kbuf_set->kaddr,
-			       kbuf_set->dma_addr, DMA_ATTR_NON_CONSISTENT);
-		list_del(&kbuf_set->list);
-		kfree(kbuf_set);
+open_failed:
+#ifdef IPU_IRQ_POLL
+	if (list_empty(&psys->fhs) && psys->isr_thread) {
+		kthread_stop(psys->isr_thread);
+		psys->isr_thread = NULL;
 	}
-	mutex_destroy(&fh->bs_mutex);
+#endif
 	mutex_destroy(&fh->mutex);
 	kfree(fh);
-	return -ENOMEM;
+	return rval;
 }
 
 static int ipu_psys_release(struct inode *inode, struct file *file)
@@ -523,76 +516,52 @@ static int ipu_psys_release(struct inode *inode, struct file *file)
 	struct ipu_psys *psys = inode_to_ipu_psys(inode);
 	struct ipu_psys_fh *fh = file->private_data;
 	struct ipu_psys_kbuffer *kbuf, *kbuf0;
-	struct ipu_psys_kcmd *kcmd, *kcmd0;
-	struct ipu_psys_buffer_set *kbuf_set, *kbuf_set0;
-	int p;
-
-	mutex_lock(&psys->mutex);
-	mutex_lock(&fh->mutex);
 
-	/*
-	 * Set pg_user to NULL so that completed kcmds don't write
-	 * their result to user space anymore.
-	 */
-	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++)
-		list_for_each_entry(kcmd, &fh->kcmds[p], list)
-			kcmd->pg_user = NULL;
-
-	/* Prevent scheduler from running more kcmds */
-	memset(fh->new_kcmd_tail, 0, sizeof(fh->new_kcmd_tail));
-
-	/* Wait until kcmds are completed in this queue and free them */
-	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
-		fh->new_kcmd_tail[p] = NULL;
-		list_for_each_entry_safe(kcmd, kcmd0, &fh->kcmds[p], list) {
-			ipu_psys_kcmd_abort(psys, kcmd, -EIO);
-			ipu_psys_kcmd_free(kcmd);
-		}
-	}
-
-	mutex_lock(&fh->bs_mutex);
-	list_for_each_entry_safe(kbuf_set, kbuf_set0, &fh->buf_sets, list) {
-		dma_free_attrs(&psys->adev->dev,
-			       kbuf_set->size, kbuf_set->kaddr,
-			       kbuf_set->dma_addr, DMA_ATTR_NON_CONSISTENT);
-		list_del(&kbuf_set->list);
-		kfree(kbuf_set);
-	}
-	mutex_unlock(&fh->bs_mutex);
+	ipu_psys_fh_deinit(fh);
 
+	mutex_lock(&fh->mutex);
 	/* clean up buffers */
-	list_for_each_entry_safe(kbuf, kbuf0, &fh->bufmap, list) {
-		list_del(&kbuf->list);
-		/* Unmap and release buffers */
-		if (kbuf->dbuf && kbuf->db_attach) {
-			struct dma_buf *dbuf;
-
-			dma_buf_vunmap(kbuf->dbuf, kbuf->kaddr);
-			dma_buf_unmap_attachment(kbuf->db_attach, kbuf->sgt,
-						 DMA_BIDIRECTIONAL);
-			dma_buf_detach(kbuf->dbuf, kbuf->db_attach);
-			dbuf = kbuf->dbuf;
-			kbuf->dbuf = NULL;
-			kbuf->db_attach = NULL;
-			dma_buf_put(dbuf);
-		} else {
-			if (kbuf->db_attach)
-				ipu_psys_put_userpages(kbuf->db_attach->priv);
-			kfree(kbuf);
+	if (!list_empty(&fh->bufmap)) {
+		list_for_each_entry_safe(kbuf, kbuf0, &fh->bufmap, list) {
+			list_del(&kbuf->list);
+			   /* Unmap and release buffers */
+			if (kbuf->dbuf && kbuf->db_attach) {
+				struct dma_buf *dbuf;
+#ifdef CONFIG_VIDEO_INTEL_IPU_MOCK
+				ipu_fw_psys_unregister_ctx_addr(kbuf->kaddr,
+								kbuf->dma_addr);
+#endif
+				kbuf->valid = false;
+				dma_buf_vunmap(kbuf->dbuf, kbuf->kaddr);
+				dma_buf_unmap_attachment(kbuf->db_attach,
+							 kbuf->sgt,
+							 DMA_BIDIRECTIONAL);
+				dma_buf_detach(kbuf->dbuf, kbuf->db_attach);
+				dbuf = kbuf->dbuf;
+				kbuf->dbuf = NULL;
+				kbuf->db_attach = NULL;
+				dma_buf_put(dbuf);
+			} else {
+				if (kbuf->db_attach)
+					ipu_psys_put_userpages(
+						kbuf->db_attach->priv);
+				kfree(kbuf);
+			}
 		}
 	}
+	mutex_unlock(&fh->mutex);
+	mutex_destroy(&fh->mutex);
 
+	mutex_lock(&psys->mutex);
 	list_del(&fh->list);
 
-	/* disable runtime autosuspend for the last fh */
-	if (list_empty(&psys->fhs))
-		pm_runtime_dont_use_autosuspend(&psys->adev->dev);
-
-	mutex_unlock(&fh->mutex);
+#ifdef IPU_IRQ_POLL
+	if (list_empty(&psys->fhs) && psys->isr_thread) {
+		kthread_stop(psys->isr_thread);
+		psys->isr_thread = NULL;
+	}
+#endif
 	mutex_unlock(&psys->mutex);
-
-	mutex_destroy(&fh->bs_mutex);
-	mutex_destroy(&fh->mutex);
 	kfree(fh);
 
 	return 0;
@@ -647,8 +616,6 @@ static int ipu_psys_getbuf(struct ipu_psys_buffer *buf, struct ipu_psys_fh *fh)
 
 	kbuf->fd = ret;
 	buf->base.fd = ret;
-	kbuf->psys = psys;
-	kbuf->fh = fh;
 	kbuf->flags = buf->flags &= ~IPU_BUFFER_FLAG_USERPTR;
 	kbuf->flags = buf->flags |= IPU_BUFFER_FLAG_DMA_HANDLE;
 
@@ -666,73 +633,6 @@ static int ipu_psys_putbuf(struct ipu_psys_buffer *buf, struct ipu_psys_fh *fh)
 	return 0;
 }
 
-static struct ipu_psys_kcmd *__ipu_get_completed_kcmd(struct ipu_psys *psys,
-						      struct ipu_psys_fh *fh)
-{
-	int p;
-
-	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
-		struct ipu_psys_kcmd *kcmd;
-
-		if (list_empty(&fh->kcmds[p]))
-			continue;
-		kcmd = list_first_entry(&fh->kcmds[p],
-					struct ipu_psys_kcmd, list);
-		if (kcmd->state != KCMD_STATE_COMPLETE)
-			continue;
-		/* Found a kcmd in completed state */
-		return kcmd;
-	}
-
-	return NULL;
-}
-
-static struct ipu_psys_kcmd *ipu_get_completed_kcmd(struct ipu_psys *psys,
-						    struct ipu_psys_fh *fh)
-{
-	struct ipu_psys_kcmd *kcmd;
-
-	mutex_lock(&fh->mutex);
-	kcmd = __ipu_get_completed_kcmd(psys, fh);
-	mutex_unlock(&fh->mutex);
-
-	return kcmd;
-}
-
-static long ipu_ioctl_dqevent(struct ipu_psys_event *event,
-			      struct ipu_psys_fh *fh, unsigned int f_flags)
-{
-	struct ipu_psys *psys = fh->psys;
-	struct ipu_psys_kcmd *kcmd = NULL;
-	int rval;
-
-	dev_dbg(&psys->adev->dev, "IOC_DQEVENT\n");
-
-	if (!(f_flags & O_NONBLOCK)) {
-		rval = wait_event_interruptible(fh->wait,
-						(kcmd =
-						 ipu_get_completed_kcmd(psys,
-									fh)));
-		if (rval == -ERESTARTSYS)
-			return rval;
-	}
-
-	mutex_lock(&fh->mutex);
-	if (!kcmd) {
-		kcmd = __ipu_get_completed_kcmd(psys, fh);
-		if (!kcmd) {
-			mutex_unlock(&fh->mutex);
-			return -ENODATA;
-		}
-	}
-
-	*event = kcmd->ev;
-	ipu_psys_kcmd_free(kcmd);
-	mutex_unlock(&fh->mutex);
-
-	return 0;
-}
-
 static long ipu_psys_mapbuf(int fd, struct ipu_psys_fh *fh)
 {
 	struct ipu_psys *psys = fh->psys;
@@ -754,8 +654,6 @@ static long ipu_psys_mapbuf(int fd, struct ipu_psys_fh *fh)
 			return -ENOMEM;
 		}
 
-		kbuf->psys = psys;
-		kbuf->fh = fh;
 		list_add_tail(&kbuf->list, &fh->bufmap);
 	}
 
@@ -880,7 +778,7 @@ static unsigned int ipu_psys_poll(struct file *file,
 
 	poll_wait(file, &fh->wait, wait);
 
-	if (ipu_get_completed_kcmd(psys, fh))
+	if (ipu_get_completed_kcmd(fh))
 		res = POLLIN;
 
 	dev_dbg(&psys->adev->dev, "ipu psys poll res %u\n", res);
diff --git a/drivers/media/pci/intel/ipu4/ipu4-psys.c b/drivers/media/pci/intel/ipu4/ipu4-psys.c
index 82e80d1..73e2b7c 100644
--- a/drivers/media/pci/intel/ipu4/ipu4-psys.c
+++ b/drivers/media/pci/intel/ipu4/ipu4-psys.c
@@ -113,6 +113,11 @@ struct ipu_trace_block psys_trace_blocks[] = {
 	}
 };
 
+static int ipu_psys_kcmd_abort(struct ipu_psys *psys,
+			       struct ipu_psys_kcmd *kcmd);
+static int ipu_psys_kcmd_queue(struct ipu_psys *psys,
+			       struct ipu_psys_kcmd *kcmd);
+
 static void set_sp_info_bits(void *base)
 {
 	int i;
@@ -204,11 +209,6 @@ void ipu_psys_kcmd_free(struct ipu_psys_kcmd *kcmd)
 		kcmd->kpg->pg_size = 0;
 	spin_unlock_irqrestore(&psys->pgs_lock, flags);
 
-	mutex_lock(&kcmd->fh->bs_mutex);
-	if (kcmd->kbuf_set)
-		kcmd->kbuf_set->buf_set_size = 0;
-	mutex_unlock(&kcmd->fh->bs_mutex);
-
 	kfree(kcmd->pg_manifest);
 	kfree(kcmd->kbufs);
 	kfree(kcmd->buffers);
@@ -353,9 +353,9 @@ static void ipu_psys_kcmd_run(struct ipu_psys *psys)
 
 	if (ret != -ENOSPC || !psys->active_kcmds) {
 		dev_err(&psys->adev->dev,
-			"kcmd %p failed to alloc resources (running (%d, psys->active_kcmds = %d))\n",
-			kcmd, ret, psys->active_kcmds);
-		ipu_psys_kcmd_abort(psys, kcmd, ret);
+		  "kcmd %p failed to alloc resources %d, active_kcmds %d\n",
+		  kcmd, ret, psys->active_kcmds);
+		ipu_psys_kcmd_abort(psys, kcmd);
 		return;
 	}
 }
@@ -484,7 +484,7 @@ void ipu_psys_run_next(struct ipu_psys *psys)
 
 				mutex_lock(&fh->mutex);
 
-				kcmd = fh->new_kcmd_tail[p];
+				kcmd = fh->sched.new_kcmd_tail[p];
 				/*
 				 * If concurrency is disabled and there are
 				 * already commands running on the PSYS, do not
@@ -505,14 +505,15 @@ void ipu_psys_run_next(struct ipu_psys *psys)
 					goto next;
 
 				/* Update pointer to the first new kcmd */
-				fh->new_kcmd_tail[p] = NULL;
-				while (kcmd != list_last_entry(&fh->kcmds[p],
-							       struct
-							       ipu_psys_kcmd,
-							       list)) {
+				fh->sched.new_kcmd_tail[p] = NULL;
+				while (kcmd != list_last_entry(
+						&fh->sched.kcmds[p],
+						struct ipu_psys_kcmd,
+						list)) {
 					kcmd = list_next_entry(kcmd, list);
 					if (kcmd->state == KCMD_STATE_NEW) {
-						fh->new_kcmd_tail[p] = kcmd;
+						fh->sched.new_kcmd_tail[p] =
+							kcmd;
 						break;
 					}
 				}
@@ -533,8 +534,7 @@ void ipu_psys_run_next(struct ipu_psys *psys)
  * Move kcmd into completed state. If kcmd is currently running,
  * abort it.
  */
-int ipu_psys_kcmd_abort(struct ipu_psys *psys,
-			struct ipu_psys_kcmd *kcmd, int error)
+int ipu_psys_kcmd_abort(struct ipu_psys *psys, struct ipu_psys_kcmd *kcmd)
 {
 	int ret = 0;
 
@@ -653,8 +653,8 @@ static void ipu_psys_flush_kcmds(struct ipu_psys *psys, int error)
 	list_for_each_entry(fh, &psys->fhs, list) {
 		mutex_lock(&fh->mutex);
 		for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
-			fh->new_kcmd_tail[p] = NULL;
-			list_for_each_entry(kcmd, &fh->kcmds[p], list) {
+			fh->sched.new_kcmd_tail[p] = NULL;
+			list_for_each_entry(kcmd, &fh->sched.kcmds[p], list) {
 				if (kcmd->state == KCMD_STATE_COMPLETE)
 					continue;
 				ipu_psys_kcmd_complete(psys, kcmd, error);
@@ -714,8 +714,8 @@ void ipu_psys_watchdog_work(struct work_struct *work)
 		for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
 			struct ipu_psys_kcmd *kcmd;
 
-			list_for_each_entry(kcmd, &fh->kcmds[p], list) {
-				if (fh->new_kcmd_tail[p] == kcmd)
+			list_for_each_entry(kcmd, &fh->sched.kcmds[p], list) {
+				if (fh->sched.new_kcmd_tail[p] == kcmd)
 					break;
 				if (kcmd->state != KCMD_STATE_RUNNING)
 					continue;
@@ -726,7 +726,7 @@ void ipu_psys_watchdog_work(struct work_struct *work)
 				dev_err(&psys->adev->dev,
 					"kcmd:0x%llx[0x%llx] taking too long\n",
 					kcmd->user_token, kcmd->issue_id);
-				r = ipu_psys_kcmd_abort(psys, kcmd, -ETIME);
+				r = ipu_psys_kcmd_abort(psys, kcmd);
 				if (r)
 					goto stop_failed;
 			}
@@ -819,7 +819,7 @@ static bool ipu_psys_kcmd_is_valid(struct ipu_psys *psys,
 	list_for_each_entry(fh, &psys->fhs, list) {
 		mutex_lock(&fh->mutex);
 		for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
-			list_for_each_entry(kcmd0, &fh->kcmds[p], list) {
+			list_for_each_entry(kcmd0, &fh->sched.kcmds[p], list) {
 				if (kcmd0 == kcmd) {
 					mutex_unlock(&fh->mutex);
 					return true;
@@ -832,7 +832,8 @@ static bool ipu_psys_kcmd_is_valid(struct ipu_psys *psys,
 	return false;
 }
 
-int ipu_psys_kcmd_queue(struct ipu_psys *psys, struct ipu_psys_kcmd *kcmd)
+static int ipu_psys_kcmd_queue(struct ipu_psys *psys,
+			       struct ipu_psys_kcmd *kcmd)
 {
 	int ret;
 
@@ -926,9 +927,10 @@ int ipu_psys_kcmd_new(struct ipu_psys_command *cmd, struct ipu_psys_fh *fh)
 		goto error;
 
 	mutex_lock(&fh->mutex);
-	list_add_tail(&kcmd->list, &fh->kcmds[cmd->priority]);
-	if (!fh->new_kcmd_tail[cmd->priority] && kcmd->state == KCMD_STATE_NEW) {
-		fh->new_kcmd_tail[cmd->priority] = kcmd;
+	list_add_tail(&kcmd->list, &fh->sched.kcmds[cmd->priority]);
+	if (!fh->sched.new_kcmd_tail[cmd->priority] &&
+	    kcmd->state == KCMD_STATE_NEW) {
+		fh->sched.new_kcmd_tail[cmd->priority] = kcmd;
 		/* Kick command scheduler thread */
 		atomic_set(&psys->wakeup_sched_thread_count, 1);
 		wake_up_interruptible(&psys->sched_cmd_wq);
@@ -986,3 +988,122 @@ void ipu_psys_handle_events(struct ipu_psys *psys)
 		wake_up_interruptible(&psys->sched_cmd_wq);
 	} while (1);
 }
+
+int ipu_psys_fh_init(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	int p;
+
+	pm_runtime_use_autosuspend(&psys->adev->dev);
+	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++)
+		INIT_LIST_HEAD(&fh->sched.kcmds[p]);
+
+	return 0;
+}
+
+int ipu_psys_fh_deinit(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_kcmd *kcmd, *kcmd0;
+	int p;
+
+	mutex_lock(&psys->mutex);
+	mutex_lock(&fh->mutex);
+
+	/*
+	 * Set pg_user to NULL so that completed kcmds don't write
+	 * their result to user space anymore.
+	 */
+	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++)
+		list_for_each_entry(kcmd, &fh->sched.kcmds[p], list)
+			kcmd->pg_user = NULL;
+
+	/* Prevent scheduler from running more kcmds */
+	memset(fh->sched.new_kcmd_tail, 0,
+		sizeof(fh->sched.new_kcmd_tail));
+
+	/* Wait until kcmds are completed in this queue and free them */
+	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
+		fh->sched.new_kcmd_tail[p] = NULL;
+		list_for_each_entry_safe(
+			kcmd, kcmd0, &fh->sched.kcmds[p], list) {
+			ipu_psys_kcmd_abort(psys, kcmd);
+			ipu_psys_kcmd_free(kcmd);
+		}
+	}
+
+	/* disable runtime autosuspend for the last fh */
+	if (list_empty(&psys->fhs))
+		pm_runtime_dont_use_autosuspend(&psys->adev->dev);
+
+	mutex_unlock(&fh->mutex);
+	mutex_unlock(&psys->mutex);
+
+	return 0;
+}
+
+static struct ipu_psys_kcmd *__ipu_get_completed_kcmd(struct ipu_psys_fh *fh)
+{
+	int p;
+
+	for (p = 0; p < IPU_PSYS_CMD_PRIORITY_NUM; p++) {
+		struct ipu_psys_kcmd *kcmd;
+
+		if (list_empty(&fh->sched.kcmds[p]))
+			continue;
+
+		kcmd = list_first_entry(&fh->sched.kcmds[p],
+					struct ipu_psys_kcmd, list);
+		if (kcmd->state != KCMD_STATE_COMPLETE)
+			continue;
+		/* Found a kcmd in completed state */
+		return kcmd;
+
+	}
+
+	return NULL;
+}
+
+struct ipu_psys_kcmd *ipu_get_completed_kcmd(struct ipu_psys_fh *fh)
+{
+	struct ipu_psys_kcmd *kcmd;
+
+	mutex_lock(&fh->mutex);
+	kcmd = __ipu_get_completed_kcmd(fh);
+	mutex_unlock(&fh->mutex);
+
+	return kcmd;
+}
+
+long ipu_ioctl_dqevent(struct ipu_psys_event *event,
+			      struct ipu_psys_fh *fh, unsigned int f_flags)
+{
+	struct ipu_psys *psys = fh->psys;
+	struct ipu_psys_kcmd *kcmd = NULL;
+	int rval;
+
+	dev_dbg(&psys->adev->dev, "IOC_DQEVENT\n");
+
+	if (!(f_flags & O_NONBLOCK)) {
+		rval = wait_event_interruptible(fh->wait,
+						(kcmd =
+						 ipu_get_completed_kcmd(fh)));
+		if (rval == -ERESTARTSYS)
+			return rval;
+	}
+
+	mutex_lock(&fh->mutex);
+	if (!kcmd) {
+		kcmd = __ipu_get_completed_kcmd(fh);
+		if (!kcmd) {
+			mutex_unlock(&fh->mutex);
+			return -ENODATA;
+		}
+	}
+
+	*event = kcmd->ev;
+	ipu_psys_kcmd_free(kcmd);
+	mutex_unlock(&fh->mutex);
+
+	return 0;
+}
-- 
2.7.4

