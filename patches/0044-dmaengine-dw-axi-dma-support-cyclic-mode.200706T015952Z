From beacc09a3cf774f4c9f3010b2098a15c1a883320 Mon Sep 17 00:00:00 2001
From: Sia Jee Heng <jee.heng.sia@intel.com>
Date: Fri, 26 Jun 2020 13:45:43 +0800
Subject: [PATCH 044/131] dmaengine: dw-axi-dma: support cyclic mode

Enabled dma cyclic mode to run on KMB platform.

Signed-off-by: Sia Jee Heng <jee.heng.sia@intel.com>
---
 .../dma/dw-axi-dmac/dw-axi-dmac-platform.c    | 239 +++++++++++++++++-
 drivers/dma/dw-axi-dmac/dw-axi-dmac.h         |   1 +
 2 files changed, 232 insertions(+), 8 deletions(-)

diff --git a/drivers/dma/dw-axi-dmac/dw-axi-dmac-platform.c b/drivers/dma/dw-axi-dmac/dw-axi-dmac-platform.c
index 0a4b5f28a389..caed78b66a8b 100644
--- a/drivers/dma/dw-axi-dmac/dw-axi-dmac-platform.c
+++ b/drivers/dma/dw-axi-dmac/dw-axi-dmac-platform.c
@@ -910,6 +910,204 @@ static void axi_chan_list_dump_lli(struct axi_dma_chan *chan,
 		axi_chan_dump_lli(chan, desc);
 }
 
+static struct dma_async_tx_descriptor *
+dw_chan_prep_dma_cyclic(struct dma_chan *dchan, dma_addr_t buf_addr,
+			size_t buf_len, size_t period_len,
+			enum dma_transfer_direction direction,
+			unsigned long flags)
+{
+	struct axi_dma_chan *chan = dchan_to_axi_dma_chan(dchan);
+	struct axi_dma_desc *first = NULL, *prev = NULL;
+	unsigned int reg_width;
+	unsigned int mem_width;
+	unsigned int data_width = BIT(chan->chip->dw->hdata->m_data_width);
+	dma_addr_t reg;
+	u32 reg_value = 0;
+	unsigned int i = 0;
+	u32 ctllo, ctlhi;
+	size_t total_len = 0;
+	size_t block_ts, max_block_ts;
+	u8 lms = 0; // Select AXI0 master for LLI fetching
+	u32 offset;
+
+	if (unlikely(!is_slave_direction(direction)))
+		return NULL;
+
+	chan->direction = direction;
+	chan->cyclic = 0x1;
+
+	max_block_ts = chan->chip->dw->hdata->block_size[chan->id];
+
+	axi_set_hw_channel(chan->chip, chan->hw_hs_num);
+
+	switch (direction) {
+	case DMA_MEM_TO_DEV:
+		reg_width = __ffs(chan->slave_config.dst_addr_width);
+
+		chan->reg_width = reg_width;
+		if (reg_width == DWAXIDMAC_TRANS_WIDTH_16) {
+			offset = DMAC_APB_HALFWORD_WR_CH_EN;
+			reg_value = axi_dma_apb_ioread32(chan->chip, offset);
+			reg_value |= 0x1 << chan->id;
+			axi_dma_apb_iowrite32(chan->chip,
+				DMAC_APB_HALFWORD_WR_CH_EN, reg_value);
+		} else if (reg_width == DWAXIDMAC_TRANS_WIDTH_8) {
+			offset = DMAC_APB_BYTE_WR_CH_EN;
+			reg_value = axi_dma_apb_ioread32(chan->chip, offset);
+			reg_value |= 0x1 << chan->id;
+			axi_dma_apb_iowrite32(chan->chip, offset, reg_value);
+		}
+		reg = chan->slave_config.dst_addr;
+
+		ctllo = axi_dma_prepare_ctllo(chan) |
+			reg_width << CH_CTL_L_DST_WIDTH_POS |
+			DWAXIDMAC_CH_CTL_L_NOINC << CH_CTL_L_DST_INC_POS |
+			DWAXIDMAC_CH_CTL_L_INC << CH_CTL_L_SRC_INC_POS;
+
+		for (i = 0; i < buf_len / period_len; i++) {
+			struct axi_dma_desc *desc;
+			size_t xfer_len;
+			u32 mem, len;
+
+			mem_width = __ffs(data_width | mem | len);
+			if (mem_width > DWAXIDMAC_TRANS_WIDTH_32)
+				mem_width = DWAXIDMAC_TRANS_WIDTH_32;
+
+			xfer_len = period_len;
+			block_ts = period_len;
+
+			desc = axi_desc_get(chan);
+			if (unlikely(!desc))
+				goto err_desc_get;
+
+			if (block_ts > max_block_ts) {
+				block_ts = max_block_ts;
+				xfer_len = max_block_ts << mem_width;
+			}
+
+			ctlhi = axi_dma_prepare_ctlhi(chan);
+
+			ctllo |= mem_width << CH_CTL_L_SRC_WIDTH_POS;
+			write_desc_sar(desc, buf_addr + i * period_len);
+			write_desc_dar(desc, reg);
+			desc->lli.block_ts_lo = period_len / 4;
+			desc->lli.ctl_hi = cpu_to_le32(ctlhi);
+			desc->lli.ctl_lo = cpu_to_le32(ctllo);
+
+			set_desc_src_master(desc);
+			set_desc_src_master(desc);
+
+			// Manage transfer list (xfer_list)
+			if (!first) {
+				first = desc;
+			} else {
+				write_desc_llp(prev, desc->vd.tx.phys | lms);
+				list_add_tail(&desc->xfer_list,
+					      &first->xfer_list);
+			}
+			prev = desc;
+			if (i == ((buf_len / period_len) - 1))
+				write_desc_llp(prev, first->vd.tx.phys | lms);
+
+			total_len += xfer_len;
+
+			set_desc_last(desc);
+		}
+		break;
+	case DMA_DEV_TO_MEM:
+		reg_width = __ffs(chan->slave_config.src_addr_width);
+
+		chan->reg_width = reg_width;
+		if (reg_width == DWAXIDMAC_TRANS_WIDTH_16) {
+			offset = DMAC_APB_HALFWORD_WR_CH_EN;
+			reg_value = axi_dma_apb_ioread32(chan->chip, offset);
+			reg_value |= 0x1 << chan->id;
+			axi_dma_apb_iowrite32(chan->chip, offset, reg_value);
+		} else if (reg_width == DWAXIDMAC_TRANS_WIDTH_8) {
+			offset = DMAC_APB_BYTE_WR_CH_EN;
+			reg_value = axi_dma_apb_ioread32(chan->chip, offset);
+			reg_value |= 0x1 << chan->id;
+			axi_dma_apb_iowrite32(chan->chip, offset, reg_value);
+		}
+		reg = chan->slave_config.src_addr;
+		if (reg_width > DWAXIDMAC_TRANS_WIDTH_32)
+			reg_width = DWAXIDMAC_TRANS_WIDTH_32;
+		ctllo = axi_dma_prepare_ctllo(chan) |
+			reg_width << CH_CTL_L_SRC_WIDTH_POS |
+			DWAXIDMAC_CH_CTL_L_INC << CH_CTL_L_DST_INC_POS |
+			// Workaround
+			DWAXIDMAC_CH_CTL_L_NOINC << CH_CTL_L_SRC_INC_POS;
+
+		for (i = 0; i < buf_len / period_len; i++) {
+			struct axi_dma_desc *desc;
+			size_t xfer_len;
+			u32 mem, len;
+
+			desc = axi_desc_get(chan);
+			if (unlikely(!desc))
+				goto err_desc_get;
+
+			xfer_len = len;
+			block_ts = xfer_len >> reg_width;
+			if (block_ts > max_block_ts) {
+				block_ts = max_block_ts;
+				xfer_len = max_block_ts << reg_width;
+			}
+			xfer_len = period_len;
+			block_ts = period_len;
+			mem_width = __ffs(data_width | mem | xfer_len);
+			if (mem_width > DWAXIDMAC_TRANS_WIDTH_32)
+				mem_width = DWAXIDMAC_TRANS_WIDTH_32;
+
+			ctlhi = axi_dma_prepare_ctlhi(chan);
+			ctllo |= mem_width << CH_CTL_L_DST_WIDTH_POS;
+
+			write_desc_sar(desc, reg);
+			write_desc_dar(desc, buf_addr + i * period_len);
+			if (reg_width == DWAXIDMAC_TRANS_WIDTH_16)
+				desc->lli.block_ts_lo = period_len / 2;
+			else if (reg_width >= DWAXIDMAC_TRANS_WIDTH_32)
+				desc->lli.block_ts_lo = period_len / 4;
+			desc->lli.ctl_hi = cpu_to_le32(ctlhi);
+			desc->lli.ctl_lo = cpu_to_le32(ctllo);
+
+			set_desc_src_master(desc);
+			set_desc_src_master(desc);
+
+			// Manage transfer list (xfer_list)
+			if (!first) {
+				first = desc;
+			} else {
+				write_desc_llp(prev, desc->vd.tx.phys | lms);
+				list_add_tail(&desc->xfer_list,
+					      &first->xfer_list);
+			}
+			prev = desc;
+			if (i == ((buf_len / period_len) - 1))
+				write_desc_llp(prev, first->vd.tx.phys | lms);
+
+			total_len += xfer_len;
+
+			// TODO: check if needed
+			set_desc_last(desc);
+		}
+		break;
+	default:
+		return NULL;
+	}
+
+	if (unlikely(!first))
+		return NULL;
+
+	return vchan_tx_prep(&chan->vc, &first->vd, flags);
+
+err_desc_get:
+	if (first)
+		axi_desc_put(first);
+
+	return NULL;
+}
+
 static noinline void axi_chan_handle_err(struct axi_dma_chan *chan, u32 status)
 {
 	struct virt_dma_desc *vd;
@@ -945,6 +1143,8 @@ static void axi_chan_block_xfer_complete(struct axi_dma_chan *chan)
 {
 	struct virt_dma_desc *vd;
 	unsigned long flags;
+	struct axi_dma_desc *desc_head;
+	struct axi_dma_desc *desc;
 
 	spin_lock_irqsave(&chan->vc.lock, flags);
 	if (unlikely(axi_chan_is_hw_enable(chan))) {
@@ -955,13 +1155,32 @@ static void axi_chan_block_xfer_complete(struct axi_dma_chan *chan)
 
 	/* The completed descriptor currently is in the head of vc list */
 	vd = vchan_next_desc(&chan->vc);
-	/* Remove the completed descriptor from issued list before completing */
-	list_del(&vd->node);
-	vchan_cookie_complete(vd);
 
-	/* Submit queued descriptors after processing the completed ones */
-	axi_chan_start_first_queued(chan);
+	if (chan->cyclic) {
+		vchan_cyclic_callback(vd);
+		if (chan->desc) {
+			desc = list_next_entry(chan->desc, xfer_list);
+		} else {
+			desc_head = vd_to_axi_desc(vd);
+			desc = list_next_entry(desc_head, xfer_list);
+		}
+
+		if (desc) {
+			axi_chan_irq_clear(chan, desc->lli.status_lo);//TBD
+			desc->lli.ctl_hi |= CH_CTL_H_LLI_VALID;//TBD
+			chan->desc = desc;
+			axi_chan_block_xfer_start(chan, desc);
+		}
+	} else {
+		/* Remove the completed descriptor from issued list before */
+		/* completing */
+		list_del(&vd->node);
+		vchan_cookie_complete(vd);
 
+		/* Submit queued descriptors after processing the completed */
+		/* ones */
+		axi_chan_start_first_queued(chan);
+	}
 	spin_unlock_irqrestore(&chan->vc.lock, flags);
 }
 
@@ -1021,6 +1240,9 @@ static int dma_chan_terminate_all(struct dma_chan *dchan)
 		}
 	}
 
+	axi_chan_disable(chan);
+	chan->cyclic = 0x0;
+	axi_free_hw_channel(chan->chip, chan->hw_hs_num);
 	vchan_get_all_descriptors(&chan->vc, &head);
 
 	/*
@@ -1313,13 +1535,14 @@ static int dw_probe(struct platform_device *pdev)
 	dma_cap_set(DMA_MEMCPY, dw->dma.cap_mask);
 	dma_cap_set(DMA_SLAVE, dw->dma.cap_mask);
 	dma_cap_set(DMA_PRIVATE, dw->dma.cap_mask);
-
+	dma_cap_set(DMA_CYCLIC, dw->dma.cap_mask);
 	/* DMA capabilities */
 	dw->dma.chancnt = hdata->nr_channels;
 	dw->dma.src_addr_widths = AXI_DMA_BUSWIDTHS;
 	dw->dma.dst_addr_widths = AXI_DMA_BUSWIDTHS;
 	dw->dma.directions = BIT(DMA_DEV_TO_MEM) | BIT(DMA_MEM_TO_DEV) |
-							BIT(DMA_MEM_TO_MEM);
+			     BIT(DMA_MEM_TO_MEM);
+	dw->dma.residue_granularity = DMA_RESIDUE_GRANULARITY_BURST;
 
 	dw->dma.dev = chip->dev;
 	dw->dma.device_tx_status = dma_chan_tx_status;
@@ -1334,7 +1557,7 @@ static int dw_probe(struct platform_device *pdev)
 	dw->dma.device_prep_dma_memcpy = dma_chan_prep_dma_memcpy;
 	dw->dma.device_prep_slave_sg = dma_chan_prep_slave_sg;
 	dw->dma.device_config = dma_chan_slave_config;
-
+	dw->dma.device_prep_dma_cyclic = dw_chan_prep_dma_cyclic;
 	platform_set_drvdata(pdev, chip);
 
 	pm_runtime_enable(chip->dev);
diff --git a/drivers/dma/dw-axi-dmac/dw-axi-dmac.h b/drivers/dma/dw-axi-dmac/dw-axi-dmac.h
index c09f2120d3d5..82f25ec7cb8e 100644
--- a/drivers/dma/dw-axi-dmac/dw-axi-dmac.h
+++ b/drivers/dma/dw-axi-dmac/dw-axi-dmac.h
@@ -38,6 +38,7 @@ struct axi_dma_chan {
 	void __iomem			*chan_regs;
 	u8				id;
 	u8				hw_hs_num;
+	u16				cyclic;
 	u32				reg_width;
 	atomic_t			descs_allocated;
 
-- 
2.17.1

