From 186ab6ffbf2430cd58e5c9a63f3303103aca5da9 Mon Sep 17 00:00:00 2001
From: "Wong, Vincent Por Yin" <vincent.por.yin.wong@intel.com>
Date: Fri, 9 Aug 2019 22:17:52 +0800
Subject: [PATCH 101/104] net: stmmac: Zero-copy transmit and XDP_TX support

- AF_XDP ZC support, uses the same TX queue for packet queueing
- support for AF_XDP ZC Receive path's XDP_TX

Signed-off-by: Wong, Vincent Por Yin <vincent.por.yin.wong@intel.com>
---
 drivers/net/ethernet/stmicro/stmmac/stmmac.h  |   4 +
 .../net/ethernet/stmicro/stmmac/stmmac_main.c |  55 +++++-
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.c  | 156 ++++++++++++++++++
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.h  |   5 +
 4 files changed, 216 insertions(+), 4 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac.h b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
index 1bb4c4351e61..d7e331c9f222 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
@@ -60,6 +60,9 @@ struct stmmac_tx_queue {
 	struct sk_buff **tx_skbuff;
 	struct stmmac_tx_info *tx_skbuff_dma;
 	struct xdp_frame **xdpf;
+	struct xdp_umem *xsk_umem;
+	bool *is_zc_pkt;
+	u32 xdpzc_inv_len_pkt;
 	unsigned int cur_tx;
 	unsigned int dirty_tx;
 	dma_addr_t dma_tx_phy;
@@ -308,6 +311,7 @@ void stmmac_rx_vlan(struct net_device *dev, struct sk_buff *skb);
 void stmmac_txrx_ring_enable(struct stmmac_priv *priv, u16 qid);
 void stmmac_txrx_ring_disable(struct stmmac_priv *priv, u16 qid);
 void stmmac_free_tx_buffer(struct stmmac_priv *priv, u32 queue, int i);
+int stmmac_xsk_async_xmit(struct net_device *dev, u32 qid);
 
 #if IS_ENABLED(CONFIG_STMMAC_SELFTESTS)
 void stmmac_selftest_run(struct net_device *dev,
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 9991e5ef8b59..a3d2bfeeee6f 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -1432,9 +1432,8 @@ static void init_dma_tx_desc_ring_q(struct stmmac_priv *priv, u32 queue)
 		tx_q->tx_skbuff_dma[i].len = 0;
 		tx_q->tx_skbuff_dma[i].last_segment = false;
 		tx_q->tx_skbuff[i] = NULL;
-
-		if (stmmac_enabled_xdp(priv))
-			tx_q->xdpf[i] = NULL;
+		tx_q->xdpf[i] = NULL;
+		tx_q->is_zc_pkt[i] = false;
 	}
 
 	tx_q->dirty_tx = 0;
@@ -1577,6 +1576,8 @@ static void free_dma_tx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
 
 	kfree(tx_q->tx_skbuff_dma);
 	kfree(tx_q->tx_skbuff);
+	kfree(tx_q->xdpf);
+	kfree(tx_q->is_zc_pkt);
 }
 
 /**
@@ -1712,6 +1713,12 @@ static int alloc_dma_tx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
 	if (!tx_q->xdpf)
 		goto err_dma;
 
+	tx_q->is_zc_pkt = kcalloc(priv->dma_tx_size,
+				  sizeof(bool),
+				  GFP_KERNEL);
+	if (!tx_q->is_zc_pkt)
+		goto err_dma;
+
 	if (priv->extend_desc) {
 		tx_q->dma_etx = dma_alloc_coherent(priv->device,
 						   priv->dma_tx_size *
@@ -1744,8 +1751,10 @@ static int alloc_dma_tx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
 err_dma:
 	kfree(tx_q->tx_skbuff);
 	kfree(tx_q->xdpf);
+	kfree(tx_q->is_zc_pkt);
 	tx_q->tx_skbuff = NULL;
 	tx_q->xdpf = NULL;
+	tx_q->is_zc_pkt = NULL;
 	return -ENOMEM;
 }
 
@@ -2016,6 +2025,11 @@ static void stmmac_free_tx_queue(struct stmmac_priv *priv, u16 qid)
 	struct stmmac_tx_queue *tx_q = &priv->tx_queue[qid];
 	unsigned int entry = tx_q->dirty_tx;
 
+	if (tx_q->xsk_umem) {
+		stmmac_xsk_free_tx_ring(tx_q);
+		goto out;
+	}
+
 	while (entry != tx_q->cur_tx) {
 		struct sk_buff *skb = tx_q->tx_skbuff[entry];
 
@@ -2044,6 +2058,7 @@ static void stmmac_free_tx_queue(struct stmmac_priv *priv, u16 qid)
 			tx_q->tx_skbuff_dma[entry].buf = 0;
 			tx_q->tx_skbuff_dma[entry].len = 0;
 			tx_q->tx_skbuff_dma[entry].map_as_page = false;
+			tx_q->is_zc_pkt[entry] = false;
 		}
 
 		tx_q->tx_skbuff_dma[entry].last_segment = false;
@@ -2055,6 +2070,7 @@ static void stmmac_free_tx_queue(struct stmmac_priv *priv, u16 qid)
 	/* Reset Byte Queue Limits (BQL) for the queue */
 	netdev_tx_reset_queue(netdev_get_tx_queue(priv->dev, qid));
 
+out:
 	/* Reset cur_tx and dirty_tx */
 	tx_q->cur_tx = 0;
 	tx_q->dirty_tx = 0;
@@ -2083,6 +2099,10 @@ static void stmmac_configure_tx_queue(struct stmmac_priv *priv, u16 qid)
 {
 	struct stmmac_tx_queue *tx_q = &priv->tx_queue[qid];
 
+	tx_q->xsk_umem = NULL;
+	if (queue_is_xdp(qid))
+		tx_q->xsk_umem = stmmac_xsk_umem(priv, qid);
+
 	/* Init Tx descriptor */
 	init_dma_tx_desc_ring_q(priv, qid);
 	stmmac_clear_tx_descriptors(priv, qid);
@@ -2260,6 +2280,7 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 	struct stmmac_tx_queue *tx_q = &priv->tx_queue[queue];
 	unsigned int bytes_compl = 0, pkts_compl = 0;
 	unsigned int entry, count = 0;
+	unsigned int xsk_frames = 0;
 
 	__netif_tx_lock_bh(netdev_get_tx_queue(priv->dev, queue));
 
@@ -2365,6 +2386,8 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 			bytes_compl += tx_q->xdpf[entry]->len;
 			xdp_return_frame(tx_q->xdpf[entry]);
 			tx_q->xdpf[entry] = NULL;
+		} else if (stmmac_enabled_xdp(priv) && tx_q->is_zc_pkt[entry]) {
+			xsk_frames++;
 		}
 
 		stmmac_release_tx_desc(priv, p, priv->mode);
@@ -2396,6 +2419,17 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 
 	__netif_tx_unlock_bh(netdev_get_tx_queue(priv->dev, queue));
 
+	if (queue_is_xdp(queue) && stmmac_enabled_xdp(priv) && tx_q->xsk_umem) {
+		if (xsk_frames) {
+			xsk_umem_complete_tx(tx_q->xsk_umem, xsk_frames
+					     + tx_q->xdpzc_inv_len_pkt);
+			tx_q->xdpzc_inv_len_pkt = 0;
+		}
+
+		/* TODO: Create a separate thread for below */
+		stmmac_xdp_xmit_zc(tx_q, STMMAC_DEFAULT_TX_WORK);
+	}
+
 	return count;
 }
 
@@ -3724,6 +3758,7 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 
 	tx_q->tx_skbuff_dma[first_entry].buf = des;
 	tx_q->tx_skbuff_dma[first_entry].len = skb_headlen(skb);
+	tx_q->is_zc_pkt[first_entry] = false;
 
 	if (priv->dma_cap.addr64 <= 32) {
 		first->des0 = cpu_to_le32(des);
@@ -3757,6 +3792,7 @@ static netdev_tx_t stmmac_tso_xmit(struct sk_buff *skb, struct net_device *dev)
 		tx_q->tx_skbuff_dma[tx_q->cur_tx].buf = des;
 		tx_q->tx_skbuff_dma[tx_q->cur_tx].len = skb_frag_size(frag);
 		tx_q->tx_skbuff_dma[tx_q->cur_tx].map_as_page = true;
+		tx_q->is_zc_pkt[tx_q->cur_tx] = false;
 	}
 
 	tx_q->tx_skbuff_dma[tx_q->cur_tx].last_segment = true;
@@ -3975,6 +4011,7 @@ static netdev_tx_t stmmac_xmit(struct sk_buff *skb, struct net_device *dev)
 			goto dma_map_err; /* should reuse desc w/o issues */
 
 		tx_q->tx_skbuff_dma[entry].buf = des;
+		tx_q->is_zc_pkt[entry] = false;
 
 		stmmac_set_desc_addr(priv, desc, des);
 
@@ -4238,6 +4275,11 @@ static int stmmac_xdp_setup(struct net_device *dev, struct bpf_prog *prog)
 	if (old_prog)
 		bpf_prog_put(old_prog);
 
+	if (need_reset && prog)
+		for (i = 0; i < priv->plat->rx_queues_to_use; i++)
+			if (priv->rx_queue[i].xsk_umem)
+				(void)stmmac_xsk_async_xmit(priv->dev, i);
+
 	return 0;
 }
 
@@ -4319,6 +4361,9 @@ static int stmmac_xdp(struct net_device *dev, struct netdev_bpf *xdp)
 	case XDP_QUERY_PROG:
 		xdp->prog_id = priv->xdp_prog ? priv->xdp_prog->aux->id : 0;
 		return 0;
+	case XDP_SETUP_XSK_UMEM:
+		return stmmac_xsk_umem_setup(priv, xdp->xsk.umem,
+					     xdp->xsk.queue_id);
 	default:
 		return -EINVAL;
 	}
@@ -4545,7 +4590,8 @@ static int stmmac_napi_poll_tx(struct napi_struct *napi, int budget)
 
 	priv->xstats.napi_poll++;
 
-	work_done = stmmac_tx_clean(priv, priv->dma_tx_size, chan);
+	work_done = stmmac_tx_clean(priv, budget, chan);
+
 	work_done = min(work_done, budget);
 
 	if (work_done < budget)
@@ -5264,6 +5310,7 @@ static const struct net_device_ops stmmac_netdev_ops = {
 	.ndo_vlan_rx_add_vid = stmmac_vlan_rx_add_vid,
 	.ndo_vlan_rx_kill_vid = stmmac_vlan_rx_kill_vid,
 	.ndo_bpf = stmmac_xdp,
+	.ndo_xsk_async_xmit = stmmac_xsk_async_xmit,
 };
 
 static void stmmac_reset_subtask(struct stmmac_priv *priv)
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
index 856b5e99d7a8..09c29f174946 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
@@ -107,6 +107,8 @@ static int stmmac_xsk_umem_enable(struct stmmac_priv *priv,
 
 static int stmmac_xsk_umem_disable(struct stmmac_priv *priv, u16 qid)
 {
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[qid];
+	struct stmmac_tx_queue *tx_q = &priv->tx_queue[qid];
 	struct xdp_umem *umem;
 	bool if_running;
 
@@ -124,6 +126,11 @@ static int stmmac_xsk_umem_disable(struct stmmac_priv *priv, u16 qid)
 	/* UMEM is shared for both Tx & Rx, we unmap once */
 	stmmac_xsk_umem_dma_unmap(priv, umem);
 
+	if (rx_q->xsk_umem)
+		rx_q->xsk_umem = NULL;
+	else if (tx_q->xsk_umem)
+		tx_q->xsk_umem = NULL;
+
 	if (if_running)
 		stmmac_txrx_ring_enable(priv, qid);
 
@@ -152,6 +159,14 @@ int stmmac_run_xdp_zc(struct stmmac_priv *priv, struct stmmac_rx_queue *rx_q,
 	switch (act) {
 	case XDP_PASS:
 		break;
+	case XDP_TX:
+		xdpf = convert_to_xdp_frame(xdp);
+		if (unlikely(!xdpf)) {
+			result = STMMAC_XDP_DROP;
+			break;
+		}
+		result = stmmac_xdp_xmit_queue(priv, rx_q->queue_index, xdpf);
+		break;
 	case XDP_REDIRECT:
 		err = xdp_do_redirect(priv->dev, xdp, xdp_prog);
 		result = (err >= 0) ? STMMAC_XDP_REDIRECT : STMMAC_XDP_DROP;
@@ -647,3 +662,144 @@ void stmmac_xsk_free_rx_ring(struct stmmac_rx_queue *rx_q)
 		}
 	}
 }
+
+bool stmmac_xdp_xmit_zc(struct stmmac_tx_queue *tx_q, unsigned int budget)
+{
+	struct stmmac_priv *priv = tx_q->priv_data;
+	struct dma_desc *tx_desc = NULL;
+	u32 queue = tx_q->queue_index;
+	bool work_done = true;
+	struct xdp_desc desc;
+	dma_addr_t dma;
+
+	while (budget-- > 0) {
+		if (unlikely(!stmmac_tx_avail(priv, queue)) ||
+		    !netif_carrier_ok(priv->dev)) {
+			work_done = false;
+			break;
+		}
+
+		if (!xsk_umem_consume_tx(tx_q->xsk_umem, &desc))
+			break;
+
+		if (!desc.len || desc.len > ((priv->plat->tx_fifo_size ?
+		    priv->plat->tx_fifo_size : priv->dma_cap.tx_fifo_size) /
+		    priv->plat->tx_queues_to_use)) {
+			tx_q->xdpzc_inv_len_pkt++;
+			break;
+		}
+
+		dma = xdp_umem_get_dma(tx_q->xsk_umem, desc.addr);
+
+		dma_sync_single_for_device(priv->device, dma, desc.len,
+					   DMA_BIDIRECTIONAL);
+
+		tx_q->xdpf[tx_q->cur_tx] = NULL;
+		tx_q->is_zc_pkt[tx_q->cur_tx] = true;
+
+		/* Fill Tx descriptor */
+		if (priv->extend_desc)
+			tx_desc = (struct dma_desc *)(tx_q->dma_etx +
+						      tx_q->cur_tx);
+		else if (priv->enhanced_tx_desc)
+			tx_desc = &(tx_q->dma_enhtx + tx_q->cur_tx)->basic;
+		else
+			tx_desc = tx_q->dma_tx + tx_q->cur_tx;
+
+		stmmac_set_desc_addr(priv, tx_desc, dma);
+
+		/* Prepare the descriptor and set the own bit too */
+		stmmac_prepare_tx_desc(priv,
+				       tx_desc,
+				       1, /* First Descriptor */
+				       desc.len,
+				       1, /* Checksum offload */
+				       priv->mode,
+				       1, /* OWN bit */
+				       1, /* Last Descriptor */
+				       desc.len);
+
+		tx_q->cur_tx = STMMAC_GET_ENTRY(tx_q->cur_tx,
+						priv->dma_tx_size);
+	}
+
+	if (tx_desc) {
+		stmmac_enable_dma_transmission(priv, priv->ioaddr);
+		if (priv->extend_desc)
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_extended_desc));
+		else if (priv->enhanced_tx_desc)
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_enhanced_tx_desc));
+		else
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_desc));
+		stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr,
+				       queue);
+		xsk_umem_consume_tx_done(tx_q->xsk_umem);
+	}
+
+	return !!budget && work_done;
+}
+
+/* ndo_xsk_async_xmit, also used to trigger a zc transmit. DW EQoS MAC does not
+ * have a way for SW to trigger INTR directly, so we rely on napi
+ */
+int stmmac_xsk_async_xmit(struct net_device *dev, u32 qid)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	struct stmmac_channel *ch = &priv->channel[qid];
+
+	if (test_bit(STMMAC_DOWN, &priv->state))
+		return -ENETDOWN;
+
+	if (!READ_ONCE(priv->xdp_prog))
+		return -ENXIO;
+
+	/* XDP max queues is based on the maximum queues availableq */
+	if (qid >= priv->plat->rx_queues_to_use ||
+	    qid >= priv->plat->tx_queues_to_use)
+		return -ENXIO;
+
+	if (queue_is_xdp(qid) &&
+	    !napi_if_scheduled_mark_missed(&ch->tx_napi)) {
+		/* DW EQoS MAC does not have a way for SW to trigger INTR */
+		if (likely(napi_schedule_prep(&ch->tx_napi)))
+			__napi_schedule(&ch->tx_napi);
+	}
+
+	return 0;
+}
+
+/* After hw transmits the packet, reclaim the buffer for next packet */
+static void stmmac_free_tx_xdp_buffer(struct stmmac_priv *priv,
+				      struct stmmac_tx_queue *tx_q,
+				      int entry)
+{
+	xdp_return_frame(tx_q->xdpf[entry]);
+
+	stmmac_free_tx_buffer(priv, tx_q->queue_index, entry);
+}
+
+void stmmac_xsk_free_tx_ring(struct stmmac_tx_queue *tx_q)
+{
+	struct stmmac_priv *priv = tx_q->priv_data;
+	struct xdp_umem *umem = tx_q->xsk_umem;
+	unsigned int entry = tx_q->dirty_tx;
+	u32 xsk_frames = 0;
+
+	while (entry != tx_q->cur_tx) {
+		/* Don't increment if packet is from run_xdp_zc's XDP_TX */
+		if (tx_q->xdpf[entry])
+			stmmac_free_tx_xdp_buffer(priv, tx_q, entry);
+		else
+			xsk_frames++;
+
+		tx_q->xdpf[entry] = NULL;
+
+		entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
+	}
+
+	if (xsk_frames)
+		xsk_umem_complete_tx(umem, xsk_frames);
+}
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
index e6287b6e0109..ca658d6fce3e 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
@@ -8,6 +8,7 @@
 #define STMMAC_XDP_DROP		BIT(0)
 #define STMMAC_XDP_TX		BIT(1)
 #define STMMAC_XDP_REDIRECT	BIT(2)
+#define STMMAC_DEFAULT_TX_WORK	256
 
 #define queue_is_xdp(qid) (priv->is_xdp[(qid)])
 #define enable_queue_xdp(qid) { priv->is_xdp[(qid)] = true; }
@@ -36,4 +37,8 @@ void stmmac_reuse_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
 			       struct stmmac_rx_buffer *obi);
 int stmmac_rx_zc(struct stmmac_priv *priv, const int budget, u32 qid);
 void stmmac_xsk_free_rx_ring(struct stmmac_rx_queue *rx_q);
+void stmmac_xsk_free_tx_ring(struct stmmac_tx_queue *tx_q);
+int stmmac_xsk_async_xmit(struct net_device *dev, u32 qid);
+bool stmmac_xdp_xmit_zc(struct stmmac_tx_queue *tx_q, unsigned int budget);
+
 #endif /* __STMMAC_XSK_H__ */
-- 
2.17.1

