From 81c3d43d21d05446521f6c9c12819d3e4118c66e Mon Sep 17 00:00:00 2001
From: Min He <min.he@intel.com>
Date: Thu, 4 Jan 2018 21:48:47 +0800
Subject: [PATCH 0968/2367] drm/i915/gvt: show pid/hw_id of current DomU
 process in debugfs

v1: show pid and hw id of current DomU process when showing shadow context
status
This allows us to identify which process a domu workload has come from.
v2: expose HW context id to debugfs
This patch expose the HW context id to the debugfs node, so that vtune
can utilize this context id to match with the one exposed by MD API.
v3: When storing DomU pid and hw id in the HWS page, offset them by the
vgpu id.
When there were multiple DomUs running, they would all write their pid
to the same address in the HWS page. When we checked i915_context_status
each shadow context would show the same current pid and hw id. By
offsetting the writes by the DomU's ID, we can see the details for each
shadow context correctly. This fixes defect 201282.

Change-Id: I106fae75af5963f043286acd604d3bab02b87c17
Signed-off-by: Min He <min.he@intel.com>
Signed-off-by: Daniel van der Wath <danielx.j.van.der.wath@intel.com>
Signed-off-by: Fei Jiang <fei.jiang@intel.com>
Reviewed-by: Singh, Satyeshwar <satyeshwar.singh@intel.com>
Reviewed-by: Abes, Brahim <brahimx.abes@intel.com>
Reviewed-by: He, Min <min.he@intel.com>
Reviewed-on: https://android.intel.com/613918
Reviewed-by: Dong, Eddie <eddie.dong@intel.com>
Tested-by: Dong, Eddie <eddie.dong@intel.com>
---
 drivers/gpu/drm/i915/gvt/scheduler.c | 36 ++++++++++++++++++++++++++++++++++++
 drivers/gpu/drm/i915/i915_debugfs.c  | 28 +++++++++++++++++++++++++++-
 2 files changed, 63 insertions(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/i915/gvt/scheduler.c b/drivers/gpu/drm/i915/gvt/scheduler.c
index 5a0fb7e..9d8d45b 100644
--- a/drivers/gpu/drm/i915/gvt/scheduler.c
+++ b/drivers/gpu/drm/i915/gvt/scheduler.c
@@ -249,6 +249,36 @@ int intel_gvt_scan_and_shadow_workload(struct intel_vgpu_workload *workload)
 	return ret;
 }
 
+static void gen8_shadow_pid_cid(struct intel_vgpu_workload *workload)
+{
+	int ring_id = workload->ring_id;
+	struct drm_i915_private *dev_priv = workload->vgpu->gvt->dev_priv;
+	struct intel_engine_cs *engine = dev_priv->engine[ring_id];
+	u32 *cs;
+
+	/* Copy the PID and CID from the guest's HWS page to the host's one */
+	cs = intel_ring_begin(workload->req, 16);
+	*cs++ = MI_LOAD_REGISTER_MEM_GEN8 | MI_SRM_LRM_GLOBAL_GTT;
+	*cs++ = i915_mmio_reg_offset(NOPID);
+	*cs++ = (workload->ctx_desc.lrca << GTT_PAGE_SHIFT) + I915_GEM_HWS_PID_ADDR;
+	*cs++ = 0;
+	*cs++ = MI_STORE_REGISTER_MEM_GEN8 | MI_SRM_LRM_GLOBAL_GTT;
+	*cs++ = i915_mmio_reg_offset(NOPID);
+	*cs++ = engine->status_page.ggtt_offset + I915_GEM_HWS_PID_ADDR +
+		(workload->vgpu->id << MI_STORE_DWORD_INDEX_SHIFT);
+	*cs++ = 0;
+	*cs++ = MI_LOAD_REGISTER_MEM_GEN8 | MI_SRM_LRM_GLOBAL_GTT;
+	*cs++ = i915_mmio_reg_offset(NOPID);
+	*cs++ = (workload->ctx_desc.lrca << GTT_PAGE_SHIFT) + I915_GEM_HWS_CID_ADDR;
+	*cs++ = 0;
+	*cs++ = MI_STORE_REGISTER_MEM_GEN8 | MI_SRM_LRM_GLOBAL_GTT;
+	*cs++ = i915_mmio_reg_offset(NOPID);
+	*cs++ = engine->status_page.ggtt_offset + I915_GEM_HWS_CID_ADDR +
+		(workload->vgpu->id << MI_STORE_DWORD_INDEX_SHIFT);
+	*cs++ = 0;
+	intel_ring_advance(workload->req, cs);
+}
+
 static int dispatch_workload(struct intel_vgpu_workload *workload)
 {
 	int ring_id = workload->ring_id;
@@ -273,6 +303,8 @@ static int dispatch_workload(struct intel_vgpu_workload *workload)
 	if (ret)
 		goto out;
 
+	gen8_shadow_pid_cid(workload);
+
 	if (workload->prepare) {
 		mutex_unlock(&dev_priv->drm.struct_mutex);
 		mutex_lock(&vgpu->gvt->lock);
@@ -705,6 +737,10 @@ int intel_vgpu_init_gvt_context(struct intel_vgpu *vgpu)
 	if (IS_ERR(vgpu->shadow_ctx))
 		return PTR_ERR(vgpu->shadow_ctx);
 
+	if (!vgpu->shadow_ctx->name) {
+		vgpu->shadow_ctx->name = kasprintf(GFP_KERNEL, "Shadow Context %d", vgpu->id);
+	}
+
 	vgpu->shadow_ctx->engine[RCS].initialised = true;
 
 	bitmap_zero(vgpu->shadow_ctx_desc_updated, I915_NUM_ENGINES);
diff --git a/drivers/gpu/drm/i915/i915_debugfs.c b/drivers/gpu/drm/i915/i915_debugfs.c
index ae75b5e..4233746 100644
--- a/drivers/gpu/drm/i915/i915_debugfs.c
+++ b/drivers/gpu/drm/i915/i915_debugfs.c
@@ -1910,6 +1910,19 @@ static void describe_ctx_ring(struct seq_file *m, struct intel_ring *ring)
 		   ring->space, ring->head, ring->tail);
 }
 
+static void describe_ctx_ring_shadowed(struct seq_file *m,
+		struct i915_gem_context *ctx, struct intel_ring *ring,
+		struct intel_engine_cs *engine)
+{
+	int pid, cid, vgt_id;
+
+	sscanf(ctx->name, "Shadow Context %d", &vgt_id);
+	pid = intel_read_status_page(engine, I915_GEM_HWS_PID_INDEX + vgt_id);
+	cid = intel_read_status_page(engine, I915_GEM_HWS_CID_INDEX + vgt_id);
+	seq_printf(m, " (Current DomU Process PID: %d, CID: %d)",
+			pid, cid);
+}
+
 static int i915_context_status(struct seq_file *m, void *unused)
 {
 	struct drm_i915_private *dev_priv = node_to_i915(m->private);
@@ -1924,6 +1937,7 @@ static int i915_context_status(struct seq_file *m, void *unused)
 		return ret;
 
 	list_for_each_entry(ctx, &dev_priv->contexts.list, link) {
+		bool is_shadow_context = false;
 		seq_printf(m, "HW context %u ", ctx->hw_id);
 		if (ctx->pid) {
 			struct task_struct *task;
@@ -1934,6 +1948,9 @@ static int i915_context_status(struct seq_file *m, void *unused)
 					   task->comm, task->pid);
 				put_task_struct(task);
 			}
+		} else if (ctx->name && !strncmp(ctx->name, "Shadow Context", 14)) {
+			seq_puts(m, "DomU Shadow Context ");
+			is_shadow_context = true;
 		} else if (IS_ERR(ctx->file_priv)) {
 			seq_puts(m, "(deleted) ");
 		} else {
@@ -1945,13 +1962,22 @@ static int i915_context_status(struct seq_file *m, void *unused)
 
 		for_each_engine(engine, dev_priv, id) {
 			struct intel_context *ce = &ctx->engine[engine->id];
+			u64 lrc_desc = intel_lr_context_descriptor(ctx,
+					engine);
 
 			seq_printf(m, "%s: ", engine->name);
+			seq_printf(m, "ctx id 0x%x ", (uint32_t)((lrc_desc >> 12) &
+					0xFFFFF));
 			seq_putc(m, ce->initialised ? 'I' : 'i');
 			if (ce->state)
 				describe_obj(m, ce->state->obj);
-			if (ce->ring)
+			if (ce->ring) {
 				describe_ctx_ring(m, ce->ring);
+				if(is_shadow_context)
+					describe_ctx_ring_shadowed(m, ctx,
+							ce->ring, engine);
+			}
+
 			seq_putc(m, '\n');
 		}
 
-- 
2.7.4

