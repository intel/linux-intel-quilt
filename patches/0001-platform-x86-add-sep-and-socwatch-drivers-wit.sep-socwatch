From 2f552ad5a3795b238f812fc00ad2188acd008cd8 Mon Sep 17 00:00:00 2001
From: Jon Moeller <jon.moeller@intel.com>
Date: Tue, 13 Nov 2018 01:25:24 -0600
Subject: [PATCH 01/27] platform/x86: add sep and socwatch drivers without
 socperf.

Signed-off-by: Jon Moeller <jon.moeller@intel.com>
---
 drivers/platform/x86/Kconfig                  |    3 +
 drivers/platform/x86/Makefile                 |    4 +
 drivers/platform/x86/sepdk/Kconfig            |   63 +
 drivers/platform/x86/sepdk/Makefile           |    5 +
 drivers/platform/x86/sepdk/inc/apic.h         |  114 +
 drivers/platform/x86/sepdk/inc/asm_helper.h   |  186 +
 drivers/platform/x86/sepdk/inc/chap.h         |   31 +
 drivers/platform/x86/sepdk/inc/control.h      |  510 ++
 drivers/platform/x86/sepdk/inc/core2.h        |   49 +
 drivers/platform/x86/sepdk/inc/cpumon.h       |   53 +
 .../platform/x86/sepdk/inc/ecb_iterators.h    |  581 ++
 drivers/platform/x86/sepdk/inc/eventmux.h     |   42 +
 drivers/platform/x86/sepdk/inc/gfx.h          |   39 +
 drivers/platform/x86/sepdk/inc/gmch.h         |   31 +
 .../platform/x86/sepdk/inc/haswellunc_sa.h    |   57 +
 drivers/platform/x86/sepdk/inc/jkt_unc_ha.h   |   37 +
 .../platform/x86/sepdk/inc/jkt_unc_qpill.h    |   64 +
 drivers/platform/x86/sepdk/inc/linuxos.h      |   79 +
 drivers/platform/x86/sepdk/inc/lwpmudrv.h     |  551 ++
 drivers/platform/x86/sepdk/inc/msrdefs.h      |   81 +
 drivers/platform/x86/sepdk/inc/output.h       |  120 +
 drivers/platform/x86/sepdk/inc/pci.h          |  133 +
 drivers/platform/x86/sepdk/inc/pebs.h         |  494 ++
 drivers/platform/x86/sepdk/inc/perfver4.h     |   51 +
 drivers/platform/x86/sepdk/inc/pmi.h          |   65 +
 .../platform/x86/sepdk/inc/sepdrv_p_state.h   |   34 +
 drivers/platform/x86/sepdk/inc/silvermont.h   |   41 +
 drivers/platform/x86/sepdk/inc/sys_info.h     |   71 +
 drivers/platform/x86/sepdk/inc/unc_common.h   |  161 +
 drivers/platform/x86/sepdk/inc/unc_gt.h       |   86 +
 drivers/platform/x86/sepdk/inc/utility.h      |  637 ++
 .../x86/sepdk/inc/valleyview_sochap.h         |   60 +
 .../x86/sepdk/include/error_reporting_utils.h |  167 +
 .../x86/sepdk/include/lwpmudrv_chipset.h      |  274 +
 .../x86/sepdk/include/lwpmudrv_defines.h      |  507 ++
 .../platform/x86/sepdk/include/lwpmudrv_ecb.h | 1116 +++
 .../platform/x86/sepdk/include/lwpmudrv_gfx.h |   33 +
 .../x86/sepdk/include/lwpmudrv_ioctl.h        |  284 +
 .../platform/x86/sepdk/include/lwpmudrv_pwr.h |  100 +
 .../x86/sepdk/include/lwpmudrv_struct.h       | 2059 +++++
 .../x86/sepdk/include/lwpmudrv_types.h        |  159 +
 .../x86/sepdk/include/lwpmudrv_version.h      |  111 +
 .../platform/x86/sepdk/include/pax_shared.h   |  180 +
 .../platform/x86/sepdk/include/rise_errors.h  |  326 +
 drivers/platform/x86/sepdk/pax/Makefile       |    4 +
 drivers/platform/x86/sepdk/pax/pax.c          |  967 +++
 drivers/platform/x86/sepdk/pax/pax.h          |   33 +
 drivers/platform/x86/sepdk/sep/Makefile       |   67 +
 drivers/platform/x86/sepdk/sep/apic.c         |  228 +
 drivers/platform/x86/sepdk/sep/chap.c         |  474 ++
 drivers/platform/x86/sepdk/sep/control.c      |  896 ++
 drivers/platform/x86/sepdk/sep/core2.c        | 2137 +++++
 drivers/platform/x86/sepdk/sep/cpumon.c       |  357 +
 drivers/platform/x86/sepdk/sep/eventmux.c     |  446 +
 drivers/platform/x86/sepdk/sep/gfx.c          |  261 +
 drivers/platform/x86/sepdk/sep/gmch.c         |  505 ++
 drivers/platform/x86/sepdk/sep/linuxos.c      | 1477 ++++
 drivers/platform/x86/sepdk/sep/lwpmudrv.c     | 7537 +++++++++++++++++
 drivers/platform/x86/sepdk/sep/output.c       | 1177 +++
 drivers/platform/x86/sepdk/sep/pci.c          |  661 ++
 drivers/platform/x86/sepdk/sep/pebs.c         | 1957 +++++
 drivers/platform/x86/sepdk/sep/perfver4.c     | 1972 +++++
 drivers/platform/x86/sepdk/sep/pmi.c          |  640 ++
 .../platform/x86/sepdk/sep/sepdrv_p_state.c   |   88 +
 drivers/platform/x86/sepdk/sep/silvermont.c   | 1113 +++
 drivers/platform/x86/sepdk/sep/sys32.S        |  200 +
 drivers/platform/x86/sepdk/sep/sys64.S        |  140 +
 drivers/platform/x86/sepdk/sep/sys_info.c     | 1111 +++
 drivers/platform/x86/sepdk/sep/unc_common.c   |  388 +
 drivers/platform/x86/sepdk/sep/unc_gt.c       |  470 +
 drivers/platform/x86/sepdk/sep/unc_mmio.c     | 1083 +++
 drivers/platform/x86/sepdk/sep/unc_msr.c      |  347 +
 drivers/platform/x86/sepdk/sep/unc_pci.c      |  491 ++
 drivers/platform/x86/sepdk/sep/unc_power.c    |  444 +
 drivers/platform/x86/sepdk/sep/unc_sa.c       |  173 +
 drivers/platform/x86/sepdk/sep/utility.c      | 1157 +++
 .../x86/sepdk/sep/valleyview_sochap.c         |  301 +
 drivers/platform/x86/socwatch/Kconfig         |    6 +
 drivers/platform/x86/socwatch/Makefile        |   22 +
 .../platform/x86/socwatch/inc/sw_collector.h  |  136 +
 .../platform/x86/socwatch/inc/sw_defines.h    |  156 +
 .../platform/x86/socwatch/inc/sw_file_ops.h   |   70 +
 .../x86/socwatch/inc/sw_hardware_io.h         |  118 +
 .../platform/x86/socwatch/inc/sw_internal.h   |  138 +
 drivers/platform/x86/socwatch/inc/sw_ioctl.h  |  303 +
 .../x86/socwatch/inc/sw_kernel_defines.h      |  164 +
 drivers/platform/x86/socwatch/inc/sw_list.h   |   76 +
 .../platform/x86/socwatch/inc/sw_lock_defs.h  |   98 +
 drivers/platform/x86/socwatch/inc/sw_mem.h    |   82 +
 .../x86/socwatch/inc/sw_ops_provider.h        |   62 +
 .../x86/socwatch/inc/sw_output_buffer.h       |  136 +
 .../socwatch/inc/sw_overhead_measurements.h   |  189 +
 .../platform/x86/socwatch/inc/sw_structs.h    |  500 ++
 drivers/platform/x86/socwatch/inc/sw_telem.h  |   74 +
 .../socwatch/inc/sw_trace_notifier_provider.h |   82 +
 .../x86/socwatch/inc/sw_tracepoint_handlers.h |  142 +
 drivers/platform/x86/socwatch/inc/sw_types.h  |  152 +
 .../platform/x86/socwatch/inc/sw_version.h    |   74 +
 drivers/platform/x86/socwatch/sw_collector.c  |  706 ++
 drivers/platform/x86/socwatch/sw_driver.c     | 1472 ++++
 drivers/platform/x86/socwatch/sw_file_ops.c   |  364 +
 .../platform/x86/socwatch/sw_hardware_io.c    |  188 +
 drivers/platform/x86/socwatch/sw_internal.c   |  238 +
 drivers/platform/x86/socwatch/sw_mem.c        |  331 +
 .../platform/x86/socwatch/sw_ops_provider.c   | 1225 +++
 .../platform/x86/socwatch/sw_output_buffer.c  |  598 ++
 drivers/platform/x86/socwatch/sw_reader.c     |  163 +
 drivers/platform/x86/socwatch/sw_telem.c      |  493 ++
 .../x86/socwatch/sw_trace_notifier_provider.c | 2233 +++++
 .../x86/socwatch/sw_tracepoint_handlers.c     |  399 +
 drivers/platform/x86/socwatchhv/Kconfig       |    6 +
 drivers/platform/x86/socwatchhv/Makefile      |   20 +
 drivers/platform/x86/socwatchhv/control.c     |  141 +
 .../platform/x86/socwatchhv/inc/asm_helper.h  |  158 +
 drivers/platform/x86/socwatchhv/inc/control.h |  194 +
 .../platform/x86/socwatchhv/inc/pw_types.h    |  132 +
 .../platform/x86/socwatchhv/inc/pw_version.h  |   67 +
 .../platform/x86/socwatchhv/inc/sw_defines.h  |  156 +
 .../platform/x86/socwatchhv/inc/sw_ioctl.h    |  303 +
 .../x86/socwatchhv/inc/sw_kernel_defines.h    |  164 +
 .../platform/x86/socwatchhv/inc/sw_structs.h  |  501 ++
 .../platform/x86/socwatchhv/inc/sw_types.h    |  152 +
 .../platform/x86/socwatchhv/inc/sw_version.h  |   74 +
 .../platform/x86/socwatchhv/inc/swhv_acrn.h   |  117 +
 .../x86/socwatchhv/inc/swhv_acrn_sbuf.h       |  186 +
 .../x86/socwatchhv/inc/swhv_defines.h         |  111 +
 .../platform/x86/socwatchhv/inc/swhv_driver.h |  109 +
 .../platform/x86/socwatchhv/inc/swhv_ioctl.h  |  164 +
 .../x86/socwatchhv/inc/swhv_structs.h         |  234 +
 drivers/platform/x86/socwatchhv/swhv_acrn.c   |  747 ++
 drivers/platform/x86/socwatchhv/swhv_driver.c |  375 +
 131 files changed, 54452 insertions(+)
 create mode 100755 drivers/platform/x86/sepdk/Kconfig
 create mode 100755 drivers/platform/x86/sepdk/Makefile
 create mode 100644 drivers/platform/x86/sepdk/inc/apic.h
 create mode 100644 drivers/platform/x86/sepdk/inc/asm_helper.h
 create mode 100644 drivers/platform/x86/sepdk/inc/chap.h
 create mode 100644 drivers/platform/x86/sepdk/inc/control.h
 create mode 100644 drivers/platform/x86/sepdk/inc/core2.h
 create mode 100644 drivers/platform/x86/sepdk/inc/cpumon.h
 create mode 100644 drivers/platform/x86/sepdk/inc/ecb_iterators.h
 create mode 100644 drivers/platform/x86/sepdk/inc/eventmux.h
 create mode 100644 drivers/platform/x86/sepdk/inc/gfx.h
 create mode 100644 drivers/platform/x86/sepdk/inc/gmch.h
 create mode 100644 drivers/platform/x86/sepdk/inc/haswellunc_sa.h
 create mode 100644 drivers/platform/x86/sepdk/inc/jkt_unc_ha.h
 create mode 100644 drivers/platform/x86/sepdk/inc/jkt_unc_qpill.h
 create mode 100644 drivers/platform/x86/sepdk/inc/linuxos.h
 create mode 100644 drivers/platform/x86/sepdk/inc/lwpmudrv.h
 create mode 100644 drivers/platform/x86/sepdk/inc/msrdefs.h
 create mode 100644 drivers/platform/x86/sepdk/inc/output.h
 create mode 100644 drivers/platform/x86/sepdk/inc/pci.h
 create mode 100644 drivers/platform/x86/sepdk/inc/pebs.h
 create mode 100644 drivers/platform/x86/sepdk/inc/perfver4.h
 create mode 100644 drivers/platform/x86/sepdk/inc/pmi.h
 create mode 100644 drivers/platform/x86/sepdk/inc/sepdrv_p_state.h
 create mode 100644 drivers/platform/x86/sepdk/inc/silvermont.h
 create mode 100644 drivers/platform/x86/sepdk/inc/sys_info.h
 create mode 100644 drivers/platform/x86/sepdk/inc/unc_common.h
 create mode 100644 drivers/platform/x86/sepdk/inc/unc_gt.h
 create mode 100644 drivers/platform/x86/sepdk/inc/utility.h
 create mode 100644 drivers/platform/x86/sepdk/inc/valleyview_sochap.h
 create mode 100644 drivers/platform/x86/sepdk/include/error_reporting_utils.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_chipset.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_defines.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_ecb.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_gfx.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_ioctl.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_pwr.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_struct.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_types.h
 create mode 100644 drivers/platform/x86/sepdk/include/lwpmudrv_version.h
 create mode 100644 drivers/platform/x86/sepdk/include/pax_shared.h
 create mode 100644 drivers/platform/x86/sepdk/include/rise_errors.h
 create mode 100755 drivers/platform/x86/sepdk/pax/Makefile
 create mode 100755 drivers/platform/x86/sepdk/pax/pax.c
 create mode 100755 drivers/platform/x86/sepdk/pax/pax.h
 create mode 100755 drivers/platform/x86/sepdk/sep/Makefile
 create mode 100755 drivers/platform/x86/sepdk/sep/apic.c
 create mode 100755 drivers/platform/x86/sepdk/sep/chap.c
 create mode 100755 drivers/platform/x86/sepdk/sep/control.c
 create mode 100755 drivers/platform/x86/sepdk/sep/core2.c
 create mode 100755 drivers/platform/x86/sepdk/sep/cpumon.c
 create mode 100755 drivers/platform/x86/sepdk/sep/eventmux.c
 create mode 100755 drivers/platform/x86/sepdk/sep/gfx.c
 create mode 100755 drivers/platform/x86/sepdk/sep/gmch.c
 create mode 100755 drivers/platform/x86/sepdk/sep/linuxos.c
 create mode 100755 drivers/platform/x86/sepdk/sep/lwpmudrv.c
 create mode 100755 drivers/platform/x86/sepdk/sep/output.c
 create mode 100755 drivers/platform/x86/sepdk/sep/pci.c
 create mode 100755 drivers/platform/x86/sepdk/sep/pebs.c
 create mode 100755 drivers/platform/x86/sepdk/sep/perfver4.c
 create mode 100755 drivers/platform/x86/sepdk/sep/pmi.c
 create mode 100755 drivers/platform/x86/sepdk/sep/sepdrv_p_state.c
 create mode 100755 drivers/platform/x86/sepdk/sep/silvermont.c
 create mode 100755 drivers/platform/x86/sepdk/sep/sys32.S
 create mode 100755 drivers/platform/x86/sepdk/sep/sys64.S
 create mode 100755 drivers/platform/x86/sepdk/sep/sys_info.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_common.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_gt.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_mmio.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_msr.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_pci.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_power.c
 create mode 100755 drivers/platform/x86/sepdk/sep/unc_sa.c
 create mode 100755 drivers/platform/x86/sepdk/sep/utility.c
 create mode 100755 drivers/platform/x86/sepdk/sep/valleyview_sochap.c
 create mode 100644 drivers/platform/x86/socwatch/Kconfig
 create mode 100644 drivers/platform/x86/socwatch/Makefile
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_collector.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_defines.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_file_ops.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_hardware_io.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_internal.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_ioctl.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_list.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_lock_defs.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_mem.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_ops_provider.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_output_buffer.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_structs.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_telem.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_types.h
 create mode 100644 drivers/platform/x86/socwatch/inc/sw_version.h
 create mode 100644 drivers/platform/x86/socwatch/sw_collector.c
 create mode 100644 drivers/platform/x86/socwatch/sw_driver.c
 create mode 100644 drivers/platform/x86/socwatch/sw_file_ops.c
 create mode 100644 drivers/platform/x86/socwatch/sw_hardware_io.c
 create mode 100644 drivers/platform/x86/socwatch/sw_internal.c
 create mode 100644 drivers/platform/x86/socwatch/sw_mem.c
 create mode 100644 drivers/platform/x86/socwatch/sw_ops_provider.c
 create mode 100644 drivers/platform/x86/socwatch/sw_output_buffer.c
 create mode 100644 drivers/platform/x86/socwatch/sw_reader.c
 create mode 100644 drivers/platform/x86/socwatch/sw_telem.c
 create mode 100644 drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
 create mode 100644 drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
 create mode 100644 drivers/platform/x86/socwatchhv/Kconfig
 create mode 100644 drivers/platform/x86/socwatchhv/Makefile
 create mode 100644 drivers/platform/x86/socwatchhv/control.c
 create mode 100644 drivers/platform/x86/socwatchhv/inc/asm_helper.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/control.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/pw_types.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/pw_version.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_defines.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_ioctl.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_kernel_defines.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_structs.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_types.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/sw_version.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_acrn.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_acrn_sbuf.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_defines.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_driver.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_ioctl.h
 create mode 100644 drivers/platform/x86/socwatchhv/inc/swhv_structs.h
 create mode 100644 drivers/platform/x86/socwatchhv/swhv_acrn.c
 create mode 100644 drivers/platform/x86/socwatchhv/swhv_driver.c

diff --git a/drivers/platform/x86/Kconfig b/drivers/platform/x86/Kconfig
index 1b67bb578f9f..e52c76c4526d 100644
--- a/drivers/platform/x86/Kconfig
+++ b/drivers/platform/x86/Kconfig
@@ -1342,3 +1342,6 @@ config PMC_ATOM
        def_bool y
        depends on PCI
        select COMMON_CLK
+source "drivers/platform/x86/socwatch/Kconfig"
+source "drivers/platform/x86/socwatchhv/Kconfig"
+source "drivers/platform/x86/sepdk/Kconfig"
diff --git a/drivers/platform/x86/Makefile b/drivers/platform/x86/Makefile
index 415104033060..f9fac98188c8 100644
--- a/drivers/platform/x86/Makefile
+++ b/drivers/platform/x86/Makefile
@@ -100,3 +100,7 @@ obj-$(CONFIG_I2C_MULTI_INSTANTIATE)	+= i2c-multi-instantiate.o
 obj-$(CONFIG_INTEL_ATOMISP2_PM)	+= intel_atomisp2_pm.o
 obj-$(CONFIG_PCENGINES_APU2)	+= pcengines-apuv2.o
 obj-$(CONFIG_INTEL_SPEED_SELECT_INTERFACE) += intel_speed_select_if/
+obj-$(CONFIG_INTEL_SOCWATCH) += socwatch/
+obj-$(CONFIG_INTEL_SOCWATCH_HV) += socwatchhv/
+obj-$(CONFIG_INTEL_SEP) += sepdk/
+
diff --git a/drivers/platform/x86/sepdk/Kconfig b/drivers/platform/x86/sepdk/Kconfig
new file mode 100755
index 000000000000..ed9c94ec3d07
--- /dev/null
+++ b/drivers/platform/x86/sepdk/Kconfig
@@ -0,0 +1,63 @@
+#
+# THE SEP KERNEL DRIVER UNDER LINUX*
+#
+config INTEL_SEP
+	bool "Sampling Enabling Product (SEP)"
+	help
+	  SEP is a command line tool for doing hardware-based sampling using
+	  event-based sampling (EBS).
+	depends on X86 || X86_64
+
+config SEP
+	tristate "SEP kernel driver"
+	depends on INTEL_SEP
+	default m
+
+config SEP_ACRN
+	tristate "SEP kernel driver"
+	depends on INTEL_SEP && ACRN_VHM
+	default m
+
+config SEP_PAX
+	tristate "PAX kernel driver from SEP"
+	depends on INTEL_SEP
+	depends on SEP
+	default m
+
+config SEP_PER_USER_MODE
+	bool "Use Per User Mode on SEP"
+	depends on INTEL_SEP
+	default n
+
+choice
+	prompt "Choose log mode"
+	default SEP_STANDARD_MODE
+	depends on INTEL_SEP
+	help
+	  This option allows to select logging mode.
+
+config SEP_STANDARD_MODE
+	bool "Use standard logging mode"
+
+config SEP_MINLOG_MODE
+	bool "Use min logging mode"
+	help
+	  WARNING: Using minimal logging mode.
+	  This may make troubleshooting more difficult.
+
+config SEP_MAXLOG_MODE
+	bool "Use max logging mode"
+	help
+	  WARNING: Using maximal logging mode.
+	  This may increase overhead
+
+endchoice
+
+config SEP_PRIVATE_BUILD
+	bool "Is this build an Internal and Private build"
+	depends on INTEL_SEP
+	default y
+	help
+	  Select Yes if this is an Intel Internal Build
+
+
diff --git a/drivers/platform/x86/sepdk/Makefile b/drivers/platform/x86/sepdk/Makefile
new file mode 100755
index 000000000000..c8992312a9bb
--- /dev/null
+++ b/drivers/platform/x86/sepdk/Makefile
@@ -0,0 +1,5 @@
+
+obj-$(CONFIG_SEP)		+= sep/
+obj-$(CONFIG_SEP_PAX)		+= pax/
+
+
diff --git a/drivers/platform/x86/sepdk/inc/apic.h b/drivers/platform/x86/sepdk/inc/apic.h
new file mode 100644
index 000000000000..2b7f1c70dab5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/apic.h
@@ -0,0 +1,114 @@
+/* ****************************************************************************
+ *	Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *	This file is part of SEP Development Kit
+ *
+ *	SEP Development Kit is free software; you can redistribute it
+ *	and/or modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ *
+ *	SEP Development Kit is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	As a special exception, you may use this file as part of a free software
+ *	library without restriction.  Specifically, if other files instantiate
+ *	templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *	this file does not by itself cause the resulting executable to be
+ *	covered by the GNU General Public License.  This exception does not
+ *	however invalidate any other reasons why the executable file might be
+ *	covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _APIC_H_
+#define _APIC_H_
+
+#include <stddef.h>
+#include <linux/irq.h>
+
+typedef U64 * PHYSICAL_ADDRESS;
+
+/**
+ * Data Types and Macros
+ */
+
+/*
+ * APIC registers and constants
+ */
+
+// APIC base MSR
+#define DRV_APIC_BASE_MSR 0x001b
+
+// APIC registers
+#define DRV_APIC_LCL_ID 0x0020
+#define DRV_APIC_LCL_TSKPRI 0x0080
+#define DRV_APIC_LCL_PPR 0x00a0
+#define DRV_APIC_LCL_EOI 0x00b0
+#define DRV_APIC_LCL_LDEST 0x00d0
+#define DRV_APIC_LCL_DSTFMT 0x00e0
+#define DRV_APIC_LCL_SVR 0x00f0
+#define DRV_APIC_LCL_ICR 0x0300
+#define DRV_APIC_LVT_TIMER 0x0320
+#define DRV_APIC_LVT_PMI 0x0340
+#define DRV_APIC_LVT_LINT0 0x0350
+#define DRV_APIC_LVT_LINT1 0x0360
+#define DRV_APIC_LVT_ERROR 0x0370
+
+#define DRV_APIC_LCL_ID_MSR 0x802
+#define DRV_APIC_LCL_TSKPRI_MSR 0x808
+#define DRV_APIC_LCL_PPR_MSR 0x80a
+#define DRV_APIC_LCL_EOI_MSR 0x80b
+#define DRV_APIC_LCL_LDEST_MSR 0x80d
+#define DRV_APIC_LCL_DSTFMT_MSR 0x80e
+#define DRV_APIC_LCL_SVR_MSR 0x80f
+#define DRV_APIC_LCL_ICR_MSR 0x830
+#define DRV_APIC_LVT_TIMER_MSR 0x832
+#define DRV_APIC_LVT_PMI_MSR 0x834
+#define DRV_APIC_LVT_LINT0_MSR 0x835
+#define DRV_APIC_LVT_LINT1_MSR 0x836
+#define DRV_APIC_LVT_ERROR_MSR 0x837
+
+// masks for LVT
+#define DRV_LVT_MASK 0x10000
+#define DRV_LVT_EDGE 0x00000
+#define DRV_LVT_LEVEL 0x08000
+#define DRV_LVT_EXTINT 0x00700
+#define DRV_LVT_NMI 0x00400
+
+// task priorities
+#define DRV_APIC_TSKPRI_LO 0x0000
+#define DRV_APIC_TSKPRI_HI 0x00f0
+
+#define DRV_X2APIC_ENABLED 0xc00LL
+
+//// Interrupt vector for PMU overflow event
+//
+//     Choose the highest unused IDT vector possible so that our
+//     callback routine runs at the highest priority allowed;
+//     must avoid using pre-defined vectors in,
+//              include/asm/irq.h
+//              include/asm/hw_irq.h
+//              include/asm/irq_vectors.h
+//
+// FIRST_DEVICE_VECTOR should be valid for kernels 2.6.33 and earlier
+#define CPU_PERF_VECTOR DRV_LVT_NMI
+// Has the APIC Been enabled
+#define DRV_APIC_BASE_GLOBAL_ENABLED(a) ((a)&1 << 11)
+#define DRV_APIC_VIRTUAL_WIRE_ENABLED(a) ((a)&0x100)
+
+/**
+ * Function Declarations
+ */
+
+/*
+ * APIC control functions
+ */
+extern VOID APIC_Enable_Pmi(void);
+extern VOID APIC_Init(PVOID param);
+extern VOID APIC_Install_Interrupt_Handler(PVOID param);
+extern VOID APIC_Restore_LVTPC(PVOID param);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/asm_helper.h b/drivers/platform/x86/sepdk/inc/asm_helper.h
new file mode 100644
index 000000000000..fd4eabf95dd9
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/asm_helper.h
@@ -0,0 +1,186 @@
+/* ****************************************************************************
+ *	Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *	This file is part of SEP Development Kit
+ *
+ *	SEP Development Kit is free software; you can redistribute it
+ *	and/or modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ *
+ *	SEP Development Kit is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	As a special exception, you may use this file as part of a free software
+ *	library without restriction.  Specifically, if other files instantiate
+ *	templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *	this file does not by itself cause the resulting executable to be
+ *	covered by the GNU General Public License.  This exception does not
+ *	however invalidate any other reasons why the executable file might be
+ *	covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _ASM_HELPER_H_
+#define _ASM_HELPER_H_
+
+#include <linux/version.h>
+
+#if KERNEL_VERSION(4, 1, 0) > LINUX_VERSION_CODE
+
+#include <asm/dwarf2.h>
+#include <asm/calling.h>
+
+#else
+
+#ifdef CONFIG_AS_CFI
+
+#define CFI_STARTPROC         .cfi_startproc
+#define CFI_ENDPROC           .cfi_endproc
+#define CFI_ADJUST_CFA_OFFSET .cfi_adjust_cfa_offset
+#define CFI_REL_OFFSET        .cfi_rel_offset
+#define CFI_RESTORE           .cfi_restore
+
+#else
+
+.macro cfi_ignore a = 0, b = 0, c = 0, d = 0
+.endm
+
+#define CFI_STARTPROC           cfi_ignore
+#define CFI_ENDPROC             cfi_ignore
+#define CFI_ADJUST_CFA_OFFSET   cfi_ignore
+#define CFI_REL_OFFSET          cfi_ignore
+#define CFI_RESTORE             cfi_ignore
+#endif
+
+#ifdef CONFIG_X86_64
+	.macro SAVE_C_REGS_HELPER offset = 0 rax = 1 rcx = 1 r8910 = 1 r11 = 1
+	.if \r11
+	movq %r11, 6*8+\offset(%rsp)
+	CFI_REL_OFFSET r11, \offset
+	.endif
+	.if \r8910
+	movq %r10, 7*8+\offset(%rsp)
+	CFI_REL_OFFSET r10, \offset
+
+	movq %r9,  8*8+\offset(%rsp)
+	CFI_REL_OFFSET r9, \offset
+
+	movq %r8,  9*8+\offset(%rsp)
+	CFI_REL_OFFSET r8, \offset
+	.endif
+	.if \rax
+	movq %rax, 10*8+\offset(%rsp)
+	CFI_REL_OFFSET rax, \offset
+	.endif
+	.if \rcx
+	movq %rcx, 11*8+\offset(%rsp)
+	CFI_REL_OFFSET rcx, \offset
+	.endif
+	movq %rdx, 12*8+\offset(%rsp)
+	CFI_REL_OFFSET rdx, \offset
+
+	movq %rsi, 13*8+\offset(%rsp)
+	CFI_REL_OFFSET rsi, \offset
+
+	movq %rdi, 14*8+\offset(%rsp)
+	CFI_REL_OFFSET rdi, \offset
+	.endm
+	.macro SAVE_C_REGS offset = 0
+	SAVE_C_REGS_HELPER \offset, 1, 1, 1, 1
+	.endm
+	.macro SAVE_EXTRA_REGS offset = 0
+	movq %r15, 0*8+\offset(%rsp)
+	CFI_REL_OFFSET r15, \offset
+
+	movq %r14, 1*8+\offset(%rsp)
+	CFI_REL_OFFSET r14, \offset
+
+	movq %r13, 2*8+\offset(%rsp)
+	CFI_REL_OFFSET r13, \offset
+
+	movq %r12, 3*8+\offset(%rsp)
+	CFI_REL_OFFSET r12, \offset
+
+	movq %rbp, 4*8+\offset(%rsp)
+	CFI_REL_OFFSET rbp, \offset
+
+	movq %rbx, 5*8+\offset(%rsp)
+	CFI_REL_OFFSET rbx, \offset
+	.endm
+
+	.macro RESTORE_EXTRA_REGS offset = 0
+	movq 0*8+\offset(%rsp), %r15
+	CFI_RESTORE r15
+	movq 1*8+\offset(%rsp), %r14
+	CFI_RESTORE r14
+	movq 2*8+\offset(%rsp), %r13
+	CFI_RESTORE r13
+	movq 3*8+\offset(%rsp), %r12
+	CFI_RESTORE r12
+	movq 4*8+\offset(%rsp), %rbp
+	CFI_RESTORE rbp
+	movq 5*8+\offset(%rsp), %rbx
+	CFI_RESTORE rbx
+	.endm
+	.macro RESTORE_C_REGS_HELPER rstor_rax = 1, rstor_rcx = 1, rstor_r11 = 1, rstor_r8910 = 1, rstor_rdx = 1
+	.if \rstor_r11
+	movq 6*8(%rsp), %r11
+	CFI_RESTORE r11
+	.endif
+	.if \rstor_r8910
+	movq 7*8(%rsp), %r10
+	CFI_RESTORE r10
+	movq 8*8(%rsp), %r9
+	CFI_RESTORE r9
+	movq 9*8(%rsp), %r8
+	CFI_RESTORE r8
+	.endif
+	.if \rstor_rax
+	movq 10*8(%rsp), %rax
+	CFI_RESTORE rax
+	.endif
+	.if \rstor_rcx
+	movq 11*8(%rsp), %rcx
+	CFI_RESTORE rcx
+	.endif
+	.if \rstor_rdx
+	movq 12*8(%rsp), %rdx
+	CFI_RESTORE rdx
+	.endif
+	movq 13*8(%rsp), %rsi
+	CFI_RESTORE rsi
+	movq 14*8(%rsp), %rdi
+	CFI_RESTORE rdi
+	.endm
+	.macro RESTORE_C_REGS
+	RESTORE_C_REGS_HELPER 1, 1, 1, 1, 1
+	.endm
+
+	.macro ALLOC_PT_GPREGS_ON_STACK addskip = 0
+	subq    $15*8+\addskip, %rsp
+	CFI_ADJUST_CFA_OFFSET 15*8+\addskip
+	.endm
+
+	.macro REMOVE_PT_GPREGS_FROM_STACK addskip = 0
+	addq $15*8+\addskip, %rsp
+	CFI_ADJUST_CFA_OFFSET - (15*8+\addskip)
+	.endm
+
+	.macro SAVE_ALL
+	ALLOC_PT_GPREGS_ON_STACK
+	SAVE_C_REGS
+	SAVE_EXTRA_REGS
+	.endm
+
+	.macro RESTORE_ALL
+	RESTORE_EXTRA_REGS
+	RESTORE_C_REGS
+	REMOVE_PT_GPREGS_FROM_STACK
+	.endm
+#endif //CONFIG_X86_64
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/chap.h b/drivers/platform/x86/sepdk/inc/chap.h
new file mode 100644
index 000000000000..823aa9058cd5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/chap.h
@@ -0,0 +1,31 @@
+/* ****************************************************************************
+ *	Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *	This file is part of SEP Development Kit
+ *
+ *	SEP Development Kit is free software; you can redistribute it
+ *	and/or modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ *
+ *	SEP Development Kit is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	As a special exception, you may use this file as part of a free software
+ *	library without restriction.  Specifically, if other files instantiate
+ *	templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *	this file does not by itself cause the resulting executable to be
+ *	covered by the GNU General Public License.  This exception does not
+ *	however invalidate any other reasons why the executable file might be
+ *	covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _CHAP_H_
+#define _CHAP_H_
+
+extern CS_DISPATCH_NODE chap_dispatch;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/control.h b/drivers/platform/x86/sepdk/inc/control.h
new file mode 100644
index 000000000000..ecc93ceab221
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/control.h
@@ -0,0 +1,510 @@
+/* ****************************************************************************
+ *	Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *	This file is part of SEP Development Kit
+ *
+ *	SEP Development Kit is free software; you can redistribute it
+ *	and/or modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ *
+ *	SEP Development Kit is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	As a special exception, you may use this file as part of a free software
+ *	library without restriction.  Specifically, if other files instantiate
+ *	templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *	this file does not by itself cause the resulting executable to be
+ *	covered by the GNU General Public License.  This exception does not
+ *	however invalidate any other reasons why the executable file might be
+ *	covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+
+#ifndef _CONTROL_H_
+#define _CONTROL_H_
+
+#include <linux/smp.h>
+#include <linux/timer.h>
+#if defined(DRV_IA32)
+#include <asm/apic.h>
+#endif
+#include <asm/io.h>
+#if defined(DRV_IA32)
+#include <asm/msr.h>
+#endif
+#include <asm/atomic.h>
+#include <linux/interrupt.h>
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv.h"
+#include "lwpmudrv_types.h"
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#endif
+
+// large memory allocation will be used if the requested size (in bytes) is
+// above this threshold
+#define MAX_KMALLOC_SIZE ((1 << 17) - 1)
+
+// check whether Linux driver should use unlocked ioctls (not protected by BKL)
+#if defined(HAVE_UNLOCKED_IOCTL)
+#define DRV_USE_UNLOCKED_IOCTL
+#endif
+#if defined(DRV_USE_UNLOCKED_IOCTL)
+#define IOCTL_OP .unlocked_ioctl
+#define IOCTL_OP_TYPE long
+#define IOCTL_USE_INODE
+#else
+#define IOCTL_OP .ioctl
+#define IOCTL_OP_TYPE S32
+#define IOCTL_USE_INODE struct inode *inode,
+#endif
+
+// Information about the state of the driver
+typedef struct GLOBAL_STATE_NODE_S GLOBAL_STATE_NODE;
+typedef GLOBAL_STATE_NODE  * GLOBAL_STATE;
+struct GLOBAL_STATE_NODE_S {
+	volatile S32 cpu_count;
+	volatile S32 dpc_count;
+
+	S32 num_cpus; // Number of CPUs in the system
+	S32 active_cpus; // Number of active CPUs - some cores can be
+					 // deactivated by the user / admin
+	S32 num_em_groups;
+	S32 num_descriptors;
+
+	volatile S32 current_phase;
+
+	U32 num_modules;
+};
+
+// Access Macros
+#define GLOBAL_STATE_num_cpus(x) ((x).num_cpus)
+#define GLOBAL_STATE_active_cpus(x) ((x).active_cpus)
+#define GLOBAL_STATE_cpu_count(x) ((x).cpu_count)
+#define GLOBAL_STATE_dpc_count(x) ((x).dpc_count)
+#define GLOBAL_STATE_num_em_groups(x) ((x).num_em_groups)
+#define GLOBAL_STATE_num_descriptors(x) ((x).num_descriptors)
+#define GLOBAL_STATE_current_phase(x) ((x).current_phase)
+#define GLOBAL_STATE_sampler_id(x) ((x).sampler_id)
+#define GLOBAL_STATE_num_modules(x) ((x).num_modules)
+
+/*
+ *
+ *
+ * CPU State data structure and access macros
+ *
+ */
+typedef struct CPU_STATE_NODE_S CPU_STATE_NODE;
+typedef CPU_STATE_NODE * CPU_STATE;
+struct CPU_STATE_NODE_S {
+	S32 apic_id; // Processor ID on the system bus
+	PVOID apic_linear_addr; // linear address of local apic
+	PVOID apic_physical_addr; // physical address of local apic
+
+	PVOID idt_base; // local IDT base address
+	atomic_t in_interrupt;
+
+#if defined(DRV_IA32)
+	U64 saved_ih; // saved perfvector to restore
+#endif
+#if defined(DRV_EM64T)
+	PVOID saved_ih; // saved perfvector to restore
+#endif
+
+	U64 last_mperf; // previous value of MPERF, for calculating delta MPERF
+	U64 last_aperf; // previous value of APERF, for calculating delta MPERF
+	DRV_BOOL last_p_state_valid; // are the previous values valid?
+				//(e.g., the first measurement does not have
+				// a previous value for calculating the delta
+	DRV_BOOL p_state_counting; //Flag to mark PMI interrupt from fixed event
+
+	S64 *em_tables; // holds the data that is saved/restored
+		// during event multiplexing
+	U32 em_table_offset;
+
+	struct timer_list *em_timer;
+	U32 current_group;
+	S32 trigger_count;
+	S32 trigger_event_num;
+
+	DISPATCH dispatch;
+	PVOID lbr_area;
+	PVOID old_dts_buffer;
+	PVOID dts_buffer;
+	U32 dts_buffer_size;
+	U32 dts_buffer_offset;
+	U32 initial_mask;
+	U32 accept_interrupt;
+
+#if defined(BUILD_CHIPSET)
+	// Chipset counter stuff
+	U32 chipset_count_init; //flag to initialize the last MCH and ICH array
+	U64 last_mch_count[8];
+	U64 last_ich_count[8];
+	U64 last_gmch_count[MAX_CHIPSET_COUNTERS];
+	U64 last_mmio_count[32]; // it's 9 now but next generation may have 29
+
+#endif
+
+	U64 *pmu_state; // holds PMU state (e.g.,  MSRs) that will be
+			// saved before and restored after collection
+	S32 socket_master;
+	S32 core_master;
+	S32 thr_master;
+	U64 num_samples;
+	U64 reset_mask;
+	U64 group_swap;
+	U64 last_visa_count[16];
+	U16 cpu_module_num;
+	U16 cpu_module_master;
+	S32 system_master;
+	DRV_BOOL offlined;
+	U32 nmi_handled;
+	struct tasklet_struct nmi_tasklet;
+	U32 em_timer_delay;
+	U32 core_type;
+};
+
+#define CPU_STATE_apic_id(cpu) ((cpu)->apic_id)
+#define CPU_STATE_apic_linear_addr(cpu) ((cpu)->apic_linear_addr)
+#define CPU_STATE_apic_physical_addr(cpu) ((cpu)->apic_physical_addr)
+#define CPU_STATE_idt_base(cpu) ((cpu)->idt_base)
+#define CPU_STATE_in_interrupt(cpu) ((cpu)->in_interrupt)
+#define CPU_STATE_saved_ih(cpu) ((cpu)->saved_ih)
+#define CPU_STATE_saved_ih_hi(cpu) ((cpu)->saved_ih_hi)
+#define CPU_STATE_dpc(cpu) ((cpu)->dpc)
+#define CPU_STATE_em_tables(cpu) ((cpu)->em_tables)
+#define CPU_STATE_em_table_offset(cpu) ((cpu)->em_table_offset)
+#define CPU_STATE_pmu_state(cpu) ((cpu)->pmu_state)
+#define CPU_STATE_em_dpc(cpu) ((cpu)->em_dpc)
+#define CPU_STATE_em_timer(cpu) ((cpu)->em_timer)
+#define CPU_STATE_current_group(cpu) ((cpu)->current_group)
+#define CPU_STATE_trigger_count(cpu) ((cpu)->trigger_count)
+#define CPU_STATE_trigger_event_num(cpu) ((cpu)->trigger_event_num)
+#define CPU_STATE_dispatch(cpu) ((cpu)->dispatch)
+#define CPU_STATE_lbr(cpu) ((cpu)->lbr)
+#define CPU_STATE_old_dts_buffer(cpu) ((cpu)->old_dts_buffer)
+#define CPU_STATE_dts_buffer(cpu) ((cpu)->dts_buffer)
+#define CPU_STATE_dts_buffer_size(cpu) ((cpu)->dts_buffer_size)
+#define CPU_STATE_dts_buffer_offset(cpu) ((cpu)->dts_buffer_offset)
+#define CPU_STATE_initial_mask(cpu) ((cpu)->initial_mask)
+#define CPU_STATE_accept_interrupt(cpu) ((cpu)->accept_interrupt)
+#define CPU_STATE_msr_value(cpu) ((cpu)->msr_value)
+#define CPU_STATE_msr_addr(cpu) ((cpu)->msr_addr)
+#define CPU_STATE_socket_master(cpu) ((cpu)->socket_master)
+#define CPU_STATE_core_master(cpu) ((cpu)->core_master)
+#define CPU_STATE_thr_master(cpu) ((cpu)->thr_master)
+#define CPU_STATE_num_samples(cpu) ((cpu)->num_samples)
+#define CPU_STATE_reset_mask(cpu) ((cpu)->reset_mask)
+#define CPU_STATE_group_swap(cpu) ((cpu)->group_swap)
+#define CPU_STATE_last_mperf(cpu) ((cpu)->last_mperf)
+#define CPU_STATE_last_aperf(cpu) ((cpu)->last_aperf)
+#define CPU_STATE_last_p_state_valid(cpu) ((cpu)->last_p_state_valid)
+#define CPU_STATE_cpu_module_num(cpu) ((cpu)->cpu_module_num)
+#define CPU_STATE_cpu_module_master(cpu) ((cpu)->cpu_module_master)
+#define CPU_STATE_p_state_counting(cpu) ((cpu)->p_state_counting)
+#define CPU_STATE_system_master(cpu) ((cpu)->system_master)
+#define CPU_STATE_offlined(cpu) ((cpu)->offlined)
+#define CPU_STATE_nmi_handled(cpu) ((cpu)->nmi_handled)
+#define CPU_STATE_nmi_tasklet(cpu) ((cpu)->nmi_tasklet)
+#define CPU_STATE_em_timer_delay(cpu) ((cpu)->em_timer_delay)
+#define CPU_STATE_core_type(cpu) ((cpu)->core_type)
+
+/*
+ * For storing data for --read/--write-msr command line options
+ */
+typedef struct MSR_DATA_NODE_S MSR_DATA_NODE;
+typedef MSR_DATA_NODE * MSR_DATA;
+struct MSR_DATA_NODE_S {
+	U64 value; // Used for emon,  for read/write-msr value
+	U64 addr;
+};
+
+#define MSR_DATA_value(md) ((md)->value)
+#define MSR_DATA_addr(md) ((md)->addr)
+
+/*
+ * Memory Allocation tracker
+ *
+ * Currently used to track large memory allocations
+ */
+
+typedef struct MEM_EL_NODE_S MEM_EL_NODE;
+typedef MEM_EL_NODE * MEM_EL;
+struct MEM_EL_NODE_S {
+	PVOID address; // pointer to piece of memory we're tracking
+	S32 size; // size (bytes) of the piece of memory
+	U32 is_addr_vmalloc;
+	// flag to check if the memory is allocated using vmalloc
+};
+
+// accessors for MEM_EL defined in terms of MEM_TRACKER below
+
+#define MEM_EL_MAX_ARRAY_SIZE 32 // minimum is 1,  nominal is 64
+
+typedef struct MEM_TRACKER_NODE_S MEM_TRACKER_NODE;
+typedef MEM_TRACKER_NODE * MEM_TRACKER;
+struct MEM_TRACKER_NODE_S {
+	U16 max_size; // MAX number of elements in the array
+	U16 elements; // number of elements available in this array
+	U16 node_vmalloc;
+	// flag to check whether the node struct is allocated using vmalloc
+	U16 array_vmalloc;
+	// flag to check whether the list of mem el is allocated using vmalloc
+	MEM_EL mem; // array of large memory items we're tracking
+	MEM_TRACKER prev, next; // enables bi-directional scanning linked list
+};
+#define MEM_TRACKER_max_size(mt) ((mt)->max_size)
+#define MEM_TRACKER_node_vmalloc(mt) ((mt)->node_vmalloc)
+#define MEM_TRACKER_array_vmalloc(mt) ((mt)->array_vmalloc)
+#define MEM_TRACKER_elements(mt) ((mt)->elements)
+#define MEM_TRACKER_mem(mt) ((mt)->mem)
+#define MEM_TRACKER_prev(mt) ((mt)->prev)
+#define MEM_TRACKER_next(mt) ((mt)->next)
+#define MEM_TRACKER_mem_address(mt, i) ((MEM_TRACKER_mem(mt)[(i)].address))
+#define MEM_TRACKER_mem_size(mt, i) ((MEM_TRACKER_mem(mt)[(i)].size))
+#define MEM_TRACKER_mem_vmalloc(mt, i)                                         \
+	((MEM_TRACKER_mem(mt)[(i)].is_addr_vmalloc))
+
+/****************************************************************************
+ ** Global State variables exported
+ ***************************************************************************/
+extern CPU_STATE pcb;
+extern U64 *cpu_tsc;
+extern GLOBAL_STATE_NODE driver_state;
+extern MSR_DATA msr_data;
+extern U32 *core_to_package_map;
+extern U32 *core_to_dev_map;
+extern U32 *core_to_phys_core_map;
+extern U32 *core_to_thread_map;
+extern U32 *threads_per_core;
+extern U32 num_packages;
+extern U64 *restore_bl_bypass;
+extern U32 **restore_ha_direct2core;
+extern U32 **restore_qpi_direct2core;
+extern U32 *occupied_core_ids;
+/****************************************************************************
+ **  Handy Short cuts
+ ***************************************************************************/
+
+/*
+ * CONTROL_THIS_CPU()
+ *     Parameters
+ *         None
+ *     Returns
+ *         CPU number of the processor being executed on
+ *
+ */
+#define CONTROL_THIS_CPU() smp_processor_id()
+
+/*
+ * CONTROL_THIS_RAW_CPU()
+ *     Parameters
+ *         None
+ *     Returns
+ *         CPU number of the processor being executed on
+ *
+ */
+#define CONTROL_THIS_RAW_CPU() (raw_smp_processor_id())
+/****************************************************************************
+ **  Interface definitions
+ ***************************************************************************/
+
+/*
+ *  Execution Control Functions
+ */
+
+extern VOID CONTROL_Invoke_Cpu(S32 cpuid, VOID (*func)(PVOID), PVOID ctx);
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_Service(func,  ctx,  blocking,  exclude)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each func invocation
+ * @param    blocking - Wait for invoked function to complete
+ * @param    exclude  - exclude the current core from executing the code
+ *
+ * @returns  none
+ *
+ * @brief    Service routine to handle all kinds of parallel invoke on all
+ *			 CPU calls
+ *
+ * <I>Special Notes:</I>
+ *         Invoke the function provided in parallel in either a
+ * blocking/non-blocking mode. The current core may be excluded if desired.
+ * NOTE - Do not call this function directly from source code.  Use the aliases
+ * CONTROL_Invoke_Parallel(),  CONTROL_Invoke_Parallel_NB(),
+ * CONTROL_Invoke_Parallel_XS().
+ *
+ */
+extern VOID CONTROL_Invoke_Parallel_Service(VOID (*func)(PVOID), PVOID ctx,
+					    S32 blocking, S32 exclude);
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel(func,  ctx)
+ *
+ * @param    func - function to be invoked by each core in the system
+ * @param    ctx  - pointer to the parameter block for each function invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel.
+ *           Wait for all the functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel,  including the CPU
+ *        that the control is being invoked on
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel(a, b)                                          \
+	CONTROL_Invoke_Parallel_Service((a), (b), TRUE, FALSE)
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_NB(func,  ctx)
+ *
+ * @param    func - function to be invoked by each core in the system
+ * @param    ctx  - pointer to the parameter block for each function invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel.
+ *           DO NOT Wait for all the functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel,  including the CPU
+ *        that the control is being invoked on
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel_NB(a, b)                                       \
+	CONTROL_Invoke_Parallel_Service((a), (b), FALSE, FALSE)
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_XS(func,  ctx)
+ *
+ * @param    func - function to be invoked by each core in the system
+ * @param    ctx  - pointer to the parameter block for each function invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel.
+ *           Wait for all the functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel,  excluding the CPU
+ *        that the control is being invoked on
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel_XS(a, b)                                       \
+	CONTROL_Invoke_Parallel_Service((a), (b), TRUE, TRUE)
+
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Init(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Initializes Memory Tracker
+ *
+ * <I>Special Notes:</I>
+ *           This should only be called when the
+ *           the driver is being loaded.
+ */
+extern VOID CONTROL_Memory_Tracker_Init(void);
+
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Free(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Frees memory used by Memory Tracker
+ *
+ * <I>Special Notes:</I>
+ *           This should only be called when the
+ *           driver is being unloaded.
+ */
+extern VOID CONTROL_Memory_Tracker_Free(void);
+
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Compaction(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Compacts the memory allocator if holes are detected
+ *
+ * <I>Special Notes:</I>
+ *           At end of collection (or at other safe sync point),
+ *           reclaim/compact space used by mem tracker
+ */
+extern VOID CONTROL_Memory_Tracker_Compaction(void);
+
+/*
+ * @fn PVOID CONTROL_Allocate_Memory(size)
+ *
+ * @param    IN size     - size of the memory to allocate
+ *
+ * @returns  char*       - pointer to the allocated memory block
+ *
+ * @brief    Allocate and zero memory
+ *
+ * <I>Special Notes:</I>
+ *           Allocate memory in the GFP_KERNEL pool.
+ *
+ *           Use this if memory is to be allocated within a context where
+ *           the allocator can block the allocation (e.g.,  by putting
+ *           the caller to sleep) while it tries to free up memory to
+ *           satisfy the request.  Otherwise,  if the allocation must
+ *           occur atomically (e.g.,  caller cannot sleep),  then use
+ *           CONTROL_Allocate_KMemory instead.
+ */
+extern PVOID CONTROL_Allocate_Memory(size_t size);
+
+/*
+ * @fn PVOID CONTROL_Allocate_KMemory(size)
+ *
+ * @param    IN size     - size of the memory to allocate
+ *
+ * @returns  char*       - pointer to the allocated memory block
+ *
+ * @brief    Allocate and zero memory
+ *
+ * <I>Special Notes:</I>
+ *           Allocate memory in the GFP_ATOMIC pool.
+ *
+ *           Use this if memory is to be allocated within a context where
+ *           the allocator cannot block the allocation (e.g.,  by putting
+ *           the caller to sleep) as it tries to free up memory to
+ *           satisfy the request.  Examples include interrupt handlers,
+ *           process context code holding locks,  etc.
+ */
+extern PVOID CONTROL_Allocate_KMemory(size_t size);
+
+/*
+ * @fn PVOID CONTROL_Free_Memory(location)
+ *
+ * @param    IN location  - size of the memory to allocate
+ *
+ * @returns  pointer to the allocated memory block
+ *
+ * @brief    Frees the memory block
+ *
+ * <I>Special Notes:</I>
+ *           Does not try to free memory if fed with a NULL pointer
+ *           Expected usage:
+ *               ptr = CONTROL_Free_Memory(ptr);
+ */
+extern PVOID CONTROL_Free_Memory(PVOID location);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/core2.h b/drivers/platform/x86/sepdk/inc/core2.h
new file mode 100644
index 000000000000..8a6c0835a623
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/core2.h
@@ -0,0 +1,49 @@
+/* ****************************************************************************
+ *	Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *	This file is part of SEP Development Kit
+ *
+ *	SEP Development Kit is free software; you can redistribute it
+ *	and/or modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ *
+ *	SEP Development Kit is distributed in the hope that it will be useful,
+ *	but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *	MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *	GNU General Public License for more details.
+ *
+ *	As a special exception, you may use this file as part of a free software
+ *	library without restriction.  Specifically, if other files instantiate
+ *	templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *	this file does not by itself cause the resulting executable to be
+ *	covered by the GNU General Public License.  This exception does not
+ *	however invalidate any other reasons why the executable file might be
+ *	covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _CORE2_H_
+#define _CORE2_H_
+
+#include "msrdefs.h"
+
+extern DISPATCH_NODE core2_dispatch;
+extern DISPATCH_NODE corei7_dispatch;
+extern DISPATCH_NODE corei7_dispatch_nehalem;
+extern DISPATCH_NODE corei7_dispatch_htoff_mode;
+extern DISPATCH_NODE corei7_dispatch_2;
+extern DISPATCH_NODE corei7_dispatch_htoff_mode_2;
+
+#define CORE2UNC_BLBYPASS_BITMASK 0x00000001
+#define CORE2UNC_DISABLE_BL_BYPASS_MSR 0x39C
+
+#if defined(DRV_IA32)
+#define CORE2_LBR_DATA_BITS 32
+#else
+#define CORE2_LBR_DATA_BITS 48
+#endif
+
+#define CORE2_LBR_BITMASK ((1ULL << CORE2_LBR_DATA_BITS) - 1)
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/cpumon.h b/drivers/platform/x86/sepdk/inc/cpumon.h
new file mode 100644
index 000000000000..0ce584c1c805
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/cpumon.h
@@ -0,0 +1,53 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _CPUMON_H_
+#define _CPUMON_H_
+
+#include <linux/version.h>
+#include "lwpmudrv_defines.h"
+
+/*
+ *  Defines
+ */
+
+/**
+ * Function Declarations
+ */
+
+/*
+ * CPUMON control functions
+ */
+
+extern VOID CPUMON_Install_Cpuhooks(void);
+extern VOID CPUMON_Remove_Cpuhooks(void);
+#if defined(DRV_CPU_HOTPLUG)
+extern DRV_BOOL CPUMON_is_Online_Allowed(void);
+extern DRV_BOOL CPUMON_is_Offline_Allowed(void);
+extern VOID CPUMON_Online_Cpu(PVOID parm);
+extern VOID CPUMON_Offline_Cpu(PVOID parm);
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/ecb_iterators.h b/drivers/platform/x86/sepdk/inc/ecb_iterators.h
new file mode 100644
index 000000000000..10527535925f
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/ecb_iterators.h
@@ -0,0 +1,581 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _ECB_ITERATORS_H_
+#define _ECB_ITERATORS_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+/*
+ * Loop macros to walk through the event control block
+ * Use for access only in the kernel mode
+ * To Do - Control access from kernel mode by a macro
+ */
+
+#define FOR_EACH_CCCR_REG(pecb, idx)                                           \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_cccr_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_CCCR_REG                           \
+                        }                               \
+		}                                       \
+	}
+
+#define FOR_EACH_CCCR_REG_CPU(pecb, idx, cpuid)                                \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = cpuid;                                        \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_cccr_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_CCCR_REG_CPU                       \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_CCCR_GP_REG(pecb, idx)                                        \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_cccr_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_is_gp_reg_get((pecb),          \
+							      (idx)) == 0) {   \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_CCCR_GP_REG                        \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_ESCR_REG(pecb, idx)                                           \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_escr_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_ESCR_REG                           \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_ESCR_REG_CPU(pecb, idx, cpuid)                                \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = cpuid;                                        \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_escr_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_ESCR_REG_CPU                       \
+			}                               \
+		}                                       \
+	}
+
+
+#define FOR_EACH_DATA_REG(pecb, idx)                                           \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_REG                           \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_DATA_REG_CPU(pecb, idx, cpuid)                                \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = cpuid;                                        \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_data_start(pecb) + ECB_data_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_REG_CPU                       \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_DATA_REG_UNC(pecb, device_idx, idx)                           \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[cur_grp];                      \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     (ECB_cccr_start(pecb) + ECB_cccr_pop(pecb));      \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_REG_UNC                       \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_DATA_REG_UNC_VER2(pecb, i, idx)                               \
+	{                                                                      \
+		U32 idx;                                                       \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     ECB_data_start(pecb) + ECB_data_pop(pecb);        \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_REG_UNC_VER2                  \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_DATA_GP_REG(pecb, idx)                                        \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     ECB_data_start(pecb) + ECB_data_pop(pecb);        \
+			     (idx)++) {                                        \
+				if (ECB_entries_is_gp_reg_get((pecb),          \
+							      (idx)) == 0) {   \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_GP_REG                        \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_DATA_GENERIC_REG(pecb, idx)                                   \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_data_start(pecb);                     \
+			     (idx) <                                           \
+			     ECB_data_start(pecb) + ECB_data_pop(pecb);        \
+			     (idx)++) {                                        \
+				if (ECB_entries_is_generic_reg_get(            \
+					    (pecb), (idx)) == 0) {             \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_DATA_GENERIC_REG                   \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_REG_ENTRY(pecb, idx)                                          \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = 0; (idx) < ECB_num_entries(pecb);         \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_REG_ENTRY                          \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_REG_ENTRY_UNC(pecb, device_idx, idx)                          \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((idx) = 0; (idx) < ECB_num_entries(pecb);         \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_REG_ENTRY_UNC                      \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_DATA_REG(pecb, i, device_idx, offset_delta)               \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((i) = ECB_data_start(pecb);                       \
+			     (i) < ECB_data_start(pecb) + ECB_data_pop(pecb);  \
+			     (i)++) {                                          \
+				if (ECB_entries_reg_offset((pecb), (i)) ==     \
+				    0) {                                       \
+					continue;                              \
+				}                                              \
+				(offset_delta) =                               \
+					(ECB_entries_reg_offset(pecb, i) -     \
+				 DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(    \
+						 &ECB_pcidev_entry_node(       \
+							 pecb)));
+
+#define END_FOR_EACH_PCI_DATA_REG                       \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_DATA_REG_VER2(pecb, i, device_idx, offset_delta)          \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		if ((pecb)) {                                                  \
+			for ((i) = ECB_data_start(pecb);                       \
+			     (i) < ECB_data_start(pecb) + ECB_data_pop(pecb);  \
+			     (i)++) {                                          \
+				if (ECB_entries_reg_offset((pecb), (i)) == 0) {\
+					continue;                              \
+				}                                              \
+				(offset_delta) =                               \
+					ECB_entries_reg_offset(pecb, i) -      \
+				DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio( \
+						&ECB_pcidev_entry_node(pecb));
+
+#define END_FOR_EACH_PCI_DATA_REG_VER2                  \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_DATA_REG_RAW(pecb, i, device_idx)                         \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((i) = ECB_data_start(pecb);                       \
+			     (i) < ECB_data_start(pecb) + ECB_data_pop(pecb);  \
+			     (i)++) {                                          \
+				if (ECB_entries_reg_offset((pecb), (i)) ==     \
+				    0) {                                       \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_PCI_DATA_REG_RAW                   \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_CCCR_REG_RAW(pecb, i, device_idx)                         \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((i) = ECB_cccr_start(pecb);                       \
+			     (i) < ECB_cccr_start(pecb) + ECB_cccr_pop(pecb);  \
+			     (i)++) {                                          \
+				if (ECB_entries_reg_offset((pecb), (i)) ==     \
+				    0) {                                       \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_PCI_CCCR_REG_RAW                   \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_REG_RAW(pecb, i, device_idx)                              \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((i) = 0; (i) < ECB_num_entries(pecb); (i)++) {    \
+				if (ECB_entries_reg_offset((pecb), (i)) ==     \
+				    0) {                                       \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_PCI_REG_RAW                        \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_PCI_REG_RAW_GROUP(pecb, i, device_idx, cur_grp)               \
+	{                                                                      \
+		U32 i = 0;                                                     \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((i) = 0; (i) < ECB_num_entries(pecb); (i)++) {    \
+				if (ECB_entries_reg_offset((pecb), (i)) ==     \
+				    0) {                                       \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_PCI_REG_RAW_GROUP                  \
+			}                               \
+		}                                       \
+	}
+
+#define CHECK_SAVE_RESTORE_EVENT_INDEX(prev_ei, cur_ei, evt_index)             \
+	{                                                                      \
+		if (prev_ei == -1) {                                           \
+			prev_ei = cur_ei;                                      \
+		}                                                              \
+		if (prev_ei < cur_ei) {                                        \
+			prev_ei = cur_ei;                                      \
+			evt_index++;                                           \
+		} else {                                                       \
+			evt_index = 0;                                         \
+			prev_ei = cur_ei;                                      \
+		}                                                              \
+	}
+
+#define FOR_EACH_REG_ENTRY_UNC_WRITE_MSR(pecb, device_idx, idx)                \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((idx) = 0; (idx) < ECB_num_entries(pecb);         \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_REG_ENTRY_UNC                      \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_REG_UNC_OPERATION(pecb, device_idx, idx, operation)           \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 cpu = CONTROL_THIS_CPU();                                  \
+		U32 pkg = core_to_package_map[cpu];                            \
+		U32 cur_grp =                                                  \
+			LWPMU_DEVICE_cur_group(&devices[(device_idx)])[(pkg)]; \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[(device_idx)])[(cur_grp)];                    \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_operations_register_start(            \
+				     pecb, (operation));                       \
+			     (idx) <                                           \
+			     (ECB_operations_register_start(pecb,              \
+							    (operation)) +     \
+			      ECB_operations_register_len(pecb, (operation))); \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_REG_UNC_OPERATION                  \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_NONEVENT_REG(pecb, idx)                                       \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_metric_start(pecb);                   \
+			     (idx) <                                           \
+			     ECB_metric_start(pecb) + ECB_metric_pop(pecb);    \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_NONEVENT_REG                       \
+			}                               \
+		}                                       \
+	}
+
+#define FOR_EACH_REG_CORE_OPERATION(pecb, idx, operation)                      \
+	{                                                                      \
+		U32 idx;                                                       \
+		U32 this_cpu__ = CONTROL_THIS_CPU();                           \
+		CPU_STATE pcpu__ = &pcb[this_cpu__];                           \
+		U32 cur_grp = CPU_STATE_current_group(pcpu__);                 \
+		U32 dev_idx = core_to_dev_map[this_cpu__];                     \
+		ECB pecb = LWPMU_DEVICE_PMU_register_data(                     \
+			&devices[dev_idx])[cur_grp];                           \
+		if ((pecb)) {                                                  \
+			for ((idx) = ECB_operations_register_start(            \
+				     pecb, (operation));                       \
+			     (idx) <                                           \
+			     (ECB_operations_register_start(pecb,              \
+							    (operation)) +     \
+			      ECB_operations_register_len(pecb, (operation))); \
+			     (idx)++) {                                        \
+				if (ECB_entries_reg_id((pecb), (idx)) == 0) {  \
+					continue;                              \
+				}
+
+#define END_FOR_EACH_REG_CORE_OPERATION                 \
+			}                               \
+		}                                       \
+	}
+
+#define ECB_SECTION_REG_INDEX(pecb, idx, operation)                            \
+	(ECB_operations_register_start(pecb, operation) + (idx))
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/eventmux.h b/drivers/platform/x86/sepdk/inc/eventmux.h
new file mode 100644
index 000000000000..4a96bb18ae85
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/eventmux.h
@@ -0,0 +1,42 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+/*
+ *  cvs_id[] = "$Id$"
+ */
+
+#ifndef _EVENTMUX_H_
+#define _EVENTMUX_H_
+
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_types.h"
+
+extern VOID EVENTMUX_Start(void);
+
+extern VOID EVENTMUX_Initialize(void);
+
+extern VOID EVENTMUX_Destroy(void);
+
+#endif /* _EVENTMUX_H_ */
diff --git a/drivers/platform/x86/sepdk/inc/gfx.h b/drivers/platform/x86/sepdk/inc/gfx.h
new file mode 100644
index 000000000000..2bad4d712527
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/gfx.h
@@ -0,0 +1,39 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _GFX_H_
+#define _GFX_H_
+
+#include "lwpmudrv_ioctl.h"
+
+extern OS_STATUS GFX_Read(S8 * buffer);
+
+extern OS_STATUS GFX_Set_Event_Code(IOCTL_ARGS arg);
+
+extern OS_STATUS GFX_Start(void);
+
+extern OS_STATUS GFX_Stop(void);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/gmch.h b/drivers/platform/x86/sepdk/inc/gmch.h
new file mode 100644
index 000000000000..baa35728c4bf
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/gmch.h
@@ -0,0 +1,31 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _GMCH_H_
+#define _GMCH_H_
+
+extern CS_DISPATCH_NODE gmch_dispatch;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/haswellunc_sa.h b/drivers/platform/x86/sepdk/inc/haswellunc_sa.h
new file mode 100644
index 000000000000..bd4fb6887d0c
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/haswellunc_sa.h
@@ -0,0 +1,57 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _HSWUNC_SA_H_INC_
+#define _HSWUNC_SA_H_INC_
+
+/*
+ * Local to this architecture: Haswell uncore SA unit
+ *
+ */
+#define HSWUNC_SA_DESKTOP_DID               0x000C04
+#define HSWUNC_SA_NEXT_ADDR_OFFSET          4
+#define HSWUNC_SA_BAR_ADDR_SHIFT            32
+#define HSWUNC_SA_BAR_ADDR_MASK             0x0007FFFFFF000LL
+#define HSWUNC_SA_MAX_PCI_DEVICES           16
+#define HSWUNC_SA_MAX_COUNT                 0x00000000FFFFFFFFLL
+#define HSWUNC_SA_MAX_COUNTERS              8
+
+#define HSWUNC_SA_MCHBAR_MMIO_PAGE_SIZE     (8 * 4096)
+#define HSWUNC_SA_PCIEXBAR_MMIO_PAGE_SIZE   (57 * 4096)
+#define HSWUNC_SA_OTHER_BAR_MMIO_PAGE_SIZE  4096
+#define HSWUNC_SA_GDXCBAR_OFFSET_LO         0x5420
+#define HSWUNC_SA_GDXCBAR_OFFSET_HI         0x5424
+#define HSWUNC_SA_GDXCBAR_MASK              0x7FFFFFF000LL
+#define HSWUNC_SA_CHAP_SAMPLE_DATA          0x00020000
+#define HSWUNC_SA_CHAP_STOP                 0x00040000
+#define HSWUNC_SA_CHAP_CTRL_REG_OFFSET      0x0
+
+#define HSWUNC_SA_PAGE_MASK                 0xfffffffffffff000
+#define HSWUNC_SA_PAGE_OFFSET_MASK          0xfff
+#define HSWUNC_SA_PAGE_SIZE                 0x1000
+
+extern DISPATCH_NODE hswunc_sa_dispatch;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/jkt_unc_ha.h b/drivers/platform/x86/sepdk/inc/jkt_unc_ha.h
new file mode 100644
index 000000000000..aa6bf7624075
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/jkt_unc_ha.h
@@ -0,0 +1,37 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _JKTUNC_HA_H_INC_
+#define _JKTUNC_HA_H_INC_
+
+#define JKTUNC_HA_DID           0x3C46
+#define JKTUNC_HA_DEVICE_NO     14
+#define JKTUNC_HA_FUNC_NO       1
+#define JKTUNC_HA_D2C_OFFSET    0x84
+#define JKTUNC_HA_D2C_BITMASK   0x00000002
+#define JKTUNC_HA_D2C_DID       0x3CA0
+#define JKTUNC_HA_D2C_FUNC_NO   0
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/jkt_unc_qpill.h b/drivers/platform/x86/sepdk/inc/jkt_unc_qpill.h
new file mode 100644
index 000000000000..debde4d48252
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/jkt_unc_qpill.h
@@ -0,0 +1,64 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+
+#ifndef _JKTUNC_QPILL_H_INC_
+#define _JKTUNC_QPILL_H_INC_
+
+/*
+ * Local to this architecture: JKT uncore QPILL unit
+ *
+ */
+#define JKTUNC_QPILL0_DID       0x3C41
+	// --- QPILL0 PerfMon DID --- B:D 1:8:2
+#define JKTUNC_QPILL_MM0_DID    0x3C86
+	// --- QPILL0 PerfMon MM Config DID --- B:D 1:8:6
+#define JKTUNC_QPILL1_DID       0x3C42
+	// --- QPILL1 PerfMon DID --- B:D 1:9:2
+#define JKTUNC_QPILL2_DID       0x3C44
+	// --- QPILL0 PerfMon DID --- B:D 1:8:2
+#define JKTUNC_QPILL3_DID       0x3C45
+	// --- QPILL0 PerfMon DID --- B:D 1:8:2
+#define JKTUNC_QPILL_MM1_DID    0x3C96
+	// --- QPILL1 PerfMon MM Config DID --- B:D 1:9:6
+#define JKTUNC_QPILL_MCFG_DID   0x3C28
+	// --- QPILL1 PerfMon MCFG DID --- B:D 0:5:0
+#define JKTUNC_QPILL0_D2C_DID   0x3C80
+	// --- D2C QPILL Port 1 config DID B:D:F X:8:0
+#define JKTUNC_QPILL1_D2C_DID   0x3C90
+	// --- D2C QPILL Port 2 config DID B:D:F X:9:0
+
+#define JKTUNC_QPILL_PERF_GLOBAL_CTRL 0x391
+
+#define IA32_DEBUG_CTRL             0x1D9
+
+#define JKTUNC_QPILL_D2C_OFFSET     0x80
+#define JKTUNC_QPILL_D2C_BITMASK    0x00000002
+#define JKTUNC_QPILL_FUNC_NO        2
+#define JKTUNC_QPILL_D2C_FUNC_NO    0
+
+extern DISPATCH_NODE jktunc_qpill_dispatch;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/linuxos.h b/drivers/platform/x86/sepdk/inc/linuxos.h
new file mode 100644
index 000000000000..3e4c2c96476f
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/linuxos.h
@@ -0,0 +1,79 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _LINUXOS_H_
+#define _LINUXOS_H_
+
+// defines for options parameter of samp_load_image_notify_routine()
+#define LOPTS_1ST_MODREC 0x1
+#define LOPTS_GLOBAL_MODULE 0x2
+#define LOPTS_EXE 0x4
+
+#define FOR_EACH_TASK for_each_process
+#if KERNEL_VERSION(3, 19, 00) <= LINUX_VERSION_CODE
+#define DRV_F_DENTRY f_path.dentry
+#else
+#define DRV_F_DENTRY f_dentry
+#endif
+
+#if KERNEL_VERSION(2, 6, 25) > LINUX_VERSION_CODE
+#define D_PATH(vm_file, name, maxlen)                                          \
+	d_path((vm_file)->f_dentry, (vm_file)->f_vfsmnt, (name), (maxlen))
+#else
+#define D_PATH(vm_file, name, maxlen)                                          \
+	d_path(&((vm_file)->f_path), (name), (maxlen))
+#endif
+
+#if KERNEL_VERSION(3, 7, 0) > LINUX_VERSION_CODE
+#define DRV_VM_MOD_EXECUTABLE(vma) (vma->vm_flags & VM_EXECUTABLE)
+#else
+#define DRV_VM_MOD_EXECUTABLE(vma) (linuxos_Equal_VM_Exe_File(vma))
+#define DRV_MM_EXE_FILE_PRESENT
+#endif
+
+#if KERNEL_VERSION(2, 6, 32) <= LINUX_VERSION_CODE
+#define DRV_ALLOW_VDSO
+#endif
+
+#if defined(DRV_IA32)
+#define FIND_VMA(mm, data) find_vma((mm), (U32)(data))
+#endif
+#if defined(DRV_EM64T)
+#define FIND_VMA(mm, data) find_vma((mm), (U64)(data))
+#endif
+
+extern VOID LINUXOS_Install_Hooks(void);
+
+extern VOID LINUXOS_Uninstall_Hooks(void);
+
+extern OS_STATUS LINUXOS_Enum_Process_Modules(DRV_BOOL at_end);
+
+extern DRV_BOOL LINUXOS_Check_KVM_Guest_Process(void);
+#if defined(DRV_CPU_HOTPLUG)
+extern VOID LINUXOS_Register_Hotplug(void);
+
+extern VOID LINUXOS_Unregister_Hotplug(void);
+#endif
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/lwpmudrv.h b/drivers/platform/x86/sepdk/inc/lwpmudrv.h
new file mode 100644
index 000000000000..37c8109a0e8b
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/lwpmudrv.h
@@ -0,0 +1,551 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _LWPMUDRV_H_
+#define _LWPMUDRV_H_
+
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/compat.h>
+#if KERNEL_VERSION(4, 12, 0) > LINUX_VERSION_CODE
+#include <asm/uaccess.h>
+#else
+#include <linux/uaccess.h>
+#endif
+#include <asm/cpufeature.h>
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_version.h"
+#include "lwpmudrv_struct.h"
+#include "pebs.h"
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#endif
+
+#if defined(DRV_SEP_ACRN_ON)
+#include <linux/vhm/acrn_hv_defs.h>
+#include <linux/vhm/vhm_hypercall.h>
+#endif
+
+#if defined(X86_FEATURE_KAISER) || defined(CONFIG_KAISER) ||                   \
+	defined(KAISER_HEADER_PRESENT)
+#define DRV_USE_KAISER
+#elif defined(X86_FEATURE_PTI)
+#define DRV_USE_PTI
+#endif
+
+/*
+ * Print macros for driver messages
+ */
+
+#if defined(MYDEBUG)
+#define SEP_PRINT_DEBUG(fmt, args...)                                \
+	{                                                                \
+		printk(KERN_INFO SEP_MSG_PREFIX " [DEBUG] " fmt, ##args);    \
+	}
+#else
+#define SEP_PRINT_DEBUG(fmt, args...)                                \
+	{                                                                \
+		;                                                            \
+	}
+#endif
+
+#define SEP_PRINT(fmt, args...)                                      \
+	{                                                                \
+		printk(KERN_INFO SEP_MSG_PREFIX " " fmt, ##args);            \
+	}
+
+#define SEP_PRINT_WARNING(fmt, args...)                              \
+	{                                                                \
+		printk(KERN_ALERT SEP_MSG_PREFIX " [Warning] " fmt, ##args); \
+	}
+
+#define SEP_PRINT_ERROR(fmt, args...)                                \
+	{                                                                \
+		printk(KERN_CRIT SEP_MSG_PREFIX " [ERROR] " fmt, ##args);    \
+	}
+
+// Macro to return the thread group id
+#define GET_CURRENT_TGID() (current->tgid)
+
+#define OVERFLOW_ARGS U64 *, U64 *
+
+typedef struct DRV_EVENT_MASK_NODE_S DRV_EVENT_MASK_NODE;
+typedef DRV_EVENT_MASK_NODE * DRV_EVENT_MASK;
+
+struct DRV_EVENT_MASK_NODE_S {
+	U16 event_idx; // 0 <= index < MAX_EVENTS
+	U16 reserved1;
+	union {
+		U32 bitFields1;
+		struct {
+			U32 precise : 1;
+			U32 lbr_capture : 1;
+			U32 dear_capture : 1;
+			// Indicates which events need to have additional
+			// registers read because they are DEAR events.
+			U32 iear_capture : 1;
+			// Indicates which events need to have additional
+			// registers read because they are IEAR events.
+			U32 btb_capture : 1;
+			// Indicates which events need to have additional
+			// registers read because they are BTB events.
+			U32 ipear_capture : 1;
+			// Indicates which events need to have additional
+			// registers read because they are IPEAR events.
+			U32 uncore_capture : 1;
+			U32 branch : 1;
+			// Whether event is related to branch operation or not
+			U32 perf_metrics_capture : 1;
+			// Whether the event is related to perf_metrics or not
+			U32 reserved : 23;
+		} s1;
+	} u1;
+};
+
+#define DRV_EVENT_MASK_event_idx(d)           ((d)->event_idx)
+#define DRV_EVENT_MASK_bitFields1(d)          ((d)->u1.bitFields1)
+#define DRV_EVENT_MASK_precise(d)             ((d)->u1.s1.precise)
+#define DRV_EVENT_MASK_lbr_capture(d)         ((d)->u1.s1.lbr_capture)
+#define DRV_EVENT_MASK_dear_capture(d)        ((d)->u1.s1.dear_capture)
+#define DRV_EVENT_MASK_iear_capture(d)        ((d)->u1.s1.iear_capture)
+#define DRV_EVENT_MASK_btb_capture(d)         ((d)->u1.s1.btb_capture)
+#define DRV_EVENT_MASK_ipear_capture(d)       ((d)->u1.s1.ipear_capture)
+#define DRV_EVENT_MASK_uncore_capture(d)      ((d)->u1.s1.uncore_capture)
+#define DRV_EVENT_MASK_branch(d)              ((d)->u1.s1.branch)
+#define DRV_EVENT_MASK_perf_metrics_capture(d)         \
+		((d)->u1.s1.perf_metrics_capture)
+
+#define MAX_OVERFLOW_EVENTS   16
+/* This defines the maximum number of overflow events per interrupt. \
+ * In order to reduce memory footprint, the value should be at least \
+ * the number of fixed and general PMU registers.                    \
+ * Sandybridge with HT off has 11 PMUs(3 fixed and 8 generic)
+ */
+
+typedef struct DRV_MASKS_NODE_S DRV_MASKS_NODE;
+typedef DRV_MASKS_NODE * DRV_MASKS;
+
+/*
+ * @macro DRV_EVENT_MASK_NODE_S
+ * @brief
+ * The structure is used to store overflow events when handling PMU interrupt.
+ * This approach should be more efficient than checking all event masks
+ * if there are many events to be monitored
+ * and only a few events among them have overflow per interrupt.
+ */
+struct DRV_MASKS_NODE_S {
+	DRV_EVENT_MASK_NODE eventmasks[MAX_OVERFLOW_EVENTS];
+	U8 masks_num; // 0 <= mask_num <= MAX_OVERFLOW_EVENTS
+};
+
+#define DRV_MASKS_masks_num(d) ((d)->masks_num)
+#define DRV_MASKS_eventmasks(d) ((d)->eventmasks)
+
+/*
+ *  Dispatch table for virtualized functions.
+ *  Used to enable common functionality for different
+ *  processor microarchitectures
+ */
+typedef struct DISPATCH_NODE_S DISPATCH_NODE;
+typedef DISPATCH_NODE *DISPATCH;
+
+struct DISPATCH_NODE_S {
+	VOID (*init)(PVOID);
+	VOID (*fini)(PVOID);
+	VOID (*write)(PVOID);
+	VOID (*freeze)(PVOID);
+	VOID (*restart)(PVOID);
+	VOID (*read_data)(PVOID);
+	VOID (*check_overflow)(DRV_MASKS);
+	VOID (*swap_group)(DRV_BOOL);
+	U64 (*read_lbrs)(PVOID, PVOID);
+	VOID (*cleanup)(PVOID);
+	VOID (*hw_errata)(void);
+	VOID (*read_power)(PVOID);
+	U64 (*check_overflow_errata)(ECB, U32, U64);
+	VOID (*read_counts)(PVOID, U32);
+	U64 (*check_overflow_gp_errata)(ECB, U64 *);
+	VOID (*read_ro)(PVOID, U32, U32);
+	VOID (*platform_info)(PVOID);
+	VOID (*trigger_read)(PVOID, U32);
+		// Counter reads triggered/initiated by User mode timer
+	VOID (*scan_for_uncore)(PVOID);
+	VOID (*read_metrics)(PVOID);
+};
+
+#if defined(BUILD_CHIPSET)
+/*
+ *  Dispatch table for virtualized functions.
+ *  Used to enable common functionality for different
+ *  chipset types
+ */
+typedef struct CS_DISPATCH_NODE_S CS_DISPATCH_NODE;
+typedef CS_DISPATCH_NODE *CS_DISPATCH;
+struct CS_DISPATCH_NODE_S {
+	U32  (*init_chipset)(void);
+		// initialize chipset (must be called before the others!)
+	VOID (*start_chipset)(void); // start the chipset counters
+	VOID (*read_counters)(PVOID);
+		// at interrupt time, read out the chipset counters
+	VOID (*stop_chipset)(void); // stop the chipset counters
+	VOID (*fini_chipset)(void);
+		// clean up resources and reset chipset state (called last)
+	VOID (*Trigger_Read)(void);
+		// GMCH counter reads triggered/initiated by User mode timer
+};
+extern CS_DISPATCH cs_dispatch;
+#endif
+
+/*
+ * global declarations
+ */
+
+extern VOID **PMU_register_data;
+extern VOID **desc_data;
+extern U64 *prev_counter_data;
+extern U64 *read_counter_info;
+extern U64 total_ram;
+extern U32 output_buffer_size;
+extern U32 saved_buffer_size;
+extern uid_t uid;
+extern DRV_CONFIG drv_cfg;
+extern volatile pid_t control_pid;
+extern U64 *interrupt_counts;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+
+extern DRV_BOOL multi_pebs_enabled;
+extern DRV_BOOL unc_buf_init;
+
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+
+
+/* needed for target agent support */
+extern U32 osid;
+extern DRV_BOOL sched_switch_enabled;
+
+#if defined(BUILD_CHIPSET)
+extern CHIPSET_CONFIG pma;
+#endif
+
+extern UNCORE_TOPOLOGY_INFO_NODE uncore_topology;
+extern PLATFORM_TOPOLOGY_PROG_NODE platform_topology_prog_node;
+extern wait_queue_head_t wait_exit;
+/*
+ * end of declarations
+ */
+
+/*!
+ * @struct LWPMU_DEVICE_NODE_S
+ * @brief  Struct to hold fields per device
+ *           PMU_register_data_unc - MSR info
+ *           dispatch_unc          - dispatch table
+ *           em_groups_counts_unc  - # groups
+ *           pcfg_unc              - config struct
+ */
+typedef struct LWPMU_DEVICE_NODE_S LWPMU_DEVICE_NODE;
+typedef LWPMU_DEVICE_NODE * LWPMU_DEVICE;
+
+struct LWPMU_DEVICE_NODE_S {
+	VOID **PMU_register_data;
+	DISPATCH dispatch;
+	S32 em_groups_count;
+	VOID *pcfg;
+	U64 **unc_prev_value;
+	U64 ***unc_acc_value;
+	U64 counter_mask;
+	U64 num_events;
+	U32 num_units;
+	VOID *ec;
+	S32 *cur_group;
+	S32 pci_dev_node_index;
+	U32 device_type;
+	LBR lbr;
+	PWR pwr;
+	PEBS_INFO_NODE pebs_info_node;
+};
+
+#define LWPMU_DEVICE_PMU_register_data(dev)  ((dev)->PMU_register_data)
+#define LWPMU_DEVICE_dispatch(dev)           ((dev)->dispatch)
+#define LWPMU_DEVICE_em_groups_count(dev)    ((dev)->em_groups_count)
+#define LWPMU_DEVICE_pcfg(dev)               ((dev)->pcfg)
+#define LWPMU_DEVICE_prev_value(dev)         ((dev)->unc_prev_value)
+#define LWPMU_DEVICE_acc_value(dev)          ((dev)->unc_acc_value)
+#define LWPMU_DEVICE_counter_mask(dev)       ((dev)->counter_mask)
+#define LWPMU_DEVICE_num_events(dev)         ((dev)->num_events)
+#define LWPMU_DEVICE_num_units(dev)          ((dev)->num_units)
+#define LWPMU_DEVICE_ec(dev)                 ((dev)->ec)
+#define LWPMU_DEVICE_cur_group(dev)          ((dev)->cur_group)
+#define LWPMU_DEVICE_pci_dev_node_index(dev) ((dev)->pci_dev_node_index)
+#define LWPMU_DEVICE_device_type(dev)        ((dev)->device_type)
+#define LWPMU_DEVICE_lbr(dev)                ((dev)->lbr)
+#define LWPMU_DEVICE_pwr(dev)                ((dev)->pwr)
+#define LWPMU_DEVICE_pebs_dispatch(dev)   ((dev)->pebs_info_node.pebs_dispatch)
+
+#define LWPMU_DEVICE_pebs_record_size(dev)                                     \
+	((dev)->pebs_info_node.pebs_record_size)
+#define LWPMU_DEVICE_apebs_basic_offset(dev)                                   \
+	((dev)->pebs_info_node.apebs_basic_offset)
+#define LWPMU_DEVICE_apebs_mem_offset(dev)                                     \
+	((dev)->pebs_info_node.apebs_mem_offset)
+#define LWPMU_DEVICE_apebs_gpr_offset(dev)                                     \
+	((dev)->pebs_info_node.apebs_gpr_offset)
+#define LWPMU_DEVICE_apebs_xmm_offset(dev)                                     \
+	((dev)->pebs_info_node.apebs_xmm_offset)
+#define LWPMU_DEVICE_apebs_lbr_offset(dev)                                     \
+	((dev)->pebs_info_node.apebs_lbr_offset)
+
+extern U32 num_devices;
+extern U32 cur_device;
+extern LWPMU_DEVICE devices;
+extern U64 *pmu_state;
+
+// Handy macro
+#define TSC_SKEW(this_cpu) (cpu_tsc[this_cpu] - cpu_tsc[0])
+
+/*
+ *  The IDT / GDT descriptor for use in identifying code segments
+ */
+#if defined(DRV_EM64T)
+#pragma pack(push, 1)
+typedef struct _idtgdtDesc {
+	U16 idtgdt_limit;
+	PVOID idtgdt_base;
+} IDTGDT_DESC;
+#pragma pack(pop)
+
+extern IDTGDT_DESC gdt_desc;
+#endif
+
+extern DRV_BOOL NMI_mode;
+extern DRV_BOOL KVM_guest_mode;
+
+#if defined(DRV_SEP_ACRN_ON)
+#define SBUF_MAX_SIZE (1ULL << 22)
+#define SBUF_HEAD_SIZE 64
+
+#define TRACE_SBUF_SIZE (4 * 1024 * 1024)
+#define TRACE_ELEMENT_SIZE 32 /* byte */
+#define TRACE_ELEMENT_NUM                                                     \
+	((TRACE_SBUF_SIZE - SBUF_HEAD_SIZE) / TRACE_ELEMENT_SIZE)
+
+#define COLLECTOR_SEP 0
+#define COLLECTOR_SOCWATCH 1
+
+enum PROFILING_FEATURE {
+	CORE_PMU_SAMPLING = 0,
+	CORE_PMU_COUNTING,
+	PEBS_PMU_SAMPLING,
+	LBR_PMU_SAMPLING,
+	UNCORE_PMU_SAMPLING,
+	VM_SWITCH_TRACING,
+	// Add socwatch feature
+};
+
+enum sbuf_type {
+	ACRN_TRACE,
+	ACRN_HVLOG,
+	ACRN_SEP,
+	ACRN_SOCWATCH,
+	ACRN_SBUF_TYPE_MAX,
+};
+
+struct data_header {
+	int32_t collector_id;
+	uint16_t cpu_id;
+	uint16_t data_type;
+	uint64_t tsc; /* TSC */
+	uint64_t payload_size;
+	uint64_t reserved;
+} __aligned(32);
+
+#define PROFILING_DATA_HEADER_SIZE (sizeof(struct data_header))
+
+struct core_pmu_sample {
+	/** context where PMI is triggered */
+	uint32_t os_id;
+	/** the task id */
+	uint32_t task_id;
+	/** instruction pointer */
+	uint64_t rip;
+	/** the task name */
+	char task[16];
+	/** physical core ID */
+	uint32_t cpu_id;
+	/** the process id */
+	uint32_t process_id;
+	/** perf global status msr value (for overflow status) */
+	uint64_t overflow_status;
+	/** rflags */
+	uint32_t rflags;
+	/** code segment */
+	uint32_t cs;
+} __aligned(32);
+
+#define CORE_PMU_SAMPLE_SIZE (sizeof(struct core_pmu_sample))
+
+#define NUM_LBR_ENTRY 32
+
+struct lbr_pmu_sample {
+	/* LBR TOS */
+	uint64_t lbr_tos;
+	/* LBR FROM IP */
+	uint64_t lbr_from_ip[NUM_LBR_ENTRY];
+	/* LBR TO IP */
+	uint64_t lbr_to_ip[NUM_LBR_ENTRY];
+	/* LBR info */
+	uint64_t lbr_info[NUM_LBR_ENTRY];
+} __aligned(32);
+
+#define LBR_PMU_SAMPLE_SIZE (sizeof(struct lbr_pmu_sample))
+
+struct pmu_sample {
+	/* core pmu sample */
+	struct core_pmu_sample csample;
+	/* lbr pmu sample */
+	struct lbr_pmu_sample lsample;
+} __aligned(32);
+
+#define PMU_SAMPLE_SIZE (sizeof(struct pmu_sample))
+
+struct vm_switch_trace {
+	uint64_t vmenter_tsc;
+	uint64_t vmexit_tsc;
+	uint64_t vmexit_reason;
+	int32_t os_id;
+} __aligned(32);
+
+#define VM_SWITCH_TRACE_SIZE (sizeof(struct vm_switch_trace))
+
+typedef struct shared_buf shared_buf_t;
+typedef struct profiling_control profiling_control_t;
+typedef struct data_header data_header_t;
+typedef struct core_pmu_sample core_pmu_sample_t;
+typedef struct vm_switch_trace vm_switch_trace_t;
+
+shared_buf_t *sbuf_allocate(uint32_t ele_num, uint32_t ele_size);
+void sbuf_free(shared_buf_t *sbuf);
+int sbuf_get(shared_buf_t *sbuf, uint8_t *data);
+int sbuf_share_setup(uint32_t pcpu_id, uint32_t sbuf_id, shared_buf_t *sbuf);
+
+extern shared_buf_t **samp_buf_per_cpu;
+
+#define MAX_NR_PCPUS 8
+#define MAX_NR_VCPUS 8
+#define MAX_NR_VMS 6
+#define MAX_MSR_LIST_NUM 15
+#define MAX_GROUP_NUM 1
+
+enum MSR_OP_STATUS { MSR_OP_READY = 0, MSR_OP_REQUESTED, MSR_OP_HANDLED };
+
+enum MSR_OP_TYPE {
+	MSR_OP_NONE = 0,
+	MSR_OP_READ,
+	MSR_OP_WRITE,
+	MSR_OP_READ_CLEAR
+};
+
+enum PMU_MSR_TYPE { PMU_MSR_CCCR = 0, PMU_MSR_ESCR, PMU_MSR_DATA };
+
+struct profiling_msr_op {
+	/* value to write or location to write into */
+	uint64_t value;
+	/* MSR address to read/write; last entry will have value of -1 */
+	uint32_t msr_id;
+	/* parameter; usage depends on operation */
+	uint16_t param;
+	uint8_t op_type;
+	uint8_t reg_type;
+};
+
+struct profiling_msr_ops_list {
+	int32_t collector_id;
+	uint32_t num_entries;
+	int32_t msr_op_state;
+	struct profiling_msr_op entries[MAX_MSR_LIST_NUM];
+};
+
+struct profiling_vcpu_pcpu_map {
+	int32_t vcpu_id;
+	int32_t pcpu_id;
+	int32_t apic_id;
+};
+
+struct profiling_vm_info {
+	int32_t vm_id;
+	u_char guid[16];
+	char vm_name[16];
+	int32_t num_vcpus;
+	struct profiling_vcpu_pcpu_map cpu_map[MAX_NR_VCPUS];
+};
+
+struct profiling_vm_info_list {
+	int32_t num_vms;
+	struct profiling_vm_info vm_list[MAX_NR_VMS];
+};
+
+struct profiling_version_info {
+	int32_t major;
+	int32_t minor;
+	int64_t supported_features;
+	int64_t reserved;
+};
+
+struct profiling_control {
+	int32_t collector_id;
+	int32_t reserved;
+	uint64_t switches;
+};
+
+struct profiling_pmi_config {
+	uint32_t num_groups;
+	uint32_t trigger_count;
+	struct profiling_msr_op initial_list[MAX_GROUP_NUM][MAX_MSR_LIST_NUM];
+	struct profiling_msr_op start_list[MAX_GROUP_NUM][MAX_MSR_LIST_NUM];
+	struct profiling_msr_op stop_list[MAX_GROUP_NUM][MAX_MSR_LIST_NUM];
+	struct profiling_msr_op entry_list[MAX_GROUP_NUM][MAX_MSR_LIST_NUM];
+	struct profiling_msr_op exit_list[MAX_GROUP_NUM][MAX_MSR_LIST_NUM];
+};
+
+struct profiling_vmsw_config {
+	int32_t collector_id;
+	struct profiling_msr_op initial_list[MAX_MSR_LIST_NUM];
+	struct profiling_msr_op entry_list[MAX_MSR_LIST_NUM];
+	struct profiling_msr_op exit_list[MAX_MSR_LIST_NUM];
+};
+
+struct profiling_pcpuid {
+	uint32_t leaf;
+	uint32_t subleaf;
+	uint32_t eax;
+	uint32_t ebx;
+	uint32_t ecx;
+	uint32_t edx;
+};
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/msrdefs.h b/drivers/platform/x86/sepdk/inc/msrdefs.h
new file mode 100644
index 000000000000..40986ea111bb
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/msrdefs.h
@@ -0,0 +1,81 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _MSRDEFS_H_
+#define _MSRDEFS_H_
+
+/*
+ * Arch Perf monitoring version 3
+ */
+#define IA32_PMC0 0x0C1
+#define IA32_PMC1 0x0C2
+#define IA32_PMC2 0x0C3
+#define IA32_PMC3 0x0C4
+#define IA32_PMC4 0x0C5
+#define IA32_PMC5 0x0C6
+#define IA32_PMC6 0x0C7
+#define IA32_PMC7 0x0C8
+#define IA32_FULL_PMC0 0x4C1
+#define IA32_FULL_PMC1 0x4C2
+#define IA32_PERFEVTSEL0 0x186
+#define IA32_PERFEVTSEL1 0x187
+#define IA32_FIXED_CTR0 0x309
+#define IA32_FIXED_CTR1 0x30A
+#define IA32_FIXED_CTR2 0x30B
+#define IA32_FIXED_CTR3 0x30C
+#define IA32_PERF_CAPABILITIES 0x345
+#define IA32_FIXED_CTRL 0x38D
+#define IA32_PERF_GLOBAL_STATUS 0x38E
+#define IA32_PERF_GLOBAL_CTRL 0x38F
+#define IA32_PERF_GLOBAL_OVF_CTRL 0x390
+#define IA32_PEBS_ENABLE 0x3F1
+#define IA32_MISC_ENABLE 0x1A0
+#define IA32_DS_AREA 0x600
+#define IA32_DEBUG_CTRL 0x1D9
+#undef IA32_LBR_FILTER_SELECT
+#define IA32_LBR_FILTER_SELECT 0x1c8
+#define IA32_PEBS_FRONTEND 0x3F7
+#define IA32_PERF_METRICS 0x329
+
+#define COMPOUND_CTR_CTL 0x306
+#define COMPOUND_PERF_CTR 0x307
+#define COMPOUND_CTR_OVF_BIT 0x800
+#define COMPOUND_CTR_OVF_SHIFT 12
+
+#define FIXED_CORE_CYCLE_GLOBAL_CTRL_MASK 0x200000000
+#define FIXED_CORE_CYCLE_FIXED_CTRL_MASK 0xF0
+
+// REG INDEX inside GLOBAL CTRL SECTION
+enum { GLOBAL_CTRL_REG_INDEX = 0,
+	GLOBAL_OVF_CTRL_REG_INDEX,
+	PEBS_ENABLE_REG_INDEX,
+	DEBUG_CTRL_REG_INDEX,
+	FIXED_CTRL_REG_INDEX,
+};
+
+// REG INDEX inside GLOBAL STATUS SECTION
+enum { GLOBAL_STATUS_REG_INDEX = 0,};
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/output.h b/drivers/platform/x86/sepdk/inc/output.h
new file mode 100644
index 000000000000..483e0b5fb5d5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/output.h
@@ -0,0 +1,120 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _OUTPUT_H_
+#define _OUTPUT_H_
+
+#include <linux/timer.h>
+#include <linux/vmalloc.h>
+
+/*
+ * Initial allocation
+ * Size of buffer     = 512KB (2^19)
+ * number of buffers  = 2
+ * The max size of the buffer cannot exceed 1<<22 i.e. 4MB
+ */
+#define OUTPUT_SMALL_BUFFER (1 << 15)
+#define OUTPUT_LARGE_BUFFER (1 << 19)
+#define OUTPUT_CP_BUFFER (1 << 22)
+#define OUTPUT_MEMORY_THRESHOLD 0x8000000
+
+extern U32 output_buffer_size;
+extern U32 saved_buffer_size;
+#define OUTPUT_BUFFER_SIZE output_buffer_size
+#define OUTPUT_NUM_BUFFERS 2
+#if defined(DRV_ANDROID)
+#define MODULE_BUFF_SIZE 1
+#else
+#define MODULE_BUFF_SIZE 2
+#endif
+
+/*
+ *  Data type declarations and accessors macros
+ */
+typedef struct {
+	spinlock_t buffer_lock;
+	U32 remaining_buffer_size;
+	U32 current_buffer;
+	U32 total_buffer_size;
+	U32 next_buffer[OUTPUT_NUM_BUFFERS];
+	U32 buffer_full[OUTPUT_NUM_BUFFERS];
+	U8 *buffer[OUTPUT_NUM_BUFFERS];
+	U32 signal_full;
+	DRV_BOOL tasklet_queued;
+} OUTPUT_NODE, *OUTPUT;
+
+#define OUTPUT_buffer_lock(x)           ((x)->buffer_lock)
+#define OUTPUT_remaining_buffer_size(x) ((x)->remaining_buffer_size)
+#define OUTPUT_total_buffer_size(x)     ((x)->total_buffer_size)
+#define OUTPUT_buffer(x, y)             ((x)->buffer[(y)])
+#define OUTPUT_buffer_full(x, y)        ((x)->buffer_full[(y)])
+#define OUTPUT_current_buffer(x)        ((x)->current_buffer)
+#define OUTPUT_signal_full(x)           ((x)->signal_full)
+#define OUTPUT_tasklet_queued(x)        ((x)->tasklet_queued)
+/*
+ *  Add an array of control buffer for per-cpu
+ */
+typedef struct {
+	wait_queue_head_t queue;
+	OUTPUT_NODE outbuf;
+	U32 sample_count;
+} BUFFER_DESC_NODE, *BUFFER_DESC;
+
+#define BUFFER_DESC_queue(a) ((a)->queue)
+#define BUFFER_DESC_outbuf(a) ((a)->outbuf)
+#define BUFFER_DESC_sample_count(a) ((a)->sample_count)
+
+extern BUFFER_DESC cpu_buf; // actually an array of BUFFER_DESC_NODE
+extern BUFFER_DESC unc_buf;
+extern BUFFER_DESC module_buf;
+extern BUFFER_DESC cpu_sideband_buf;
+/*
+ *  Interface Functions
+ */
+
+extern int OUTPUT_Module_Fill(PVOID data, U16 size, U8 in_notification);
+extern OS_STATUS OUTPUT_Initialize(void);
+extern OS_STATUS OUTPUT_Initialize_UNC(void);
+extern void OUTPUT_Cleanup(void);
+extern void OUTPUT_Cleanup(void);
+extern int OUTPUT_Destroy(void);
+extern int OUTPUT_Flush(void);
+
+extern ssize_t OUTPUT_Module_Read(struct file *filp, char __user *buf,
+			size_t count, loff_t *f_pos);
+
+extern ssize_t OUTPUT_Sample_Read(struct file *filp, char __user *buf,
+			size_t count, loff_t *f_pos);
+
+extern ssize_t OUTPUT_UncSample_Read(struct file *filp, char __user *buf,
+				size_t count, loff_t *f_pos);
+
+extern ssize_t OUTPUT_SidebandInfo_Read(struct file *filp, char __user *buf,
+				size_t count, loff_t *f_pos);
+
+extern void *OUTPUT_Reserve_Buffer_Space(BUFFER_DESC bd, U32 size,
+		DRV_BOOL defer, U8 in_notification, S32 cpu_idx);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/pci.h b/drivers/platform/x86/sepdk/inc/pci.h
new file mode 100644
index 000000000000..44d5304d86a5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/pci.h
@@ -0,0 +1,133 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _PCI_H_
+#define _PCI_H_
+
+#include "lwpmudrv_defines.h"
+
+/*
+ * PCI Config Address macros
+ */
+#define PCI_ENABLE 0x80000000
+
+#define PCI_ADDR_IO 0xCF8
+#define PCI_DATA_IO 0xCFC
+
+#define BIT0 0x1
+#define BIT1 0x2
+
+/*
+ * Macro for forming a PCI configuration address
+ */
+#define FORM_PCI_ADDR(bus, dev, fun, off)                                  \
+	(((PCI_ENABLE)) | ((bus & 0xFF) << 16) | ((dev & 0x1F) << 11) |    \
+	((fun & 0x07) << 8) | ((off & 0xFF) << 0))
+
+#define VENDOR_ID_MASK 0x0000FFFF
+#define DEVICE_ID_MASK 0xFFFF0000
+#define DEVICE_ID_BITSHIFT 16
+#define LOWER_4_BYTES_MASK 0x00000000FFFFFFFF
+#define MAX_BUSNO 256
+#define NEXT_ADDR_OFFSET 4
+#define NEXT_ADDR_SHIFT 32
+#define DRV_IS_PCI_VENDOR_ID_INTEL 0x8086
+#define MAX_PCI_DEVS 32
+
+#define CONTINUE_IF_NOT_GENUINE_INTEL_DEVICE(value, vendor_id, device_id)   \
+	{                                                                   \
+		vendor_id = value & VENDOR_ID_MASK;                         \
+		device_id = (value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT; \
+		if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {              \
+			continue;                                           \
+		}                                                           \
+	}
+
+#define CHECK_IF_GENUINE_INTEL_DEVICE(value, vendor_id, device_id, valid)   \
+	{                                                                   \
+		vendor_id = value & VENDOR_ID_MASK;                         \
+		device_id = (value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT; \
+		valid = 1;                                                  \
+		if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {              \
+			valid = 0;                                          \
+		}                                                           \
+	}
+
+typedef struct SEP_MMIO_NODE_S SEP_MMIO_NODE;
+
+struct SEP_MMIO_NODE_S {
+	U64 physical_address;
+	U64 virtual_address;
+	U64 map_token;
+	U32 size;
+};
+
+#define SEP_MMIO_NODE_physical_address(x) ((x)->physical_address)
+#define SEP_MMIO_NODE_virtual_address(x) ((x)->virtual_address)
+#define SEP_MMIO_NODE_map_token(x) ((x)->map_token)
+#define SEP_MMIO_NODE_size(x) ((x)->size)
+
+extern OS_STATUS PCI_Map_Memory(SEP_MMIO_NODE *node, U64 phy_address,
+				U32 map_size);
+
+extern void PCI_Unmap_Memory(SEP_MMIO_NODE *node);
+
+extern int PCI_Read_From_Memory_Address(U32 addr, U32 *val);
+
+extern int PCI_Write_To_Memory_Address(U32 addr, U32 val);
+
+/*** UNIVERSAL PCI ACCESSORS ***/
+
+extern VOID PCI_Initialize(void);
+
+extern U32 PCI_Read_U32(U32 bus, U32 device, U32 function, U32 offset);
+
+extern U32 PCI_Read_U32_Valid(U32 bus, U32 device, U32 function, U32 offset,
+			      U32 invalid_value);
+
+extern U64 PCI_Read_U64(U32 bus, U32 device, U32 function, U32 offset);
+
+extern U64 PCI_Read_U64_Valid(U32 bus, U32 device, U32 function, U32 offset,
+			      U64 invalid_value);
+
+extern U32 PCI_Write_U32(U32 bus, U32 device, U32 function, U32 offset,
+			 U32 value);
+
+extern U32 PCI_Write_U64(U32 bus, U32 device, U32 function, U32 offset,
+			 U64 value);
+
+/*** UNIVERSAL MMIO ACCESSORS ***/
+
+extern U32 PCI_MMIO_Read_U32(U64 virtual_address_base, U32 offset);
+
+extern U64 PCI_MMIO_Read_U64(U64 virtual_address_base, U32 offset);
+
+extern void PCI_MMIO_Write_U32(U64 virtual_address_base, U32 offset,
+				U32 value);
+
+extern void PCI_MMIO_Write_U64(U64 virtual_address_base, U32 offset,
+				U64 value);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/pebs.h b/drivers/platform/x86/sepdk/inc/pebs.h
new file mode 100644
index 000000000000..7a7bbe10e2ba
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/pebs.h
@@ -0,0 +1,494 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _PEBS_H_
+#define _PEBS_H_
+
+
+typedef struct PEBS_REC_NODE_S PEBS_REC_NODE;
+
+struct PEBS_REC_NODE_S {
+	U64 r_flags; // Offset 0x00
+	U64 linear_ip; // Offset 0x08
+	U64 rax; // Offset 0x10
+	U64 rbx; // Offset 0x18
+	U64 rcx; // Offset 0x20
+	U64 rdx; // Offset 0x28
+	U64 rsi; // Offset 0x30
+	U64 rdi; // Offset 0x38
+	U64 rbp; // Offset 0x40
+	U64 rsp; // Offset 0x48
+	U64 r8; // Offset 0x50
+	U64 r9; // Offset 0x58
+	U64 r10; // Offset 0x60
+	U64 r11; // Offset 0x68
+	U64 r12; // Offset 0x70
+	U64 r13; // Offset 0x78
+	U64 r14; // Offset 0x80
+	U64 r15; // Offset 0x88
+};
+
+typedef struct PEBS_REC_EXT_NODE_S PEBS_REC_EXT_NODE;
+typedef PEBS_REC_EXT_NODE * PEBS_REC_EXT;
+struct PEBS_REC_EXT_NODE_S {
+	PEBS_REC_NODE pebs_basic; // Offset 0x00 to 0x88
+	U64 glob_perf_overflow; // Offset 0x90
+	U64 data_linear_address; // Offset 0x98
+	U64 data_source; // Offset 0xA0
+	U64 latency; // Offset 0xA8
+};
+
+#define PEBS_REC_EXT_r_flags(x) ((x)->pebs_basic.r_flags)
+#define PEBS_REC_EXT_linear_ip(x) ((x)->pebs_basic.linear_ip)
+#define PEBS_REC_EXT_rax(x) 	((x)->pebs_basic.rax)
+#define PEBS_REC_EXT_rbx(x) 	((x)->pebs_basic.rbx)
+#define PEBS_REC_EXT_rcx(x) 	((x)->pebs_basic.rcx)
+#define PEBS_REC_EXT_rdx(x) 	((x)->pebs_basic.rdx)
+#define PEBS_REC_EXT_rsi(x)		((x)->pebs_basic.rsi)
+#define PEBS_REC_EXT_rdi(x) 	((x)->pebs_basic.rdi)
+#define PEBS_REC_EXT_rbp(x) 	((x)->pebs_basic.rbp)
+#define PEBS_REC_EXT_rsp(x) 	((x)->pebs_basic.rsp)
+#define PEBS_REC_EXT_r8(x) 		((x)->pebs_basic.r8)
+#define PEBS_REC_EXT_r9(x) 		((x)->pebs_basic.r9)
+#define PEBS_REC_EXT_r10(x) 	((x)->pebs_basic.r10)
+#define PEBS_REC_EXT_r11(x) 	((x)->pebs_basic.r11)
+#define PEBS_REC_EXT_r12(x) 	((x)->pebs_basic.r12)
+#define PEBS_REC_EXT_r13(x) 	((x)->pebs_basic.r13)
+#define PEBS_REC_EXT_r14(x) 	((x)->pebs_basic.r14)
+#define PEBS_REC_EXT_r15(x) 	((x)->pebs_basic.r15)
+#define PEBS_REC_EXT_glob_perf_overflow(x) ((x)->glob_perf_overflow)
+#define PEBS_REC_EXT_data_linear_address(x) ((x)->data_linear_address)
+#define PEBS_REC_EXT_data_source(x) ((x)->data_source)
+#define PEBS_REC_EXT_latency(x) ((x)->latency)
+
+typedef struct PEBS_REC_EXT1_NODE_S PEBS_REC_EXT1_NODE;
+typedef PEBS_REC_EXT1_NODE * PEBS_REC_EXT1;
+struct PEBS_REC_EXT1_NODE_S {
+	PEBS_REC_EXT_NODE pebs_ext;
+	U64 eventing_ip; //Offset 0xB0
+	U64 hle_info; //Offset 0xB8
+};
+
+#define PEBS_REC_EXT1_r_flags(x) ((x)->pebs_ext.pebs_basic.r_flags)
+#define PEBS_REC_EXT1_linear_ip(x) ((x)->pebs_ext.pebs_basic.linear_ip)
+#define PEBS_REC_EXT1_rax(x) ((x)->pebs_ext.pebs_basic.rax)
+#define PEBS_REC_EXT1_rbx(x) ((x)->pebs_ext.pebs_basic.rbx)
+#define PEBS_REC_EXT1_rcx(x) ((x)->pebs_ext.pebs_basic.rcx)
+#define PEBS_REC_EXT1_rdx(x) ((x)->pebs_ext.pebs_basic.rdx)
+#define PEBS_REC_EXT1_rsi(x) ((x)->pebs_ext.pebs_basic.rsi)
+#define PEBS_REC_EXT1_rdi(x) ((x)->pebs_ext.pebs_basic.rdi)
+#define PEBS_REC_EXT1_rbp(x) ((x)->pebs_ext.pebs_basic.rbp)
+#define PEBS_REC_EXT1_rsp(x) ((x)->pebs_ext.pebs_basic.rsp)
+#define PEBS_REC_EXT1_r8(x) ((x)->pebs_ext.pebs_basic.r8)
+#define PEBS_REC_EXT1_r9(x) ((x)->pebs_ext.pebs_basic.r9)
+#define PEBS_REC_EXT1_r10(x) ((x)->pebs_ext.pebs_basic.r10)
+#define PEBS_REC_EXT1_r11(x) ((x)->pebs_ext.pebs_basic.r11)
+#define PEBS_REC_EXT1_r12(x) ((x)->pebs_ext.pebs_basic.r12)
+#define PEBS_REC_EXT1_r13(x) ((x)->pebs_ext.pebs_basic.r13)
+#define PEBS_REC_EXT1_r14(x) ((x)->pebs_ext.pebs_basic.r14)
+#define PEBS_REC_EXT1_r15(x) ((x)->pebs_ext.pebs_basic.r15)
+#define PEBS_REC_EXT1_glob_perf_overflow(x) ((x)->pebs_ext.glob_perf_overflow)
+#define PEBS_REC_EXT1_data_linear_address(x)                      \
+	((x)->pebs_ext.data_linear_address)
+#define PEBS_REC_EXT1_data_source(x) ((x)->pebs_ext.data_source)
+#define PEBS_REC_EXT1_latency(x) ((x)->pebs_ext.latency)
+#define PEBS_REC_EXT1_eventing_ip(x) ((x)->eventing_ip)
+#define PEBS_REC_EXT1_hle_info(x) ((x)->hle_info)
+
+typedef struct PEBS_REC_EXT2_NODE_S PEBS_REC_EXT2_NODE;
+typedef PEBS_REC_EXT2_NODE * PEBS_REC_EXT2;
+struct PEBS_REC_EXT2_NODE_S {
+	PEBS_REC_EXT1_NODE pebs_ext1;
+	U64 tsc; //Offset 0xC0
+};
+
+#define PEBS_REC_EXT2_r_flags(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r_flags)
+#define PEBS_REC_EXT2_linear_ip(x)                                            \
+	((x)->pebs_ext1->pebs_ext.pebs_basic.linear_ip)
+#define PEBS_REC_EXT2_rax(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rax)
+#define PEBS_REC_EXT2_rbx(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rbx)
+#define PEBS_REC_EXT2_rcx(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rcx)
+#define PEBS_REC_EXT2_rdx(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rdx)
+#define PEBS_REC_EXT2_rsi(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rsi)
+#define PEBS_REC_EXT2_rdi(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rdi)
+#define PEBS_REC_EXT2_rbp(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rbp)
+#define PEBS_REC_EXT2_rsp(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.rsp)
+#define PEBS_REC_EXT2_r8(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r8)
+#define PEBS_REC_EXT2_r9(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r9)
+#define PEBS_REC_EXT2_r10(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r10)
+#define PEBS_REC_EXT2_r11(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r11)
+#define PEBS_REC_EXT2_r12(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r12)
+#define PEBS_REC_EXT2_r13(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r13)
+#define PEBS_REC_EXT2_r14(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r14)
+#define PEBS_REC_EXT2_r15(x) ((x)->pebs_ext1->pebs_ext.pebs_basic.r15)
+#define PEBS_REC_EXT2_glob_perf_overflow(x)                                   \
+	((x)->pebs_ext1->pebs_ext.glob_perf_overflow)
+#define PEBS_REC_EXT2_data_linear_address(x)                                  \
+	((x)->pebs_ext1->pebs_ext.data_linear_address)
+#define PEBS_REC_EXT2_data_source(x) ((x)->pebs_ext1->pebs_ext.data_source)
+#define PEBS_REC_EXT2_latency(x) ((x)->pebs_ext1->pebs_ext.latency)
+#define PEBS_REC_EXT2_eventing_ip(x) ((x)->pebs_ext1->eventing_ip)
+#define PEBS_REC_EXT2_hle_info(x) ((x)->pebs_ext1->hle_info)
+#define PEBS_REC_EXT2_tsc(x) ((x)->tsc)
+
+typedef struct APEBS_CONFIG_NODE_S APEBS_CONFIG_NODE;
+typedef APEBS_CONFIG_NODE * APEBS_CONFIG;
+
+struct APEBS_CONFIG_NODE_S {
+	U8 apebs_enabled;
+	U8 collect_mem;
+	U8 collect_gpr;
+	U8 collect_xmm;
+	U8 collect_lbrs;
+	U8 precise_ip_lbrs;
+	U8 num_lbr_entries;
+	U16 basic_offset;
+	U16 mem_offset;
+	U16 gpr_offset;
+	U16 xmm_offset;
+	U16 lbr_offset;
+};
+
+#define APEBS_CONFIG_apebs_enabled(x) ((x)->apebs_enabled)
+#define APEBS_CONFIG_collect_mem(x) ((x)->collect_mem)
+#define APEBS_CONFIG_collect_gpr(x) ((x)->collect_gpr)
+#define APEBS_CONFIG_collect_xmm(x) ((x)->collect_xmm)
+#define APEBS_CONFIG_collect_lbrs(x) ((x)->collect_lbrs)
+#define APEBS_CONFIG_precise_ip_lbrs(x) ((x)->precise_ip_lbrs)
+#define APEBS_CONFIG_num_lbr_entries(x) ((x)->num_lbr_entries)
+#define APEBS_CONFIG_basic_offset(x) ((x)->basic_offset)
+#define APEBS_CONFIG_mem_offset(x) ((x)->mem_offset)
+#define APEBS_CONFIG_gpr_offset(x) ((x)->gpr_offset)
+#define APEBS_CONFIG_xmm_offset(x) ((x)->xmm_offset)
+#define APEBS_CONFIG_lbr_offset(x) ((x)->lbr_offset)
+
+typedef struct ADAPTIVE_PEBS_BASIC_INFO_NODE_S ADAPTIVE_PEBS_BASIC_INFO_NODE;
+typedef ADAPTIVE_PEBS_BASIC_INFO_NODE * ADAPTIVE_PEBS_BASIC_INFO;
+
+struct ADAPTIVE_PEBS_BASIC_INFO_NODE_S {
+	U64 record_info; // Offset 0x0
+		// [47:0] - record format, [63:48] - record size
+	U64 eventing_ip; // Offset 0x8
+	U64 applicable_counters; // Offset 0x10
+	U64 tsc; // Offset 0x18
+};
+
+#define ADAPTIVE_PEBS_BASIC_INFO_record_info(x) ((x)->record_info)
+#define ADAPTIVE_PEBS_BASIC_INFO_eventing_ip(x) ((x)->eventing_ip)
+#define ADAPTIVE_PEBS_BASIC_INFO_tsc(x) ((x)->tsc)
+#define ADAPTIVE_PEBS_BASIC_INFO_applicable_counters(x)                       \
+	((x)->applicable_counters)
+
+typedef struct ADAPTIVE_PEBS_MEM_INFO_NODE_S ADAPTIVE_PEBS_MEM_INFO_NODE;
+typedef ADAPTIVE_PEBS_MEM_INFO_NODE * ADAPTIVE_PEBS_MEM_INFO;
+
+struct ADAPTIVE_PEBS_MEM_INFO_NODE_S {
+	U64 data_linear_address; // Offset 0x20
+	U64 data_source; // Offset 0x28
+	U64 latency; // Offset 0x30
+	U64 hle_info; // Offset 0x38
+};
+
+#define ADAPTIVE_PEBS_MEM_INFO_data_linear_address(x) ((x)->data_linear_address)
+#define ADAPTIVE_PEBS_MEM_INFO_data_source(x) ((x)->data_source)
+#define ADAPTIVE_PEBS_MEM_INFO_latency(x) ((x)->latency)
+#define ADAPTIVE_PEBS_MEM_INFO_hle_info(x) ((x)->hle_info)
+
+typedef struct ADAPTIVE_PEBS_GPR_INFO_NODE_S ADAPTIVE_PEBS_GPR_INFO_NODE;
+typedef ADAPTIVE_PEBS_GPR_INFO_NODE * ADAPTIVE_PEBS_GPR_INFO;
+
+struct ADAPTIVE_PEBS_GPR_INFO_NODE_S {
+	U64 rflags; // Offset 0x40
+	U64 rip; // Offset 0x48
+	U64 rax; // Offset 0x50
+	U64 rcx; // Offset 0x58
+	U64 rdx; // Offset 0x60
+	U64 rbx; // Offset 0x68
+	U64 rsp; // Offset 0x70
+	U64 rbp; // Offset 0x78
+	U64 rsi; // Offset 0x80
+	U64 rdi; // Offset 0x88
+	U64 r8; // Offset 0x90
+	U64 r9; // Offset 0x98
+	U64 r10; // Offset 0xA0
+	U64 r11; // Offset 0xA8
+	U64 r12; // Offset 0xB0
+	U64 r13; // Offset 0xB8
+	U64 r14; // Offset 0xC0
+	U64 r15; // Offset 0xC8
+};
+
+#define ADAPTIVE_PEBS_GPR_INFO_rflags(x) ((x)->rflags)
+#define ADAPTIVE_PEBS_GPR_INFO_rip(x) ((x)->rip)
+#define ADAPTIVE_PEBS_GPR_INFO_rax(x) ((x)->rax)
+#define ADAPTIVE_PEBS_GPR_INFO_rcx(x) ((x)->rcx)
+#define ADAPTIVE_PEBS_GPR_INFO_rdx(x) ((x)->rdx)
+#define ADAPTIVE_PEBS_GPR_INFO_rbx(x) ((x)->rbx)
+#define ADAPTIVE_PEBS_GPR_INFO_rsp(x) ((x)->rsp)
+#define ADAPTIVE_PEBS_GPR_INFO_rbp(x) ((x)->rbp)
+#define ADAPTIVE_PEBS_GPR_INFO_rsi(x) ((x)->rsi)
+#define ADAPTIVE_PEBS_GPR_INFO_rdi(x) ((x)->rdi)
+#define ADAPTIVE_PEBS_GPR_INFO_r8(x) ((x)->r8)
+#define ADAPTIVE_PEBS_GPR_INFO_r9(x) ((x)->r9)
+#define ADAPTIVE_PEBS_GPR_INFO_r10(x) ((x)->r10)
+#define ADAPTIVE_PEBS_GPR_INFO_r11(x) ((x)->r11)
+#define ADAPTIVE_PEBS_GPR_INFO_r12(x) ((x)->r12)
+#define ADAPTIVE_PEBS_GPR_INFO_r13(x) ((x)->r13)
+#define ADAPTIVE_PEBS_GPR_INFO_r14(x) ((x)->r14)
+#define ADAPTIVE_PEBS_GPR_INFO_r15(x) ((x)->r15)
+
+typedef struct ADAPTIVE_PEBS_XMM_INFO_NODE_S ADAPTIVE_PEBS_XMM_INFO_NODE;
+typedef ADAPTIVE_PEBS_XMM_INFO_NODE * ADAPTIVE_PEBS_XMM_INFO;
+
+struct ADAPTIVE_PEBS_XMM_INFO_NODE_S {
+	U64 xmm0_l; // Offset 0xD0
+	U64 xmm0_h; // Offset 0xD8
+	U64 xmm1_l; // Offset 0xE0
+	U64 xmm1_h; // Offset 0xE8
+	U64 xmm2_l; // Offset 0xF0
+	U64 xmm2_h; // Offset 0xF8
+	U64 xmm3_l; // Offset 0x100
+	U64 xmm3_h; // Offset 0x108
+	U64 xmm4_l; // Offset 0x110
+	U64 xmm4_h; // Offset 0x118
+	U64 xmm5_l; // Offset 0x120
+	U64 xmm5_h; // Offset 0x128
+	U64 xmm6_l; // Offset 0x130
+	U64 xmm6_h; // Offset 0x138
+	U64 xmm7_l; // Offset 0x140
+	U64 xmm7_h; // Offset 0x148
+	U64 xmm8_l; // Offset 0x150
+	U64 xmm8_h; // Offset 0x158
+	U64 xmm9_l; // Offset 0x160
+	U64 xmm9_h; // Offset 0x168
+	U64 xmm10_l; // Offset 0x170
+	U64 xmm10_h; // Offset 0x178
+	U64 xmm11_l; // Offset 0x180
+	U64 xmm11_h; // Offset 0x188
+	U64 xmm12_l; // Offset 0x190
+	U64 xmm12_h; // Offset 0x198
+	U64 xmm13_l; // Offset 0x1A0
+	U64 xmm13_h; // Offset 0x1A8
+	U64 xmm14_l; // Offset 0x1B0
+	U64 xmm14_h; // Offset 0x1B8
+	U64 xmm15_l; // Offset 0x1C0
+	U64 xmm15_h; // Offset 0x1C8
+};
+
+#define ADAPTIVE_PEBS_XMM_INFO_xmm0_l(x) ((x)->xmm0_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm0_h(x) ((x)->xmm0_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm1_l(x) ((x)->xmm1_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm1_h(x) ((x)->xmm1_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm2_l(x) ((x)->xmm2_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm2_h(x) ((x)->xmm2_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm3_l(x) ((x)->xmm3_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm3_h(x) ((x)->xmm3_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm4_l(x) ((x)->xmm4_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm4_h(x) ((x)->xmm4_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm5_l(x) ((x)->xmm5_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm5_h(x) ((x)->xmm5_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm6_l(x) ((x)->xmm6_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm6_h(x) ((x)->xmm6_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm7_l(x) ((x)->xmm7_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm7_h(x) ((x)->xmm7_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm8_l(x) ((x)->xmm8_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm8_h(x) ((x)->xmm8_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm9_l(x) ((x)->xmm9_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm9_h(x) ((x)->xmm9_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm10_l(x) ((x)->xmm10_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm10_h(x) ((x)->xmm10_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm11_l(x) ((x)->xmm11_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm11_h(x) ((x)->xmm11_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm12_l(x) ((x)->xmm12_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm12_h(x) ((x)->xmm12_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm13_l(x) ((x)->xmm13_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm13_h(x) ((x)->xmm13_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm14_l(x) ((x)->xmm14_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm14_h(x) ((x)->xmm14_h)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm15_l(x) ((x)->xmm15_l)
+#define ADAPTIVE_PEBS_XMM_INFO_xmm15_h(x) ((x)->xmm15_h)
+
+typedef struct ADAPTIVE_PEBS_LBR_INFO_NODE_S ADAPTIVE_PEBS_LBR_INFO_NODE;
+typedef ADAPTIVE_PEBS_LBR_INFO_NODE * ADAPTIVE_PEBS_LBR_INFO;
+
+struct ADAPTIVE_PEBS_LBR_INFO_NODE_S {
+	U64 lbr_from; // Offset 0x1D0
+	U64 lbr_to; // Offset 0x1D8
+	U64 lbr_info; // Offset 0x1E0
+};
+
+#define ADAPTIVE_PEBS_LBR_INFO_lbr_from(x) ((x)->lbr_from)
+#define ADAPTIVE_PEBS_LBR_INFO_lbr_to(x) ((x)->lbr_to)
+#define ADAPTIVE_PEBS_LBR_INFO_lbr_info(x) ((x)->lbr_info)
+
+typedef struct LATENCY_INFO_NODE_S LATENCY_INFO_NODE;
+typedef LATENCY_INFO_NODE * LATENCY_INFO;
+
+struct LATENCY_INFO_NODE_S {
+	U64 linear_address;
+	U64 data_source;
+	U64 latency;
+	U64 stack_pointer;
+	U64 phys_addr;
+};
+
+#define LATENCY_INFO_linear_address(x) ((x)->linear_address)
+#define LATENCY_INFO_data_source(x) ((x)->data_source)
+#define LATENCY_INFO_latency(x) ((x)->latency)
+#define LATENCY_INFO_stack_pointer(x) ((x)->stack_pointer)
+#define LATENCY_INFO_phys_addr(x) ((x)->phys_addr)
+
+typedef struct DTS_BUFFER_EXT_NODE_S DTS_BUFFER_EXT_NODE;
+typedef DTS_BUFFER_EXT_NODE * DTS_BUFFER_EXT;
+struct DTS_BUFFER_EXT_NODE_S {
+	U64 base; // Offset 0x00
+	U64 index; // Offset 0x08
+	U64 max; // Offset 0x10
+	U64 threshold; // Offset 0x18
+	U64 pebs_base; // Offset 0x20
+	U64 pebs_index; // Offset 0x28
+	U64 pebs_max; // Offset 0x30
+	U64 pebs_threshold; // Offset 0x38
+	U64 counter_reset0; // Offset 0x40
+	U64 counter_reset1; // Offset 0x48
+	U64 counter_reset2; // Offset 0x50
+	U64 counter_reset3;
+};
+
+#define DTS_BUFFER_EXT_base(x) ((x)->base)
+#define DTS_BUFFER_EXT_index(x) ((x)->index)
+#define DTS_BUFFER_EXT_max(x) ((x)->max)
+#define DTS_BUFFER_EXT_threshold(x) ((x)->threshold)
+#define DTS_BUFFER_EXT_pebs_base(x) ((x)->pebs_base)
+#define DTS_BUFFER_EXT_pebs_index(x) ((x)->pebs_index)
+#define DTS_BUFFER_EXT_pebs_max(x) ((x)->pebs_max)
+#define DTS_BUFFER_EXT_pebs_threshold(x) ((x)->pebs_threshold)
+#define DTS_BUFFER_EXT_counter_reset0(x) ((x)->counter_reset0)
+#define DTS_BUFFER_EXT_counter_reset1(x) ((x)->counter_reset1)
+#define DTS_BUFFER_EXT_counter_reset2(x) ((x)->counter_reset2)
+#define DTS_BUFFER_EXT_counter_reset3(x) ((x)->counter_reset3)
+
+typedef struct DTS_BUFFER_EXT1_NODE_S DTS_BUFFER_EXT1_NODE;
+typedef DTS_BUFFER_EXT1_NODE * DTS_BUFFER_EXT1;
+struct DTS_BUFFER_EXT1_NODE_S {
+	DTS_BUFFER_EXT_NODE dts_buffer;
+	U64 counter_reset4; // Offset 0x60
+	U64 counter_reset5; // Offset 0x68
+	U64 counter_reset6; // Offset 0x70
+	U64 counter_reset7; // Offset 0x78
+	U64 fixed_counter_reset0; // Offset 0x80
+	U64 fixed_counter_reset1; // Offset 0x88
+	U64 fixed_counter_reset2; // Offset 0x90
+	U64 fixed_counter_reset3; // Offset 0x98
+};
+
+#define DTS_BUFFER_EXT1_base(x) ((x)->dts_buffer.base)
+#define DTS_BUFFER_EXT1_index(x) ((x)->dts_buffer.index)
+#define DTS_BUFFER_EXT1_max(x) ((x)->dts_buffer.max)
+#define DTS_BUFFER_EXT1_threshold(x) ((x)->dts_buffer.threshold)
+#define DTS_BUFFER_EXT1_pebs_base(x) ((x)->dts_buffer.pebs_base)
+#define DTS_BUFFER_EXT1_pebs_index(x) ((x)->dts_buffer.pebs_index)
+#define DTS_BUFFER_EXT1_pebs_max(x) ((x)->dts_buffer.pebs_max)
+#define DTS_BUFFER_EXT1_pebs_threshold(x) ((x)->dts_buffer.pebs_threshold)
+#define DTS_BUFFER_EXT1_counter_reset0(x) ((x)->dts_buffer.counter_reset0)
+#define DTS_BUFFER_EXT1_counter_reset1(x) ((x)->dts_buffer.counter_reset1)
+#define DTS_BUFFER_EXT1_counter_reset2(x) ((x)->dts_buffer.counter_reset2)
+#define DTS_BUFFER_EXT1_counter_reset3(x) ((x)->dts_buffer.counter_reset3)
+#define DTS_BUFFER_EXT1_counter_reset4(x) ((x)->counter_reset4)
+#define DTS_BUFFER_EXT1_counter_reset5(x) ((x)->counter_reset5)
+#define DTS_BUFFER_EXT1_counter_reset6(x) ((x)->counter_reset6)
+#define DTS_BUFFER_EXT1_counter_reset7(x) ((x)->counter_reset7)
+#define DTS_BUFFER_EXT1_fixed_counter_reset0(x) ((x)->fixed_counter_reset0)
+#define DTS_BUFFER_EXT1_fixed_counter_reset1(x) ((x)->fixed_counter_reset1)
+#define DTS_BUFFER_EXT1_fixed_counter_reset2(x) ((x)->fixed_counter_reset2)
+#define DTS_BUFFER_EXT1_fixed_counter_reset3(x) ((x)->fixed_counter_reset3)
+
+extern OS_STATUS PEBS_Initialize(U32 dev_idx);
+
+extern OS_STATUS PEBS_Allocate(void);
+
+extern VOID PEBS_Destroy(void);
+
+extern VOID PEBS_Flush_Buffer(void *);
+
+extern VOID PEBS_Reset_Counter(S32 this_cpu, U32 index, U64 value);
+
+extern VOID PEBS_Reset_Index(S32 this_cpu);
+
+extern VOID PEBS_Modify_IP(void *sample, DRV_BOOL is_64bit_addr, U32 rec_index);
+
+extern VOID PEBS_Modify_TSC(void *sample, U32 rec_index);
+
+extern U32 PEBS_Get_Num_Records_Filled(void);
+
+extern U64 PEBS_Fill_Buffer(S8 *buffer, EVENT_DESC evt_desc, U32 rec_index);
+
+extern U64 APEBS_Fill_Buffer(S8 *buffer, EVENT_DESC evt_desc, U32 rec_index);
+
+extern U64 PEBS_Overflowed(S32 this_cpu, U64 overflow_status, U32 rec_index);
+
+/*
+ *  Dispatch table for virtualized functions.
+ *  Used to enable common functionality for different
+ *  processor microarchitectures
+ */
+typedef struct PEBS_DISPATCH_NODE_S PEBS_DISPATCH_NODE;
+typedef PEBS_DISPATCH_NODE * PEBS_DISPATCH;
+struct PEBS_DISPATCH_NODE_S {
+	VOID (*initialize_threshold)(DTS_BUFFER_EXT);
+	U64 (*overflow)(S32, U64, U32);
+	VOID (*modify_ip)(void *, DRV_BOOL, U32);
+	VOID (*modify_tsc)(void *, U32);
+	U32 (*get_num_records_filled)(void);
+};
+
+typedef struct PEBS_INFO_NODE_S PEBS_INFO_NODE;
+typedef PEBS_INFO_NODE *PEBS_INFO;
+struct PEBS_INFO_NODE_S {
+	PEBS_DISPATCH pebs_dispatch;
+	U32 pebs_record_size;
+	U16 apebs_basic_offset;
+	U16 apebs_mem_offset;
+	U16 apebs_gpr_offset;
+	U16 apebs_xmm_offset;
+	U16 apebs_lbr_offset;
+};
+
+#define APEBS_RECORD_SIZE_MASK 0xFFFF000000000000ULL //[63:48]
+#define APEBS_RECORD_FORMAT_MASK 0xFFFFFFFFFFFFULL //[47:0]
+#define APEBS_MEM_RECORD_FORMAT_MASK 0x1ULL
+#define APEBS_GPR_RECORD_FORMAT_MASK 0x2ULL
+#define APEBS_XMM_RECORD_FORMAT_MASK 0x4ULL
+#define APEBS_LBR_RECORD_FORMAT_MASK 0x8ULL
+
+
+extern PEBS_DISPATCH_NODE core2_pebs;
+extern PEBS_DISPATCH_NODE core2p_pebs;
+extern PEBS_DISPATCH_NODE corei7_pebs;
+extern PEBS_DISPATCH_NODE haswell_pebs;
+extern PEBS_DISPATCH_NODE perfver4_pebs;
+extern PEBS_DISPATCH_NODE perfver4_apebs;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/perfver4.h b/drivers/platform/x86/sepdk/inc/perfver4.h
new file mode 100644
index 000000000000..74ecf54179df
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/perfver4.h
@@ -0,0 +1,51 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _PERFVER4_H_
+#define _PERFVER4_H_
+
+#include "msrdefs.h"
+
+extern DISPATCH_NODE perfver4_dispatch;
+extern DISPATCH_NODE perfver4_dispatch_htoff_mode;
+extern DISPATCH_NODE perfver4_dispatch_nonht_mode;
+
+#define PERFVER4_UNC_BLBYPASS_BITMASK 0x00000001
+#define PERFVER4_UNC_DISABLE_BL_BYPASS_MSR 0x39C
+
+#if defined(DRV_IA32)
+#define PERFVER4_LBR_DATA_BITS 32
+#else
+#define PERFVER4_LBR_DATA_BITS 57
+#endif
+
+#define PERFVER4_LBR_BITMASK ((1ULL << PERFVER4_LBR_DATA_BITS) - 1)
+
+#define PERFVER4_FROZEN_BIT_MASK 0xc00000000000000ULL
+#define PERFVER4_OVERFLOW_BIT_MASK_HT_ON 0x600000070000000FULL
+#define PERFVER4_OVERFLOW_BIT_MASK_HT_OFF 0x60000007000000FFULL
+#define PERFVER4_OVERFLOW_BIT_MASK_NON_HT 0x6000000F000000FFULL
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/pmi.h b/drivers/platform/x86/sepdk/inc/pmi.h
new file mode 100644
index 000000000000..4fd71f74ceb9
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/pmi.h
@@ -0,0 +1,65 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _PMI_H_
+#define _PMI_H_
+
+#include "lwpmudrv_defines.h"
+#include <linux/ptrace.h>
+#include <linux/version.h>
+
+#if defined(DRV_IA32)
+#if KERNEL_VERSION(2, 6, 25) > LINUX_VERSION_CODE
+#define REGS_xcs(regs) (regs->xcs)
+#define REGS_eip(regs) (regs->eip)
+#define REGS_eflags(regs) (regs->eflags)
+#else
+#define REGS_xcs(regs) (regs->cs)
+#define REGS_eip(regs) (regs->ip)
+#define REGS_eflags(regs) (regs->flags)
+#endif
+#endif
+
+#if defined(DRV_EM64T)
+#define REGS_cs(regs) (regs->cs)
+
+#if KERNEL_VERSION(2, 6, 25) > LINUX_VERSION_CODE
+#define REGS_rip(regs) (regs->rip)
+#define REGS_eflags(regs) (regs->eflags)
+#else
+#define REGS_rip(regs) (regs->ip)
+#define REGS_eflags(regs) (regs->flags)
+#endif
+#endif
+
+asmlinkage VOID PMI_Interrupt_Handler(struct pt_regs *regs);
+
+#if defined(DRV_SEP_ACRN_ON)
+S32 PMI_Buffer_Handler(PVOID data);
+#endif
+
+extern U32 pmi_Get_CSD(U32, U32 *, U32 *);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/sepdrv_p_state.h b/drivers/platform/x86/sepdk/inc/sepdrv_p_state.h
new file mode 100644
index 000000000000..2a20394c393f
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/sepdrv_p_state.h
@@ -0,0 +1,34 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _SEPDRV_P_STATE_H_
+#define _SEPDRV_P_STATE_H_
+
+#define DRV_APERF_MSR 0xE8
+#define DRV_MPERF_MSR 0xE7
+
+extern OS_STATUS SEPDRV_P_STATE_Read(S8 *buffer, CPU_STATE pcpu);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/silvermont.h b/drivers/platform/x86/sepdk/inc/silvermont.h
new file mode 100644
index 000000000000..4a35b1db5047
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/silvermont.h
@@ -0,0 +1,41 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _SILVERMONT_H_
+#define _SILVERMONT_H_
+
+#include "msrdefs.h"
+extern DISPATCH_NODE silvermont_dispatch;
+extern DISPATCH_NODE knights_dispatch;
+
+#if defined(DRV_IA32)
+#define SILVERMONT_LBR_DATA_BITS 32
+#else
+#define SILVERMONT_LBR_DATA_BITS 48
+#endif
+
+#define SILVERMONT_LBR_BITMASK ((1ULL << SILVERMONT_LBR_DATA_BITS) - 1)
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/sys_info.h b/drivers/platform/x86/sepdk/inc/sys_info.h
new file mode 100644
index 000000000000..c5dd5621a58b
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/sys_info.h
@@ -0,0 +1,71 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _SYS_INFO_H_
+#define _SYS_INFO_H_
+
+#include "lwpmudrv_defines.h"
+
+#define KNIGHTS_FAMILY 0x06
+#define KNL_MODEL 0x57
+#define KNM_MODEL 0x85
+
+#define is_Knights_family(family, model)                                 \
+	((family == KNIGHTS_FAMILY) &&                                   \
+	((model == KNL_MODEL) || (model == KNM_MODEL)))
+
+typedef struct __generic_ioctl {
+	U32 size;
+	S32 ret;
+	U64 rsv[3];
+} GENERIC_IOCTL;
+
+#define GENERIC_IOCTL_size(gio) ((gio)->size)
+#define GENERIC_IOCTL_ret(gio) ((gio)->ret)
+
+//
+// This one is unusual in that it's really a variable
+// size. The system_info field is just a easy way
+// to access the base information, but the actual size
+// when used tends to be much larger that what is
+// shown here.
+//
+typedef struct __system_info {
+	GENERIC_IOCTL gen;
+	VTSA_SYS_INFO sys_info;
+} IOCTL_SYS_INFO;
+
+extern U32 *cpu_built_sysinfo;
+
+#define IOCTL_SYS_INFO_gen(isi) ((isi)->gen)
+#define IOCTL_SYS_INFO_sys_info(isi) ((isi)->sys_info)
+
+extern U32 SYS_INFO_Build(void);
+extern void SYS_INFO_Transfer(PVOID buf_usr_to_drv,
+			 unsigned long len_usr_to_drv);
+extern void SYS_INFO_Destroy(void);
+extern void SYS_INFO_Build_Cpu(PVOID param);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/unc_common.h b/drivers/platform/x86/sepdk/inc/unc_common.h
new file mode 100644
index 000000000000..d1cc228982f0
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/unc_common.h
@@ -0,0 +1,161 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _UNC_COMMON_H_INC_
+#define _UNC_COMMON_H_INC_
+
+#include "pci.h"
+
+#define DRV_IS_PCI_VENDOR_ID_INTEL 0x8086
+#define VENDOR_ID_MASK 0x0000FFFF
+#define DEVICE_ID_MASK 0xFFFF0000
+#define DEVICE_ID_BITSHIFT 16
+
+#define UNCORE_SOCKETID_UBOX_LNID_OFFSET 0x40
+#define UNCORE_SOCKETID_UBOX_GID_OFFSET 0x54
+
+#define INVALID_BUS_NUMBER -1
+#define PCI_INVALID_VALUE 0xFFFFFFFF
+
+typedef struct DEVICE_CALLBACK_NODE_S DEVICE_CALLBACK_NODE;
+typedef DEVICE_CALLBACK_NODE * DEVICE_CALLBACK;
+
+struct DEVICE_CALLBACK_NODE_S {
+	DRV_BOOL (*is_Valid_Device)(U32);
+	DRV_BOOL (*is_Valid_For_Write)(U32, U32);
+	DRV_BOOL (*is_Unit_Ctl)(U32);
+	DRV_BOOL (*is_PMON_Ctl)(U32);
+};
+
+#define MAX_PCIDEV_UNITS 16
+#define GET_MAX_PCIDEV_ENTRIES(num_pkg)                                        \
+	((num_pkg > MAX_PCIDEV_UNITS) ? num_pkg : MAX_PCIDEV_UNITS)
+
+typedef struct UNC_PCIDEV_NODE_S UNC_PCIDEV_NODE;
+
+struct UNC_PCIDEV_NODE_S {
+	U32 num_entries;
+	U32 max_entries;
+	S32 *busno_list; // array for pcibus mapping
+	SEP_MMIO_NODE *mmio_map; // virtual memory mapping entries
+};
+
+#define UNC_PCIDEV_max_entries(x) ((x)->max_entries)
+#define UNC_PCIDEV_num_entries(x) ((x)->num_entries)
+#define UNC_PCIDEV_busno_list(x) ((x)->busno_list)
+#define UNC_PCIDEV_busno_entry(x, entry) ((x)->busno_list[entry])
+#define UNC_PCIDEV_mmio_map(x) ((x)->mmio_map)
+#define UNC_PCIDEV_mmio_map_entry(x, entry) ((x)->mmio_map[entry])
+#define UNC_PCIDEV_virtual_addr_entry(x, entry)                                \
+	(SEP_MMIO_NODE_virtual_address(&UNC_PCIDEV_mmio_map_entry(x, entry)))
+
+#define UNC_PCIDEV_is_busno_valid(x, entry)                                    \
+	(((x)->busno_list) && ((x)->num_entries > (entry)) &&                  \
+	((x)->busno_list[(entry)] != INVALID_BUS_NUMBER))
+#define UNC_PCIDEV_is_vaddr_valid(x, entry)                                    \
+	(((x)->mmio_map) && ((x)->num_entries > (entry)) &&                    \
+	((x)->mmio_map[(entry)].virtual_address))
+
+extern UNC_PCIDEV_NODE unc_pcidev_map[];
+
+#define GET_BUS_MAP(dev_node, entry)                                           \
+	(UNC_PCIDEV_busno_entry((&(unc_pcidev_map[dev_node])), entry))
+#define GET_NUM_MAP_ENTRIES(dev_node)                                          \
+	(UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node])))
+#define IS_MMIO_MAP_VALID(dev_node, entry)                                     \
+	(UNC_PCIDEV_is_vaddr_valid((&(unc_pcidev_map[dev_node])), entry))
+#define IS_BUS_MAP_VALID(dev_node, entry)                                      \
+	(UNC_PCIDEV_is_busno_valid((&(unc_pcidev_map[dev_node])), entry))
+#define virtual_address_table(dev_node, entry)                                 \
+	(UNC_PCIDEV_virtual_addr_entry(&(unc_pcidev_map[dev_node]), entry))
+
+extern OS_STATUS UNC_COMMON_Do_Bus_to_Socket_Map(U32 uncore_did, U32 dev_node,
+						U32 bus_no, U32 device_no,
+						U32 function_no);
+
+extern VOID UNC_COMMON_Dummy_Func(PVOID param);
+
+extern VOID UNC_COMMON_Read_Counts(PVOID param, U32 id);
+
+/************************************************************/
+/*
+ * UNC common PCI  based API
+ *
+ ************************************************************/
+
+extern VOID UNC_COMMON_PCI_Write_PMU(PVOID param, U32 ubox_did, U32 control_msr,
+				U32 ctl_val, U32 pci_dev_index,
+				DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_PCI_Enable_PMU(PVOID param, U32 control_msr,
+				U32 enable_val, U32 disable_val,
+				DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_PCI_Disable_PMU(PVOID param, U32 control_msr,
+				U32 enable_val, U32 disable_val,
+				DEVICE_CALLBACK callback);
+
+extern OS_STATUS UNC_COMMON_Add_Bus_Map(U32 uncore_did, U32 dev_node,
+					U32 bus_no);
+
+extern OS_STATUS UNC_COMMON_Init(void);
+
+extern VOID UNC_COMMON_Clean_Up(void);
+
+extern VOID UNC_COMMON_PCI_Trigger_Read(U32 id);
+
+extern VOID UNC_COMMON_PCI_Read_PMU_Data(PVOID param);
+
+extern VOID UNC_COMMON_PCI_Scan_For_Uncore(PVOID param, U32 dev_info_node,
+					DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_Get_Platform_Topology(U32 dev_info_node);
+
+/************************************************************/
+/*
+ * UNC common MSR  based API
+ *
+ ************************************************************/
+
+extern VOID UNC_COMMON_MSR_Write_PMU(PVOID param, U32 control_msr,
+				U64 control_val, U64 reset_val,
+				DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_MSR_Enable_PMU(PVOID param, U32 control_msr,
+				U64 control_val, U64 unit_ctl_val,
+				U64 pmon_ctl_val,
+				DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_MSR_Disable_PMU(PVOID param, U32 control_msr,
+				U64 unit_ctl_val, U64 pmon_ctl_val,
+				DEVICE_CALLBACK callback);
+
+extern VOID UNC_COMMON_MSR_Trigger_Read(U32 id);
+
+extern VOID UNC_COMMON_MSR_Read_PMU_Data(PVOID param);
+
+extern VOID UNC_COMMON_MSR_Clean_Up(PVOID param);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/unc_gt.h b/drivers/platform/x86/sepdk/inc/unc_gt.h
new file mode 100644
index 000000000000..3e95db32cfa8
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/unc_gt.h
@@ -0,0 +1,86 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _UNC_GT_H_INC_
+#define _UNC_GT_H_INC_
+
+/*
+ * Local to this architecture: SNB uncore GT unit
+ *
+ */
+#define GT_MMIO_SIZE 0x200000
+#define NEXT_ADDR_OFFSET 4
+#define UNC_GT_BAR_MASK 0xFFF00000
+#define PERF_GLOBAL_CTRL 0x391
+#define GT_CLEAR_COUNTERS 0xFFFF0000
+
+#define IA32_DEBUG_CTRL 0x1D9
+#define MAX_FREE_RUNNING_EVENTS 6
+#define GT_DID_1 0x102
+#define INTEL_VENDOR_ID 0x8086
+#define DRV_GET_PCI_VENDOR_ID(value) (value & 0x0000FFFF)
+#define DRV_GET_PCI_DEVICE_ID(value) ((value & 0xFFFF0000) >> 16)
+#define DRV_IS_INTEL_VENDOR_ID(value) (value == INTEL_VENDOR_ID)
+#define DRV_IS_GT_DEVICE_ID(value) (value == GT_DID_1)
+
+//clock gating disable values
+#define UNC_GT_GCPUNIT_REG1 0x9400
+#define UNC_GT_GCPUNIT_REG2 0x9404
+#define UNC_GT_GCPUNIT_REG3 0x9408
+#define UNC_GT_GCPUNIT_REG4 0x940c
+#define UNC_GT_GCPUNIT_REG1_VALUE 0xffffffff
+#define UNC_GT_GCPUNIT_REG2_VALUE 0xffffffff
+#define UNC_GT_GCPUNIT_REG3_VALUE 0xffe3ffff
+#define UNC_GT_GCPUNIT_REG4_VALUE 0x00000003
+//RC6 disable
+#define UNC_GT_RC6_REG1 0xa090
+#define UNC_GT_RC6_REG2 0xa094
+#define UNC_GT_RC6_REG1_OR_VALUE 0x80000000
+#define UNC_GT_RC6_REG2_VALUE 0x00000000
+extern DISPATCH_NODE unc_gt_dispatch;
+
+typedef struct GT_CTR_NODE_S GT_CTR_NODE;
+typedef GT_CTR_NODE * GT_CTR;
+struct GT_CTR_NODE_S {
+	union {
+		struct {
+			U32 low : 32;
+			U32 high : 12;
+		} bits;
+		U64 value;
+	} u;
+};
+
+#define GT_CTR_NODE_value(x) (x.u.value)
+#define GT_CTR_NODE_low(x) (x.u.bits.low)
+#define GT_CTR_NODE_high(x) (x.u.bits.high)
+#define GT_CTR_NODE_value_reset(x) x.u.value = 0
+
+#define DRV_WRITE_PCI_REG_ULONG(va, offset_delta, value)                       \
+	writel(value, (void __iomem *)((char *)(va + offset_delta)))
+#define DRV_READ_PCI_REG_ULONG(va, offset_delta)                               \
+	readl((void __iomem *)(char *)(va + offset_delta))
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/utility.h b/drivers/platform/x86/sepdk/inc/utility.h
new file mode 100644
index 000000000000..c5eca9612b00
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/utility.h
@@ -0,0 +1,637 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _UTILITY_H_
+#define _UTILITY_H_
+
+/**
+// Data Types and Macros
+*/
+#pragma pack(push, 1)
+
+#pragma pack(pop)
+
+/*
+ * Declarations
+ */
+extern DISPATCH_NODE unc_msr_dispatch;
+extern DISPATCH_NODE unc_pci_dispatch;
+extern DISPATCH_NODE unc_mmio_dispatch;
+extern DISPATCH_NODE unc_mmio_fpga_dispatch;
+extern DISPATCH_NODE unc_power_dispatch;
+
+/*
+ *  These routines have macros defined in asm/system.h
+ */
+#define SYS_Local_Irq_Enable() local_irq_enable()
+#define SYS_Local_Irq_Disable() local_irq_disable()
+#define SYS_Local_Irq_Save(flags) local_irq_save(flags)
+#define SYS_Local_Irq_Restore(flags) local_irq_restore(flags)
+
+#include <asm/msr.h>
+
+#define SYS_MMIO_Read32(base, offset)                                       \
+	((base) ? readl((void __iomem *)((UIOP)(base) + (offset))) : 0)
+extern U64 SYS_MMIO_Read64(U64 baseAddress, U64 offset);
+
+extern U64 SYS_Read_MSR(U32 msr);
+
+extern void SYS_Write_MSR(U32 msr, U64 val);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0) ||                      \
+	(LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0) &&                  \
+	defined(CONFIG_UIDGID_STRICT_TYPE_CHECKS))
+#define DRV_GET_UID(p) (p->cred->uid.val)
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
+#define DRV_GET_UID(p) (p->cred->uid)
+#else
+#define DRV_GET_UID(p) (p->uid)
+#endif
+
+extern void SYS_Perfvec_Handler(void);
+
+extern void *SYS_get_stack_ptr0(void);
+extern void *SYS_get_stack_ptr3(void);
+extern void *SYS_get_user_fp(void);
+extern short SYS_Get_cs(void);
+
+#if defined(DRV_IA32)
+extern void *SYS_Get_IDT_Base_HWR(void); /// IDT base from hardware IDTR
+extern void *SYS_Get_GDT_Base_HWR(void); /// GDT base from hardware GDTR
+extern U64 SYS_Get_TSC(void);
+
+#define SYS_Get_IDT_Base SYS_Get_IDT_Base_HWR
+#define SYS_Get_GDT_Base SYS_Get_GDT_Base_HWR
+#endif
+
+#if defined(DRV_EM64T)
+extern unsigned short SYS_Get_Code_Selector0(void);
+extern void SYS_Get_IDT_Base(void **);
+extern void SYS_Get_GDT_Base(void **);
+#endif
+
+extern void SYS_IO_Delay(void);
+#define SYS_Inb(port) inb(port)
+#define SYS_Outb(byte, port) outb(byte, port)
+
+/* typedef int                 OSSTATUS; */
+
+/*
+ * Lock implementations
+ */
+#define SYS_Locked_Inc(var) atomic_inc((var))
+#define SYS_Locked_Dec(var) atomic_dec((var))
+
+extern void UTILITY_Read_TSC(U64 *pTsc);
+
+extern void UTILITY_down_read_mm(struct mm_struct *mm);
+
+extern void UTILITY_up_read_mm(struct mm_struct *mm);
+
+extern void UTILITY_Read_Cpuid(U64 cpuid_function, U64 *rax_value,
+		U64 *rbx_value, U64 *rcx_value, U64 *rdx_value);
+
+extern DISPATCH UTILITY_Configure_CPU(U32);
+
+#if defined(DRV_IA32)
+asmlinkage void SYS_Get_CSD(U32, U32 *, U32 *);
+#endif
+
+#if defined(BUILD_CHIPSET)
+extern CS_DISPATCH UTILITY_Configure_Chipset(void);
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern unsigned long UTILITY_Find_Symbol (const char* name)
+ *
+ * @brief    Finds the address of the specified kernel symbol.
+ *
+ * @param    const char* name - name of the symbol to look for
+ *
+ * @return   Symbol address (0 if could not find)
+ *
+ * <I>Special Notes:</I>
+ *  This wrapper is needed due to kallsyms_lookup_name not being exported
+ *  in kernel version 2.6.32.*.
+ *  Careful! This code is *NOT* multithread-safe or reentrant! Should only
+ *  be called from 1 context at a time!
+ */
+extern unsigned long UTILITY_Find_Symbol(const char *name);
+
+/************************************************************************/
+/*********************** DRIVER LOG DECLARATIONS ************************/
+/************************************************************************/
+
+#define DRV_LOG_COMPILER_MEM_BARRIER() { asm volatile("" : : : "memory"); }
+
+#define DRV_LOG_DEFAULT_LOAD_VERBOSITY (LOG_CHANNEL_MOSTWHERE | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_INIT_VERBOSITY                                         \
+	(LOG_CHANNEL_MEMLOG | LOG_CHANNEL_AUXMEMLOG | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_DETECTION_VERBOSITY (DRV_LOG_DEFAULT_INIT_VERBOSITY)
+#define DRV_LOG_DEFAULT_ERROR_VERBOSITY                                        \
+	(LOG_CHANNEL_MOSTWHERE | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_STATE_CHANGE_VERBOSITY (DRV_LOG_DEFAULT_INIT_VERBOSITY)
+#define DRV_LOG_DEFAULT_MARK_VERBOSITY (LOG_CHANNEL_MOSTWHERE | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_DEBUG_VERBOSITY                                        \
+	(LOG_CHANNEL_MEMLOG | LOG_CHANNEL_PRINTK | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_FLOW_VERBOSITY                                         \
+	(LOG_CHANNEL_MEMLOG | LOG_CHANNEL_AUXMEMLOG | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_ALLOC_VERBOSITY (LOG_VERBOSITY_NONE)
+#define DRV_LOG_DEFAULT_INTERRUPT_VERBOSITY                                    \
+	(LOG_CHANNEL_MEMLOG | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_TRACE_VERBOSITY (LOG_VERBOSITY_NONE)
+#define DRV_LOG_DEFAULT_REGISTER_VERBOSITY (LOG_VERBOSITY_NONE)
+#define DRV_LOG_DEFAULT_NOTIFICATION_VERBOSITY                                 \
+	(LOG_CHANNEL_MEMLOG | LOG_CONTEXT_ALL)
+#define DRV_LOG_DEFAULT_WARNING_VERBOSITY                                      \
+	(LOG_CHANNEL_MOSTWHERE | LOG_CONTEXT_ALL)
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void UTILITY_Log (U8 category, U8 in_notification, U8 secondary,
+ *                          const char* function_name, U32 func_name_len,
+ *                           U32 line_number, const char* format_string, ...)
+ *
+ * @brief    Checks whether and where the message should be logged,
+ *		and logs it as appropriate.
+ *
+ * @param    U8  category        - message category
+ *           U8  in_notification - whether or not we are in a notification/OS
+ * callback context (information cannot be reliably obtained without passing
+ * it through the stack)
+ *           U8  secondary       - secondary information field for the message
+ *           const char* function_name   - name of the calling function
+ *           U32 func_name_len   - length of the name of the calling function
+ * (more efficient to pass it as parameter than finding it back at runtime)
+ *           U32 line_number     - line number of the call site
+ *           const char* format_string   - classical format string for
+ * printf-like functions
+ *           ...                         - elements to print
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ *  Used to keep track of the IOCTL operation currently being processed.
+ *  This information is saved in the log buffer (globally), as well as
+ *  in every log entry.
+ *  NB: only IOCTLs for which grabbing the ioctl mutex is necessary
+ *  should be kept track of this way.
+ */
+extern VOID UTILITY_Log(U8 category, U8 in_notification, U8 secondary,
+			const char *function_name, U32 func_name_len,
+			U32 line_number, const char *format_string, ...);
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern DRV_STATUS UTILITY_Driver_Log_Init (void)
+ *
+ * @brief    Allocates and initializes the driver log buffer.
+ *
+ * @param    none
+ *
+ * @return   OS_SUCCESS on success, OS_NO_MEM on error.
+ *
+ * <I>Special Notes:</I>
+ *           Should be (successfully) run before any non-LOAD log calls.
+ *           Allocates memory without going through CONTROL_Allocate (to avoid
+ *           complicating the instrumentation of CONTROL_* functions): calling
+ *           UTILITY_Driver_Log_Free is necessary to free the log structure.
+ *           Falls back to vmalloc when contiguous physical memory cannot be
+ *           allocated. This does not impact runtime behavior, but may impact
+ *           the easiness of retrieving the log from a core dump if the system
+ *           crashes.
+ */
+extern DRV_STATUS UTILITY_Driver_Log_Init(void);
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern DRV_STATUS UTILITY_Driver_Log_Free (void)
+ *
+ * @brief    Frees the driver log buffer.
+ *
+ * @param    none
+ *
+ * @return   OS_SUCCESS on success, OS_NO_MEM on error.
+ *
+ * <I>Special Notes:</I>
+ *           Should be done before unloading the driver.
+ *           See UTILITY_Driver_Log_Init for details.
+ */
+extern void UTILITY_Driver_Log_Free(void);
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern void UTILITY_Driver_Set_Active_Ioctl (U32 ioctl)
+ *
+ * @brief    Sets the 'active_ioctl' global to the specified value.
+ *
+ * @param    U32 ioctl - ioctl/drvop code to use
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ * Used to keep track of the IOCTL operation currently being processed.
+ * This information is saved in the log buffer (globally), as well as
+ * in every log entry.
+ * NB: only IOCTLs for which grabbing the ioctl mutex is necessary
+ * should be kept track of this way.
+ */
+extern void UTILITY_Driver_Set_Active_Ioctl(U32);
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern const char** UTILITY_Log_Category_Strings (void)
+ *
+ * @brief    Accessor function for the log category string array
+ *
+ * @param    none
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ *  Only needed for cosmetic purposes when adjusting category verbosities.
+ */
+extern const char **UTILITY_Log_Category_Strings(void);
+
+extern DRV_LOG_BUFFER driver_log_buffer;
+extern volatile U8 active_ioctl;
+
+#define DRV_LOG() driver_log_buffer
+#define DRV_LOG_VERBOSITY(category)                                            \
+	((DRV_LOG_BUFFER_verbosities(DRV_LOG()))[category])
+#define SEP_IN_NOTIFICATION 1
+
+#define SEP_DRV_RAW_LOG(category, in_notification, second, message, ...)       \
+	UTILITY_Log(category, in_notification, second, __func__,               \
+			sizeof(__func__), __LINE__, message, ##__VA_ARGS__)
+#define SEP_DRV_ULK_LOG(category, in_notification, second, message, ...)       \
+	UTILITY_Log(category, in_notification, second, __func__,               \
+			sizeof(__func__), __LINE__, message, ##__VA_ARGS__)
+
+#define SEP_DRV_LOG_INCREMENT_NB_ACTIVE_INTERRUPTS()                    \
+	do {                                                            \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_active_interrupts(DRV_LOG()), 1);    \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_interrupts(DRV_LOG()), 1);           \
+	} while (0)
+
+#define SEP_DRV_LOG_DECREMENT_NB_ACTIVE_INTERRUPTS()                    \
+	do {                                                            \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_active_interrupts(DRV_LOG()), -1);   \
+	} while (0)
+
+#define SEP_DRV_LOG_INCREMENT_NB_ACTIVE_NOTIFICATIONS()                 \
+	do {                                                            \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_active_notifications(DRV_LOG()), 1); \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_notifications(DRV_LOG()), 1);        \
+	} while (0)
+
+#define SEP_DRV_LOG_DECREMENT_NB_ACTIVE_NOTIFICATIONS()                 \
+	do {                                                            \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_nb_active_notifications(DRV_LOG()), -1);\
+	} while (0)
+
+#define SEP_DRV_LOG_INCREMENT_NB_STATE_TRANSITIONS()                        \
+	do {                                                                \
+		__sync_fetch_and_add(                                       \
+		&DRV_LOG_BUFFER_nb_driver_state_transitions(DRV_LOG()), 1); \
+	} while (0)
+
+#define SEP_DRV_LOG_DISAMBIGUATE()                                      \
+	do {                                                            \
+		__sync_fetch_and_add(                                   \
+		&DRV_LOG_BUFFER_disambiguator(DRV_LOG()), 1);           \
+	} while (0)
+
+/************************************************************************/
+/************************** CATEGORY LOG APIs ***************************/
+/************************************************************************/
+
+// ERROR, WARNING and LOAD are always compiled in...
+#define SEP_DRV_LOG_ERROR(message, ...)                                        \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_ERROR, 0, DRV_LOG_NOTHING, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_WARNING(message, ...)                                      \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_WARNING, 0, DRV_LOG_NOTHING, message, \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_NOTIFICATION_ERROR(in_notif, message, ...)                 \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_ERROR, in_notif, DRV_LOG_NOTHING,     \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_NOTIFICATION_WARNING(in_notif, message, ...)               \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_WARNING, in_notif, DRV_LOG_NOTHING,   \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_LOAD(message, ...)                                         \
+	do {                                                                   \
+		if (DRV_LOG()) {                                               \
+			SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_LOAD, 0,              \
+					DRV_LOG_NOTHING, message,              \
+					##__VA_ARGS__);                        \
+		} else if (DRV_LOG_DEFAULT_LOAD_VERBOSITY &                    \
+			   LOG_CHANNEL_PRINTK) {                               \
+			printk(KERN_ERR SEP_MSG_PREFIX " " message "\n",       \
+				   ##__VA_ARGS__);                             \
+		}                                                              \
+	} while (0)
+
+#if defined(DRV_MINIMAL_LOGGING) // MINIMAL LOGGING MODE
+#define SEP_DRV_LOG_INIT(message, ...)                                     \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_INIT_IN(message, ...)                                  \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_INIT_OUT(message, ...)                                 \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_DETECTION(message, ...)                                \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_MARK(message, ...)                                     \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_DEBUG(message, ...)                                    \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_DEBUG_IN(message, ...)                                 \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_DEBUG_OUT(message, ...)                                \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_FLOW_IN(message, ...)                                  \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_FLOW_OUT(message, ...)                                 \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_ALLOC(message, ...)                                    \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_ALLOC_IN(message, ...)                                 \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_ALLOC_OUT(message, ...)                                \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_INTERRUPT_IN(message, ...)                             \
+	SEP_DRV_LOG_INCREMENT_NB_ACTIVE_INTERRUPTS();
+#define SEP_DRV_LOG_INTERRUPT_OUT(message, ...)                            \
+	SEP_DRV_LOG_DECREMENT_NB_ACTIVE_INTERRUPTS();
+#define SEP_DRV_LOG_NOTIFICATION_IN(message, ...)                          \
+	SEP_DRV_LOG_INCREMENT_NB_ACTIVE_NOTIFICATIONS();
+#define SEP_DRV_LOG_NOTIFICATION_OUT(message, ...)                         \
+	SEP_DRV_LOG_DECREMENT_NB_ACTIVE_NOTIFICATIONS();
+#define SEP_DRV_LOG_STATE_TRANSITION(former_state, new_state, message, ...)  \
+	{                                                                    \
+		(void)former_state;                                          \
+		SEP_DRV_LOG_INCREMENT_NB_STATE_TRANSITIONS();                \
+		DRV_LOG_BUFFER_driver_state(DRV_LOG()) = new_state;          \
+	}
+#else // REGULAR LOGGING MODE (PART 1 / 2)
+#define SEP_DRV_LOG_INIT(message, ...)                                     \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_INIT, 0, DRV_LOG_NOTHING, message,    \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_INIT_IN(message, ...)                                  \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_INIT, 0, DRV_LOG_FLOW_IN, message,    \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_INIT_OUT(message, ...)                                 \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_INIT, 0, DRV_LOG_FLOW_OUT, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_DETECTION(message, ...)                                \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_DETECTION, 0, DRV_LOG_NOTHING,        \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_MARK(message, ...)                                     \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_MARK, 0, DRV_LOG_NOTHING, message,    \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_DEBUG(message, ...)                                    \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_DEBUG, 0, DRV_LOG_NOTHING, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_DEBUG_IN(message, ...)                                 \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_DEBUG, 0, DRV_LOG_FLOW_IN, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_DEBUG_OUT(message, ...)                                \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_DEBUG, 0, DRV_LOG_FLOW_OUT, message,  \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_FLOW_IN(message, ...)                                  \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_FLOW, 0, DRV_LOG_FLOW_IN, message,    \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_FLOW_OUT(message, ...)                                 \
+	SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_FLOW, 0, DRV_LOG_FLOW_OUT, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_ALLOC(message, ...)                                    \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_ALLOC, 0, DRV_LOG_NOTHING, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_ALLOC_IN(message, ...)                                 \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_ALLOC, 0, DRV_LOG_FLOW_IN, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_ALLOC_OUT(message, ...)                                \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_ALLOC, 0, DRV_LOG_FLOW_OUT, message,  \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_INTERRUPT_IN(message, ...)                             \
+	{                                                                  \
+		SEP_DRV_LOG_INCREMENT_NB_ACTIVE_INTERRUPTS();              \
+		SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_INTERRUPT, 0,             \
+			DRV_LOG_FLOW_IN, message, ##__VA_ARGS__);          \
+	}
+
+#define SEP_DRV_LOG_INTERRUPT_OUT(message, ...)                            \
+	{                                                                  \
+		SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_INTERRUPT, 0,             \
+			DRV_LOG_FLOW_OUT, message, ##__VA_ARGS__);         \
+		SEP_DRV_LOG_DECREMENT_NB_ACTIVE_INTERRUPTS();              \
+	}
+
+#define SEP_DRV_LOG_NOTIFICATION_IN(message, ...)                          \
+	{                                                                  \
+		SEP_DRV_LOG_INCREMENT_NB_ACTIVE_NOTIFICATIONS();           \
+		SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_NOTIFICATION, 1,          \
+			DRV_LOG_FLOW_IN, message, ##__VA_ARGS__);          \
+	}
+
+#define SEP_DRV_LOG_NOTIFICATION_OUT(message, ...)                         \
+	{                                                                  \
+		SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_NOTIFICATION, 1,          \
+			DRV_LOG_FLOW_OUT, message, ##__VA_ARGS__);         \
+		SEP_DRV_LOG_DECREMENT_NB_ACTIVE_NOTIFICATIONS();           \
+	}
+
+#define SEP_DRV_LOG_STATE_TRANSITION(former_state, new_state, message, ...) \
+	{                                                                   \
+		SEP_DRV_LOG_INCREMENT_NB_STATE_TRANSITIONS();               \
+		DRV_LOG_BUFFER_driver_state(DRV_LOG()) = new_state;         \
+		SEP_DRV_RAW_LOG(DRV_LOG_CATEGORY_STATE_CHANGE, 0,           \
+			((U8)former_state << 4) | ((U8)new_state & 0xF),    \
+			message, ##__VA_ARGS__);                            \
+	}
+
+#endif
+
+#if defined(DRV_MAXIMAL_LOGGING) // MAXIMAL LOGGING MODE
+#define SEP_DRV_LOG_TRACE(message, ...)                                    \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, 0, DRV_LOG_NOTHING, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_TRACE_IN(message, ...)                                 \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, 0, DRV_LOG_FLOW_IN, message,   \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_TRACE_OUT(message, ...)                                \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, 0, DRV_LOG_FLOW_OUT, message,  \
+			##__VA_ARGS__)
+#define SEP_DRV_LOG_REGISTER_IN(message, ...)                              \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_REGISTER, 0, DRV_LOG_FLOW_IN,         \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_REGISTER_OUT(message, ...)                             \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_REGISTER, 0, DRV_LOG_FLOW_OUT,        \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_NOTIFICATION_TRACE(in_notif, message, ...)             \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, in_notif, DRV_LOG_NOTHING,     \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_NOTIFICATION_TRACE_IN(in_notif, message, ...)          \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, in_notif, DRV_LOG_FLOW_IN,     \
+			message, ##__VA_ARGS__)
+#define SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notif, message, ...)         \
+	SEP_DRV_ULK_LOG(DRV_LOG_CATEGORY_TRACE, in_notif, DRV_LOG_FLOW_OUT,    \
+			message, ##__VA_ARGS__)
+#else // REGULAR LOGGING MODE (PART 2 / 2)
+#define SEP_DRV_LOG_TRACE(message, ...)                                    \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_TRACE_IN(message, ...)                                 \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_TRACE_OUT(message, ...)                                \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_REGISTER_IN(message, ...)                              \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_REGISTER_OUT(message, ...)                             \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_NOTIFICATION_TRACE(in_notif, message, ...)             \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_NOTIFICATION_TRACE_IN(in_notif, message, ...)          \
+	{                                                                      \
+	}
+#define SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notif, message, ...)         \
+	{                                                                      \
+	}
+#endif
+
+/************************************************************************/
+/************************* FACILITATOR MACROS ***************************/
+/************************************************************************/
+
+#define SEP_DRV_LOG_ERROR_INIT_OUT(message, ...)       \
+	{                                                  \
+		SEP_DRV_LOG_ERROR(message, ##__VA_ARGS__);     \
+		SEP_DRV_LOG_INIT_OUT(message, ##__VA_ARGS__);  \
+	}
+
+#define SEP_DRV_LOG_ERROR_FLOW_OUT(message, ...)       \
+	{                                                  \
+		SEP_DRV_LOG_ERROR(message, ##__VA_ARGS__);     \
+		SEP_DRV_LOG_FLOW_OUT(message, ##__VA_ARGS__);  \
+	}
+#define SEP_DRV_LOG_ERROR_TRACE_OUT(message, ...)      \
+	{                                                  \
+		SEP_DRV_LOG_ERROR(message, ##__VA_ARGS__);     \
+		SEP_DRV_LOG_TRACE_OUT(message, ##__VA_ARGS__); \
+	}
+#define SEP_DRV_LOG_ERROR_ALLOC_OUT(message, ...)      \
+	{                                                  \
+		SEP_DRV_LOG_ERROR(message, ##__VA_ARGS__);     \
+		SEP_DRV_LOG_ALLOC_OUT(message, ##__VA_ARGS__); \
+	}
+
+#define SEP_DRV_LOG_WARNING_FLOW_OUT(message, ...)    \
+	{                                                 \
+		SEP_DRV_LOG_WARNING(message, ##__VA_ARGS__);  \
+		SEP_DRV_LOG_FLOW_OUT(message, ##__VA_ARGS__); \
+	}
+
+#define SEP_DRV_LOG_WARNING_TRACE_OUT(message, ...)     \
+	{                                                   \
+		SEP_DRV_LOG_WARNING(message, ##__VA_ARGS__);    \
+		SEP_DRV_LOG_TRACE_OUT(message, ##__VA_ARGS__);  \
+	}
+#define SEP_DRV_LOG_WARNING_ALLOC_OUT(message, ...)      \
+	{                                                    \
+		SEP_DRV_LOG_WARNING(message, ##__VA_ARGS__);     \
+		SEP_DRV_LOG_ALLOC_OUT(message, ##__VA_ARGS__);   \
+	}
+
+#define SEP_DRV_LOG_INIT_TRACE_OUT(message, ...)        \
+	{                                                   \
+		SEP_DRV_LOG_INIT(message, ##__VA_ARGS__);       \
+		SEP_DRV_LOG_TRACE_OUT(message, ##__VA_ARGS__);  \
+	}
+
+#define SEP_DRV_LOG_WARNING_NOTIFICATION_OUT(message, ...)    \
+	{                                                         \
+		SEP_DRV_LOG_WARNING(message, ##__VA_ARGS__);          \
+		SEP_DRV_LOG_NOTIFICATION_OUT(message, ##__VA_ARGS__); \
+	}
+
+
+/************************************************************************/
+/************************* DRIVER STATE MACROS **************************/
+/************************************************************************/
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern U32 UTILITY_Change_Driver_State (U32 allowed_prior_states,
+ *                     U32 state, const char* func, U32 line_number)
+ *
+ * @brief    Updates the driver state (if the transition is legal).
+ *
+ * @param U32 allowed_prior_states - the bitmask representing the states
+ *                            from which the transition is allowed to occur
+ *        U32 state             - the destination state
+ *        const char* func      - the callsite's function's name
+ *        U32 line_number       - the callsite's line number
+ *
+ * @return   1 in case of success, 0 otherwise
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+extern U32 UTILITY_Change_Driver_State(U32 allowed_prior_states, U32 state,
+					   const char *func, U32 line_number);
+
+#define GET_DRIVER_STATE() GLOBAL_STATE_current_phase(driver_state)
+#define CHANGE_DRIVER_STATE(allowed_prior_states, state)                      \
+	UTILITY_Change_Driver_State(allowed_prior_states, state, __func__,    \
+					__LINE__)
+#define DRIVER_STATE_IN(state, states)                                        \
+	(!!(MATCHING_STATE_BIT(state) & (states)))
+
+#endif
diff --git a/drivers/platform/x86/sepdk/inc/valleyview_sochap.h b/drivers/platform/x86/sepdk/inc/valleyview_sochap.h
new file mode 100644
index 000000000000..18214ea3ca76
--- /dev/null
+++ b/drivers/platform/x86/sepdk/inc/valleyview_sochap.h
@@ -0,0 +1,60 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _VALLEYVIEW_SOCHAP_H_INC_
+#define _VALLEYVIEW_SOCHAP_H_INC_
+
+/*
+ * Local to this architecture: Valleyview uncore SA unit
+ *
+ */
+#define VLV_VISA_DESKTOP_DID 0x000C04
+#define VLV_VISA_NEXT_ADDR_OFFSET 4
+#define VLV_VISA_BAR_ADDR_SHIFT 32
+#define VLV_VISA_BAR_ADDR_MASK 0x000FFFC00000LL
+#define VLV_VISA_MAX_PCI_DEVICES 16
+#define VLV_VISA_MCR_REG_OFFSET 0xD0
+#define VLV_VISA_MDR_REG_OFFSET 0xD4
+#define VLV_VISA_MCRX_REG_OFFSET 0xD8
+#define VLV_VISA_BYTE_ENABLES 0xF
+#define VLV_VISA_OP_CODE_SHIFT 24
+#define VLV_VISA_PORT_ID_SHIFT 16
+#define VLV_VISA_OFFSET_HI_MASK 0xFF
+#define VLV_VISA_OFFSET_LO_MASK 0xFF
+#define VLV_CHAP_SIDEBAND_PORT_ID 23
+#define VLV_CHAP_SIDEBAND_WRITE_OP_CODE 1
+#define VLV_CHAP_SIDEBAND_READ_OP_CODE 0
+#define VLV_CHAP_MAX_COUNTERS 8
+#define VLV_CHAP_MAX_COUNT 0x00000000FFFFFFFFLL
+
+#define VLV_VISA_OTHER_BAR_MMIO_PAGE_SIZE 4096
+#define VLV_VISA_CHAP_SAMPLE_DATA 0x00020000
+#define VLV_VISA_CHAP_STOP 0x00040000
+#define VLV_VISA_CHAP_START 0x00110000
+#define VLV_VISA_CHAP_CTRL_REG_OFFSET 0x0
+
+extern DISPATCH_NODE valleyview_visa_dispatch;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/error_reporting_utils.h b/drivers/platform/x86/sepdk/include/error_reporting_utils.h
new file mode 100644
index 000000000000..c1e90c441cc1
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/error_reporting_utils.h
@@ -0,0 +1,167 @@
+
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2002-2018 Intel Corporation. All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef __ERROR_REPORTING_UTILS_H__
+#define __ERROR_REPORTING_UTILS_H__
+
+#define DRV_ASSERT_N_RET_VAL(ret_val)                                          \
+	{                                                                      \
+		DRV_ASSERT((ret_val) == VT_SUCCESS);                           \
+		DRV_CHECK_N_RETURN_N_FAIL(ret_val);                            \
+	}
+
+#define DRV_ASSERT_N_CONTINUE(ret_val)                                         \
+	{                                                                      \
+		if ((ret_val) != VT_SUCCESS) {                                 \
+			LOG_ERR1(VTSA_T("Operation failed with error code "),  \
+				 (ret_val));                                   \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_RETURN_N_FAIL(ret_val)                                     \
+	{                                                                      \
+		if ((ret_val) != VT_SUCCESS) {                                 \
+			LOG_ERR1(VTSA_T("Operation failed with error code "),  \
+				 (ret_val));                                   \
+			return ret_val;                                        \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_RETURN_NO_RETVAL(ret_val)                                  \
+	{                                                                      \
+		if ((ret_val) != VT_SUCCESS) {                                 \
+			LOG_ERR1(VTSA_T("Operation failed with error code "),  \
+				 (ret_val));                                   \
+			return;                                                \
+		}                                                              \
+	}
+
+#define DRV_CHECK_PTR_N_RET_VAL(ptr)                                           \
+	{                                                                      \
+		if ((ptr) == NULL) {                                           \
+			LOG_ERR0(VTSA_T("Encountered null pointer"));          \
+			return VT_SAM_ERROR;                                   \
+		}                                                              \
+	}
+
+#define DRV_CHECK_PTR_N_RET_NULL(ptr)                                          \
+	{                                                                      \
+		if ((ptr) == NULL) {                                           \
+			LOG_ERR0(VTSA_T("Encountered null pointer"));          \
+			return NULL;                                           \
+		}                                                              \
+	}
+
+#define DRV_CHECK_PTR_N_LOG_NO_RETURN(ptr)                                     \
+	{                                                                      \
+		if ((ptr) == NULL) {                                           \
+			LOG_ERR0(VTSA_T("Encountered null pointer"));          \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_LOG_NO_RETURN(ret_val)                                     \
+	{                                                                      \
+		if ((ret_val) != VT_SUCCESS) {                                 \
+			LOG_ERR1(VTSA_T("Operation failed with error code "),  \
+				 (ret_val));                                   \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_RET_NEG_ONE(ret_val)                                       \
+	{                                                                      \
+		if ((ret_val) == -1) {                                         \
+			LOG_ERR0(VTSA_T(                                       \
+				"Operation failed with error code = -1"));     \
+			return VT_SAM_ERROR;                                   \
+		}                                                              \
+	}
+
+#define DRV_REQUIRES_TRUE_COND_RET_N_FAIL(cond)                                \
+	{                                                                      \
+		if (!(cond)) {                                                 \
+			LOG_ERR0(VTSA_T("Condition check failed"));            \
+			return VT_SAM_ERROR;                                   \
+		}                                                              \
+	}
+
+#define DRV_REQUIRES_TRUE_COND_RET_ASSIGNED_VAL(cond, ret_val)                 \
+	{                                                                      \
+		if (!(cond)) {                                                 \
+			LOG_ERR0(VTSA_T("Condition check failed"));            \
+			return ret_val;                                        \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_ERR_LOG_ERR_STRNG_N_RET(rise_err)                          \
+	{                                                                      \
+		if (rise_err != VT_SUCCESS) {                                  \
+			PVOID rise_ptr = NULL;                                 \
+			const VTSA_CHAR *error_str = NULL;                     \
+			RISE_open(&rise_ptr);                                  \
+			RISE_translate_err_code(rise_ptr, rise_err,            \
+						&error_str);                   \
+			LogItW(LOG_LEVEL_ERROR | LOG_AREA_GENERAL,             \
+			       L"Operation failed with error [ %d ] = %s\n",   \
+			       rise_err, error_str);                           \
+			RISE_close(rise_ptr);                                  \
+			return rise_err;                                       \
+		}                                                              \
+	}
+
+#define DRV_CHECK_PTR_N_CLEANUP(ptr, gotolabel, ret_val)                       \
+	{                                                                      \
+		if ((ptr) == NULL) {                                           \
+			LOG_ERR0(VTSA_T("Encountered null pointer"));          \
+			ret_val = VT_SAM_ERROR;                                \
+			goto gotolabel;                                        \
+		}                                                              \
+	}
+
+#define DRV_CHECK_ON_FAIL_CLEANUP_N_RETURN(ret_val, gotolabel)                 \
+	{                                                                      \
+		if ((ret_val) != VT_SUCCESS) {                                 \
+			DRV_CHECK_N_LOG_NO_RETURN(ret_val);                    \
+			goto gotolabel;                                        \
+		}                                                              \
+	}
+
+#define DRV_CHECK_N_CLEANUP_N_RETURN_RET_NEG_ONE(ret_val, gotolabel)           \
+	{                                                                      \
+		if ((ret_val) == -1) {                                         \
+			DRV_CHECK_N_LOG_NO_RETURN(ret_val);                    \
+			goto gotolabel;                                        \
+		}                                                              \
+	}
+
+#define DRV_CHECK_PTR_ON_NULL_CLEANUP_N_RETURN(ptr, gotolabel)                 \
+	{                                                                      \
+		if ((ptr) == NULL) {                                           \
+			DRV_CHECK_PTR_N_LOG_NO_RETURN(ptr);                    \
+			goto gotolabel;                                        \
+		}                                                              \
+	}
+
+#define FREE_N_SET_NULL(ptr)                                                   \
+	{                                                                      \
+		if (ptr != NULL) {                                             \
+			free(ptr);                                             \
+			ptr = NULL;                                            \
+		}                                                              \
+	}
+
+#define DELETE_N_SET_NULL(ptr)                                                 \
+	{                                                                      \
+		delete ptr;                                                    \
+		ptr = NULL;                                                    \
+	}
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_chipset.h b/drivers/platform/x86/sepdk/include/lwpmudrv_chipset.h
new file mode 100644
index 000000000000..82531312af75
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_chipset.h
@@ -0,0 +1,274 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef _LWPMUDRV_CHIPSET_UTILS_H_
+#define _LWPMUDRV_CHIPSET_UTILS_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define MAX_CHIPSET_EVENT_NAME 64
+#define MAX_CHIPSET_COUNTERS   5
+		/* TODO: this covers 1 fixed counter                           \
+		 * plus 4 general counters on GMCH;                            \
+		 * for other chipset devices, this                             \
+		 * can vary from 8 to 32; might consider                       \
+		 * making this per-chipset-type since                          \
+		 * event-multiplexing is currently not                         \
+		 * supported for chipset collections
+		 */
+
+#if defined(_NTDDK_)
+#define CHIPSET_PHYS_ADDRESS PHYSICAL_ADDRESS
+#else
+#define CHIPSET_PHYS_ADDRESS U64
+#endif
+
+// possible values for whether chipset data is valid or not
+enum { DATA_IS_VALID, DATA_IS_INVALID, DATA_OUT_OF_RANGE };
+
+typedef struct CHIPSET_PCI_ARG_NODE_S CHIPSET_PCI_ARG_NODE;
+typedef CHIPSET_PCI_ARG_NODE * CHIPSET_PCI_ARG;
+
+struct CHIPSET_PCI_ARG_NODE_S {
+	U32 address;
+	U32 value;
+};
+
+#define CHIPSET_PCI_ARG_address(chipset_pci) ((chipset_pci)->address)
+#define CHIPSET_PCI_ARG_value(chipset_pci) ((chipset_pci)->value)
+
+typedef struct CHIPSET_PCI_SEARCH_ADDR_NODE_S CHIPSET_PCI_SEARCH_ADDR_NODE;
+typedef CHIPSET_PCI_SEARCH_ADDR_NODE * CHIPSET_PCI_SEARCH_ADDR;
+
+struct CHIPSET_PCI_SEARCH_ADDR_NODE_S {
+	U32 start;
+	U32 stop;
+	U32 increment;
+	U32 addr;
+};
+
+#define CHIPSET_PCI_SEARCH_ADDR_start(pci_search_addr)                         \
+	((pci_search_addr)->start)
+#define CHIPSET_PCI_SEARCH_ADDR_stop(pci_search_addr) ((pci_search_addr)->stop)
+#define CHIPSET_PCI_SEARCH_ADDR_increment(pci_search_addr)                     \
+	((pci_search_addr)->increment)
+#define CHIPSET_PCI_SEARCH_ADDR_address(pci_search_addr)                       \
+	((pci_search_addr)->addr)
+
+typedef struct CHIPSET_PCI_CONFIG_NODE_S CHIPSET_PCI_CONFIG_NODE;
+typedef CHIPSET_PCI_CONFIG_NODE * CHIPSET_PCI_CONFIG;
+
+struct CHIPSET_PCI_CONFIG_NODE_S {
+	U32 bus;
+	U32 device;
+	U32 function;
+	U32 offset;
+	U32 value;
+};
+
+#define CHIPSET_PCI_CONFIG_bus(pci_config) ((pci_config)->bus)
+#define CHIPSET_PCI_CONFIG_device(pci_config) ((pci_config)->device)
+#define CHIPSET_PCI_CONFIG_function(pci_config) ((pci_config)->function)
+#define CHIPSET_PCI_CONFIG_offset(pci_config) ((pci_config)->offset)
+#define CHIPSET_PCI_CONFIG_value(pci_config) ((pci_config)->value)
+
+typedef struct CHIPSET_MARKER_NODE_S CHIPSET_MARKER_NODE;
+typedef CHIPSET_MARKER_NODE * CHIPSET_MARKER;
+
+struct CHIPSET_MARKER_NODE_S {
+	U32 processor_number;
+	U32 rsvd;
+	U64 tsc;
+};
+
+#define CHIPSET_MARKER_processor_number(chipset_marker)                        \
+	((pci_config)->processor_number)
+#define CHIPSET_MARKER_tsc(chipset_marker) ((pci_config)->tsc)
+
+typedef struct CHAP_INTERFACE_NODE_S CHAP_INTERFACE_NODE;
+typedef CHAP_INTERFACE_NODE * CHAP_INTERFACE;
+
+// CHAP chipset registers
+// Offsets for registers are command-0x00, event-0x04, status-0x08, data-0x0C
+struct CHAP_INTERFACE_NODE_S {
+	U32 command_register;
+	U32 event_register;
+	U32 status_register;
+	U32 data_register;
+};
+
+#define CHAP_INTERFACE_command_register(chap) ((chap)->command_register)
+#define CHAP_INTERFACE_event_register(chap) ((chap)->event_register)
+#define CHAP_INTERFACE_status_register(chap) ((chap)->status_register)
+#define CHAP_INTERFACE_data_register(chap) ((chap)->data_register)
+
+/**************************************************************************
+ * GMCH Registers and Offsets
+ **************************************************************************
+ */
+
+// Counter registers - each counter has 4 registers
+#define GMCH_MSG_CTRL_REG 0xD0 // message control register (MCR) 0xD0-0xD3
+#define GMCH_MSG_DATA_REG 0xD4 // message data register (MDR) 0xD4-0xD7
+
+// Counter register offsets
+#define GMCH_PMON_CAPABILITIES                                                 \
+	0x0005F0F0 // when read, bit 0 enabled means GMCH counters are available
+#define GMCH_PMON_GLOBAL_CTRL                                                  \
+	0x0005F1F0 // simultaneously enables/disables fixed and general counters
+
+// Fixed counters (32-bit)
+#define GMCH_PMON_FIXED_CTR_CTRL                                               \
+	0x0005F4F0 // enables and filters the fixed counters
+#define GMCH_PMON_FIXED_CTR0                                                   \
+	0x0005E8F0 // 32-bit fixed counter for GMCH_CORE_CLKS event
+#define GMCH_PMON_FIXED_CTR_OVF_VAL                                            \
+	0xFFFFFFFFLL // overflow value for GMCH fixed counters
+
+// General counters (38-bit)
+// NOTE: lower order bits on GP counters must be read before the higher bits!
+#define GMCH_PMON_GP_CTR0_L 0x0005F8F0 // GMCH GP counter 0, low bits
+#define GMCH_PMON_GP_CTR0_H 0x0005FCF0 // GMCH GP counter 0, high bits
+#define GMCH_PMON_GP_CTR1_L 0x0005F9F0
+#define GMCH_PMON_GP_CTR1_H 0x0005FDF0
+#define GMCH_PMON_GP_CTR2_L 0x0005FAF0
+#define GMCH_PMON_GP_CTR2_H 0x0005FEF0
+#define GMCH_PMON_GP_CTR3_L 0x0005FBF0
+#define GMCH_PMON_GP_CTR3_H 0x0005FFF0
+#define GMCH_PMON_GP_CTR_OVF_VAL  0x3FFFFFFFFFLL
+	// overflow value for GMCH general counters
+
+// Register offsets for LNC
+#define LNC_GMCH_REGISTER_READ 0xD0000000
+#define LNC_GMCH_REGISTER_WRITE 0xE0000000
+
+// Register offsets for SLT
+#define SLT_GMCH_REGISTER_READ 0x10000000
+#define SLT_GMCH_REGISTER_WRITE 0x11000000
+
+// Register offsets for CDV
+#define CDV_GMCH_REGISTER_READ 0x10000000
+#define CDV_GMCH_REGISTER_WRITE 0x11000000
+
+
+typedef struct CHIPSET_EVENT_NODE_S CHIPSET_EVENT_NODE;
+typedef CHIPSET_EVENT_NODE * CHIPSET_EVENT;
+
+//chipset event
+struct CHIPSET_EVENT_NODE_S {
+	U32 event_id;
+	U32 group_id;
+	char name[MAX_CHIPSET_EVENT_NAME];
+	U32 pm;
+	U32 counter;
+};
+
+#define CHIPSET_EVENT_event_id(chipset_event) ((chipset_event)->event_id)
+#define CHIPSET_EVENT_group_id(chipset_event) ((chipset_event)->group_id)
+#define CHIPSET_EVENT_name(chipset_event) ((chipset_event)->name)
+#define CHIPSET_EVENT_pm(chipset_event) ((chipset_event)->pm)
+#define CHIPSET_EVENT_counter(chipset_event) ((chipset_event)->counter)
+
+typedef struct CHIPSET_SEGMENT_NODE_S CHIPSET_SEGMENT_NODE;
+typedef CHIPSET_SEGMENT_NODE * CHIPSET_SEGMENT;
+
+//chipset segment data
+struct CHIPSET_SEGMENT_NODE_S {
+	CHIPSET_PHYS_ADDRESS physical_address;
+	U64 virtual_address;
+	U16 size;
+	U16 number_of_counters;
+	U16 total_events;
+	U16 start_register; // (see driver for details)
+	U32 read_register; // read register offset (model dependent)
+	U32 write_register; // write register offset (model dependent)
+	CHIPSET_EVENT_NODE events[MAX_CHIPSET_COUNTERS];
+};
+
+#define CHIPSET_SEGMENT_physical_address(chipset_segment)                      \
+	((chipset_segment)->physical_address)
+#define CHIPSET_SEGMENT_virtual_address(chipset_segment)                       \
+	((chipset_segment)->virtual_address)
+#define CHIPSET_SEGMENT_size(chipset_segment) ((chipset_segment)->size)
+#define CHIPSET_SEGMENT_num_counters(chipset_segment)                          \
+	((chipset_segment)->number_of_counters)
+#define CHIPSET_SEGMENT_total_events(chipset_segment)                          \
+	((chipset_segment)->total_events)
+#define CHIPSET_SEGMENT_start_register(chipset_segment)                        \
+	((chipset_segment)->start_register)
+#define CHIPSET_SEGMENT_read_register(chipset_segment)                         \
+	((chipset_segment)->read_register)
+#define CHIPSET_SEGMENT_write_register(chipset_segment)                        \
+	((chipset_segment)->write_register)
+#define CHIPSET_SEGMENT_events(chipset_segment) ((chipset_segment)->events)
+
+typedef struct CHIPSET_CONFIG_NODE_S CHIPSET_CONFIG_NODE;
+typedef CHIPSET_CONFIG_NODE * CHIPSET_CONFIG;
+
+//chipset struct used for communication between user mode and kernel
+struct CHIPSET_CONFIG_NODE_S {
+	U32 length; // length of this entire area
+	U32 major_version;
+	U32 minor_version;
+	U32 rsvd;
+	U64 cpu_counter_mask;
+	struct {
+		U64 processor : 1; // Processor PMU
+		U64 mch_chipset : 1; // MCH Chipset
+		U64 ich_chipset : 1; // ICH Chipset
+		U64 motherboard_time_flag : 1; // Motherboard_Time requested.
+		U64 host_processor_run : 1;
+			// Each processor should manage the MCH counts they see.
+			// Turn off for Gen 4 (NOA) runs.
+		U64 mmio_noa_registers : 1; // NOA
+		U64 bnb_chipset : 1; // BNB Chipset
+		U64 gmch_chipset : 1; // GMCH Chipset
+		U64 rsvd : 56;
+	} config_flags;
+	CHIPSET_SEGMENT_NODE mch;
+	CHIPSET_SEGMENT_NODE ich;
+	CHIPSET_SEGMENT_NODE mmio;
+	CHIPSET_SEGMENT_NODE bnb;
+	CHIPSET_SEGMENT_NODE gmch;
+};
+
+#define CHIPSET_CONFIG_length(chipset) ((chipset)->length)
+#define CHIPSET_CONFIG_major_version(chipset) ((chipset)->major_version)
+#define CHIPSET_CONFIG_minor_version(chipset) ((chipset)->minor_version)
+#define CHIPSET_CONFIG_cpu_counter_mask(chipset) ((chipset)->cpu_counter_mask)
+#define CHIPSET_CONFIG_processor(chipset) ((chipset)->config_flags.processor)
+#define CHIPSET_CONFIG_mch_chipset(chipset)                                    \
+	((chipset)->config_flags.mch_chipset)
+#define CHIPSET_CONFIG_ich_chipset(chipset)                                    \
+	((chipset)->config_flags.ich_chipset)
+#define CHIPSET_CONFIG_motherboard_time(chipset)                               \
+	((chipset)->config_flags.motherboard_time_flag)
+#define CHIPSET_CONFIG_host_proc_run(chipset)                                  \
+	((chipset)->config_flags.host_processor_run)
+#define CHIPSET_CONFIG_noa_chipset(chipset)                                    \
+	((chipset)->config_flags.mmio_noa_registers)
+#define CHIPSET_CONFIG_bnb_chipset(chipset)                                    \
+	((chipset)->config_flags.bnb_chipset)
+#define CHIPSET_CONFIG_gmch_chipset(chipset)                                   \
+	((chipset)->config_flags.gmch_chipset)
+#define CHIPSET_CONFIG_mch(chipset) ((chipset)->mch)
+#define CHIPSET_CONFIG_ich(chipset) ((chipset)->ich)
+#define CHIPSET_CONFIG_noa(chipset) ((chipset)->mmio)
+#define CHIPSET_CONFIG_bnb(chipset) ((chipset)->bnb)
+#define CHIPSET_CONFIG_gmch(chipset) ((chipset)->gmch)
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_defines.h b/drivers/platform/x86/sepdk/include/lwpmudrv_defines.h
new file mode 100644
index 000000000000..d6889982ada0
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_defines.h
@@ -0,0 +1,507 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef _LWPMUDRV_DEFINES_H_
+#define _LWPMUDRV_DEFINES_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+//
+// Start off with none of the OS'es are defined
+//
+#undef DRV_OS_WINDOWS
+#undef DRV_OS_LINUX
+#undef DRV_OS_SOLARIS
+#undef DRV_OS_MAC
+#undef DRV_OS_ANDROID
+#undef DRV_OS_UNIX
+
+//
+// Make sure none of the architectures is defined here
+//
+#undef DRV_IA32
+#undef DRV_EM64T
+
+//
+// Make sure one (and only one) of the OS'es gets defined here
+//
+// Unfortunately entirex defines _WIN32 so we need to check for linux
+// first.  The definition of these flags is one and only one
+// _OS_xxx is allowed to be defined.
+//
+#if defined(__ANDROID__)
+#define DRV_OS_ANDROID
+#define DRV_OS_UNIX
+#elif defined(__linux__)
+#define DRV_OS_LINUX
+#define DRV_OS_UNIX
+#elif defined(sun)
+#define DRV_OS_SOLARIS
+#define DRV_OS_UNIX
+#elif defined(_WIN32)
+#define DRV_OS_WINDOWS
+#elif defined(__APPLE__)
+#define DRV_OS_MAC
+#define DRV_OS_UNIX
+#elif defined(__FreeBSD__)
+#define DRV_OS_FREEBSD
+#define DRV_OS_UNIX
+#else
+#error "Compiling for an unknown OS"
+#endif
+
+//
+// Make sure one (and only one) architecture is defined here
+// as well as one (and only one) pointer__ size
+//
+#if defined(_M_IX86) || defined(__i386__)
+#define DRV_IA32
+#elif defined(_M_AMD64) || defined(__x86_64__)
+#define DRV_EM64T
+#else
+#error "Unknown architecture for compilation"
+#endif
+
+//
+// Add a well defined definition of compiling for release (free) vs.
+// debug (checked). Once again, don't assume these are the only two values,
+// always have an else clause in case we want to expand this.
+//
+#if defined(DRV_OS_UNIX)
+#define WINAPI
+#endif
+
+/*
+ *  Add OS neutral defines for file processing.  This is needed in both
+ *  the user code and the kernel code for cleanliness
+ */
+#undef DRV_FILE_DESC
+#undef DRV_INVALID_FILE_DESC_VALUE
+#define DRV_ASSERT assert
+
+#if defined(DRV_OS_WINDOWS)
+
+#define DRV_FILE_DESC HANDLE
+#define DRV_INVALID_FILE_DESC_VALUE INVALID_HANDLE_VALUE
+
+#elif defined(DRV_OS_LINUX) || defined(DRV_OS_SOLARIS) ||                      \
+	defined(DRV_OS_ANDROID)
+
+#define DRV_IOCTL_FILE_DESC SIOP
+#define DRV_FILE_DESC SIOP
+#define DRV_INVALID_FILE_DESC_VALUE -1
+
+#elif defined(DRV_OS_FREEBSD)
+
+#define DRV_IOCTL_FILE_DESC S64
+#define DRV_FILE_DESC S64
+#define DRV_INVALID_FILE_DESC_VALUE -1
+
+#elif defined(DRV_OS_MAC)
+#if defined __LP64__
+#define DRV_IOCTL_FILE_DESC S64
+#define DRV_FILE_DESC S64
+#define DRV_INVALID_FILE_DESC_VALUE (S64)(-1)
+#else
+#define DRV_IOCTL_FILE_DESC S32
+#define DRV_FILE_DESC S32
+#define DRV_INVALID_FILE_DESC_VALUE (S32)(-1)
+#endif
+
+#else
+
+#error "Compiling for an unknown OS"
+
+#endif
+
+#define OUT
+#define IN
+#define INOUT
+
+//
+// VERIFY_SIZEOF let's you insert a compile-time check that the size of a data
+// type (e.g. a struct) is what you think it should be.  Usually it is
+// important to know what the actual size of your struct is, and to make sure
+// it is the same across all platforms.  So this will prevent the code from
+// compiling if something happens that you didn't expect, whether it's because
+// you counted wring, or more often because the compiler inserted padding that
+// you don't want.
+//
+// NOTE: 'elem' and 'size' must both be identifier safe, e.g. matching the
+// regular expression /^[0-9a-zA-Z_]$/.
+//
+// Example:
+//   typedef struct { void *ptr; int data; } mytype;
+//   VERIFY_SIZEOF(mytype, 8);
+//                         ^-- this is correct on 32-bit platforms, but fails
+//                             on 64-bit platforms, indicating a possible
+//                             portability issue.
+//
+#define VERIFY_SIZEOF(type, size)                                            \
+	{                                                                    \
+		enum {                                                       \
+		sizeof_##type##_eq_##size = 1 / (int)(sizeof(type) == size)  \
+		}                                                            \
+	}
+
+#if defined(DRV_OS_WINDOWS)
+#define DRV_DLLIMPORT __declspec(dllimport)
+#define DRV_DLLEXPORT __declspec(dllexport)
+#endif
+#if defined(DRV_OS_UNIX)
+#define DRV_DLLIMPORT
+#define DRV_DLLEXPORT
+#endif
+
+#if defined(DRV_OS_WINDOWS)
+#define FSI64RAW "I64"
+#define DRV_PATH_SEPARATOR "\\"
+#define L_DRV_PATH_SEPARATOR L"\\"
+#endif
+
+#if defined(DRV_OS_UNIX)
+#define FSI64RAW "ll"
+#define DRV_PATH_SEPARATOR "/"
+#define L_DRV_PATH_SEPARATOR L"/"
+#endif
+
+#define FSS64 "%" FSI64RAW "d"
+#define FSU64 "%" FSI64RAW "u"
+#define FSX64 "%" FSI64RAW "x"
+
+#if defined(DRV_OS_WINDOWS)
+#define DRV_RTLD_NOW 0
+#endif
+#if defined(DRV_OS_UNIX)
+#if defined(DRV_OS_FREEBSD)
+#define DRV_RTLD_NOW 0
+#else
+#define DRV_RTLD_NOW RTLD_NOW
+#endif
+#endif
+
+#define DRV_STRLEN (U32)(strlen)
+#define DRV_WCSLEN (U32)(wcslen)
+#define DRV_STRCSPN strcspn
+#define DRV_STRCHR strchr
+#define DRV_STRRCHR strrchr
+#define DRV_WCSRCHR wcsrchr
+
+#if defined(DRV_OS_WINDOWS)
+#define DRV_STCHARLEN DRV_WCSLEN
+#else
+#define DRV_STCHARLEN DRV_STRLEN
+#endif
+
+#if defined(DRV_OS_WINDOWS)
+#define DRV_STRCPY strcpy_s
+#define DRV_STRNCPY strncpy_s
+#define DRV_STRICMP _stricmp
+#define DRV_STRNCMP strncmp
+#define DRV_STRNICMP _strnicmp
+#define DRV_STRDUP _strdup
+#define DRV_WCSDUP _wcsdup
+#define DRV_STRCMP strcmp
+#define DRV_WCSCMP wcscmp
+#define DRV_SNPRINTF _snprintf_s
+#define DRV_SNWPRINTF _snwprintf_s
+#define DRV_VSNPRINTF _vsnprintf_s
+#define DRV_SSCANF sscanf_s
+#define DRV_STRCAT strcat_s
+#define DRV_STRNCAT strncat_s
+#define DRV_MEMCPY memcpy_s
+#define DRV_WMEMCPY wmemcpy_s
+#define DRV_STRTOK strtok_s
+#define DRV_STRTOUL strtoul
+#define DRV_STRTOULL _strtoui64
+#define DRV_STRTOQ _strtoui64
+#define DRV_FOPEN(fp, name, mode) fopen_s(&(fp), (name), (mode))
+#define DRV_WFOPEN(fp, name, mode) _wfopen_s(&(fp), (name), (mode))
+#define DRV_FCLOSE(fp)          \
+	{                           \
+		if ((fp) != NULL) {     \
+			fclose((fp));       \
+		}                       \
+	}
+#define DRV_WCSCPY wcscpy_s
+#define DRV_WCSNCPY wcsncpy_s
+#define DRV_WCSCAT wcscat_s
+#define DRV_WCSNCAT wcsncat_s
+#define DRV_WCSTOK wcstok_s
+#define DRV_WCSSTR wcsstr
+#define DRV_STRERROR strerror_s
+#define DRV_SPRINTF sprintf_s
+#define DRV_VSPRINTF vsprintf_s
+#define DRV_VSWPRINTF vswprintf_s
+#define DRV_GETENV_S getenv_s
+#define DRV_WGETENV_S wgetenv_s
+#define DRV_PUTENV(name) _putenv(name)
+#define DRV_USTRCMP(X, Y) DRV_WCSCMP(X, Y)
+#define DRV_USTRDUP(X) DRV_WCSDUP(X)
+#define DRV_ACCESS(X) _access_s(X, 4)
+#define DRV_STRSTR strstr
+
+#define DRV_STCHAR_COPY DRV_WCSNCPY
+
+#define DRV_GETENV(buf, buf_size, name) _dupenv_s(&(buf), &(buf_size), (name))
+#define DRV_WGETENV(buf, buf_size, name) _wdupenv_s(&(buf), &(buf_size), (name))
+#define DRV_SCLOSE(fp) _close(fp)
+#define DRV_WRITE(fp, buf, buf_size) _write(fp, buf, buf_size);
+#define DRV_SOPEN_S(fp, name, oflag, shflag, pmode)                          \
+	_sopen_s((fp), (name), (oflag), (shflag), (pmode))
+#endif
+
+#if defined(DRV_OS_UNIX)
+/*
+   Note: Many of the following macros have a "size" as the second argument.
+   Generally speaking, this is for compatibility with the _s versions
+   available on Windows. On Linux/Solaris/Mac, it is ignored.
+   On Windows, it is the size of the destination buffer and is used wrt
+   memory checking features available in the C runtime in debug mode.
+   Do not confuse it with the number of bytes to be copied, or such.
+
+   On Windows, this size should correspond to the number of allocated characters
+   (char or wchar_t) pointed to by the first argument. See MSDN or more details.
+*/
+#define DRV_STRICMP strcasecmp
+#define DRV_STRDUP strdup
+#define DRV_STRNDUP strndup
+#define DRV_STRCMP strcmp
+#define DRV_STRNCMP strncmp
+#define DRV_STRSTR strstr
+#define DRV_SNPRINTF(buf, buf_size, length, args...)                          \
+	snprintf((buf), (length), ##args)
+#define DRV_SNWPRINTF(buf, buf_size, length, args...)                         \
+	snwprintf((buf), (length), ##args)
+#define DRV_VSNPRINTF(buf, buf_size, length, args...)                         \
+	vsnprintf((buf), (length), ##args)
+#define DRV_SSCANF sscanf
+#define DRV_STRCPY(dst, dst_size, src) strcpy((dst), (src))
+#define DRV_STRNCPY(dst, dst_size, src, n) strncpy((dst), (src), (n))
+#define DRV_STRCAT(dst, dst_size, src) strcat((dst), (src))
+#define DRV_STRNCAT(dst, dst_size, src, n) strncat((dst), (src), (n))
+#define DRV_MEMCPY(dst, dst_size, src, n) memcpy((dst), (src), (n))
+#define DRV_STRTOK(tok, delim, context) strtok((tok), (delim))
+#define DRV_STRTOUL strtoul
+#define DRV_STRTOULL strtoull
+#define DRV_STRTOL strtol
+#define DRV_FOPEN(fp, name, mode) { (fp) = fopen((name), (mode)); }
+#define DRV_FCLOSE(fp)                     \
+	{                                  \
+		if ((fp) != NULL) {        \
+			fclose((fp));      \
+		}                          \
+	}
+
+#define DRV_WCSCPY(dst, dst_size, src) wcscpy((dst), (const wchar_t *)(src))
+#define DRV_WCSNCPY(dst, dst_size, src, count)                                 \
+	wcsncpy((dst), (const wchar_t *)(src), (count))
+#define DRV_WCSCAT(dst, dst_size, src) wcscat((dst), (const wchar_t *)(src))
+#define DRV_WCSTOK(tok, delim, context)                                        \
+	wcstok((tok), (const wchar_t *)(delim), (context))
+#define DRV_STRERROR strerror
+#define DRV_SPRINTF(dst, dst_size, args...) sprintf((dst), ##args)
+#define DRV_VSPRINTF(dst, dst_size, length, args...)                           \
+	vsprintf((dst), (length), ##args)
+#define DRV_VSWPRINTF(dst, dst_size, length, args...)                          \
+	vswprintf((dst), (length), ##args)
+#define DRV_GETENV_S(dst, dst_size) getenv(dst)
+#define DRV_WGETENV_S(dst, dst_size) wgetenv(dst)
+#define DRV_PUTENV(name) putenv(name)
+#define DRV_GETENV(buf, buf_size, name) ((buf) = getenv((name)))
+#define DRV_USTRCMP(X, Y) DRV_STRCMP(X, Y)
+#define DRV_USTRDUP(X) DRV_STRDUP(X)
+#define DRV_ACCESS(X) access(X, X_OK)
+
+#define DRV_STCHAR_COPY DRV_STRNCPY
+#endif
+
+#if defined(DRV_OS_WINDOWS)
+#define DRV_STRTOK_R(tok, delim, context) strtok_s((tok), (delim), (context))
+#else
+#define DRV_STRTOK_R(tok, delim, context) strtok_r((tok), (delim), (context))
+#endif
+
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_MAC) || defined(DRV_OS_FREEBSD)
+#define DRV_STRTOQ strtoq
+#endif
+
+#if defined(DRV_OS_ANDROID)
+#define DRV_STRTOQ strtol
+#endif
+
+#if defined(DRV_OS_SOLARIS)
+#define DRV_STRTOQ strtoll
+#endif
+
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_FREEBSD) || defined(DRV_OS_MAC)
+#define DRV_WCSDUP wcsdup
+#endif
+
+#if defined(DRV_OS_SOLARIS)
+#define DRV_WCSDUP solaris_wcsdup
+#endif
+
+#if defined(DRV_OS_ANDROID)
+#define DRV_WCSDUP android_wcsdup
+#endif
+
+/*
+ * Windows uses wchar_t and linux uses char for strings.
+ * Need an extra level of abstraction to standardize it.
+ */
+#if defined(DRV_OS_WINDOWS)
+#define DRV_STDUP DRV_WCSDUP
+#define DRV_FORMAT_STRING(x) L##x
+#define DRV_PRINT_STRING(stream, format, ...)                                  \
+	fwprintf((stream), (format), __VA_ARGS__)
+#else
+#define DRV_STDUP DRV_STRDUP
+#define DRV_FORMAT_STRING(x) x
+#define DRV_PRINT_STRING(stream, format, ...)                                  \
+	fprintf((stream), (format), __VA_ARGS__)
+#endif
+
+/*
+ * OS return types
+ */
+#if defined(DRV_OS_UNIX)
+#define OS_STATUS int
+#define OS_SUCCESS 0
+#if defined(BUILD_DRV_ESX)
+#define OS_ILLEGAL_IOCTL -1
+#define OS_NO_MEM -2
+#define OS_FAULT -3
+#define OS_INVALID -4
+#define OS_NO_SYSCALL -5
+#define OS_RESTART_SYSCALL -6
+#define OS_IN_PROGRESS -7
+#else
+#define OS_ILLEGAL_IOCTL -ENOTTY
+#define OS_NO_MEM -ENOMEM
+#define OS_FAULT -EFAULT
+#define OS_INVALID -EINVAL
+#define OS_NO_SYSCALL -ENOSYS
+#define OS_RESTART_SYSCALL -ERESTARTSYS
+#define OS_IN_PROGRESS -EALREADY
+#endif
+#endif
+#if defined(DRV_OS_WINDOWS)
+#define OS_STATUS NTSTATUS
+#define OS_SUCCESS STATUS_SUCCESS
+#define OS_ILLEGAL_IOCTL STATUS_UNSUCCESSFUL
+#define OS_NO_MEM STATUS_UNSUCCESSFUL
+#define OS_FAULT STATUS_UNSUCCESSFUL
+#define OS_INVALID STATUS_UNSUCCESSFUL
+#define OS_NO_SYSCALL STATUS_UNSUCCESSFUL
+#define OS_RESTART_SYSCALL STATUS_UNSUCCESSFUL
+#define OS_IN_PROGRESS STATUS_UNSUCCESSFUL
+#endif
+
+/****************************************************************************
+ **  Driver State defintions
+ ***************************************************************************/
+#define DRV_STATE_UNINITIALIZED 0
+#define DRV_STATE_RESERVED 1
+#define DRV_STATE_IDLE 2
+#define DRV_STATE_PAUSED 3
+#define DRV_STATE_STOPPED 4
+#define DRV_STATE_RUNNING 5
+#define DRV_STATE_PAUSING 6
+#define DRV_STATE_PREPARE_STOP 7
+#define DRV_STATE_TERMINATING 8
+
+#define MATCHING_STATE_BIT(state) ((U32)1 << state)
+#define STATE_BIT_UNINITIALIZED MATCHING_STATE_BIT(DRV_STATE_UNINITIALIZED)
+#define STATE_BIT_RESERVED MATCHING_STATE_BIT(DRV_STATE_RESERVED)
+#define STATE_BIT_IDLE MATCHING_STATE_BIT(DRV_STATE_IDLE)
+#define STATE_BIT_PAUSED MATCHING_STATE_BIT(DRV_STATE_PAUSED)
+#define STATE_BIT_STOPPED MATCHING_STATE_BIT(DRV_STATE_STOPPED)
+#define STATE_BIT_RUNNING MATCHING_STATE_BIT(DRV_STATE_RUNNING)
+#define STATE_BIT_PAUSING MATCHING_STATE_BIT(DRV_STATE_PAUSING)
+#define STATE_BIT_PREPARE_STOP MATCHING_STATE_BIT(DRV_STATE_PREPARE_STOP)
+#define STATE_BIT_TERMINATING MATCHING_STATE_BIT(DRV_STATE_TERMINATING)
+#define STATE_BIT_ANY ((U32)-1)
+
+#define IS_COLLECTING_STATE(state)                                             \
+	(!!(MATCHING_STATE_BIT(state) &                                        \
+		(STATE_BIT_RUNNING | STATE_BIT_PAUSING | STATE_BIT_PAUSED)))
+
+/*
+ *  Stop codes
+ */
+#define DRV_STOP_BASE 0
+#define DRV_STOP_NORMAL 1
+#define DRV_STOP_ASYNC 2
+#define DRV_STOP_CANCEL 3
+#define SEP_FREE(loc)                   \
+	{                               \
+		if ((loc)) {            \
+			free(loc);      \
+			loc = NULL;     \
+		}                       \
+	}
+
+#define MAX_EVENTS 256 // Limiting maximum multiplexing events to 256.
+#if defined(DRV_OS_UNIX)
+#define UNREFERENCED_PARAMETER(p) ((p) = (p))
+#endif
+
+/*
+ * Global marker names
+ */
+#define START_MARKER_NAME "SEP_START_MARKER"
+#define PAUSE_MARKER_NAME "SEP_PAUSE_MARKER"
+#define RESUME_MARKER_NAME "SEP_RESUME_MARKER"
+
+#define DRV_SOC_STRING_LEN (100 + MAX_MARKER_LENGTH)
+
+/*
+ * Temp path
+ */
+#define SEP_TMPDIR "SEP_TMP_DIR"
+#if defined(DRV_OS_WINDOWS)
+#define OS_TMPDIR "TEMP"
+#define GET_DEFAULT_TMPDIR(dir, size)                                  \
+	{                                                              \
+		GetTempPath((U32)size, dir);                           \
+	}
+#else
+#define OS_TMPDIR "TMPDIR"
+/*
+ * Unix has default tmp dir
+ */
+#if defined(DRV_OS_ANDROID)
+#define TEMP_PATH "/data"
+#else
+#define TEMP_PATH "/tmp"
+#endif
+#define GET_DEFAULT_TMPDIR(dir, size)                                       \
+	{                                                                   \
+		DRV_STRCPY((STCHAR *)dir, (U32)size, (STCHAR *)TEMP_PATH);  \
+	}
+#endif
+
+#define OS_ID_UNKNOWN -1
+#define OS_ID_NATIVE 0
+#define OS_ID_VMM 0
+#define OS_ID_MODEM 1
+#define OS_ID_ANDROID 2
+#define OS_ID_SECVM 3
+#define OS_ID_ACORN (U32)(-1)
+
+#define PERF_HW_VER4 (5)
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_ecb.h b/drivers/platform/x86/sepdk/include/lwpmudrv_ecb.h
new file mode 100644
index 000000000000..792ae65191b3
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_ecb.h
@@ -0,0 +1,1116 @@
+
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef _LWPMUDRV_ECB_UTILS_H_
+#define _LWPMUDRV_ECB_UTILS_H_
+
+#if defined(DRV_OS_WINDOWS)
+#pragma warning(disable : 4200)
+#endif
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+// control register types
+#define CCCR 1 // counter configuration control register
+#define ESCR 2 // event selection control register
+#define DATA 4 // collected as snapshot of current value
+#define DATA_RO_DELTA 8 // read-only counter collected as current-previous
+#define DATA_RO_SS 16
+// read-only counter collected as snapshot of current value
+#define METRICS 32 // hardware metrics
+
+// event multiplexing modes
+#define EM_DISABLED -1
+#define EM_TIMER_BASED 0
+#define EM_EVENT_BASED_PROFILING 1
+#define EM_TRIGGER_BASED 2
+
+// ***************************************************************************
+
+/*!\struct EVENT_DESC_NODE
+ * \var    sample_size - size of buffer in bytes to hold the sample + extras
+ * \var    max_gp_events - max number of General Purpose events per EM group
+ * \var    pebs_offset - offset in the sample to locate the pebs capture info
+ * \var    lbr_offset - offset in the sample to locate the lbr information
+ * \var    lbr_num_regs - offset in the sample to locate the number of
+ *			lbr register information
+ * \var    latency_offset_in_sample - offset in the sample to locate the
+ *			latency information
+ * \var    latency_size_in_sample  - size of latency records in the sample
+ * \var    latency_size_from_pebs_record - size of the latency data from
+ *			pebs record in the sample
+ * \var    latency_offset_in_pebs_record - offset in the sample to locate the
+ *		latency information in pebs record
+ * \var    power_offset_in_sample - offset in the sample to locate the
+ *		power information
+ * \var    ebc_offset - offset in the sample to locate the ebc count informatio
+ * \var    uncore_ebc_offset - offset in the sample to locate the uncore
+ *	ebc count information
+ *
+ * \var    ro_offset - offset of RO data in the sample
+ * \var    ro_count - total number of RO entries (including all of
+ *				IEAR/DEAR/BTB/IPEAR)
+ * \var    iear_offset - offset into RO data at which IEAR entries begin
+ * \var    dear_offset - offset into RO data at which DEAR entries begin
+ * \var    btb_offset - offset into RO data at which BTB entries begin
+ *			(these use the same PMDs)
+ * \var    ipear_offset - offset into RO data at which IPEAR entries begin
+ *			(these use the same PMDs)
+ * \var    iear_count    - number of IEAR entries
+ * \var    dear_count    - number of DEAR entries
+ * \var    btb_count    - number of BTB entries
+ * \var    ipear_count   - number of IPEAR entries
+ *
+ * \var    pwr_offset  - offset in the sample to locate the pwr count info
+ * \var    p_state_offset - offset in the sample to locate the p_state
+ *			information (APERF/MPERF)
+ *
+ * \brief  Data structure to describe the events and the mode
+ *
+ */
+
+typedef struct EVENT_DESC_NODE_S EVENT_DESC_NODE;
+typedef EVENT_DESC_NODE * EVENT_DESC;
+
+struct EVENT_DESC_NODE_S {
+	U32 sample_size;
+	U32 pebs_offset;
+	U32 pebs_size;
+	U32 lbr_offset;
+	U32 lbr_num_regs;
+	U32 latency_offset_in_sample;
+	U32 latency_size_in_sample;
+	U32 latency_size_from_pebs_record;
+	U32 latency_offset_in_pebs_record;
+	U32 power_offset_in_sample;
+	U32 ebc_offset;
+	U32 uncore_ebc_offset;
+	U32 eventing_ip_offset;
+	U32 hle_offset;
+	U32 pwr_offset;
+	U32 callstack_offset;
+	U32 callstack_size;
+	U32 p_state_offset;
+	U32 pebs_tsc_offset;
+	U32 perfmetrics_offset;
+	U32 perfmetrics_size;
+	/* ----------ADAPTIVE PEBS FIELDS --------- */
+	U16 applicable_counters_offset;
+	U16 gpr_info_offset;
+	U16 gpr_info_size;
+	U16 xmm_info_offset;
+	U16 xmm_info_size;
+	U16 lbr_info_size;
+	/*------------------------------------------*/
+	U32 reserved2;
+	U64 reserved3;
+};
+
+//
+// Accessor macros for EVENT_DESC node
+//
+#define EVENT_DESC_sample_size(ec) ((ec)->sample_size)
+#define EVENT_DESC_pebs_offset(ec) ((ec)->pebs_offset)
+#define EVENT_DESC_pebs_size(ec) ((ec)->pebs_size)
+#define EVENT_DESC_lbr_offset(ec) ((ec)->lbr_offset)
+#define EVENT_DESC_lbr_num_regs(ec) ((ec)->lbr_num_regs)
+#define EVENT_DESC_latency_offset_in_sample(ec) ((ec)->latency_offset_in_sample)
+#define EVENT_DESC_latency_size_from_pebs_record(ec)                           \
+	((ec)->latency_size_from_pebs_record)
+#define EVENT_DESC_latency_offset_in_pebs_record(ec)                           \
+	((ec)->latency_offset_in_pebs_record)
+#define EVENT_DESC_latency_size_in_sample(ec) ((ec)->latency_size_in_sample)
+#define EVENT_DESC_power_offset_in_sample(ec) ((ec)->power_offset_in_sample)
+#define EVENT_DESC_ebc_offset(ec) ((ec)->ebc_offset)
+#define EVENT_DESC_uncore_ebc_offset(ec) ((ec)->uncore_ebc_offset)
+#define EVENT_DESC_eventing_ip_offset(ec) ((ec)->eventing_ip_offset)
+#define EVENT_DESC_hle_offset(ec) ((ec)->hle_offset)
+#define EVENT_DESC_pwr_offset(ec) ((ec)->pwr_offset)
+#define EVENT_DESC_callstack_offset(ec) ((ec)->callstack_offset)
+#define EVENT_DESC_callstack_size(ec) ((ec)->callstack_size)
+#define EVENT_DESC_perfmetrics_offset(ec) ((ec)->perfmetrics_offset)
+#define EVENT_DESC_perfmetrics_size(ec) ((ec)->perfmetrics_size)
+#define EVENT_DESC_p_state_offset(ec) ((ec)->p_state_offset)
+#define EVENT_DESC_pebs_tsc_offset(ec) ((ec)->pebs_tsc_offset)
+#define EVENT_DESC_applicable_counters_offset(ec)                              \
+	((ec)->applicable_counters_offset)
+#define EVENT_DESC_gpr_info_offset(ec) ((ec)->gpr_info_offset)
+#define EVENT_DESC_gpr_info_size(ec) ((ec)->gpr_info_size)
+#define EVENT_DESC_xmm_info_offset(ec) ((ec)->xmm_info_offset)
+#define EVENT_DESC_xmm_info_size(ec) ((ec)->xmm_info_size)
+#define EVENT_DESC_lbr_info_size(ec) ((ec)->lbr_info_size)
+
+// ***************************************************************************
+
+/*!\struct EVENT_CONFIG_NODE
+ * \var    num_groups      -  The number of groups being programmed
+ * \var    em_mode         -  Is EM valid?  If so how?
+ * \var    em_time_slice   -  EM valid?  time slice in milliseconds
+ * \var    sample_size     -  size of buffer in bytes to hold the sample + extra
+ * \var    max_gp_events   -  Max number of General Purpose events per EM group
+ * \var    pebs_offset     -  offset in the sample to locate the pebs capture
+ *				information
+ * \var    lbr_offset      -  offset in the sample to locate the lbr information
+ * \var    lbr_num_regs    -  offset in the sample to locate the lbr information
+ * \var    latency_offset_in_sample      -  offset in the sample to locate the
+ *			latency information
+ * \var    latency_size_in_sample        -  size of latency records in sample
+ * \var    latency_size_from_pebs_record -  offset in sample to locate latency
+ *                                          size from pebs record
+ * \var    latency_offset_in_pebs_record -  offset in the sample to locate the
+ *                                         latency information in pebs record
+ * \var    power_offset_in_sample        -  offset in the sample to locate the
+ *					power information
+ * \var    ebc_offset                    -  offset in the sample to locate the
+ *					 ebc count information
+ *
+ * \var    pwr_offset - offset in the sample to locate the pwr count information
+ * \var    p_state_offset -  offset in the sample to locate the p_state
+ *				information (APERF/MPERF)
+ *
+ * \brief  Data structure to describe the events and the mode
+ *
+ */
+
+typedef struct EVENT_CONFIG_NODE_S EVENT_CONFIG_NODE;
+typedef EVENT_CONFIG_NODE * EVENT_CONFIG;
+
+struct EVENT_CONFIG_NODE_S {
+	U32 num_groups;
+	S32 em_mode;
+	S32 em_factor;
+	S32 em_event_num;
+	U32 sample_size;
+	U32 max_gp_events;
+	U32 max_fixed_counters;
+	U32 max_ro_counters; // maximum read-only counters
+	U32 pebs_offset;
+	U32 pebs_size;
+	U32 lbr_offset;
+	U32 lbr_num_regs;
+	U32 latency_offset_in_sample;
+	U32 latency_size_in_sample;
+	U32 latency_size_from_pebs_record;
+	U32 latency_offset_in_pebs_record;
+	U32 power_offset_in_sample;
+	U32 ebc_offset;
+	U32 num_groups_unc;
+	U32 ebc_offset_unc;
+	U32 sample_size_unc;
+	U32 eventing_ip_offset;
+	U32 hle_offset;
+	U32 pwr_offset;
+	U32 callstack_offset;
+	U32 callstack_size;
+	U32 p_state_offset;
+	U32 pebs_tsc_offset;
+	U64 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+//
+// Accessor macros for EVENT_CONFIG node
+//
+#define EVENT_CONFIG_num_groups(ec) ((ec)->num_groups)
+#define EVENT_CONFIG_mode(ec) ((ec)->em_mode)
+#define EVENT_CONFIG_em_factor(ec) ((ec)->em_factor)
+#define EVENT_CONFIG_em_event_num(ec) ((ec)->em_event_num)
+#define EVENT_CONFIG_sample_size(ec) ((ec)->sample_size)
+#define EVENT_CONFIG_max_gp_events(ec) ((ec)->max_gp_events)
+#define EVENT_CONFIG_max_fixed_counters(ec) ((ec)->max_fixed_counters)
+#define EVENT_CONFIG_max_ro_counters(ec) ((ec)->max_ro_counters)
+#define EVENT_CONFIG_pebs_offset(ec) ((ec)->pebs_offset)
+#define EVENT_CONFIG_pebs_size(ec) ((ec)->pebs_size)
+#define EVENT_CONFIG_lbr_offset(ec) ((ec)->lbr_offset)
+#define EVENT_CONFIG_lbr_num_regs(ec) ((ec)->lbr_num_regs)
+#define EVENT_CONFIG_latency_offset_in_sample(ec)                              \
+	((ec)->latency_offset_in_sample)
+#define EVENT_CONFIG_latency_size_from_pebs_record(ec)                         \
+	((ec)->latency_size_from_pebs_record)
+#define EVENT_CONFIG_latency_offset_in_pebs_record(ec)                         \
+	((ec)->latency_offset_in_pebs_record)
+#define EVENT_CONFIG_latency_size_in_sample(ec) ((ec)->latency_size_in_sample)
+#define EVENT_CONFIG_power_offset_in_sample(ec) ((ec)->power_offset_in_sample)
+#define EVENT_CONFIG_ebc_offset(ec) ((ec)->ebc_offset)
+#define EVENT_CONFIG_num_groups_unc(ec) ((ec)->num_groups_unc)
+#define EVENT_CONFIG_ebc_offset_unc(ec) ((ec)->ebc_offset_unc)
+#define EVENT_CONFIG_sample_size_unc(ec) ((ec)->sample_size_unc)
+#define EVENT_CONFIG_eventing_ip_offset(ec) ((ec)->eventing_ip_offset)
+#define EVENT_CONFIG_hle_offset(ec) ((ec)->hle_offset)
+#define EVENT_CONFIG_pwr_offset(ec) ((ec)->pwr_offset)
+#define EVENT_CONFIG_callstack_offset(ec) ((ec)->callstack_offset)
+#define EVENT_CONFIG_callstack_size(ec) ((ec)->callstack_size)
+#define EVENT_CONFIG_p_state_offset(ec) ((ec)->p_state_offset)
+#define EVENT_CONFIG_pebs_tsc_offset(ec) ((ec)->pebs_tsc_offset)
+
+typedef enum { UNC_MUX = 1, UNC_COUNTER } UNC_SA_PROG_TYPE;
+
+typedef enum {
+	UNC_PCICFG = 1,
+	UNC_MMIO,
+	UNC_STOP,
+	UNC_MEMORY,
+	UNC_STATUS
+} UNC_SA_CONFIG_TYPE;
+
+typedef enum {
+	UNC_MCHBAR = 1,
+	UNC_DMIBAR,
+	UNC_PCIEXBAR,
+	UNC_GTTMMADR,
+	UNC_GDXCBAR,
+	UNC_CHAPADR,
+	UNC_SOCPCI,
+	UNC_NPKBAR
+} UNC_SA_BAR_TYPE;
+
+typedef enum { UNC_OP_READ = 1, UNC_OP_WRITE, UNC_OP_RMW } UNC_SA_OPERATION;
+
+typedef enum {
+	STATIC_COUNTER = 1,
+	FREERUN_COUNTER,
+	PROG_FREERUN_COUNTER
+} COUNTER_TYPES;
+
+typedef enum {
+	PACKAGE_EVENT = 1,
+	MODULE_EVENT,
+	THREAD_EVENT,
+	SYSTEM_EVENT
+} EVENT_SCOPE_TYPES;
+
+typedef enum {
+	DEVICE_CORE = 1, // CORE DEVICE
+	DEVICE_HETERO,
+	DEVICE_UNC_CBO = 10, // UNCORE DEVICES START
+	DEVICE_UNC_HA,
+	DEVICE_UNC_IMC,
+	DEVICE_UNC_IRP,
+	DEVICE_UNC_NCU,
+	DEVICE_UNC_PCU,
+	DEVICE_UNC_POWER,
+	DEVICE_UNC_QPI,
+	DEVICE_UNC_R2PCIE,
+	DEVICE_UNC_R3QPI,
+	DEVICE_UNC_SBOX,
+	DEVICE_UNC_GT,
+	DEVICE_UNC_UBOX,
+	DEVICE_UNC_WBOX,
+	DEVICE_UNC_COREI7,
+	DEVICE_UNC_CHA,
+	DEVICE_UNC_EDC,
+	DEVICE_UNC_IIO,
+	DEVICE_UNC_M2M,
+	DEVICE_UNC_EDRAM,
+	DEVICE_UNC_FPGA_CACHE,
+	DEVICE_UNC_FPGA_FAB,
+	DEVICE_UNC_FPGA_THERMAL,
+	DEVICE_UNC_FPGA_POWER,
+	DEVICE_UNC_FPGA_GB,
+	DEVICE_UNC_TELEMETRY = 150, // TELEMETRY DEVICE
+	DEVICE_UNC_CHAP = 200, // CHIPSET DEVICES START
+	DEVICE_UNC_GMCH,
+	DEVICE_UNC_GFX,
+	DEVICE_UNC_SOCPERF = 300, // UNCORE VISA DEVICES START
+	DEVICE_UNC_HFI_RXE = 400, // STL HFI
+	DEVICE_UNC_HFI_TXE,
+} DEVICE_TYPES;
+
+typedef enum {
+	LBR_ENTRY_TOS = 0,
+	LBR_ENTRY_FROM_IP,
+	LBR_ENTRY_TO_IP,
+	LBR_ENTRY_INFO
+} LBR_ENTRY_TYPE;
+
+// ***************************************************************************
+
+/*!\struct EVENT_REG_ID_NODE
+ * \var    reg_id      -  MSR index to r/w
+ * \var    pci_id     PCI based register and its details to operate on
+ */
+typedef struct EVENT_REG_ID_NODE_S EVENT_REG_ID_NODE;
+typedef EVENT_REG_ID_NODE * EVENT_REG_ID;
+
+struct EVENT_REG_ID_NODE_S {
+	U32 reg_id;
+	U32 pci_bus_no;
+	U32 pci_dev_no;
+	U32 pci_func_no;
+	U32 data_size;
+	U32 bar_index; // Points to the index (MMIO_INDEX_LIST)
+		// of bar memory map list to be used in mmio_bar_list of ECB
+	U32 reserved1;
+	U32 reserved2;
+	U64 reserved3;
+};
+
+// ***************************************************************************
+
+typedef enum {
+	PMU_REG_RW_READ = 1,
+	PMU_REG_RW_WRITE,
+	PMU_REG_RW_READ_WRITE,
+} PMU_REG_RW_TYPES;
+
+typedef enum {
+	PMU_REG_PROG_MSR = 1,
+	PMU_REG_PROG_PCI,
+	PMU_REG_PROG_MMIO,
+} PMU_REG_PROG_TYPES;
+
+typedef enum {
+	PMU_REG_GLOBAL_CTRL = 1,
+	PMU_REG_UNIT_CTRL,
+	PMU_REG_UNIT_STATUS,
+	PMU_REG_DATA,
+	PMU_REG_EVENT_SELECT,
+	PMU_REG_FILTER,
+	PMU_REG_FIXED_CTRL,
+} PMU_REG_TYPES;
+
+/*!\struct EVENT_REG_NODE
+ * \var    reg_type             - register type
+ * \var    event_id_index       - event ID index
+ * \var    event_reg_id         - register ID/pci register details
+ * \var    desc_id              - desc ID
+ * \var    flags                - flags
+ * \var    reg_value            - register value
+ * \var    max_bits             - max bits
+ * \var    scheduled            - boolean to specify if this event node has
+ *				  been scheduled already
+ * \var    bus_no               - PCI bus number
+ * \var    dev_no               - PCI device number
+ * \var    func_no              - PCI function number
+ * \var    counter_type         - Event counter type - static/freerun
+ * \var    event_scope          - Event scope - package/module/thread
+ * \var    reg_prog_type        - Register Programming type
+ * \var    reg_rw_type          - Register Read/Write type
+ * \var    reg_order            - Register order in the programming sequence
+ * \var
+ * \brief  Data structure to describe the event registers
+ *
+ */
+
+typedef struct EVENT_REG_NODE_S EVENT_REG_NODE;
+typedef EVENT_REG_NODE  * EVENT_REG;
+
+struct EVENT_REG_NODE_S {
+	U8 reg_type;
+	U8 unit_id;
+	U16 event_id_index;
+	U16 counter_event_offset;
+	U16 reserved1;
+	EVENT_REG_ID_NODE event_reg_id;
+	U64 reg_value;
+	U16 desc_id;
+	U16 flags;
+	U32 reserved2;
+	U64 max_bits;
+	U8 scheduled;
+	S8 secondary_pci_offset_shift;
+	U16 secondary_pci_offset_offset; // offset of the offset...
+	U32 counter_type;
+	U32 event_scope;
+	U8 reg_prog_type;
+	U8 reg_rw_type;
+	U8 reg_order;
+	U8 bit_position;
+	U64 secondary_pci_offset_mask;
+	U32 core_event_id;
+	U32 uncore_buffer_offset_in_package;
+	U32 uncore_buffer_offset_in_system;
+	U32 reserved3;
+	U64 reserved4;
+	U64 reserved5;
+	U64 reserved6;
+};
+
+//
+// Accessor macros for EVENT_REG node
+// Note: the flags field is not directly addressible to prevent hackery
+//
+#define EVENT_REG_reg_type(x, i) ((x)[(i)].reg_type)
+#define EVENT_REG_event_id_index(x, i) ((x)[(i)].event_id_index)
+#define EVENT_REG_unit_id(x, i) ((x)[(i)].unit_id)
+#define EVENT_REG_counter_event_offset(x, i) ((x)[(i)].counter_event_offset)
+#define EVENT_REG_reg_id(x, i) ((x)[(i)].event_reg_id.reg_id)
+#define EVENT_REG_bus_no(x, i) ((x)[(i)].event_reg_id.pci_bus_no)
+#define EVENT_REG_dev_no(x, i) ((x)[(i)].event_reg_id.pci_dev_no)
+#define EVENT_REG_func_no(x, i) ((x)[(i)].event_reg_id.pci_func_no)
+#define EVENT_REG_offset(x, i)                                                 \
+	((x)[(i)].event_reg_id.reg_id) // points to the reg_id
+#define EVENT_REG_data_size(x, i) ((x)[(i)].event_reg_id.data_size)
+#define EVENT_REG_desc_id(x, i) ((x)[(i)].desc_id)
+#define EVENT_REG_flags(x, i) ((x)[(i)].flags)
+#define EVENT_REG_reg_value(x, i) ((x)[(i)].reg_value)
+#define EVENT_REG_max_bits(x, i) ((x)[(i)].max_bits)
+#define EVENT_REG_scheduled(x, i) ((x)[(i)].scheduled)
+#define EVENT_REG_secondary_pci_offset_shift(x, i)                             \
+	((x)[(i)].secondary_pci_offset_shift)
+#define EVENT_REG_secondary_pci_offset_offset(x, i)                            \
+	((x)[(i)].secondary_pci_offset_offset)
+#define EVENT_REG_secondary_pci_offset_mask(x, i)                              \
+	((x)[(i)].secondary_pci_offset_mask)
+
+#define EVENT_REG_counter_type(x, i) ((x)[(i)].counter_type)
+#define EVENT_REG_event_scope(x, i) ((x)[(i)].event_scope)
+#define EVENT_REG_reg_prog_type(x, i) ((x)[(i)].reg_prog_type)
+#define EVENT_REG_reg_rw_type(x, i) ((x)[(i)].reg_rw_type)
+#define EVENT_REG_reg_order(x, i) ((x)[(i)].reg_order)
+#define EVENT_REG_bit_position(x, i) ((x)[(i)].bit_position)
+
+#define EVENT_REG_core_event_id(x, i) ((x)[(i)].core_event_id)
+#define EVENT_REG_uncore_buffer_offset_in_package(x, i)                        \
+	((x)[(i)].uncore_buffer_offset_in_package)
+#define EVENT_REG_uncore_buffer_offset_in_system(x, i)                         \
+	((x)[(i)].uncore_buffer_offset_in_system)
+
+//
+// Config bits
+//
+#define EVENT_REG_precise_bit 0x00000001
+#define EVENT_REG_global_bit 0x00000002
+#define EVENT_REG_uncore_bit 0x00000004
+#define EVENT_REG_uncore_q_rst_bit 0x00000008
+#define EVENT_REG_latency_bit 0x00000010
+#define EVENT_REG_is_gp_reg_bit 0x00000020
+#define EVENT_REG_clean_up_bit 0x00000040
+#define EVENT_REG_em_trigger_bit 0x00000080
+#define EVENT_REG_lbr_value_bit 0x00000100
+#define EVENT_REG_fixed_reg_bit 0x00000200
+#define EVENT_REG_multi_pkg_evt_bit 0x00001000
+#define EVENT_REG_branch_evt_bit 0x00002000
+
+//
+// Accessor macros for config bits
+//
+#define EVENT_REG_precise_get(x, i) ((x)[(i)].flags & EVENT_REG_precise_bit)
+#define EVENT_REG_precise_set(x, i) ((x)[(i)].flags |= EVENT_REG_precise_bit)
+#define EVENT_REG_precise_clear(x, i) ((x)[(i)].flags &= ~EVENT_REG_precise_bit)
+
+#define EVENT_REG_global_get(x, i) ((x)[(i)].flags & EVENT_REG_global_bit)
+#define EVENT_REG_global_set(x, i) ((x)[(i)].flags |= EVENT_REG_global_bit)
+#define EVENT_REG_global_clear(x, i) ((x)[(i)].flags &= ~EVENT_REG_global_bit)
+
+#define EVENT_REG_uncore_get(x, i) ((x)[(i)].flags & EVENT_REG_uncore_bit)
+#define EVENT_REG_uncore_set(x, i) ((x)[(i)].flags |= EVENT_REG_uncore_bit)
+#define EVENT_REG_uncore_clear(x, i) ((x)[(i)].flags &= ~EVENT_REG_uncore_bit)
+
+#define EVENT_REG_uncore_q_rst_get(x, i)                                       \
+	((x)[(i)].flags & EVENT_REG_uncore_q_rst_bit)
+#define EVENT_REG_uncore_q_rst_set(x, i)                                       \
+	((x)[(i)].flags |= EVENT_REG_uncore_q_rst_bit)
+#define EVENT_REG_uncore_q_rst_clear(x, i)                                     \
+	((x)[(i)].flags &= ~EVENT_REG_uncore_q_rst_bit)
+
+#define EVENT_REG_latency_get(x, i) ((x)[(i)].flags & EVENT_REG_latency_bit)
+#define EVENT_REG_latency_set(x, i) ((x)[(i)].flags |= EVENT_REG_latency_bit)
+#define EVENT_REG_latency_clear(x, i) ((x)[(i)].flags &= ~EVENT_REG_latency_bit)
+
+#define EVENT_REG_is_gp_reg_get(x, i) ((x)[(i)].flags & EVENT_REG_is_gp_reg_bit)
+#define EVENT_REG_is_gp_reg_set(x, i)                                          \
+	((x)[(i)].flags |= EVENT_REG_is_gp_reg_bit)
+#define EVENT_REG_is_gp_reg_clear(x, i)                                        \
+	((x)[(i)].flags &= ~EVENT_REG_is_gp_reg_bit)
+
+#define EVENT_REG_lbr_value_get(x, i) ((x)[(i)].flags & EVENT_REG_lbr_value_bit)
+#define EVENT_REG_lbr_value_set(x, i)                                          \
+	((x)[(i)].flags |= EVENT_REG_lbr_value_bit)
+#define EVENT_REG_lbr_value_clear(x, i)                                        \
+	((x)[(i)].flags &= ~EVENT_REG_lbr_value_bit)
+
+#define EVENT_REG_fixed_reg_get(x, i) ((x)[(i)].flags & EVENT_REG_fixed_reg_bit)
+#define EVENT_REG_fixed_reg_set(x, i)                                          \
+	((x)[(i)].flags |= EVENT_REG_fixed_reg_bit)
+#define EVENT_REG_fixed_reg_clear(x, i)                                        \
+	((x)[(i)].flags &= ~EVENT_REG_fixed_reg_bit)
+
+#define EVENT_REG_multi_pkg_evt_bit_get(x, i)                                  \
+	((x)[(i)].flags & EVENT_REG_multi_pkg_evt_bit)
+#define EVENT_REG_multi_pkg_evt_bit_set(x, i)                                  \
+	((x)[(i)].flags |= EVENT_REG_multi_pkg_evt_bit)
+#define EVENT_REG_multi_pkg_evt_bit_clear(x, i)                                \
+	((x)[(i)].flags &= ~EVENT_REG_multi_pkg_evt_bit)
+
+#define EVENT_REG_clean_up_get(x, i) ((x)[(i)].flags & EVENT_REG_clean_up_bit)
+#define EVENT_REG_clean_up_set(x, i) ((x)[(i)].flags |= EVENT_REG_clean_up_bit)
+#define EVENT_REG_clean_up_clear(x, i)                                         \
+	((x)[(i)].flags &= ~EVENT_REG_clean_up_bit)
+
+#define EVENT_REG_em_trigger_get(x, i)                                         \
+	((x)[(i)].flags & EVENT_REG_em_trigger_bit)
+#define EVENT_REG_em_trigger_set(x, i)                                         \
+	((x)[(i)].flags |= EVENT_REG_em_trigger_bit)
+#define EVENT_REG_em_trigger_clear(x, i)                                       \
+	((x)[(i)].flags &= ~EVENT_REG_em_trigger_bit)
+
+#define EVENT_REG_branch_evt_get(x, i)                                         \
+	((x)[(i)].flags & EVENT_REG_branch_evt_bit)
+#define EVENT_REG_branch_evt_set(x, i)                                         \
+	((x)[(i)].flags |= EVENT_REG_branch_evt_bit)
+#define EVENT_REG_branch_evt_clear(x, i)                                       \
+	((x)[(i)].flags &= ~EVENT_REG_branch_evt_bit)
+
+// ***************************************************************************
+
+/*!\struct DRV_PCI_DEVICE_ENTRY_NODE_S
+ * \var    bus_no          -  PCI bus no to read
+ * \var    dev_no          -  PCI device no to read
+ * \var    func_no            PCI device no to read
+ * \var    bar_offset         BASE Address Register offset of the PCI based PMU
+ * \var    bit_offset         Bit offset of the same
+ * \var    size               size of read/write
+ * \var    bar_address        the actual BAR present
+ * \var    enable_offset      Offset info to enable/disable
+ * \var    enabled            Status of enable/disable
+ * \brief  Data structure to describe the PCI Device
+ *
+ */
+
+typedef struct DRV_PCI_DEVICE_ENTRY_NODE_S DRV_PCI_DEVICE_ENTRY_NODE;
+typedef DRV_PCI_DEVICE_ENTRY_NODE * DRV_PCI_DEVICE_ENTRY;
+
+struct DRV_PCI_DEVICE_ENTRY_NODE_S {
+	U32 bus_no;
+	U32 dev_no;
+	U32 func_no;
+	U32 bar_offset;
+	U64 bar_mask;
+	U32 bit_offset;
+	U32 size;
+	U64 bar_address;
+	U32 enable_offset;
+	U32 enabled;
+	U32 base_offset_for_mmio;
+	U32 operation;
+	U32 bar_name;
+	U32 prog_type;
+	U32 config_type;
+	S8 bar_shift; // positive shifts right, negative shifts left
+	U8 reserved0;
+	U16 reserved1;
+	U64 value;
+	U64 mask;
+	U64 virtual_address;
+	U32 port_id;
+	U32 op_code;
+	U32 device_id;
+	U16 bar_num;
+	U16 feature_id;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+//
+// Accessor macros for DRV_PCI_DEVICE_NODE node
+//
+#define DRV_PCI_DEVICE_ENTRY_bus_no(x) ((x)->bus_no)
+#define DRV_PCI_DEVICE_ENTRY_dev_no(x) ((x)->dev_no)
+#define DRV_PCI_DEVICE_ENTRY_func_no(x) ((x)->func_no)
+#define DRV_PCI_DEVICE_ENTRY_bar_offset(x) ((x)->bar_offset)
+#define DRV_PCI_DEVICE_ENTRY_bar_mask(x) ((x)->bar_mask)
+#define DRV_PCI_DEVICE_ENTRY_bit_offset(x) ((x)->bit_offset)
+#define DRV_PCI_DEVICE_ENTRY_size(x) ((x)->size)
+#define DRV_PCI_DEVICE_ENTRY_bar_address(x) ((x)->bar_address)
+#define DRV_PCI_DEVICE_ENTRY_enable_offset(x) ((x)->enable_offset)
+#define DRV_PCI_DEVICE_ENTRY_enable(x) ((x)->enabled)
+#define DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(x) ((x)->base_offset_for_mmio)
+#define DRV_PCI_DEVICE_ENTRY_operation(x) ((x)->operation)
+#define DRV_PCI_DEVICE_ENTRY_bar_name(x) ((x)->bar_name)
+#define DRV_PCI_DEVICE_ENTRY_prog_type(x) ((x)->prog_type)
+#define DRV_PCI_DEVICE_ENTRY_config_type(x) ((x)->config_type)
+#define DRV_PCI_DEVICE_ENTRY_bar_shift(x) ((x)->bar_shift)
+#define DRV_PCI_DEVICE_ENTRY_value(x) ((x)->value)
+#define DRV_PCI_DEVICE_ENTRY_mask(x) ((x)->mask)
+#define DRV_PCI_DEVICE_ENTRY_virtual_address(x) ((x)->virtual_address)
+#define DRV_PCI_DEVICE_ENTRY_port_id(x) ((x)->port_id)
+#define DRV_PCI_DEVICE_ENTRY_op_code(x) ((x)->op_code)
+#define DRV_PCI_DEVICE_ENTRY_device_id(x) ((x)->device_id)
+#define DRV_PCI_DEVICE_ENTRY_bar_num(x) ((x)->bar_num)
+#define DRV_PCI_DEVICE_ENTRY_feature_id(x) ((x)->feature_id)
+
+// ***************************************************************************
+typedef enum {
+	PMU_OPERATION_INITIALIZE = 0,
+	PMU_OPERATION_WRITE,
+	PMU_OPERATION_ENABLE,
+	PMU_OPERATION_DISABLE,
+	PMU_OPERATION_READ,
+	PMU_OPERATION_CLEANUP,
+	PMU_OPERATION_READ_LBRS,
+	PMU_OPERATION_GLOBAL_REGS,
+	PMU_OPERATION_CTRL_GP,
+	PMU_OPERATION_DATA_FIXED,
+	PMU_OPERATION_DATA_GP,
+	PMU_OPERATION_OCR,
+	PMU_OPERATION_HW_ERRATA,
+	PMU_OPERATION_CHECK_OVERFLOW_GP_ERRATA,
+	PMU_OPERATION_CHECK_OVERFLOW_ERRATA,
+	PMU_OPERATION_ALL_REG,
+	PMU_OPERATION_DATA_ALL,
+	PMU_OPERATION_GLOBAL_STATUS,
+	PMU_OPERATION_METRICS,
+} PMU_OPERATION_TYPES;
+#define MAX_OPERATION_TYPES 32
+
+/*!\struct PMU_OPERATIONS_NODE
+ * \var operation_type - Type of operation from enumeration PMU_OPERATION_TYPES
+ * \var register_start - Start index of the registers for a specific operation
+ * \var register_len   - Number of registers for a specific operation
+ *
+ * \brief
+ * Structure for defining start and end indices in the ECB entries array for
+ * each type of operation performed in the driver
+ * initialize, write, read, enable, disable, etc.
+ */
+typedef struct PMU_OPERATIONS_NODE_S PMU_OPERATIONS_NODE;
+typedef PMU_OPERATIONS_NODE * PMU_OPERATIONS;
+struct PMU_OPERATIONS_NODE_S {
+	U32 operation_type;
+	U32 register_start;
+	U32 register_len;
+	U32 reserved1;
+	U32 reserved2;
+	U32 reserved3;
+};
+#define PMU_OPERATIONS_operation_type(x) ((x)->operation_type)
+#define PMU_OPERATIONS_register_start(x) ((x)->register_start)
+#define PMU_OPERATIONS_register_len(x) ((x)->register_len)
+#define PMU_OPER_operation_type(x, i) ((x)[(i)].operation_type)
+#define PMU_OPER_register_start(x, i) ((x)[(i)].register_start)
+#define PMU_OPER_register_len(x, i) ((x)[(i)].register_len)
+
+typedef enum {
+	ECB_MMIO_BAR1 = 1,
+	ECB_MMIO_BAR2 = 2,
+	ECB_MMIO_BAR3 = 3,
+	ECB_MMIO_BAR4 = 4,
+	ECB_MMIO_BAR5 = 5,
+	ECB_MMIO_BAR6 = 6,
+	ECB_MMIO_BAR7 = 7,
+	ECB_MMIO_BAR8 = 8,
+} MMIO_INDEX_LIST;
+#define MAX_MMIO_BARS 8
+
+/*!\struct MMIO_BAR_INFO_NODE
+ */
+typedef struct MMIO_BAR_INFO_NODE_S MMIO_BAR_INFO_NODE;
+typedef MMIO_BAR_INFO_NODE * MMIO_BAR_INFO;
+
+struct MMIO_BAR_INFO_NODE_S {
+	U32 bus_no;
+	U32 dev_no;
+	U32 func_no;
+	U32 offset;
+	U32 addr_size;
+	U32 map_size;
+	S8 bar_shift;
+	U8 reserved1;
+	U16 reserved2;
+	U32 reserved3;
+	U32 reserved4;
+	U32 reserved5;
+	U64 bar_mask;
+	U64 base_mmio_offset;
+	U64 physical_address;
+	U64 virtual_address;
+	U64 reserved6;
+	U64 reserved7;
+};
+
+/*!\struct ECB_NODE_S
+ * \var    num_entries -       Total number of entries in "entries".
+ * \var    group_id    -       Group ID.
+ * \var    num_events  -       Number of events in this group.
+ * \var    cccr_start  -       Starting index of counter configuration control
+ *			       registers in "entries".
+ * \var    cccr_pop    -       Number of counter configuration control
+ *			       registers in "entries".
+ * \var    escr_start  -       Starting index of event selection control
+ *			       registers in "entries".
+ * \var    escr_pop    -       Number of event selection control registers
+ *				in "entries".
+ * \var    data_start  -       Starting index of data registers in "entries".
+ * \var    data_pop    -       Number of data registers in "entries".
+ * \var    pcidev_entry_node   PCI device details for one device
+ * \var    entries     - .     All the register nodes required for programming
+ *
+ * \brief
+ */
+
+typedef struct ECB_NODE_S ECB_NODE;
+typedef ECB_NODE * ECB;
+
+struct ECB_NODE_S {
+	U8 version;
+	U8 reserved1;
+	U16 reserved2;
+	U32 num_entries;
+	U32 group_id;
+	U32 num_events;
+	U32 cccr_start;
+	U32 cccr_pop;
+	U32 escr_start;
+	U32 escr_pop;
+	U32 data_start;
+	U32 data_pop;
+	U16 flags;
+	U8 pmu_timer_interval;
+	U8 reserved3;
+	U32 size_of_allocation;
+	U32 group_offset;
+	U32 reserved4;
+	DRV_PCI_DEVICE_ENTRY_NODE pcidev_entry_node;
+	U32 num_pci_devices;
+	U32 pcidev_list_offset;
+	DRV_PCI_DEVICE_ENTRY pcidev_entry_list;
+	U32 device_type;
+	U32 dev_node;
+	PMU_OPERATIONS_NODE operations[MAX_OPERATION_TYPES];
+	U32 descriptor_id;
+	U32 reserved5;
+	U32 metric_start;
+	U32 metric_pop;
+	MMIO_BAR_INFO_NODE mmio_bar_list[MAX_MMIO_BARS];
+	U64 reserved6;
+	U64 reserved7;
+	U64 reserved8;
+	EVENT_REG_NODE entries[];
+};
+
+//
+// Accessor macros for ECB node
+//
+#define ECB_version(x) ((x)->version)
+#define ECB_num_entries(x) ((x)->num_entries)
+#define ECB_group_id(x) ((x)->group_id)
+#define ECB_num_events(x) ((x)->num_events)
+#define ECB_cccr_start(x) ((x)->cccr_start)
+#define ECB_cccr_pop(x) ((x)->cccr_pop)
+#define ECB_escr_start(x) ((x)->escr_start)
+#define ECB_escr_pop(x) ((x)->escr_pop)
+#define ECB_data_start(x) ((x)->data_start)
+#define ECB_data_pop(x) ((x)->data_pop)
+#define ECB_metric_start(x) ((x)->metric_start)
+#define ECB_metric_pop(x) ((x)->metric_pop)
+#define ECB_pcidev_entry_node(x) ((x)->pcidev_entry_node)
+#define ECB_num_pci_devices(x) ((x)->num_pci_devices)
+#define ECB_pcidev_list_offset(x) ((x)->pcidev_list_offset)
+#define ECB_pcidev_entry_list(x) ((x)->pcidev_entry_list)
+#define ECB_flags(x) ((x)->flags)
+#define ECB_pmu_timer_interval(x) ((x)->pmu_timer_interval)
+#define ECB_size_of_allocation(x) ((x)->size_of_allocation)
+#define ECB_group_offset(x) ((x)->group_offset)
+#define ECB_device_type(x) ((x)->device_type)
+#define ECB_dev_node(x) ((x)->dev_node)
+#define ECB_operations(x) ((x)->operations)
+#define ECB_descriptor_id(x) ((x)->descriptor_id)
+#define ECB_entries(x) ((x)->entries)
+
+// for flag bit field
+#define ECB_direct2core_bit 0x0001
+#define ECB_bl_bypass_bit 0x0002
+#define ECB_pci_id_offset_bit 0x0003
+#define ECB_pcu_ccst_debug 0x0004
+
+#define ECB_VERSION 2
+
+#define ECB_CONSTRUCT(x, num_entries, group_id, cccr_start, escr_start,  \
+			data_start, size_of_allocation)                  \
+	{                                                                \
+		ECB_num_entries((x)) = (num_entries);                    \
+		ECB_group_id((x)) = (group_id);                          \
+		ECB_cccr_start((x)) = (cccr_start);                      \
+		ECB_cccr_pop((x)) = 0;                                   \
+		ECB_escr_start((x)) = (escr_start);                      \
+		ECB_escr_pop((x)) = 0;                                   \
+		ECB_data_start((x)) = (data_start);                      \
+		ECB_data_pop((x)) = 0;                                   \
+		ECB_metric_start((x)) = 0;                               \
+		ECB_metric_pop((x)) = 0;                                 \
+		ECB_num_pci_devices((x)) = 0;                            \
+		ECB_version((x)) = ECB_VERSION;                          \
+		ECB_size_of_allocation((x)) = (size_of_allocation);      \
+	}
+
+#define ECB_CONSTRUCT2(x, num_entries, group_id, size_of_allocation)      \
+	{                                                                 \
+		ECB_num_entries((x)) = (num_entries);                     \
+		ECB_group_id((x)) = (group_id);                           \
+		ECB_num_pci_devices((x)) = 0;                             \
+		ECB_version((x)) = ECB_VERSION;                           \
+		ECB_size_of_allocation((x)) = (size_of_allocation);       \
+	}
+
+#define ECB_CONSTRUCT1(x, num_entries, group_id, cccr_start, escr_start,  \
+			data_start, num_pci_devices, size_of_allocation)  \
+	{                                                                 \
+		ECB_num_entries((x)) = (num_entries);                     \
+		ECB_group_id((x)) = (group_id);                           \
+		ECB_cccr_start((x)) = (cccr_start);                       \
+		ECB_cccr_pop((x)) = 0;                                    \
+		ECB_escr_start((x)) = (escr_start);                       \
+		ECB_escr_pop((x)) = 0;                                    \
+		ECB_data_start((x)) = (data_start);                       \
+		ECB_data_pop((x)) = 0;                                    \
+		ECB_metric_start((x)) = 0;                                \
+		ECB_metric_pop((x)) = 0;                                  \
+		ECB_num_pci_devices((x)) = (num_pci_devices);             \
+		ECB_version((x)) = ECB_VERSION;                           \
+		ECB_size_of_allocation((x)) = (size_of_allocation);       \
+	}
+
+
+//
+// Accessor macros for ECB node entries
+//
+#define ECB_entries_reg_type(x, i) EVENT_REG_reg_type((ECB_entries(x)), (i))
+#define ECB_entries_event_id_index(x, i)                                       \
+	EVENT_REG_event_id_index((ECB_entries(x)), (i))
+#define ECB_entries_unit_id(x, i) EVENT_REG_unit_id((ECB_entries(x)), (i))
+#define ECB_entries_counter_event_offset(x, i)                                 \
+	EVENT_REG_counter_event_offset((ECB_entries(x)), (i))
+#define ECB_entries_reg_id(x, i) EVENT_REG_reg_id((ECB_entries(x)), (i))
+#define ECB_entries_reg_prog_type(x, i)                                        \
+	EVENT_REG_reg_prog_type((ECB_entries(x)), (i))
+#define ECB_entries_reg_offset(x, i) EVENT_REG_offset((ECB_entries(x)), (i))
+#define ECB_entries_reg_data_size(x, i)                                        \
+	EVENT_REG_data_size((ECB_entries(x)), (i))
+#define ECB_entries_desc_id(x, i) EVENT_REG_desc_id((ECB_entries(x)), i)
+#define ECB_entries_flags(x, i) EVENT_REG_flags((ECB_entries(x)), i)
+#define ECB_entries_reg_order(x, i) EVENT_REG_reg_order((ECB_entries(x)), i)
+#define ECB_entries_reg_value(x, i) EVENT_REG_reg_value((ECB_entries(x)), (i))
+#define ECB_entries_max_bits(x, i) EVENT_REG_max_bits((ECB_entries(x)), (i))
+#define ECB_entries_scheduled(x, i) EVENT_REG_scheduled((ECB_entries(x)), (i))
+#define ECB_entries_counter_event_offset(x, i)                                 \
+	EVENT_REG_counter_event_offset((ECB_entries(x)), (i))
+#define ECB_entries_bit_position(x, i)                                         \
+	EVENT_REG_bit_position((ECB_entries(x)), (i))
+// PCI config-specific fields
+#define ECB_entries_bus_no(x, i) EVENT_REG_bus_no((ECB_entries(x)), (i))
+#define ECB_entries_dev_no(x, i) EVENT_REG_dev_no((ECB_entries(x)), (i))
+#define ECB_entries_func_no(x, i) EVENT_REG_func_no((ECB_entries(x)), (i))
+#define ECB_entries_counter_type(x, i)                                         \
+	EVENT_REG_counter_type((ECB_entries(x)), (i))
+#define ECB_entries_event_scope(x, i)                                          \
+	EVENT_REG_event_scope((ECB_entries(x)), (i))
+#define ECB_entries_precise_get(x, i)                                          \
+	EVENT_REG_precise_get((ECB_entries(x)), (i))
+#define ECB_entries_global_get(x, i) EVENT_REG_global_get((ECB_entries(x)), (i))
+#define ECB_entries_uncore_get(x, i) EVENT_REG_uncore_get((ECB_entries(x)), (i))
+#define ECB_entries_uncore_q_rst_get(x, i)                                     \
+	EVENT_REG_uncore_q_rst_get((ECB_entries(x)), (i))
+#define ECB_entries_is_gp_reg_get(x, i)                                        \
+	EVENT_REG_is_gp_reg_get((ECB_entries(x)), (i))
+#define ECB_entries_lbr_value_get(x, i)                                        \
+	EVENT_REG_lbr_value_get((ECB_entries(x)), (i))
+#define ECB_entries_fixed_reg_get(x, i)                                        \
+	EVENT_REG_fixed_reg_get((ECB_entries(x)), (i))
+#define ECB_entries_is_multi_pkg_bit_set(x, i)                                 \
+	EVENT_REG_multi_pkg_evt_bit_get((ECB_entries(x)), (i))
+#define ECB_entries_clean_up_get(x, i)                                         \
+	EVENT_REG_clean_up_get((ECB_entries(x)), (i))
+#define ECB_entries_em_trigger_get(x, i)                                       \
+	EVENT_REG_em_trigger_get((ECB_entries(x)), (i))
+#define ECB_entries_branch_evt_get(x, i)                                       \
+	EVENT_REG_branch_evt_get((ECB_entries(x)), (i))
+#define ECB_entries_reg_rw_type(x, i)                                          \
+	EVENT_REG_reg_rw_type((ECB_entries(x)), (i))
+#define ECB_entries_secondary_pci_offset_offset(x, i)                          \
+	EVENT_REG_secondary_pci_offset_offset((ECB_entries(x)), (i))
+#define ECB_entries_secondary_pci_offset_shift(x, i)                           \
+	EVENT_REG_secondary_pci_offset_shift((ECB_entries(x)), (i))
+#define ECB_entries_secondary_pci_offset_mask(x, i)                            \
+	EVENT_REG_secondary_pci_offset_mask((ECB_entries(x)), (i))
+#define ECB_operations_operation_type(x, i)                                    \
+	PMU_OPER_operation_type((ECB_operations(x)), (i))
+#define ECB_operations_register_start(x, i)                                    \
+	PMU_OPER_register_start((ECB_operations(x)), (i))
+#define ECB_operations_register_len(x, i)                                      \
+	PMU_OPER_register_len((ECB_operations(x)), (i))
+
+#define ECB_entries_core_event_id(x, i)                                        \
+	EVENT_REG_core_event_id((ECB_entries(x)), (i))
+#define ECB_entries_uncore_buffer_offset_in_package(x, i)                      \
+	EVENT_REG_uncore_buffer_offset_in_package((ECB_entries(x)), (i))
+#define ECB_entries_uncore_buffer_offset_in_system(x, i)                       \
+	EVENT_REG_uncore_buffer_offset_in_system((ECB_entries(x)), (i))
+
+#define ECB_SET_OPERATIONS(x, operation_type, start, len)                   \
+	{                                                                   \
+		ECB_operations_operation_type(x, operation_type)            \
+			= operation_type;                                   \
+		ECB_operations_register_start(x, operation_type) = start;   \
+		ECB_operations_register_len(x, operation_type) = len;       \
+	}
+
+
+// ***************************************************************************
+
+/*!\struct  LBR_ENTRY_NODE_S
+ * \var     etype       TOS = 0; FROM = 1; TO = 2
+ * \var     type_index
+ * \var     reg_id
+ */
+
+typedef struct LBR_ENTRY_NODE_S LBR_ENTRY_NODE;
+typedef LBR_ENTRY_NODE * LBR_ENTRY;
+
+struct LBR_ENTRY_NODE_S {
+	U16 etype;
+	U16 type_index;
+	U32 reg_id;
+};
+
+//
+// Accessor macros for LBR entries
+//
+#define LBR_ENTRY_NODE_etype(lentry) ((lentry).etype)
+#define LBR_ENTRY_NODE_type_index(lentry) ((lentry).type_index)
+#define LBR_ENTRY_NODE_reg_id(lentry) ((lentry).reg_id)
+
+// ***************************************************************************
+
+/*!\struct LBR_NODE_S
+ * \var    num_entries     -  The number of entries
+ * \var    entries         -  The entries in the list
+ *
+ * \brief  Data structure to describe the LBR registers that need to be read
+ *
+ */
+
+typedef struct LBR_NODE_S LBR_NODE;
+typedef LBR_NODE * LBR;
+
+struct LBR_NODE_S {
+	U32 size;
+	U32 num_entries;
+	LBR_ENTRY_NODE entries[];
+};
+
+//
+// Accessor macros for LBR node
+//
+#define LBR_size(lbr) ((lbr)->size)
+#define LBR_num_entries(lbr) ((lbr)->num_entries)
+#define LBR_entries_etype(lbr, idx) ((lbr)->entries[idx].etype)
+#define LBR_entries_type_index(lbr, idx) ((lbr)->entries[idx].type_index)
+#define LBR_entries_reg_id(lbr, idx) ((lbr)->entries[idx].reg_id)
+
+// ***************************************************************************
+
+/*!\struct  PWR_ENTRY_NODE_S
+ * \var     etype       none as yet
+ * \var     type_index
+ * \var     reg_id
+ */
+
+typedef struct PWR_ENTRY_NODE_S PWR_ENTRY_NODE;
+typedef PWR_ENTRY_NODE * PWR_ENTRY;
+
+struct PWR_ENTRY_NODE_S {
+	U16 etype;
+	U16 type_index;
+	U32 reg_id;
+};
+
+//
+// Accessor macros for PWR entries
+//
+#define PWR_ENTRY_NODE_etype(lentry) ((lentry).etype)
+#define PWR_ENTRY_NODE_type_index(lentry) ((lentry).type_index)
+#define PWR_ENTRY_NODE_reg_id(lentry) ((lentry).reg_id)
+
+// ***************************************************************************
+
+/*!\struct PWR_NODE_S
+ * \var    num_entries     -  The number of entries
+ * \var    entries         -  The entries in the list
+ *
+ * \brief  Data structure to describe the PWR registers that need to be read
+ *
+ */
+
+typedef struct PWR_NODE_S PWR_NODE;
+typedef PWR_NODE * PWR;
+
+struct PWR_NODE_S {
+	U32 size;
+	U32 num_entries;
+	PWR_ENTRY_NODE entries[];
+};
+
+//
+// Accessor macros for PWR node
+//
+#define PWR_size(lbr) ((lbr)->size)
+#define PWR_num_entries(lbr) ((lbr)->num_entries)
+#define PWR_entries_etype(lbr, idx) ((lbr)->entries[idx].etype)
+#define PWR_entries_type_index(lbr, idx) ((lbr)->entries[idx].type_index)
+#define PWR_entries_reg_id(lbr, idx) ((lbr)->entries[idx].reg_id)
+
+// ***************************************************************************
+
+/*!\struct  RO_ENTRY_NODE_S
+ * \var     type       - DEAR, IEAR, BTB.
+ */
+
+typedef struct RO_ENTRY_NODE_S RO_ENTRY_NODE;
+typedef RO_ENTRY_NODE * RO_ENTRY;
+
+struct RO_ENTRY_NODE_S {
+	U32 reg_id;
+};
+
+//
+// Accessor macros for RO entries
+//
+#define RO_ENTRY_NODE_reg_id(lentry) ((lentry).reg_id)
+
+// ***************************************************************************
+
+/*!\struct RO_NODE_S
+ * \var    size            - The total size including header and entries.
+ * \var    num_entries     - The number of entries.
+ * \var    entries         - The entries in the list.
+ *
+ * \brief  Data structure to describe the RO registers that need to be read.
+ *
+ */
+
+typedef struct RO_NODE_S RO_NODE;
+typedef RO_NODE * RO;
+
+struct RO_NODE_S {
+	U32 size;
+	U32 num_entries;
+	RO_ENTRY_NODE entries[];
+};
+
+//
+// Accessor macros for RO node
+//
+#define RO_size(ro) ((ro)->size)
+#define RO_num_entries(ro) ((ro)->num_entries)
+#define RO_entries_reg_id(ro, idx) ((ro)->entries[idx].reg_id)
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_gfx.h b/drivers/platform/x86/sepdk/include/lwpmudrv_gfx.h
new file mode 100644
index 000000000000..fe6583e2c44c
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_gfx.h
@@ -0,0 +1,33 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2011-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef _LWPMUDRV_GFX_H_
+#define _LWPMUDRV_GFX_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define GFX_BASE_ADDRESS 0xFF200000
+#define GFX_BASE_NEW_OFFSET 0x00080000
+#define GFX_PERF_REG 0x040 // location of GFX counter relative to base
+#define GFX_NUM_COUNTERS 9 // max number of GFX counters per counter group
+#define GFX_CTR_OVF_VAL 0xFFFFFFFF // overflow value for GFX counters
+
+#define GFX_REG_CTR_CTRL 0x01FF
+#define GFX_CTRL_DISABLE 0x1E00
+
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_ioctl.h b/drivers/platform/x86/sepdk/include/lwpmudrv_ioctl.h
new file mode 100644
index 000000000000..a8d32466a4bd
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_ioctl.h
@@ -0,0 +1,284 @@
+/****
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ****/
+
+#ifndef _LWPMUDRV_IOCTL_H_
+#define _LWPMUDRV_IOCTL_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+//SEP Driver Operation defines
+/*
+	"NOTE THAT the definition must be identical across all OSes"
+	"DO NOT add any OS specific compile flag"
+*/
+#define DRV_OPERATION_START 1
+#define DRV_OPERATION_STOP 2
+#define DRV_OPERATION_INIT_PMU 3
+#define DRV_OPERATION_INIT 4
+#define DRV_OPERATION_EM_GROUPS 5
+#define DRV_OPERATION_SET_CPU_MASK 17
+#define DRV_OPERATION_PCI_READ 18
+#define DRV_OPERATION_PCI_WRITE 19
+#define DRV_OPERATION_READ_PCI_CONFIG 20
+#define DRV_OPERATION_FD_PHYS 21
+#define DRV_OPERATION_WRITE_PCI_CONFIG 22
+#define DRV_OPERATION_INSERT_MARKER 23
+#define DRV_OPERATION_GET_NORMALIZED_TSC 24
+#define DRV_OPERATION_EM_CONFIG_NEXT 25
+#define DRV_OPERATION_SYS_CONFIG 26
+#define DRV_OPERATION_TSC_SKEW_INFO 27
+#define DRV_OPERATION_NUM_CORES 28
+#define DRV_OPERATION_COLLECT_SYS_CONFIG 29
+#define DRV_OPERATION_GET_SYS_CONFIG 30
+#define DRV_OPERATION_PAUSE 31
+#define DRV_OPERATION_RESUME 32
+#define DRV_OPERATION_SET_ASYNC_EVENT 33
+#define DRV_OPERATION_ASYNC_STOP 34
+#define DRV_OPERATION_TERMINATE 35
+#define DRV_OPERATION_READ_MSRS 36
+#define DRV_OPERATION_LBR_INFO 37
+#define DRV_OPERATION_RESERVE 38
+#define DRV_OPERATION_MARK 39
+#define DRV_OPERATION_AWAIT_STOP 40
+#define DRV_OPERATION_SEED_NAME 41
+#define DRV_OPERATION_KERNEL_CS 42
+#define DRV_OPERATION_SET_UID 43
+#define DRV_OPERATION_VERSION 51
+#define DRV_OPERATION_CHIPSET_INIT 52
+#define DRV_OPERATION_GET_CHIPSET_DEVICE_ID 53
+#define DRV_OPERATION_SWITCH_GROUP 54
+#define DRV_OPERATION_GET_NUM_CORE_CTRS 55
+#define DRV_OPERATION_PWR_INFO 56
+#define DRV_OPERATION_NUM_DESCRIPTOR 57
+#define DRV_OPERATION_DESC_NEXT 58
+#define DRV_OPERATION_MARK_OFF 59
+#define DRV_OPERATION_CREATE_MARKER 60
+#define DRV_OPERATION_GET_DRIVER_STATE 61
+#define DRV_OPERATION_READ_SWITCH_GROUP 62
+#define DRV_OPERATION_EM_GROUPS_UNC 63
+#define DRV_OPERATION_EM_CONFIG_NEXT_UNC 64
+#define DRV_OPERATION_INIT_UNC 65
+#define DRV_OPERATION_RO_INFO 66
+#define DRV_OPERATION_READ_MSR 67
+#define DRV_OPERATION_WRITE_MSR 68
+#define DRV_OPERATION_THREAD_SET_NAME 69
+#define DRV_OPERATION_GET_PLATFORM_INFO 70
+#define DRV_OPERATION_GET_NORMALIZED_TSC_STANDALONE 71
+#define DRV_OPERATION_READ_AND_RESET 72
+#define DRV_OPERATION_SET_CPU_TOPOLOGY 73
+#define DRV_OPERATION_INIT_NUM_DEV 74
+#define DRV_OPERATION_SET_GFX_EVENT 75
+#define DRV_OPERATION_GET_NUM_SAMPLES 76
+#define DRV_OPERATION_SET_PWR_EVENT 77
+#define DRV_OPERATION_SET_DEVICE_NUM_UNITS 78
+#define DRV_OPERATION_TIMER_TRIGGER_READ 79
+#define DRV_OPERATION_GET_INTERVAL_COUNTS 80
+#define DRV_OPERATION_FLUSH 81
+#define DRV_OPERATION_SET_SCAN_UNCORE_TOPOLOGY_INFO 82
+#define DRV_OPERATION_GET_UNCORE_TOPOLOGY 83
+#define DRV_OPERATION_GET_MARKER_ID 84
+#define DRV_OPERATION_GET_SAMPLE_DROP_INFO 85
+#define DRV_OPERATION_GET_DRV_SETUP_INFO 86
+#define DRV_OPERATION_GET_PLATFORM_TOPOLOGY 87
+#define DRV_OPERATION_GET_THREAD_COUNT 88
+#define DRV_OPERATION_GET_THREAD_INFO 89
+#define DRV_OPERATION_GET_DRIVER_LOG 90
+#define DRV_OPERATION_CONTROL_DRIVER_LOG 91
+#define DRV_OPERATION_SET_OSID 92
+#define DRV_OPERATION_GET_AGENT_MODE 93
+#define DRV_OPERATION_INIT_DRIVER 94
+#define DRV_OPERATION_SET_EMON_BUFFER_DRIVER_HELPER 95
+// Only used by MAC OS
+#define DRV_OPERATION_GET_ASLR_OFFSET 997 // this may not need
+#define DRV_OPERATION_SET_OSX_VERSION 998
+#define DRV_OPERATION_PROVIDE_FUNCTION_PTRS 999
+
+// IOCTL_SETUP
+
+// IOCTL_ARGS
+typedef struct IOCTL_ARGS_NODE_S IOCTL_ARGS_NODE;
+typedef IOCTL_ARGS_NODE * IOCTL_ARGS;
+
+#if defined(DRV_EM64T)
+struct IOCTL_ARGS_NODE_S {
+	U64 len_drv_to_usr;
+	// buffer send from driver(target) to user(host), stands for read buffer
+	char *buf_drv_to_usr;
+	// length of the driver(target) to user(host) buffer
+	U64 len_usr_to_drv;
+	// buffer send from user(host) to driver(target) stands for write buffer
+	char *buf_usr_to_drv; // length of user(host) to driver(target) buffer
+	U32 command;
+};
+#endif
+#if defined(DRV_IA32)
+struct IOCTL_ARGS_NODE_S {
+	U64 len_drv_to_usr;
+	// buffer send from driver(target) to user(host),stands for read buffer
+	char *buf_drv_to_usr; // length of driver(target) to user(host) buffer
+	char *reserved1;
+	U64 len_usr_to_drv;
+	// send from user(host) to driver(target),stands for write buffer
+	char *buf_usr_to_drv; // length of user(host) to driver(target) buffer
+	char *reserved2;
+	U32 command;
+};
+#endif
+
+#if defined(DRV_OS_WINDOWS)
+
+//
+// NtDeviceIoControlFile IoControlCode values for this device.
+//
+// Warning:  Remember that the low two bits of the code specify how the
+//           buffers are passed to the driver!
+//
+// 16 bit device type. 12 bit function codes
+#define LWPMUDRV_IOCTL_DEVICE_TYPE 0xA000
+// values 0-32768 reserved for Microsoft
+#define LWPMUDRV_IOCTL_FUNCTION 0x0A00 // values 0-2047  reserved for Microsoft
+
+//
+// Basic CTL CODE macro to reduce typographical errors
+// Use for FILE_READ_ACCESS
+//
+#define LWPMUDRV_CTL_READ_CODE(x)                                              \
+	CTL_CODE(LWPMUDRV_IOCTL_DEVICE_TYPE, LWPMUDRV_IOCTL_FUNCTION + (x),    \
+		 METHOD_BUFFERED, FILE_READ_ACCESS)
+
+/* Refernece https://docs.microsoft.com/en-us/windows-hardware/drivers/kernel/defining-i-o-control-codes
+   CTL_CODE (DeviceType, Function, Method, Access) generates 32 bit code
+	------------------------------------------------- ----------------
+	|   31   | 30 ... 16 | 15      14 |   13   | 12  ... 2 | 1      0 |
+	-------------------------------------------------------------------
+	| common | device    | req access | custom | func code | transfer |
+	|        |  type     |            |        |           |   type   |
+	-------------------------------------------------------------------
+*/
+#define LWPMUDRV_DEVICE_TYPE(x) ((x & 0xFFFF0000) >> 16)
+#define LWPMUDRV_METHOD(x) (x & 3)
+#define LWPMUDRV_FUNCTION(x) (((x >> 2) & 0x00000FFF) - 0x0A00)
+
+#define LWPMUDRV_IOCTL_CODE(x) LWPMUDRV_CTL_READ_CODE(x)
+
+#elif defined(SEP_ESX)
+
+typedef struct CPU_ARGS_NODE_S CPU_ARGS_NODE;
+typedef CPU_ARGS_NODE * CPU_ARGS;
+struct CPU_ARGS_NODE_S {
+	U64 len_drv_to_usr;
+	char *buf_drv_to_usr;
+	U32 command;
+	U32 CPU_ID;
+	U32 BUCKET_ID;
+};
+
+// IOCTL_SETUP
+#define LWPMU_IOC_MAGIC 99
+#define OS_SUCCESS 0
+#define OS_STATUS int
+//#define OS_ILLEGAL_IOCTL  -ENOTTY
+//#define OS_NO_MEM         -ENOMEM
+//#define OS_FAULT          -EFAULT
+
+#define LWPMUDRV_IOCTL_IO(x) (x)
+#define LWPMUDRV_IOCTL_IOR(x) (x)
+#define LWPMUDRV_IOCTL_IOW(x) (x)
+#define LWPMUDRV_IOCTL_IORW(x) (x)
+
+#elif defined(DRV_OS_LINUX) || defined(DRV_OS_SOLARIS) ||                      \
+	defined(DRV_OS_ANDROID)
+// IOCTL_ARGS
+
+// COMPAT IOCTL_ARGS
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+typedef struct IOCTL_COMPAT_ARGS_NODE_S IOCTL_COMPAT_ARGS_NODE;
+typedef IOCTL_COMPAT_ARGS_NODE * IOCTL_COMPAT_ARGS;
+struct IOCTL_COMPAT_ARGS_NODE_S {
+	U64 len_drv_to_usr;
+	compat_uptr_t buf_drv_to_usr;
+	U64 len_usr_to_drv;
+	compat_uptr_t buf_usr_to_drv;
+};
+#endif
+
+// COMPAT IOCTL_SETUP
+//
+#define LWPMU_IOC_MAGIC 99
+
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+#define LWPMUDRV_IOCTL_IO(x) _IO(LWPMU_IOC_MAGIC, (x))
+#define LWPMUDRV_IOCTL_IOR(x) _IOR(LWPMU_IOC_MAGIC, (x), compat_uptr_t)
+#define LWPMUDRV_IOCTL_IOW(x) _IOW(LWPMU_IOC_MAGIC, (x), compat_uptr_t)
+#define LWPMUDRV_IOCTL_IORW(x) _IOW(LWPMU_IOC_MAGIC, (x), compat_uptr_t)
+#else
+#define LWPMUDRV_IOCTL_IO(x) _IO(LWPMU_IOC_MAGIC, (x))
+#define LWPMUDRV_IOCTL_IOR(x) _IOR(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS)
+#define LWPMUDRV_IOCTL_IOW(x) _IOW(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS)
+#define LWPMUDRV_IOCTL_IORW(x) _IOW(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS)
+#endif
+
+#elif defined(DRV_OS_FREEBSD)
+
+// IOCTL_SETUP
+//
+#define LWPMU_IOC_MAGIC 99
+
+/* FreeBSD is very strict about IOR/IOW/IOWR specifications on IOCTLs.
+ * Since these IOCTLs all pass down the real read/write buffer lengths
+ *  and addresses inside of an IOCTL_ARGS_NODE data structure, we
+ *  need to specify all of these as _IOW so that the kernel will
+ *  view it as userspace passing the data to the driver, rather than
+ *  the reverse.  There are also some cases where Linux is passing
+ *  a smaller type than IOCTL_ARGS_NODE, even though its really
+ *  passing an IOCTL_ARGS_NODE.  These needed to be fixed for FreeBSD.
+ */
+#define LWPMUDRV_IOCTL_IO(x) _IO(LWPMU_IOC_MAGIC, (x))
+#define LWPMUDRV_IOCTL_IOR(x) _IOW(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS_NODE)
+#define LWPMUDRV_IOCTL_IOW(x) _IOW(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS_NODE)
+#define LWPMUDRV_IOCTL_IORW(x) _IOW(LWPMU_IOC_MAGIC, (x), IOCTL_ARGS_NODE)
+
+#elif defined(DRV_OS_MAC)
+
+typedef struct CPU_ARGS_NODE_S CPU_ARGS_NODE;
+typedef CPU_ARGS_NODE * CPU_ARGS;
+struct CPU_ARGS_NODE_S {
+	U64 len_drv_to_usr;
+	char *buf_drv_to_usr;
+	U32 command;
+	U32 CPU_ID;
+	U32 BUCKET_ID;
+};
+
+// IOCTL_SETUP
+//
+#define LWPMU_IOC_MAGIC 99
+#define OS_SUCCESS 0
+#define OS_STATUS int
+#define OS_ILLEGAL_IOCTL -ENOTTY
+#define OS_NO_MEM -ENOMEM
+#define OS_FAULT -EFAULT
+
+// Task file Opcodes.
+// keeping the definitions as IOCTL but in MAC OSX
+// these are really OpCodes consumed by Execute command.
+
+#else
+#error "unknown OS in lwpmudrv_ioctl.h"
+#endif
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_pwr.h b/drivers/platform/x86/sepdk/include/lwpmudrv_pwr.h
new file mode 100644
index 000000000000..e26a478a9bb1
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_pwr.h
@@ -0,0 +1,100 @@
+/****
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2011-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+****/
+
+#ifndef _LWPMUDRV_PWR_H_
+#define _LWPMUDRV_PWR_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#define MAX_EVENT_NAME_LEN 512
+#define MAX_EVENT_DESC_LEN 1024
+
+// Power event groups
+enum PWR_EVENT_GROUPS {
+	IO_DEV_STATES = 1,
+	MMIO_DEV_STATES,
+	MMIO_SYS_STATES,
+	MMIO_IPC_DEV_RES,
+	MMIO_IPC_SYS_RES
+};
+
+typedef struct PWR_EVENT_INFO_NODE_S PWR_EVENT_INFO_NODE;
+typedef PWR_EVENT_INFO_NODE * PWR_EVENT_INFO;
+
+struct PWR_EVENT_INFO_NODE_S {
+	U32 event_id;
+	U32 group_id;
+	char name[MAX_EVENT_NAME_LEN];
+	char desc[MAX_EVENT_DESC_LEN];
+	U32 io_baseaddr1;
+	U32 io_range1;
+	U32 io_baseaddr2;
+	U32 io_range2;
+	U32 offset;
+	U32 virtual_address;
+};
+
+#define PWR_EVENT_INFO_event_id(pwr_event) ((pwr_event)->event_id)
+#define PWR_EVENT_INFO_group_id(pwr_event) ((pwr_event)->group_id)
+#define PWR_EVENT_INFO_name(pwr_event) ((pwr_event)->name)
+#define PWR_EVENT_INFO_desc(pwr_event) ((pwr_event)->desc)
+#define PWR_EVENT_INFO_io_baseaddr1(pwr_event) ((pwr_event)->io_baseaddr1)
+#define PWR_EVENT_INFO_io_range1(pwr_event) ((pwr_event)->io_range1)
+#define PWR_EVENT_INFO_io_baseaddr2(pwr_event) ((pwr_event)->io_baseaddr2)
+#define PWR_EVENT_INFO_io_range2(pwr_event) ((pwr_event)->io_range2)
+#define PWR_EVENT_INFO_offset(pwr_event) ((pwr_event)->offset)
+#define PWR_EVENT_INFO_virtual_address(pwr_event) ((pwr_event)->virtual_address)
+
+// IPC register offsets
+#define IPC_BASE_ADDRESS 0xFF11C000
+#define IPC_CMD_OFFSET 0x00000000
+#define IPC_STS_OFFSET 0x00000004
+#define IPC_SPTR_OFFSET 0x00000008
+#define IPC_DPTR_OFFSET 0x0000000C
+#define IPC_WBUF_OFFSET 0x00000080
+#define IPC_RBUF_OFFSET 0x00000090
+#define IPC_MAX_ADDR 0x100
+
+// Write 3bytes in IPC_WBUF (2bytes for address and 1byte for value)
+#define IPC_ADC_WRITE_1 0x000300FF
+// Write 2bytes in IPC_WBUF (2bytes for address) and read 1byte from IPC_RBUF
+#define IPC_ADC_READ_1 0x000210FF
+
+// IPC commands
+#define IPC_MESSAGE_MSIC 0xFF
+#define IPC_MESSAGE_CC 0xEF
+#define IPC_MESSAGE_D_RESIDENCY 0xEA
+#define IPC_MESSAGE_S_RESIDENCY 0xEB
+
+// IPC subcommands
+#define IPC_COMMAND_WRITE 0x0
+#define IPC_COMMAND_READ 0x1
+#define IPC_COMMAND_START_RESIDENCY 0x0
+#define IPC_COMMAND_STOP_RESIDENCY 0x1
+#define IPC_COMMAND_DUMP_RESIDENCY 0x2
+
+// IPC commands for S state residency counter
+#define S_RESIDENCY_BASE_ADDRESS 0xFFFF71E0
+#define S_RESIDENCY_MAX_COUNTERS 0x4
+#define S_RESIDENCY_MAX_STATES 0x3
+// IPC commands for D state residency counter
+#define D_RESIDENCY_BASE_ADDRESS 0xFFFF7000
+#define D_RESIDENCY_MAX_COUNTERS 0x78 // 40 LSS * 3 D states = 120
+#define D_RESIDENCY_MAX_STATES 0x3
+#define D_RESIDENCY_MAX_LSS 0x28 // 40 LSS
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif /* _LWPMUDRV_PWR_H_ */
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_struct.h b/drivers/platform/x86/sepdk/include/lwpmudrv_struct.h
new file mode 100644
index 000000000000..c76ef5fa0e67
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_struct.h
@@ -0,0 +1,2059 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+***/
+
+#ifndef _LWPMUDRV_STRUCT_UTILS_H_
+#define _LWPMUDRV_STRUCT_UTILS_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+// processor execution modes
+#define MODE_UNKNOWN 99
+// the following defines must start at 0
+#define MODE_64BIT 3
+#define MODE_32BIT 2
+#define MODE_16BIT 1
+#define MODE_V86 0
+
+// sampling methods
+#define SM_RTC 2020 // real time clock
+#define SM_VTD 2021 // OS Virtual Timer Device
+#define SM_NMI 2022 // non-maskable interrupt time based
+#define SM_EBS 2023 // event based sampling
+#define SM_EBC 2024 // event based counting
+
+// sampling mechanism bitmap definitions
+#define INTERRUPT_RTC 0x1
+#define INTERRUPT_VTD 0x2
+#define INTERRUPT_NMI 0x4
+#define INTERRUPT_EBS 0x8
+
+// Device types
+#define DEV_CORE 0x01
+#define DEV_UNC 0x02
+
+// eflags defines
+#define EFLAGS_VM 0x00020000 // V86 mode
+#define EFLAGS_IOPL0 0
+#define EFLAGS_IOPL1 0x00001000
+#define EFLAGS_IOPL2 0x00002000
+#define EFLAGS_IOPL3 0x00003000
+#define MAX_EMON_GROUPS 1000
+#define MAX_PCI_BUSNO 256
+#define MAX_DEVICES 30
+#define MAX_REGS 64
+#define MAX_EMON_GROUPS 1000
+#define MAX_PCI_DEVNO 32
+#define MAX_PCI_FUNCNO 8
+#define MAX_PCI_DEVUNIT 16
+#define MAX_TURBO_VALUES 32
+#define REG_BIT_MASK 0xFFFFFFFFFFFFFFFFULL
+
+extern float freq_multiplier;
+
+// Enumeration for invoking dispatch on multiple cpus or not
+typedef enum { DRV_MULTIPLE_INSTANCE = 0, DRV_SINGLE_INSTANCE } DRV_PROG_TYPE;
+
+typedef struct DRV_CONFIG_NODE_S DRV_CONFIG_NODE;
+typedef DRV_CONFIG_NODE * DRV_CONFIG;
+
+struct DRV_CONFIG_NODE_S {
+	U32 size;
+	U16 version;
+	U16 reserved1;
+	U32 num_events;
+	U32 num_chipset_events;
+	U32 chipset_offset;
+	S32 seed_name_len;
+	union {
+		S8 *seed_name;
+		U64 dummy1;
+	} u1;
+	union {
+		S8 *cpu_mask;
+		U64 dummy2;
+	} u2;
+	union {
+		U64 collection_config;
+		struct {
+			U64 start_paused : 1;
+			U64 counting_mode : 1;
+			U64 enable_chipset : 1;
+			U64 enable_gfx : 1;
+			U64 enable_pwr : 1;
+			U64 emon_mode : 1;
+			U64 debug_inject : 1;
+			U64 virt_phys_translation : 1;
+			U64 enable_p_state : 1;
+			U64 enable_cp_mode : 1;
+			U64 read_pstate_msrs : 1;
+			U64 use_pcl : 1;
+			U64 enable_ebc : 1;
+			U64 enable_tbc : 1;
+			U64 ds_area_available : 1;
+			U64 per_cpu_tsc : 1;
+			U64 reserved_field1 : 48;
+		} s1;
+	} u3;
+	U64 target_pid;
+	U32 os_of_interest;
+	U16 unc_timer_interval;
+	U16 unc_em_factor;
+	S32 p_state_trigger_index;
+	DRV_BOOL multi_pebs_enabled;
+	U32 reserved2;
+	U32 reserved3;
+	U64 reserved4;
+	U64 reserved5;
+	U64 reserved6;
+};
+
+#define DRV_CONFIG_size(cfg) ((cfg)->size)
+#define DRV_CONFIG_version(cfg) ((cfg)->version)
+#define DRV_CONFIG_num_events(cfg) ((cfg)->num_events)
+#define DRV_CONFIG_num_chipset_events(cfg) ((cfg)->num_chipset_events)
+#define DRV_CONFIG_chipset_offset(cfg) ((cfg)->chipset_offset)
+
+#define DRV_CONFIG_seed_name(cfg) ((cfg)->u1.seed_name)
+#define DRV_CONFIG_seed_name_len(cfg) ((cfg)->seed_name_len)
+#define DRV_CONFIG_cpu_mask(cfg) ((cfg)->u2.cpu_mask)
+#define DRV_CONFIG_start_paused(cfg) ((cfg)->u3.s1.start_paused)
+#define DRV_CONFIG_counting_mode(cfg) ((cfg)->u3.s1.counting_mode)
+#define DRV_CONFIG_enable_chipset(cfg) ((cfg)->u3.s1.enable_chipset)
+#define DRV_CONFIG_enable_gfx(cfg) ((cfg)->u3.s1.enable_gfx)
+#define DRV_CONFIG_enable_pwr(cfg) ((cfg)->u3.s1.enable_pwr)
+#define DRV_CONFIG_emon_mode(cfg) ((cfg)->u3.s1.emon_mode)
+#define DRV_CONFIG_debug_inject(cfg) ((cfg)->u3.s1.debug_inject)
+#define DRV_CONFIG_virt_phys_translation(cfg)                                  \
+	((cfg)->u3.s1.virt_phys_translation)
+#define DRV_CONFIG_enable_p_state(cfg) ((cfg)->u3.s1.enable_p_state)
+#define DRV_CONFIG_enable_cp_mode(cfg) ((cfg)->u3.s1.enable_cp_mode)
+#define DRV_CONFIG_read_pstate_msrs(cfg) ((cfg)->u3.s1.read_pstate_msrs)
+#define DRV_CONFIG_use_pcl(cfg) ((cfg)->u3.s1.use_pcl)
+#define DRV_CONFIG_event_based_counts(cfg) ((cfg)->u3.s1.enable_ebc)
+#define DRV_CONFIG_timer_based_counts(cfg) ((cfg)->u3.s1.enable_tbc)
+#define DRV_CONFIG_ds_area_available(cfg) ((cfg)->u3.s1.ds_area_available)
+#define DRV_CONFIG_per_cpu_tsc(cfg) ((cfg)->u3.s1.per_cpu_tsc)
+#define DRV_CONFIG_target_pid(cfg) ((cfg)->target_pid)
+#define DRV_CONFIG_os_of_interest(cfg) ((cfg)->os_of_interest)
+#define DRV_CONFIG_unc_timer_interval(cfg) ((cfg)->unc_timer_interval)
+#define DRV_CONFIG_unc_em_factor(cfg) ((cfg)->unc_em_factor)
+#define DRV_CONFIG_p_state_trigger_index(cfg) ((cfg)->p_state_trigger_index)
+#define DRV_CONFIG_multi_pebs_enabled(cfg) ((cfg)->multi_pebs_enabled)
+
+#define DRV_CONFIG_VERSION 1
+
+typedef struct DEV_CONFIG_NODE_S DEV_CONFIG_NODE;
+typedef DEV_CONFIG_NODE * DEV_CONFIG;
+
+struct DEV_CONFIG_NODE_S {
+	U16 size;
+	U16 version;
+	U32 dispatch_id;
+	U32 pebs_mode;
+	U32 pebs_record_num;
+	U32 results_offset; // to store the offset for this device's results
+	U32 max_gp_counters;
+	U32 device_type;
+	U32 core_type;
+	union {
+		U64 enable_bit_fields;
+		struct {
+			U64 pebs_capture : 1;
+			U64 collect_lbrs : 1;
+			U64 collect_callstacks : 1;
+			U64 collect_kernel_callstacks : 1;
+			U64 latency_capture : 1;
+			U64 power_capture : 1;
+			U64 htoff_mode : 1;
+			U64 eventing_ip_capture : 1;
+			U64 hle_capture : 1;
+			U64 precise_ip_lbrs : 1;
+			U64 store_lbrs : 1;
+			U64 tsc_capture : 1;
+			U64 enable_perf_metrics : 1;
+			U64 enable_adaptive_pebs : 1;
+			U64 apebs_collect_mem_info : 1;
+			U64 apebs_collect_gpr : 1;
+			U64 apebs_collect_xmm : 1;
+			U64 apebs_collect_lbrs : 1;
+			U64 collect_fixed_counter_pebs : 1;
+			U64 collect_os_callstacks : 1;
+			U64 reserved_field1 : 44;
+		} s1;
+	} u1;
+	U32 emon_unc_offset[MAX_EMON_GROUPS];
+	U32 ebc_group_id_offset;
+	U8 num_perf_metrics;
+	U8 apebs_num_lbr_entries;
+	U16 emon_perf_metrics_offset;
+	U32 device_scope;
+	U32 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define DEV_CONFIG_dispatch_id(cfg) ((cfg)->dispatch_id)
+#define DEV_CONFIG_pebs_mode(cfg) ((cfg)->pebs_mode)
+#define DEV_CONFIG_pebs_record_num(cfg) ((cfg)->pebs_record_num)
+#define DEV_CONFIG_results_offset(cfg) ((cfg)->results_offset)
+#define DEV_CONFIG_max_gp_counters(cfg) ((cfg)->max_gp_counters)
+
+#define DEV_CONFIG_device_type(cfg) ((cfg)->device_type)
+#define DEV_CONFIG_core_type(cfg) ((cfg)->core_type)
+
+#define DEV_CONFIG_pebs_capture(cfg) ((cfg)->u1.s1.pebs_capture)
+#define DEV_CONFIG_collect_lbrs(cfg) ((cfg)->u1.s1.collect_lbrs)
+#define DEV_CONFIG_collect_callstacks(cfg) ((cfg)->u1.s1.collect_callstacks)
+#define DEV_CONFIG_collect_kernel_callstacks(cfg)                              \
+	((cfg)->u1.s1.collect_kernel_callstacks)
+#define DEV_CONFIG_latency_capture(cfg) ((cfg)->u1.s1.latency_capture)
+#define DEV_CONFIG_power_capture(cfg) ((cfg)->u1.s1.power_capture)
+#define DEV_CONFIG_htoff_mode(cfg) ((cfg)->u1.s1.htoff_mode)
+#define DEV_CONFIG_eventing_ip_capture(cfg) ((cfg)->u1.s1.eventing_ip_capture)
+#define DEV_CONFIG_hle_capture(cfg) ((cfg)->u1.s1.hle_capture)
+#define DEV_CONFIG_precise_ip_lbrs(cfg) ((cfg)->u1.s1.precise_ip_lbrs)
+#define DEV_CONFIG_store_lbrs(cfg) ((cfg)->u1.s1.store_lbrs)
+#define DEV_CONFIG_tsc_capture(cfg) ((cfg)->u1.s1.tsc_capture)
+#define DEV_CONFIG_enable_perf_metrics(cfg) ((cfg)->u1.s1.enable_perf_metrics)
+#define DEV_CONFIG_enable_adaptive_pebs(cfg) ((cfg)->u1.s1.enable_adaptive_pebs)
+#define DEV_CONFIG_apebs_collect_mem_info(cfg)                                 \
+	((cfg)->u1.s1.apebs_collect_mem_info)
+#define DEV_CONFIG_apebs_collect_gpr(cfg) ((cfg)->u1.s1.apebs_collect_gpr)
+#define DEV_CONFIG_apebs_collect_xmm(cfg) ((cfg)->u1.s1.apebs_collect_xmm)
+#define DEV_CONFIG_apebs_collect_lbrs(cfg) ((cfg)->u1.s1.apebs_collect_lbrs)
+#define DEV_CONFIG_collect_fixed_counter_pebs(cfg)                             \
+	((cfg)->u1.s1.collect_fixed_counter_pebs)
+#define DEV_CONFIG_collect_os_callstacks(cfg)                                  \
+	((cfg)->u1.s1.collect_os_callstacks)
+#define DEV_CONFIG_enable_bit_fields(cfg) ((cfg)->u1.enable_bit_fields)
+#define DEV_CONFIG_emon_unc_offset(cfg, grp_num)                               \
+	((cfg)->emon_unc_offset[grp_num])
+#define DEV_CONFIG_ebc_group_id_offset(cfg) ((cfg)->ebc_group_id_offset)
+#define DEV_CONFIG_num_perf_metrics(cfg) ((cfg)->num_perf_metrics)
+#define DEV_CONFIG_apebs_num_lbr_entries(cfg) ((cfg)->apebs_num_lbr_entries)
+#define DEV_CONFIG_emon_perf_metrics_offset(cfg)                               \
+	((cfg)->emon_perf_metrics_offset)
+#define DEV_CONFIG_device_scope(cfg) ((cfg)->device_scope)
+
+typedef struct DEV_UNC_CONFIG_NODE_S DEV_UNC_CONFIG_NODE;
+typedef DEV_UNC_CONFIG_NODE * DEV_UNC_CONFIG;
+
+struct DEV_UNC_CONFIG_NODE_S {
+	U16 size;
+	U16 version;
+	U32 dispatch_id;
+	U32 results_offset;
+	U32 device_type;
+	U32 device_scope;
+	U32 reserved1;
+	U32 emon_unc_offset[MAX_EMON_GROUPS];
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define DEV_UNC_CONFIG_dispatch_id(cfg) ((cfg)->dispatch_id)
+#define DEV_UNC_CONFIG_results_offset(cfg) ((cfg)->results_offset)
+#define DEV_UNC_CONFIG_emon_unc_offset(cfg, grp_num)                           \
+	((cfg)->emon_unc_offset[grp_num])
+#define DEV_UNC_CONFIG_device_type(cfg) ((cfg)->device_type)
+#define DEV_UNC_CONFIG_device_scope(cfg) ((cfg)->device_scope)
+
+/*
+ *    X86 processor code descriptor
+ */
+typedef struct CodeDescriptor_s {
+	union {
+		U32 lowWord; // low dword of descriptor
+		struct { // low broken out by fields
+			U16 limitLow; // segment limit 15:00
+			U16 baseLow; // segment base 15:00
+		} s1;
+	} u1;
+	union {
+		U32 highWord; // high word of descriptor
+		struct { // high broken out by bit fields
+			U32 baseMid : 8; // base 23:16
+			U32 accessed : 1; // accessed
+			U32 readable : 1; // readable
+			U32 conforming : 1; // conforming code segment
+			U32 oneOne : 2; // always 11
+			U32 dpl : 2; // Dpl
+			U32 pres : 1; // present bit
+			U32 limitHi : 4; // limit 19:16
+			U32 sys : 1; // available for use by system
+			U32 reserved_0 : 1; // reserved, always 0
+			U32 default_size : 1;
+			// default operation size (1=32bit, 0=16bit)
+			U32 granularity : 1; // granularity (1=32 bit, 0=20 bit)
+			U32 baseHi : 8; // base hi 31:24
+		} s2;
+	} u2;
+} CodeDescriptor;
+
+/*
+ *  Module record. These are emitted whenever a DLL/EXE is loaded or unloaded.
+ *  The filename fields may be 0 on an unload.  The records reperesent a module
+ *  for a certain span of time, delineated by the load / unload samplecounts.
+ *  Note:
+ *  The structure contains 64 bit fields which may cause the compiler to pad the
+ *  length of the structure to an 8 byte boundary.
+ */
+typedef struct ModuleRecord_s {
+	U16 recLength; // total length of this record (including this length,
+		// always U32 multiple)  output from sampler is variable
+		// length (pathname at end of record) sampfile builder moves
+		// path names to a separate "literal pool" area
+		// so that these records become fixed length, and can be treated
+		// as an array see modrecFixedLen in header
+
+	U16 segmentType : 2;
+	// V86, 16, 32, 64 (see MODE_ defines), maybe inaccurate for Win95
+	// .. a 16 bit module may become a 32 bit module, inferred by
+	// ..looking at 1st sample record that matches the module selector
+	U16 loadEvent : 1; // 0 for load, 1 for unload
+	U16 processed : 1; // 0 for load, 1 for unload
+	U16 reserved0 : 12;
+
+	U16 selector; // code selector or V86 segment
+	U16 segmentNameLength;
+	// length of the segment name if the segmentNameSet bit is set
+	U32 segmentNumber;
+	// segment number, Win95 can have multiple pieces for one module
+	union {
+		U32 flags; // all the flags as one dword
+		struct {
+			U32 exe : 1; // this module is an exe
+			U32 globalModule : 1;
+			// globally loaded module.  There may be multiple
+			// module records for a global module, but the samples
+			// will only point to the 1st one, the others will be
+			// ignored.  NT's Kernel32 is an example of this.
+			// REVISIT this??
+			U32 bogusWin95 : 1;
+			// "bogus" win95 module.  By bogus, we mean a
+			// module that has a pid of 0, no length and no base.
+			// Selector actually used as a 32 bit module.
+			U32 pidRecIndexRaw : 1; // pidRecIndex is raw OS pid
+			U32 sampleFound : 1;
+			// at least one sample referenced this module
+			U32 tscUsed : 1; // tsc set when record written
+			U32 duplicate : 1;
+			// 1st pass analysis has determined this is a
+			// duplicate load
+			U32 globalModuleTB5 : 1;
+			// module mapped into all processes on system
+			U32 segmentNameSet : 1;
+			// set if the segment name was collected
+			// (initially done for xbox collections)
+			U32 firstModuleRecInProcess : 1;
+			// if the pidCreatesTrackedInModuleRecs flag is set
+			//  in the SampleHeaderEx struct and this flag
+			//  is set, the associated module indicates
+			//  the beginning of a new process
+			U32 source : 1;
+			// 0 for path in target system,
+			// 1 for path in host system
+			U32 unknownLoadAddress : 1;
+			// for 0 valid loadAddr64 value,
+			// 1 for invalid loadAddr64 value
+			U32 reserved1 : 20;
+		} s1;
+	} u2;
+	U64 length64; // module length
+	U64 loadAddr64; // load address
+	U32 pidRecIndex;
+	// process ID rec index (index into  start of pid record section)
+	// .. (see pidRecIndexRaw).  If pidRecIndex == 0 and pidRecIndexRaw == 1
+	// ..then this is a kernel or global module.  Can validly
+	// ..be 0 if not raw (array index).  Use ReturnPid() to access this
+	// ..field
+	U32 osid; // OS identifier
+	U64 unloadTsc; // TSC collected on an unload event
+	U32 path; // module path name (section offset on disk)
+		// ..when initally written by sampler name is at end of this
+		// ..struct, when merged with main file names are pooled at end
+		// ..of ModuleRecord Section so ModulesRecords can be
+		// ..fixed length
+	U16 pathLength; // path name length (inludes terminating \0)
+	U16 filenameOffset; // offset into path name of base filename
+	U32 segmentName; // offset to the segmentName from the beginning of the
+		//  module section in a processed module section
+		//  (s/b 0 in a raw module record)
+		// in a raw module record, the segment name will follow the
+		//  module name and the module name's terminating NULL char
+	U32 page_offset_high;
+	U64 tsc; // time stamp counter module event occurred
+	U32 parent_pid; // Parent PID of the process
+	U32 page_offset_low;
+} ModuleRecord;
+
+#define MR_unloadTscSet(x, y) { (x)->unloadTsc = (y); }
+#define MR_unloadTscGet(x) ((x)->unloadTsc)
+
+#define MR_page_offset_Set(x, y)                                    \
+	{                                                           \
+		(x)->page_offset_low = (y)&0xFFFFFFFF;              \
+		(x)->page_offset_high = ((y) >> 32) & 0xFFFFFFFF;   \
+	}
+
+#define MR_page_offset_Get(x)                                                  \
+	((((U64)(x)->page_offset_high) << 32) | (x)->page_offset_low)
+
+// Accessor macros for ModuleRecord
+#define MODULE_RECORD_rec_length(x) ((x)->recLength)
+#define MODULE_RECORD_segment_type(x) ((x)->segmentType)
+#define MODULE_RECORD_load_event(x) ((x)->loadEvent)
+#define MODULE_RECORD_processed(x) ((x)->processed)
+#define MODULE_RECORD_selector(x) ((x)->selector)
+#define MODULE_RECORD_segment_name_length(x) ((x)->segmentNameLength)
+#define MODULE_RECORD_segment_number(x) ((x)->segmentNumber)
+#define MODULE_RECORD_flags(x) ((x)->u2.flags)
+#define MODULE_RECORD_exe(x) ((x)->u2.s1.exe)
+#define MODULE_RECORD_global_module(x) ((x)->u2.s1.globalModule)
+#define MODULE_RECORD_bogus_win95(x) ((x)->u2.s1.bogusWin95)
+#define MODULE_RECORD_pid_rec_index_raw(x) ((x)->u2.s1.pidRecIndexRaw)
+#define MODULE_RECORD_sample_found(x) ((x)->u2.s1.sampleFound)
+#define MODULE_RECORD_tsc_used(x) ((x)->u2.s1.tscUsed)
+#define MODULE_RECORD_duplicate(x) ((x)->u2.s1.duplicate)
+#define MODULE_RECORD_global_module_tb5(x) ((x)->u2.s1.globalModuleTB5)
+#define MODULE_RECORD_segment_name_set(x) ((x)->u2.s1.segmentNameSet)
+#define MODULE_RECORD_first_module_rec_in_process(x)                           \
+	((x)->u2.s1.firstModuleRecInProcess)
+#define MODULE_RECORD_source(x) ((x)->u2.s1.source)
+#define MODULE_RECORD_unknown_load_address(x) ((x)->u2.s1.unknownLoadAddress)
+#define MODULE_RECORD_length64(x) ((x)->length64)
+#define MODULE_RECORD_load_addr64(x) ((x)->loadAddr64)
+#define MODULE_RECORD_pid_rec_index(x) ((x)->pidRecIndex)
+#define MODULE_RECORD_load_sample_count(x) ((x)->u5.s2.loadSampleCount)
+#define MODULE_RECORD_unload_sample_count(x) ((x)->u5.s2.unloadSampleCount)
+#define MODULE_RECORD_unload_tsc(x) ((x)->unloadTsc)
+#define MODULE_RECORD_path(x) ((x)->path)
+#define MODULE_RECORD_path_length(x) ((x)->pathLength)
+#define MODULE_RECORD_filename_offset(x) ((x)->filenameOffset)
+#define MODULE_RECORD_segment_name(x) ((x)->segmentName)
+#define MODULE_RECORD_tsc(x) ((x)->tsc)
+#define MODULE_RECORD_parent_pid(x) ((x)->parent_pid)
+#define MODULE_RECORD_osid(x) ((x)->osid)
+
+/*
+ *  Sample record.  Size can be determined by looking at the header record.
+ *  There can be up to 3 sections.  The SampleFileHeader defines the presence
+ *  of sections and their offsets. Within a sample file, all of the sample
+ *  records have the same number of sections and the same size.  However,
+ *  different sample record sections and sizes can exist in different
+ *  sample files.  Since recording counters and the time stamp counter for
+ *  each sample can be space consuming, the user can determine whether or not
+ *  this information is kept at sample collection time.
+ */
+
+typedef struct SampleRecordPC_s { // Program Counter section
+	U32 descriptor_id;
+	U32 osid; // OS identifier
+	union {
+		struct {
+			U64 iip; // IA64 interrupt instruction pointer
+			U64 ipsr; // IA64 interrupt processor status register
+		} s1;
+		struct {
+			U32 eip; // IA32 instruction pointer
+			U32 eflags; // IA32 eflags
+			CodeDescriptor csd; // IA32 code seg descriptor(8 bytes)
+		} s2;
+	} u1;
+	U16 cs; // IA32 cs (0 for IA64)
+	union {
+		U16 cpuAndOS; // cpu and OS info as one word
+		struct { // cpu and OS info broken out
+			U16 cpuNum : 12; // cpu number (0 - 4096)
+			U16 notVmid0 : 1;
+			// win95, vmid0 flag(1 means NOT vmid 0)
+			U16 codeMode : 2; // processor mode, see MODE_ defines
+			U16 uncore_valid : 1;
+			// identifies if the uncore count is valid
+		} s3;
+	} u2;
+	U32 tid; // OS thread ID  (may get reused, see tidIsRaw)
+	U32 pidRecIndex; // process ID rec index (index into start of pid
+		// record section) .. can validly be 0 if not raw
+		// (array index).  Use ReturnPid() to
+		// ..access this field .. (see pidRecIndexRaw)
+	union {
+		U32 bitFields2;
+		struct {
+			U32 mrIndex : 20;
+			// module record index (index into start of
+			// module rec section) .. (see mrIndexNone)
+			U32 eventIndex : 8; // index into the Events section
+			U32 tidIsRaw : 1; // tid is raw OS tid
+			U32 IA64PC : 1; // TRUE=this is a IA64 PC sample record
+			U32 pidRecIndexRaw : 1; // pidRecIndex is raw OS pid
+			U32 mrIndexNone : 1; // no mrIndex (unknown module)
+		} s4;
+	} u3;
+	U64 tsc; // processor timestamp counter
+} SampleRecordPC, *PSampleRecordPC;
+
+#define SAMPLE_RECORD_descriptor_id(x) ((x)->descriptor_id)
+#define SAMPLE_RECORD_osid(x) ((x)->osid)
+#define SAMPLE_RECORD_iip(x) ((x)->u1.s1.iip)
+#define SAMPLE_RECORD_ipsr(x) ((x)->u1.s1.ipsr)
+#define SAMPLE_RECORD_eip(x) ((x)->u1.s2.eip)
+#define SAMPLE_RECORD_eflags(x) ((x)->u1.s2.eflags)
+#define SAMPLE_RECORD_csd(x) ((x)->u1.s2.csd)
+#define SAMPLE_RECORD_cs(x) ((x)->cs)
+#define SAMPLE_RECORD_cpu_and_os(x) ((x)->u2.cpuAndOS)
+#define SAMPLE_RECORD_cpu_num(x) ((x)->u2.s3.cpuNum)
+#define SAMPLE_RECORD_uncore_valid(x) ((x)->u2.s3.uncore_valid)
+#define SAMPLE_RECORD_not_vmid0(x) ((x)->u2.s3.notVmid0)
+#define SAMPLE_RECORD_code_mode(x) ((x)->u2.s3.codeMode)
+#define SAMPLE_RECORD_tid(x) ((x)->tid)
+#define SAMPLE_RECORD_pid_rec_index(x) ((x)->pidRecIndex)
+#define SAMPLE_RECORD_bit_fields2(x) ((x)->u3.bitFields2)
+#define SAMPLE_RECORD_mr_index(x) ((x)->u3.s4.mrIndex)
+#define SAMPLE_RECORD_event_index(x) ((x)->u3.s4.eventIndex)
+#define SAMPLE_RECORD_tid_is_raw(x) ((x)->u3.s4.tidIsRaw)
+#define SAMPLE_RECORD_ia64_pc(x) ((x)->u3.s4.IA64PC)
+#define SAMPLE_RECORD_pid_rec_index_raw(x) ((x)->u3.s4.pidRecIndexRaw)
+#define SAMPLE_RECORD_mr_index_none(x) ((x)->u3.s4.mrIndexNone)
+#define SAMPLE_RECORD_tsc(x) ((x)->tsc)
+
+// end of SampleRecord sections
+
+/* Uncore Sample Record definition. This is a skinny sample record used by
+ * uncore boxes to record samples.
+ * The sample record consists of a descriptor id, cpu info and timestamp.
+ */
+
+typedef struct UncoreSampleRecordPC_s {
+	U32 descriptor_id;
+	U32 osid;
+	U16 cpuNum;
+	U16 pkgNum;
+	union {
+		U32 flags;
+		struct {
+			U32 uncore_valid : 1;
+			// identifies if the uncore count is valid
+			U32 reserved1 : 31;
+		} s1;
+	} u1;
+	U64 reserved2;
+	U64 tsc; // processor timestamp counter
+} UncoreSampleRecordPC, *PUnocreSampleRecordPC;
+
+#define UNCORE_SAMPLE_RECORD_descriptor_id(x) ((x)->descriptor_id)
+#define UNCORE_SAMPLE_RECORD_osid(x) ((x)->osid)
+#define UNCORE_SAMPLE_RECORD_cpu_num(x) ((x)->cpuNum)
+#define UNCORE_SAMPLE_RECORD_pkg_num(x) ((x)->pkgNum)
+#define UNCORE_SAMPLE_RECORD_uncore_valid(x) ((x)->u1.s1.uncore_valid)
+#define UNCORE_SAMPLE_RECORD_tsc(x) ((x)->tsc)
+
+// end of UncoreSampleRecord section
+
+// Definitions for user markers data
+// The instances of these structures will be written to user markers temp file
+#define MARKER_DEFAULT_TYPE "Default_Marker"
+#define MARKER_DEFAULT_ID 0
+#define MAX_MARKER_LENGTH 136
+
+#define MARK_ID 4
+#define MARK_DATA 2
+#define THREAD_INFO 8
+
+/*
+ *  Common Register descriptions
+ */
+
+/*
+ *  Bits used in the debug control register
+ */
+#define DEBUG_CTL_LBR 0x0000001
+#define DEBUG_CTL_BTF 0x0000002
+#define DEBUG_CTL_TR 0x0000040
+#define DEBUG_CTL_BTS 0x0000080
+#define DEBUG_CTL_BTINT 0x0000100
+#define DEBUG_CTL_BT_OFF_OS 0x0000200
+#define DEBUG_CTL_BTS_OFF_USR 0x0000400
+#define DEBUG_CTL_FRZ_LBR_ON_PMI 0x0000800
+#define DEBUG_CTL_FRZ_PMON_ON_PMI 0x0001000
+#define DEBUG_CTL_ENABLE_UNCORE_PMI_BIT 0x0002000
+
+#define DEBUG_CTL_NODE_lbr_get(reg) ((reg)&DEBUG_CTL_LBR)
+#define DEBUG_CTL_NODE_lbr_set(reg) ((reg) |= DEBUG_CTL_LBR)
+#define DEBUG_CTL_NODE_lbr_clear(reg) ((reg) &= ~DEBUG_CTL_LBR)
+
+#define DEBUG_CTL_NODE_btf_get(reg) ((reg)&DEBUG_CTL_BTF)
+#define DEBUG_CTL_NODE_btf_set(reg) ((reg) |= DEBUG_CTL_BTF)
+#define DEBUG_CTL_NODE_btf_clear(reg) ((reg) &= ~DEBUG_CTL_BTF)
+
+#define DEBUG_CTL_NODE_tr_get(reg) ((reg)&DEBUG_CTL_TR)
+#define DEBUG_CTL_NODE_tr_set(reg) ((reg) |= DEBUG_CTL_TR)
+#define DEBUG_CTL_NODE_tr_clear(reg) ((reg) &= ~DEBUG_CTL_TR)
+
+#define DEBUG_CTL_NODE_bts_get(reg) ((reg)&DEBUG_CTL_BTS)
+#define DEBUG_CTL_NODE_bts_set(reg) ((reg) |= DEBUG_CTL_BTS)
+#define DEBUG_CTL_NODE_bts_clear(reg) ((reg) &= ~DEBUG_CTL_BTS)
+
+#define DEBUG_CTL_NODE_btint_get(reg) ((reg)&DEBUG_CTL_BTINT)
+#define DEBUG_CTL_NODE_btint_set(reg) ((reg) |= DEBUG_CTL_BTINT)
+#define DEBUG_CTL_NODE_btint_clear(reg) ((reg) &= ~DEBUG_CTL_BTINT)
+
+#define DEBUG_CTL_NODE_bts_off_os_get(reg) ((reg)&DEBUG_CTL_BTS_OFF_OS)
+#define DEBUG_CTL_NODE_bts_off_os_set(reg) ((reg) |= DEBUG_CTL_BTS_OFF_OS)
+#define DEBUG_CTL_NODE_bts_off_os_clear(reg) ((reg) &= ~DEBUG_CTL_BTS_OFF_OS)
+
+#define DEBUG_CTL_NODE_bts_off_usr_get(reg) ((reg)&DEBUG_CTL_BTS_OFF_USR)
+#define DEBUG_CTL_NODE_bts_off_usr_set(reg) ((reg) |= DEBUG_CTL_BTS_OFF_USR)
+#define DEBUG_CTL_NODE_bts_off_usr_clear(reg) ((reg) &= ~DEBUG_CTL_BTS_OFF_USR)
+
+#define DEBUG_CTL_NODE_frz_lbr_on_pmi_get(reg) ((reg)&DEBUG_CTL_FRZ_LBR_ON_PMI)
+#define DEBUG_CTL_NODE_frz_lbr_on_pmi_set(reg)                                 \
+	((reg) |= DEBUG_CTL_FRZ_LBR_ON_PMI)
+#define DEBUG_CTL_NODE_frz_lbr_on_pmi_clear(reg)                               \
+	((reg) &= ~DEBUG_CTL_FRZ_LBR_ON_PMI)
+
+#define DEBUG_CTL_NODE_frz_pmon_on_pmi_get(reg)                                \
+	((reg)&DEBUG_CTL_FRZ_PMON_ON_PMI)
+#define DEBUG_CTL_NODE_frz_pmon_on_pmi_set(reg)                                \
+	((reg) |= DEBUG_CTL_FRZ_PMON_ON_PMI)
+#define DEBUG_CTL_NODE_frz_pmon_on_pmi_clear(reg)                              \
+	((reg) &= ~DEBUG_CTL_FRZ_PMON_ON_PMI)
+
+#define DEBUG_CTL_NODE_enable_uncore_pmi_get(reg)                              \
+	((reg)&DEBUG_CTL_ENABLE_UNCORE_PMI)
+#define DEBUG_CTL_NODE_enable_uncore_pmi_set(reg)                              \
+	((reg) |= DEBUG_CTL_ENABLE_UNCORE_PMI)
+#define DEBUG_CTL_NODE_enable_uncore_pmi_clear(reg)                            \
+	((reg) &= ~DEBUG_CTL_ENABLE_UNCORE_PMI)
+
+/*
+ * @macro SEP_VERSION_NODE_S
+ * @brief
+ * This structure supports versioning in Sep. The field major indicates major,
+ * version minor indicates the minor version and api indicates the api version
+ * for the current sep build. This structure is initialized at the time when
+ * the driver is loaded.
+ */
+
+typedef struct SEP_VERSION_NODE_S SEP_VERSION_NODE;
+typedef SEP_VERSION_NODE * SEP_VERSION;
+
+struct SEP_VERSION_NODE_S {
+	union {
+		U32 sep_version;
+		struct {
+			S32 major : 8;
+			S32 minor : 8;
+			S32 api : 8;
+			S32 update : 8;
+		} s1;
+	} u1;
+};
+
+#define SEP_VERSION_NODE_sep_version(version) ((version)->u1.sep_version)
+#define SEP_VERSION_NODE_major(version) ((version)->u1.s1.major)
+#define SEP_VERSION_NODE_minor(version) ((version)->u1.s1.minor)
+#define SEP_VERSION_NODE_api(version) ((version)->u1.s1.api)
+#define SEP_VERSION_NODE_update(version) ((version)->u1.s1.update)
+
+/*
+ *  The VTSA_SYS_INFO_STRUCT information that is shared across kernel mode
+ *  and user mode code, very specifically for tb5 file generation
+ */
+
+typedef enum {
+	GT_UNK = 0,
+	GT_PER_CPU,
+	GT_PER_CHIPSET,
+	GT_CPUID,
+	GT_NODE,
+	GT_SYSTEM,
+	GT_SAMPLE_RECORD_INFO
+} GEN_ENTRY_TYPES;
+
+typedef enum {
+	GST_UNK = 0,
+	GST_X86,
+	GST_ITANIUM,
+	GST_SA, //strong arm
+	GST_XSC,
+	GST_EM64T,
+	GST_CS860
+} GEN_ENTRY_SUBTYPES;
+
+typedef struct __fixed_size_pointer {
+	union {
+		U64 fs_force_alignment;
+		struct {
+			U32 fs_unused;
+			U32 is_ptr : 1;
+		} s1;
+	} u1;
+	union {
+		U64 fs_offset;
+		void *fs_ptr;
+	} u2;
+} VTSA_FIXED_SIZE_PTR;
+
+#define VTSA_FIXED_SIZE_PTR_is_ptr(fsp) ((fsp)->u1.s1.is_ptr)
+#define VTSA_FIXED_SIZE_PTR_fs_offset(fsp) ((fsp)->u2.fs_offset)
+#define VTSA_FIXED_SIZE_PTR_fs_ptr(fsp) ((fsp)->u2.fs_ptr)
+
+typedef struct __generic_array_header {
+	//
+	// Information realted to the generic header
+	//
+	U32 hdr_size; // size of this generic header
+		// (for versioning and real data starts
+		//  after the header)
+
+	U32 next_field_hdr_padding; // make sure next field is 8-byte aligned
+
+	//
+	// VTSA_FIXED_SIZE_PTR should always be on an 8-byte boundary...
+	//
+	// pointer to the next generic header if there is one
+	//
+	VTSA_FIXED_SIZE_PTR hdr_next_gen_hdr;
+
+	U32 hdr_reserved[7]; // padding for future use - force to 64 bytes...
+
+	//
+	// Information related to the array this header is describing
+	//
+	U32 array_num_entries;
+	U32 array_entry_size;
+	U16 array_type; // from the GEN_ENTRY_TYPES enumeration
+	U16 array_subtype; // from the GEN_ENTRY_SUBTYPES enumeration
+} VTSA_GEN_ARRAY_HDR;
+
+#define VTSA_GEN_ARRAY_HDR_hdr_size(gah) ((gah)->hdr_size)
+#define VTSA_GEN_ARRAY_HDR_hdr_next_gen_hdr(gah) ((gah)->hdr_next_gen_hdr)
+#define VTSA_GEN_ARRAY_HDR_array_num_entries(gah) ((gah)->array_num_entries)
+#define VTSA_GEN_ARRAY_HDR_array_entry_size(gah) ((gah)->array_entry_size)
+#define VTSA_GEN_ARRAY_HDR_array_type(gah) ((gah)->array_type)
+#define VTSA_GEN_ARRAY_HDR_array_subtype(gah) ((gah)->array_subtype)
+
+typedef struct __cpuid_x86 {
+	U32 cpuid_eax_input;
+	U32 cpuid_eax;
+	U32 cpuid_ebx;
+	U32 cpuid_ecx;
+	U32 cpuid_edx;
+} VTSA_CPUID_X86;
+
+#define VTSA_CPUID_X86_cpuid_eax_input(cid) ((cid)->cpuid_eax_input)
+#define VTSA_CPUID_X86_cpuid_eax(cid) ((cid)->cpuid_eax)
+#define VTSA_CPUID_X86_cpuid_ebx(cid) ((cid)->cpuid_ebx)
+#define VTSA_CPUID_X86_cpuid_ecx(cid) ((cid)->cpuid_ecx)
+#define VTSA_CPUID_X86_cpuid_edx(cid) ((cid)->cpuid_edx)
+
+typedef struct __cpuid_ipf {
+	U64 cpuid_select;
+	U64 cpuid_val;
+} VTSA_CPUID_IPF;
+
+#define VTSA_CPUID_IPF_cpuid_select(cid) ((cid)->cpuid_select)
+#define VTSA_CPUID_IPF_cpuid_val(cid) ((cid)->cpuid_val)
+
+typedef struct __generic_per_cpu {
+	//
+	// per cpu information
+	//
+	U32 cpu_number; // cpu number (as defined by the OS)
+	U32 cpu_speed_mhz; // cpu speed (in Mhz)
+	U32 cpu_fsb_mhz; // Front Side Bus speed (in Mhz) (if known)
+	U32 cpu_cache_L2;
+	// ??? USER: cpu L2 (marketing definition) cache size (if known)
+
+	//
+	// And pointer to other structures. Keep this on an 8-byte boundary
+	//
+	// "pointer" to generic array header that should contain
+	// cpuid information for this cpu
+	//
+	VTSA_FIXED_SIZE_PTR cpu_cpuid_array;
+
+	S64 cpu_tsc_offset;
+	// TSC offset from CPU 0 computed as (TSC CPU N - TSC CPU 0)
+	//
+	// intel processor number (from mkting).
+	// Currently 3 decimal digits (3xx, 5xx and 7xx)
+	//
+	U32 cpu_intel_processor_number;
+
+	U32 cpu_cache_L3;
+	// ??? USER: cpu L3 (marketing definition) cache size (if known)
+
+	U64 platform_id;
+
+	//
+	// package/mapping information
+	//
+	// The hierarchy for uniquely identifying a logical processor
+	// in a system is node number/id (from the node structure),
+	// package number, core number, and thread number.
+	// Core number is for identifying a core within a package.
+	//
+	// Actually, on Itanium getting all this information is
+	// pretty involved with complicated algorithm using PAL calls.
+	// I don't know how important all this stuff is to the user.
+	// Maybe we can just have the place holder now and figure out
+	// how to fill them later.
+	//
+	U16 cpu_package_num; // package number for this cpu (if known)
+	U16 cpu_core_num; // core number (if known)
+	U16 cpu_hw_thread_num; // hw thread number inside the core (if known)
+
+	U16 cpu_threads_per_core; // total number of h/w threads per core
+	U16 cpu_module_id; // Processor module number
+	U16 cpu_num_modules; // Number of processor modules
+	U32 cpu_core_type; // Core type for hetero
+	U32 arch_perfmon_ver;
+	U32 num_gp_counters;
+	U32 num_fixed_counters;
+	U32 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+
+} VTSA_GEN_PER_CPU;
+
+#define VTSA_GEN_PER_CPU_cpu_number(p_cpu) ((p_cpu)->cpu_number)
+#define VTSA_GEN_PER_CPU_cpu_speed_mhz(p_cpu) ((p_cpu)->cpu_speed_mhz)
+#define VTSA_GEN_PER_CPU_cpu_fsb_mhz(p_cpu) ((p_cpu)->cpu_fsb_mhz)
+#define VTSA_GEN_PER_CPU_cpu_cache_L2(p_cpu) ((p_cpu)->cpu_cache_L2)
+#define VTSA_GEN_PER_CPU_cpu_cpuid_array(p_cpu) ((p_cpu)->cpu_cpuid_array)
+#define VTSA_GEN_PER_CPU_cpu_tsc_offset(p_cpu) ((p_cpu)->cpu_tsc_offset)
+#define VTSA_GEN_PER_CPU_cpu_intel_processor_number(p_cpu)                     \
+	((p_cpu)->cpu_intel_processor_number)
+#define VTSA_GEN_PER_CPU_cpu_cache_L3(p_cpu) ((p_cpu)->cpu_cache_L3)
+#define VTSA_GEN_PER_CPU_platform_id(p_cpu) ((p_cpu)->platform_id)
+#define VTSA_GEN_PER_CPU_cpu_package_num(p_cpu) ((p_cpu)->cpu_package_num)
+#define VTSA_GEN_PER_CPU_cpu_core_num(p_cpu) ((p_cpu)->cpu_core_num)
+#define VTSA_GEN_PER_CPU_cpu_hw_thread_num(p_cpu) ((p_cpu)->cpu_hw_thread_num)
+#define VTSA_GEN_PER_CPU_cpu_threads_per_core(p_cpu)                           \
+	((p_cpu)->cpu_threads_per_core)
+#define VTSA_GEN_PER_CPU_cpu_module_num(p_cpu) ((p_cpu)->cpu_module_id)
+#define VTSA_GEN_PER_CPU_cpu_num_modules(p_cpu) ((p_cpu)->cpu_num_modules)
+#define VTSA_GEN_PER_CPU_cpu_core_type(p_cpu) ((p_cpu)->cpu_core_type)
+#define VTSA_GEN_PER_CPU_arch_perfmon_ver(p_cpu) ((p_cpu)->arch_perfmon_ver)
+#define VTSA_GEN_PER_CPU_num_gp_counters(p_cpu) ((p_cpu)->num_gp_counters)
+#define VTSA_GEN_PER_CPU_num_fixed_counters(p_cpu) ((p_cpu)->num_fixed_counters)
+
+typedef struct __node_info {
+	U32 node_type_from_shell;
+	U32 node_id; // The node number/id (if known)
+
+	U32 node_num_available; // total number cpus on this node
+	U32 node_num_used; // USER: number used based on cpu mask at time of run
+
+	U64 node_physical_memory;
+	// amount of physical memory (bytes) on this node
+
+	//
+	// pointer to the first generic header that
+	// contains the per-cpu information
+	//
+	// Keep the VTSA_FIXED_SIZE_PTR on an 8-byte boundary...
+	//
+	VTSA_FIXED_SIZE_PTR node_percpu_array;
+
+	U32 node_reserved[2]; // leave some space
+
+} VTSA_NODE_INFO;
+
+#define VTSA_NODE_INFO_node_type_from_shell(vni) ((vni)->node_type_from_shell)
+#define VTSA_NODE_INFO_node_id(vni) ((vni)->node_id)
+#define VTSA_NODE_INFO_node_num_available(vni) ((vni)->node_num_available)
+#define VTSA_NODE_INFO_node_num_used(vni) ((vni)->node_num_used)
+#define VTSA_NODE_INFO_node_physical_memory(vni) ((vni)->node_physical_memory)
+#define VTSA_NODE_INFO_node_percpu_array(vni) ((vni)->node_percpu_array)
+
+typedef struct __sys_info {
+	//
+	// Keep this on an 8-byte boundary
+	//
+	VTSA_FIXED_SIZE_PTR node_array; // the per-node information
+
+	U64 min_app_address;
+	// USER: lower allowed user space address (if known)
+	U64 max_app_address;
+	// USER: upper allowed user space address (if known)
+	U32 page_size; // Current page size
+	U32 allocation_granularity;
+	// USER: Granularity of allocation requests (if known)
+	U32 reserved1; // added for future fields
+	U32 reserved2; // alignment purpose
+	U64 reserved3[3]; // added for future fields
+
+} VTSA_SYS_INFO;
+
+#define VTSA_SYS_INFO_node_array(sys_info) ((sys_info)->node_array)
+#define VTSA_SYS_INFO_min_app_address(sys_info) ((sys_info)->min_app_address)
+#define VTSA_SYS_INFO_max_app_address(sys_info) ((sys_info)->max_app_address)
+#define VTSA_SYS_INFO_page_size(sys_info) ((sys_info)->page_size)
+#define VTSA_SYS_INFO_allocation_granularity(sys_info)                         \
+	((sys_info)->allocation_granularity)
+
+typedef struct DRV_TOPOLOGY_INFO_NODE_S DRV_TOPOLOGY_INFO_NODE;
+typedef DRV_TOPOLOGY_INFO_NODE * DRV_TOPOLOGY_INFO;
+
+struct DRV_TOPOLOGY_INFO_NODE_S {
+	U32 cpu_number; // cpu number (as defined by the OS)
+	U16 cpu_package_num; // package number for this cpu (if known)
+	U16 cpu_core_num; // core number (if known)
+	U16 cpu_hw_thread_num; // T0 or T1 if HT enabled
+	U16 reserved1;
+	S32 socket_master;
+	S32 core_master;
+	S32 thr_master;
+	U32 cpu_module_num;
+	U32 cpu_module_master;
+	U32 cpu_num_modules;
+	U32 cpu_core_type;
+	U32 arch_perfmon_ver;
+	U32 num_gp_counters;
+	U32 num_fixed_counters;
+	U32 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define DRV_TOPOLOGY_INFO_cpu_number(dti) ((dti)->cpu_number)
+#define DRV_TOPOLOGY_INFO_cpu_package_num(dti) ((dti)->cpu_package_num)
+#define DRV_TOPOLOGY_INFO_cpu_core_num(dti) ((dti)->cpu_core_num)
+#define DRV_TOPOLOGY_INFO_socket_master(dti) ((dti)->socket_master)
+#define DRV_TOPOLOGY_INFO_core_master(dti) ((dti)->core_master)
+#define DRV_TOPOLOGY_INFO_thr_master(dti) ((dti)->thr_master)
+#define DRV_TOPOLOGY_INFO_cpu_hw_thread_num(dti) ((dti)->cpu_hw_thread_num)
+#define DRV_TOPOLOGY_INFO_cpu_module_num(dti) ((dti)->cpu_module_num)
+#define DRV_TOPOLOGY_INFO_cpu_module_master(dti) ((dti)->cpu_module_master)
+#define DRV_TOPOLOGY_INFO_cpu_num_modules(dti) ((dti)->cpu_num_modules)
+#define DRV_TOPOLOGY_INFO_cpu_core_type(dti) ((dti)->cpu_core_type)
+#define DRV_TOPOLOGY_INFO_arch_perfmon_ver(dti) ((dti)->arch_perfmon_ver)
+#define DRV_TOPOLOGY_INFO_num_gp_counters(dti) ((dti)->num_gp_counters)
+#define DRV_TOPOLOGY_INFO_num_fixed_counters(dti) ((dti)->num_fixed_counters)
+
+#define VALUE_TO_BE_DISCOVERED 0
+
+// dimm information
+typedef struct DRV_DIMM_INFO_NODE_S DRV_DIMM_INFO_NODE;
+typedef DRV_DIMM_INFO_NODE * DRV_DIMM_INFO;
+
+struct DRV_DIMM_INFO_NODE_S {
+	U32 platform_id;
+	U32 channel_num;
+	U32 rank_num;
+	U32 value;
+	U8 mc_num;
+	U8 dimm_valid;
+	U8 valid_value;
+	U8 rank_value;
+	U8 density_value;
+	U8 width_value;
+	U16 socket_num;
+	U64 reserved1;
+	U64 reserved2;
+};
+
+#define DRV_DIMM_INFO_platform_id(di) ((di)->platform_id)
+#define DRV_DIMM_INFO_channel_num(di) ((di)->channel_num)
+#define DRV_DIMM_INFO_rank_num(di) ((di)->rank_num)
+#define DRV_DIMM_INFO_value(di) ((di)->value)
+#define DRV_DIMM_INFO_mc_num(di) ((di)->mc_num)
+#define DRV_DIMM_INFO_dimm_valid(di) ((di)->dimm_valid)
+#define DRV_DIMM_INFO_valid_value(di) ((di)->valid_value)
+#define DRV_DIMM_INFO_rank_value(di) ((di)->rank_value)
+#define DRV_DIMM_INFO_density_value(di) ((di)->density_value)
+#define DRV_DIMM_INFO_width_value(di) ((di)->width_value)
+#define DRV_DIMM_INFO_socket_num(di) ((di)->socket_num)
+
+//platform information. need to get from driver
+#define MAX_PACKAGES 16
+#define MAX_CHANNELS 8
+#define MAX_RANKS 3
+
+typedef struct DRV_PLATFORM_INFO_NODE_S DRV_PLATFORM_INFO_NODE;
+typedef DRV_PLATFORM_INFO_NODE * DRV_PLATFORM_INFO;
+
+struct DRV_PLATFORM_INFO_NODE_S {
+	U64 info; // platform info
+	U64 ddr_freq_index; // freq table index
+	U8 misc_valid; // misc enabled valid bit
+	U8 reserved1; // added for alignment purpose
+	U16 reserved2;
+	U32 vmm_timer_freq; // timer frequency from VMM on SoFIA (in HZ)
+	U64 misc_info; // misc enabled info
+	U64 ufs_freq; // ufs frequency (HSX only)
+	DRV_DIMM_INFO_NODE dimm_info[MAX_PACKAGES * MAX_CHANNELS * MAX_RANKS];
+	U64 energy_multiplier; // Value of energy multiplier
+	U64 reserved3;
+	U64 reserved4;
+	U64 reserved5;
+	U64 reserved6;
+};
+
+#define DRV_PLATFORM_INFO_info(data) ((data)->info)
+#define DRV_PLATFORM_INFO_ddr_freq_index(data) ((data)->ddr_freq_index)
+#define DRV_PLATFORM_INFO_misc_valid(data) ((data)->misc_valid)
+#define DRV_PLATFORM_INFO_misc_info(data) ((data)->misc_info)
+#define DRV_PLATFORM_INFO_ufs_freq(data) ((data)->ufs_freq)
+#define DRV_PLATFORM_INFO_dimm_info(data) ((data)->dimm_info)
+#define DRV_PLATFORM_INFO_energy_multiplier(data) ((data)->energy_multiplier)
+#define DRV_PLATFORM_INFO_vmm_timer_freq(data) ((data)->vmm_timer_freq)
+
+//platform information. need to get from Platform picker
+typedef struct PLATFORM_FREQ_INFO_NODE_S PLATFORM_FREQ_INFO_NODE;
+typedef PLATFORM_FREQ_INFO_NODE * PLATFORM_FREQ_INFO;
+
+struct PLATFORM_FREQ_INFO_NODE_S {
+	float multiplier; // freq multiplier
+	double *table; // freq table
+	U32 table_size; // freq table size
+	U64 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+#define PLATFORM_FREQ_INFO_multiplier(data) ((data)->multiplier)
+#define PLATFORM_FREQ_INFO_table(data) ((data)->table)
+#define PLATFORM_FREQ_INFO_table_size(data) ((data)->table_size)
+
+typedef struct DEVICE_INFO_NODE_S DEVICE_INFO_NODE;
+typedef DEVICE_INFO_NODE * DEVICE_INFO; //NEEDED in PP
+
+struct DEVICE_INFO_NODE_S {
+	S8 *dll_name;
+	PVOID dll_handle;
+	S8 *cpu_name;
+	S8 *pmu_name;
+	DRV_STCHAR *event_db_file_name;
+	//PLATFORM_IDENTITY plat_identity;
+	// is undefined right now. Please take this as structure containing U64
+	U32 plat_type;
+	// device type (e.g., DEVICE_INFO_CORE, etc. ... see enum below)
+	U32 plat_sub_type;
+	// cti_type (e.g., CTI_Sandybridge, etc., ... see env_info_types.h)
+	S32 dispatch_id;
+	// this will be set in user mode dlls and will be unique across all
+	// IPF, IA32 (including MIDS).
+	ECB *ecb;
+	EVENT_CONFIG ec;
+	DEV_CONFIG pcfg;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 num_of_groups;
+	U32 size_of_alloc; // size of each event control block
+	PVOID drv_event;
+	U32 num_events;
+	U32 event_id_index;
+	// event id index of device
+	// (basically how many events processed before this device)
+	U32 num_counters;
+	U32 group_index;
+	U32 num_packages;
+	U32 num_units;
+	U32 device_type;
+	U32 core_type;
+	U32 pmu_clone_id; // cti_type of platform to impersonate in device DLLs
+	U32 device_scope;
+	U32 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+};
+
+#define MAX_EVENT_NAME_LENGTH 256
+
+#define DEVICE_INFO_dll_name(pdev) ((pdev)->dll_name)
+#define DEVICE_INFO_dll_handle(pdev) ((pdev)->dll_handle)
+#define DEVICE_INFO_cpu_name(pdev) ((pdev)->cpu_name)
+#define DEVICE_INFO_pmu_name(pdev) ((pdev)->pmu_name)
+#define DEVICE_INFO_event_db_file_name(pdev) ((pdev)->event_db_file_name)
+#define DEVICE_INFO_plat_type(pdev) ((pdev)->plat_type)
+#define DEVICE_INFO_plat_sub_type(pdev) ((pdev)->plat_sub_type)
+#define DEVICE_INFO_pmu_clone_id(pdev) ((pdev)->pmu_clone_id)
+#define DEVICE_INFO_dispatch_id(pdev) ((pdev)->dispatch_id)
+#define DEVICE_INFO_ecb(pdev) ((pdev)->ecb)
+#define DEVICE_INFO_ec(pdev) ((pdev)->ec)
+#define DEVICE_INFO_pcfg(pdev) ((pdev)->pcfg)
+#define DEVICE_INFO_pcfg_unc(pdev) ((pdev)->pcfg_unc)
+#define DEVICE_INFO_num_groups(pdev) ((pdev)->num_of_groups)
+#define DEVICE_INFO_size_of_alloc(pdev) ((pdev)->size_of_alloc)
+#define DEVICE_INFO_drv_event(pdev) ((pdev)->drv_event)
+#define DEVICE_INFO_num_events(pdev) ((pdev)->num_events)
+#define DEVICE_INFO_event_id_index(pdev) ((pdev)->event_id_index)
+#define DEVICE_INFO_num_counters(pdev) ((pdev)->num_counters)
+#define DEVICE_INFO_group_index(pdev) ((pdev)->group_index)
+#define DEVICE_INFO_num_packages(pdev) ((pdev)->num_packages)
+#define DEVICE_INFO_num_units(pdev) ((pdev)->num_units)
+#define DEVICE_INFO_device_type(pdev) ((pdev)->device_type)
+#define DEVICE_INFO_core_type(pdev) ((pdev)->core_type)
+#define DEVICE_INFO_device_scope(pdev) ((pdev)->device_scope)
+
+typedef struct DEVICE_INFO_DATA_NODE_S DEVICE_INFO_DATA_NODE;
+typedef DEVICE_INFO_DATA_NODE * DEVICE_INFO_DATA; //NEEDED in PP
+
+struct DEVICE_INFO_DATA_NODE_S {
+	DEVICE_INFO pdev_info;
+	U32 num_elements;
+	U32 num_allocated;
+	U64 reserved1;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define DEVICE_INFO_DATA_pdev_info(d) ((d)->pdev_info)
+#define DEVICE_INFO_DATA_num_elements(d) ((d)->num_elements)
+#define DEVICE_INFO_DATA_num_allocated(d) ((d)->num_allocated)
+
+typedef enum {
+	DEVICE_INFO_CORE = 0,
+	DEVICE_INFO_UNCORE = 1,
+	DEVICE_INFO_CHIPSET = 2,
+	DEVICE_INFO_GFX = 3,
+	DEVICE_INFO_PWR = 4,
+	DEVICE_INFO_TELEMETRY = 5
+} DEVICE_INFO_TYPE;
+
+typedef enum {
+	INVALID_TERMINATE_TYPE = 0,
+	STOP_TERMINATE,
+	CANCEL_TERMINATE
+} ABNORMAL_TERMINATE_TYPE;
+
+typedef enum {
+	DEVICE_SCOPE_PACKAGE = 0,
+	DEVICE_SCOPE_SYSTEM = 1
+} DEVICE_SCOPE_TYPE;
+
+typedef struct PCIFUNC_INFO_NODE_S PCIFUNC_INFO_NODE;
+typedef PCIFUNC_INFO_NODE * PCIFUNC_INFO;
+
+struct PCIFUNC_INFO_NODE_S {
+	U32 valid;
+	U32 num_entries;
+	// the number of entries found with same <dev_no, func_no>
+	// but difference bus_no.
+	U64 deviceId;
+	U64 reserved1;
+	U64 reserved2;
+};
+
+#define PCIFUNC_INFO_NODE_funcno(x) ((x)->funcno)
+#define PCIFUNC_INFO_NODE_valid(x) ((x)->valid)
+#define PCIFUNC_INFO_NODE_deviceId(x) ((x)->deviceId)
+#define PCIFUNC_INFO_NODE_num_entries(x) ((x)->num_entries)
+
+typedef struct PCIDEV_INFO_NODE_S PCIDEV_INFO_NODE;
+typedef PCIDEV_INFO_NODE * PCIDEV_INFO;
+
+struct PCIDEV_INFO_NODE_S {
+	PCIFUNC_INFO_NODE func_info[MAX_PCI_FUNCNO];
+	U32 valid;
+	U32 dispatch_id;
+	U64 reserved1;
+	U64 reserved2;
+};
+
+#define PCIDEV_INFO_NODE_func_info(x, i) ((x).func_info[i])
+#define PCIDEV_INFO_NODE_valid(x) ((x).valid)
+
+typedef struct UNCORE_PCIDEV_NODE_S UNCORE_PCIDEV_NODE;
+
+struct UNCORE_PCIDEV_NODE_S {
+	PCIDEV_INFO_NODE pcidev[MAX_PCI_DEVNO];
+	U32 dispatch_id;
+	U32 scan;
+	U32 num_uncore_units;
+	U32 num_deviceid_entries;
+	U8 dimm_device1;
+	U8 dimm_device2;
+	U16 reserved1;
+	U32 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+	U32 deviceid_list[MAX_PCI_DEVNO];
+};
+
+// Structure used to perform uncore device discovery
+
+typedef struct UNCORE_TOPOLOGY_INFO_NODE_S UNCORE_TOPOLOGY_INFO_NODE;
+typedef UNCORE_TOPOLOGY_INFO_NODE * UNCORE_TOPOLOGY_INFO;
+
+struct UNCORE_TOPOLOGY_INFO_NODE_S {
+	UNCORE_PCIDEV_NODE device[MAX_DEVICES];
+};
+
+#define UNCORE_TOPOLOGY_INFO_device(x, dev_index) ((x)->device[dev_index])
+#define UNCORE_TOPOLOGY_INFO_device_dispatch_id(x, dev_index)                  \
+	((x)->device[dev_index].dispatch_id)
+#define UNCORE_TOPOLOGY_INFO_device_scan(x, dev_index)                         \
+	((x)->device[dev_index].scan)
+#define UNCORE_TOPOLOGY_INFO_pcidev_valid(x, dev_index, devno)                 \
+	((x)->device[dev_index].pcidev[devno].valid)
+#define UNCORE_TOPOLOGY_INFO_pcidev_dispatch_id(x, dev_index, devno)           \
+	((x)->device[dev_index].pcidev[devno].dispatch_id)
+#define UNCORE_TOPOLOGY_INFO_pcidev(x, dev_index, devno)                       \
+	((x)->device[dev_index].pcidev[devno])
+#define UNCORE_TOPOLOGY_INFO_num_uncore_units(x, dev_index)                    \
+	((x)->device[dev_index].num_uncore_units)
+#define UNCORE_TOPOLOGY_INFO_num_deviceid_entries(x, dev_index)                \
+	((x)->device[dev_index].num_deviceid_entries)
+#define UNCORE_TOPOLOGY_INFO_dimm_device1(x, dev_index)                        \
+	((x)->device[dev_index].dimm_device1)
+#define UNCORE_TOPOLOGY_INFO_dimm_device2(x, dev_index)                        \
+	((x)->device[dev_index].dimm_device2)
+#define UNCORE_TOPOLOGY_INFO_deviceid(x, dev_index, deviceid_idx)              \
+	((x)->device[dev_index].deviceid_list[deviceid_idx])
+#define UNCORE_TOPOLOGY_INFO_pcidev_set_funcno_valid(x, dev_index, devno,      \
+						     funcno)                   \
+	((x)->device[dev_index].pcidev[devno].func_info[funcno].valid = 1)
+#define UNCORE_TOPOLOGY_INFO_pcidev_is_found_in_platform(x, dev_index, devno,  \
+							 funcno)               \
+	((x)->device[dev_index].pcidev[devno].func_info[funcno].num_entries)
+#define UNCORE_TOPOLOGY_INFO_pcidev_is_devno_funcno_valid(x, dev_index, devno, \
+							  funcno)              \
+	((x)->device[dev_index].pcidev[devno].func_info[funcno].valid ? TRUE : \
+									FALSE)
+#define UNCORE_TOPOLOGY_INFO_pcidev_is_device_found(x, dev_index, devno,       \
+						    funcno)                    \
+	((x)->device[dev_index].pcidev[devno].func_info[funcno].num_entries > 0)
+
+#define UNCORE_TOPOLOGY_INFO_pcidev_num_entries_found(x, dev_index, devno,     \
+						      funcno)                  \
+	((x)->device[dev_index].pcidev[devno].func_info[funcno].num_entries)
+
+typedef enum {
+	CORE_TOPOLOGY_NODE = 0,
+	UNCORE_TOPOLOGY_NODE_IMC = 1,
+	UNCORE_TOPOLOGY_NODE_UBOX = 2,
+	UNCORE_TOPOLOGY_NODE_QPI = 3,
+	MAX_TOPOLOGY_DEV = 4,
+	// When you adding new topo node to this enum,
+	// make sue MAX_TOPOLOGY_DEV is always the last one.
+} UNCORE_TOPOLOGY_NODE_INDEX_TYPE;
+
+typedef struct PLATFORM_TOPOLOGY_REG_NODE_S PLATFORM_TOPOLOGY_REG_NODE;
+typedef PLATFORM_TOPOLOGY_REG_NODE * PLATFORM_TOPOLOGY_REG;
+
+struct PLATFORM_TOPOLOGY_REG_NODE_S {
+	U32 bus;
+	U32 device;
+	U32 function;
+	U32 reg_id;
+	U64 reg_mask;
+	U64 reg_value[MAX_PACKAGES];
+	U8 reg_type;
+	U8 device_valid;
+	U16 reserved1;
+	U32 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define PLATFORM_TOPOLOGY_REG_bus(x, i) ((x)[(i)].bus)
+#define PLATFORM_TOPOLOGY_REG_device(x, i) ((x)[(i)].device)
+#define PLATFORM_TOPOLOGY_REG_function(x, i) ((x)[(i)].function)
+#define PLATFORM_TOPOLOGY_REG_reg_id(x, i) ((x)[(i)].reg_id)
+#define PLATFORM_TOPOLOGY_REG_reg_mask(x, i) ((x)[(i)].reg_mask)
+#define PLATFORM_TOPOLOGY_REG_reg_type(x, i) ((x)[(i)].reg_type)
+#define PLATFORM_TOPOLOGY_REG_device_valid(x, i) ((x)[(i)].device_valid)
+#define PLATFORM_TOPOLOGY_REG_reg_value(x, i, package_no)                      \
+	((x)[(i)].reg_value[package_no])
+
+typedef struct PLATFORM_TOPOLOGY_DISCOVERY_NODE_S
+	PLATFORM_TOPOLOGY_DISCOVERY_NODE;
+typedef PLATFORM_TOPOLOGY_DISCOVERY_NODE * PLATFORM_TOPOLOGY_DISCOVERY;
+
+struct PLATFORM_TOPOLOGY_DISCOVERY_NODE_S {
+	U32 device_index;
+	U32 device_id;
+	U32 num_registers;
+	U8 scope;
+	U8 prog_valid;
+	U16 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+	U64 reserved5;
+	PLATFORM_TOPOLOGY_REG_NODE topology_regs[MAX_REGS];
+};
+
+//Structure used to discover the uncore device topology_device
+
+typedef struct PLATFORM_TOPOLOGY_PROG_NODE_S PLATFORM_TOPOLOGY_PROG_NODE;
+typedef PLATFORM_TOPOLOGY_PROG_NODE * PLATFORM_TOPOLOGY_PROG;
+
+struct PLATFORM_TOPOLOGY_PROG_NODE_S {
+	U32 num_devices;
+	PLATFORM_TOPOLOGY_DISCOVERY_NODE topology_device[MAX_TOPOLOGY_DEV];
+};
+
+#define PLATFORM_TOPOLOGY_PROG_num_devices(x) ((x)->num_devices)
+#define PLATFORM_TOPOLOGY_PROG_topology_device(x, dev_index)                   \
+	((x)->topology_device[dev_index])
+#define PLATFORM_TOPOLOGY_PROG_topology_device_device_index(x, dev_index)      \
+	((x)->topology_device[dev_index].device_index)
+#define PLATFORM_TOPOLOGY_PROG_topology_device_device_id(x, dev_index)         \
+	((x)->topology_device[dev_index].device_id)
+#define PLATFORM_TOPOLOGY_PROG_topology_device_scope(x, dev_index)             \
+	((x)->topology_device[dev_index].scope)
+#define PLATFORM_TOPOLOGY_PROG_topology_device_num_registers(x, dev_index)     \
+	((x)->topology_device[dev_index].num_registers)
+#define PLATFORM_TOPOLOGY_PROG_topology_device_prog_valid(x, dev_index)        \
+	((x)->topology_device[dev_index].prog_valid)
+#define PLATFORM_TOPOLOGY_PROG_topology_topology_regs(x, dev_index)            \
+	((x)->topology_device[dev_index].topology_regs)
+
+typedef struct FPGA_GB_DISCOVERY_NODE_S FPGA_GB_DISCOVERY_NODE;
+
+struct FPGA_GB_DISCOVERY_NODE_S {
+	U16 bar_num;
+	U16 feature_id;
+	U32 device_id;
+	U64 afu_id_l;
+	U64 afu_id_h;
+	U32 feature_offset;
+	U32 feature_len;
+	U8 scan;
+	U8 valid;
+	U16 reserved1;
+	U32 reserved2;
+};
+
+typedef struct FPGA_GB_DEV_NODE_S FPGA_GB_DEV_NODE;
+typedef FPGA_GB_DEV_NODE * FPGA_GB_DEV;
+
+struct FPGA_GB_DEV_NODE_S {
+	U32 num_devices;
+	FPGA_GB_DISCOVERY_NODE fpga_gb_device[MAX_DEVICES];
+};
+
+#define FPGA_GB_DEV_num_devices(x) ((x)->num_devices)
+#define FPGA_GB_DEV_device(x, dev_index) ((x)->fpga_gb_device[dev_index])
+#define FPGA_GB_DEV_bar_num(x, dev_index)                                      \
+	((x)->fpga_gb_device[dev_index].bar_num)
+#define FPGA_GB_DEV_feature_id(x, dev_index)                                   \
+	((x)->fpga_gb_device[dev_index].feature_id)
+#define FPGA_GB_DEV_device_id(x, dev_index)                                    \
+	((x)->fpga_gb_device[dev_index].device_id)
+#define FPGA_GB_DEV_afu_id_low(x, dev_index)                                   \
+	((x)->fpga_gb_device[dev_index].afu_id_l)
+#define FPGA_GB_DEV_afu_id_high(x, dev_index)                                  \
+	((x)->fpga_gb_device[dev_index].afu_id_h)
+#define FPGA_GB_DEV_feature_offset(x, dev_index)                               \
+	((x)->fpga_gb_device[dev_index].feature_offset)
+#define FPGA_GB_DEV_feature_len(x, dev_index)                                  \
+	((x)->fpga_gb_device[dev_index].feature_len)
+#define FPGA_GB_DEV_scan(x, dev_index) ((x)->fpga_gb_device[dev_index].scan)
+#define FPGA_GB_DEV_valid(x, dev_index) ((x)->fpga_gb_device[dev_index].valid)
+
+typedef enum {
+	UNCORE_TOPOLOGY_INFO_NODE_IMC = 0,
+	UNCORE_TOPOLOGY_INFO_NODE_QPILL = 1,
+	UNCORE_TOPOLOGY_INFO_NODE_HA = 2,
+	UNCORE_TOPOLOGY_INFO_NODE_R3 = 3,
+	UNCORE_TOPOLOGY_INFO_NODE_R2 = 4,
+	UNCORE_TOPOLOGY_INFO_NODE_IRP = 5,
+	UNCORE_TOPOLOGY_INFO_NODE_IMC_UCLK = 6,
+	UNCORE_TOPOLOGY_INFO_NODE_EDC_ECLK = 7,
+	UNCORE_TOPOLOGY_INFO_NODE_EDC_UCLK = 8,
+	UNCORE_TOPOLOGY_INFO_NODE_M2M = 9,
+	UNCORE_TOPOLOGY_INFO_NODE_HFI_RXE = 10,
+	UNCORE_TOPOLOGY_INFO_NODE_HFI_TXE = 11,
+	UNCORE_TOPOLOGY_INFO_NODE_FPGA_CACHE = 12,
+	UNCORE_TOPOLOGY_INFO_NODE_FPGA_FAB = 13,
+	UNCORE_TOPOLOGY_INFO_NODE_FPGA_THERMAL = 14,
+	UNCORE_TOPOLOGY_INFO_NODE_FPGA_POWER = 15,
+} UNCORE_TOPOLOGY_INFO_NODE_INDEX_TYPE;
+
+typedef struct SIDEBAND_INFO_NODE_S SIDEBAND_INFO_NODE;
+typedef SIDEBAND_INFO_NODE * SIDEBAND_INFO;
+
+struct SIDEBAND_INFO_NODE_S {
+	U32 tid;
+	U32 pid;
+	U64 tsc;
+};
+
+#define SIDEBAND_INFO_pid(x) ((x)->pid)
+#define SIDEBAND_INFO_tid(x) ((x)->tid)
+#define SIDEBAND_INFO_tsc(x) ((x)->tsc)
+
+typedef struct SAMPLE_DROP_NODE_S SAMPLE_DROP_NODE;
+typedef SAMPLE_DROP_NODE * SAMPLE_DROP;
+
+struct SAMPLE_DROP_NODE_S {
+	U32 os_id;
+	U32 cpu_id;
+	U32 sampled;
+	U32 dropped;
+};
+
+#define SAMPLE_DROP_os_id(x) ((x)->os_id)
+#define SAMPLE_DROP_cpu_id(x) ((x)->cpu_id)
+#define SAMPLE_DROP_sampled(x) ((x)->sampled)
+#define SAMPLE_DROP_dropped(x) ((x)->dropped)
+
+#define MAX_SAMPLE_DROP_NODES 20
+
+typedef struct SAMPLE_DROP_INFO_NODE_S SAMPLE_DROP_INFO_NODE;
+typedef SAMPLE_DROP_INFO_NODE * SAMPLE_DROP_INFO;
+
+struct SAMPLE_DROP_INFO_NODE_S {
+	U32 size;
+	SAMPLE_DROP_NODE drop_info[MAX_SAMPLE_DROP_NODES];
+};
+
+#define SAMPLE_DROP_INFO_size(x) ((x)->size)
+#define SAMPLE_DROP_INFO_drop_info(x, index) ((x)->drop_info[index])
+
+#define IS_PEBS_SAMPLE_RECORD(sample_record)                                   \
+	((SAMPLE_RECORD_pid_rec_index(sample_record) == (U32)-1) &&            \
+	 (SAMPLE_RECORD_tid(sample_record) == (U32)-1))
+
+/*
+ *  VMM vendor information
+ */
+#define KVM_SIGNATURE "KVMKVMKVM\0\0\0"
+#define XEN_SIGNATURE "XenVMMXenVMM"
+#define VMWARE_SIGNATURE "VMwareVMware"
+#define HYPERV_SIGNATURE "Microsoft Hv"
+
+#define DRV_VMM_UNKNOWN 0
+#define DRV_VMM_MOBILEVISOR 1
+#define DRV_VMM_KVM 2
+#define DRV_VMM_XEN 3
+#define DRV_VMM_HYPERV 4
+#define DRV_VMM_VMWARE 5
+#define DRV_VMM_ACRN 6
+
+/*
+ * @macro DRV_SETUP_INFO_NODE_S
+ * @brief
+ * This structure supports driver information such as NMI profiling mode.
+ */
+
+typedef struct DRV_SETUP_INFO_NODE_S DRV_SETUP_INFO_NODE;
+typedef DRV_SETUP_INFO_NODE * DRV_SETUP_INFO;
+
+struct DRV_SETUP_INFO_NODE_S {
+	union {
+		U64 modes;
+		struct {
+			U64 nmi_mode : 1;
+			U64 vmm_mode : 1;
+			U64 vmm_vendor : 8;
+			U64 vmm_guest_vm : 1;
+			U64 pebs_accessible : 1;
+			U64 cpu_hotplug_mode : 1;
+			U64 matrix_inaccessible : 1;
+			U64 page_table_isolation : 2;
+			U64 pebs_ignored_by_pti : 1;
+			U64 reserved1 : 47;
+		} s1;
+	} u1;
+	U64 reserved2;
+	U64 reserved3;
+	U64 reserved4;
+};
+
+#define DRV_SETUP_INFO_nmi_mode(info) ((info)->u1.s1.nmi_mode)
+#define DRV_SETUP_INFO_vmm_mode(info) ((info)->u1.s1.vmm_mode)
+#define DRV_SETUP_INFO_vmm_vendor(info) ((info)->u1.s1.vmm_vendor)
+#define DRV_SETUP_INFO_vmm_guest_vm(info) ((info)->u1.s1.vmm_guest_vm)
+#define DRV_SETUP_INFO_pebs_accessible(info) ((info)->u1.s1.pebs_accessible)
+#define DRV_SETUP_INFO_cpu_hotplug_mode(info) ((info)->u1.s1.cpu_hotplug_mode)
+#define DRV_SETUP_INFO_matrix_inaccessible(info)                               \
+	((info)->u1.s1.matrix_inaccessible)
+#define DRV_SETUP_INFO_page_table_isolation(info)                              \
+	((info)->u1.s1.page_table_isolation)
+#define DRV_SETUP_INFO_pebs_ignored_by_pti(info)                               \
+	((info)->u1.s1.pebs_ignored_by_pti)
+
+#define DRV_SETUP_INFO_PTI_DISABLED 0
+#define DRV_SETUP_INFO_PTI_KPTI 1
+#define DRV_SETUP_INFO_PTI_KAISER 2
+#define DRV_SETUP_INFO_PTI_VA_SHADOW 3
+#define DRV_SETUP_INFO_PTI_UNKNOWN 4
+
+/*
+  Type: task_info_t
+  Description:
+	  Represents the equivalent of a Linux Thread.
+  Fields:
+	  o  id: A unique identifier. May be `NULL_TASK_ID`.
+	  o  name: Human-readable name for this task
+	  o  executable_name: Literal path to the binary elf that this task's
+			  entry point is executing from.
+	  o  address_space_id: The unique ID for the address space this task is
+			  running in.
+  */
+struct task_info_node_s {
+	U64 id;
+	char name[32];
+	U64 address_space_id;
+};
+
+/*
+  Type: REMOTE_SWITCH
+  Description:
+	  Collection switch set on target
+*/
+typedef struct REMOTE_SWITCH_NODE_S REMOTE_SWITCH_NODE;
+typedef REMOTE_SWITCH_NODE * REMOTE_SWITCH;
+
+struct REMOTE_SWITCH_NODE_S {
+	U32 auto_mode : 1;
+	U32 adv_hotspot : 1;
+	U32 lbr_callstack : 2;
+	U32 full_pebs : 1;
+	U32 uncore_supported : 1;
+	U32 agent_mode : 2;
+	U32 sched_switch_enabled : 1;
+	U32 data_transfer_mode : 1;
+	U32 reserved1 : 22;
+	U32 reserved2;
+};
+
+#define REMOTE_SWITCH_auto_mode(x) ((x).auto_mode)
+#define REMOTE_SWITCH_adv_hotspot(x) ((x).adv_hotspot)
+#define REMOTE_SWITCH_lbr_callstack(x) ((x).lbr_callstack)
+#define REMOTE_SWITCH_full_pebs(x) ((x).full_pebs)
+#define REMOTE_SWITCH_uncore_supported(x) ((x).uncore_supported)
+#define REMOTE_SWITCH_agent_mode(x) ((x).agent_mode)
+#define REMOTE_SWITCH_sched_switch_enabled(x) ((x).sched_switch_enabled)
+#define REMOTE_SWITCH_data_transfer_mode(x) ((x).data_transfer_mode)
+
+/*
+  Type: REMOTE_OS_INFO
+  Description:
+	  Remote target OS system information
+*/
+#define OSINFOLEN 64
+typedef struct REMOTE_OS_INFO_NODE_S REMOTE_OS_INFO_NODE;
+typedef REMOTE_OS_INFO_NODE * REMOTE_OS_INFO;
+
+struct REMOTE_OS_INFO_NODE_S {
+	U32 os_family;
+	U32 reserved1;
+	S8 sysname[OSINFOLEN];
+	S8 release[OSINFOLEN];
+	S8 version[OSINFOLEN];
+};
+
+#define REMOTE_OS_INFO_os_family(x) ((x).os_family)
+#define REMOTE_OS_INFO_sysname(x) ((x).sysname)
+#define REMOTE_OS_INFO_release(x) ((x).release)
+#define REMOTE_OS_INFO_version(x) ((x).version)
+
+/*
+  Type: REMOTE_HARDWARE_INFO
+  Description:
+	  Remote target hardware information
+*/
+typedef struct REMOTE_HARDWARE_INFO_NODE_S REMOTE_HARDWARE_INFO_NODE;
+typedef REMOTE_HARDWARE_INFO_NODE * REMOTE_HARDWARE_INFO;
+
+struct REMOTE_HARDWARE_INFO_NODE_S {
+	U32 num_cpus;
+	U32 family;
+	U32 model;
+	U32 stepping;
+	U64 tsc_freq;
+	U64 reserved2;
+	U64 reserved3;
+};
+
+#define REMOTE_HARDWARE_INFO_num_cpus(x) ((x).num_cpus)
+#define REMOTE_HARDWARE_INFO_family(x) ((x).family)
+#define REMOTE_HARDWARE_INFO_model(x) ((x).model)
+#define REMOTE_HARDWARE_INFO_stepping(x) ((x).stepping)
+#define REMOTE_HARDWARE_INFO_tsc_frequency(x) ((x).tsc_freq)
+
+/*
+  Type: SEP_AGENT_MODE
+  Description:
+	  SEP mode on target agent
+*/
+typedef enum {
+	NATIVE_AGENT = 0,
+	HOST_VM_AGENT, // Service OS in ACRN
+	GUEST_VM_AGENT // User OS in ACRN
+} SEP_AGENT_MODE;
+
+/*
+  Type: DATA_TRANSFER_MODE
+  Description:
+	 Data transfer mode from target agent to remote host
+*/
+typedef enum {
+	IMMEDIATE_TRANSFER = 0,
+	DELAYED_TRANSFER // Send after collection is done
+} DATA_TRANSFER_MODE;
+
+#define MAX_NUM_OS_ALLOWED 6
+#define TARGET_IP_NAMELEN 64
+
+typedef struct TARGET_INFO_NODE_S TARGET_INFO_NODE;
+typedef TARGET_INFO_NODE * TARGET_INFO;
+
+struct TARGET_INFO_NODE_S {
+	U32 num_of_agents;
+	U32 reserved;
+	U32 os_id[MAX_NUM_OS_ALLOWED];
+	S8 ip_address[MAX_NUM_OS_ALLOWED][TARGET_IP_NAMELEN];
+	REMOTE_OS_INFO_NODE os_info[MAX_NUM_OS_ALLOWED];
+	REMOTE_HARDWARE_INFO_NODE hardware_info[MAX_NUM_OS_ALLOWED];
+	REMOTE_SWITCH_NODE remote_switch[MAX_NUM_OS_ALLOWED];
+};
+
+#define TARGET_INFO_num_of_agents(x) 	((x)->num_of_agents)
+#define TARGET_INFO_os_id(x, i) 		((x)->os_id[i])
+#define TARGET_INFO_os_info(x, i) 		((x)->os_info[i])
+#define TARGET_INFO_ip_address(x, i) 	((x)->ip_address[i])
+#define TARGET_INFO_hardware_info(x, i) ((x)->hardware_info[i])
+#define TARGET_INFO_remote_switch(x, i) ((x)->remote_switch[i])
+
+typedef struct CPU_MAP_TRACE_NODE_S CPU_MAP_TRACE_NODE;
+typedef CPU_MAP_TRACE_NODE * CPU_MAP_TRACE;
+
+struct CPU_MAP_TRACE_NODE_S {
+	U64 tsc;
+	U32 os_id;
+	U32 vcpu_id;
+	U32 pcpu_id;
+	U8 is_static : 1;
+	U8 initial : 1;
+	U8 reserved1 : 6;
+	U8 reserved2;
+	U16 reserved3;
+	U64 reserved4;
+};
+
+#define CPU_MAP_TRACE_tsc(x) ((x)->tsc)
+#define CPU_MAP_TRACE_os_id(x) ((x)->os_id)
+#define CPU_MAP_TRACE_vcpu_id(x) ((x)->vcpu_id)
+#define CPU_MAP_TRACE_pcpu_id(x) ((x)->pcpu_id)
+#define CPU_MAP_TRACE_is_static(x) ((x)->is_static)
+#define CPU_MAP_TRACE_initial(x) ((x)->initial)
+
+typedef struct VM_SWITCH_TRACE_NODE_S VM_SWITCH_TRACE_NODE;
+typedef VM_SWITCH_TRACE_NODE * VM_SWITCH_TRACE;
+
+struct VM_SWITCH_TRACE_NODE_S {
+	U64 tsc;
+	U32 from_os_id;
+	U32 to_os_id;
+	U64 reason;
+	U64 reserved1;
+	U64 reserved2;
+};
+
+#define VM_SWITCH_TRACE_tsc(x) 			((x)->tsc)
+#define VM_SWITCH_TRACE_from_os_id(x) 	((x)->from_os_id)
+#define VM_SWITCH_TRACE_to_os_id(x) 	((x)->to_os_id)
+#define VM_SWITCH_TRACE_reason(x) 		((x)->reason)
+
+typedef struct EMON_BUFFER_DRIVER_HELPER_NODE_S EMON_BUFFER_DRIVER_HELPER_NODE;
+typedef EMON_BUFFER_DRIVER_HELPER_NODE * EMON_BUFFER_DRIVER_HELPER;
+
+struct EMON_BUFFER_DRIVER_HELPER_NODE_S {
+	U32 num_entries_per_package;
+	U32 num_cpu;
+	U32 power_num_package_events;
+	U32 power_num_module_events;
+	U32 power_num_thread_events;
+	U32 power_device_offset_in_package;
+	U32 core_num_events;
+	U32 core_index_to_thread_offset_map[];
+};
+
+#define EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(x)                   \
+	((x)->num_entries_per_package)
+#define EMON_BUFFER_DRIVER_HELPER_num_cpu(x) ((x)->num_cpu)
+#define EMON_BUFFER_DRIVER_HELPER_power_num_package_events(x)                  \
+	((x)->power_num_package_events)
+#define EMON_BUFFER_DRIVER_HELPER_power_num_module_events(x)                   \
+	((x)->power_num_module_events)
+#define EMON_BUFFER_DRIVER_HELPER_power_num_thread_events(x)                   \
+	((x)->power_num_thread_events)
+#define EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(x)            \
+	((x)->power_device_offset_in_package)
+#define EMON_BUFFER_DRIVER_HELPER_core_num_events(x) ((x)->core_num_events)
+#define EMON_BUFFER_DRIVER_HELPER_core_index_to_thread_offset_map(x)           \
+	((x)->core_index_to_thread_offset_map)
+
+// EMON counts buffer follow this hardware topology:
+// package -> device -> unit/thread -> event
+
+// Calculate the CORE thread offset
+// Using for initialization: calculate the cpu_index_to_thread_offset_map
+// in emon_Create_Emon_Buffer_Descriptor()
+// EMON_BUFFER_CORE_THREAD_OFFSET =
+// package_id * num_entries_per_package  +  //package offset
+// device_offset_in_package              +  //device base offset
+// (core_id * threads_per_core + thread_id) * num_core_events + //thread offset
+#define EMON_BUFFER_CORE_THREAD_OFFSET(package_id, num_entries_per_package,    \
+				       device_offset_in_package, core_id,      \
+				       threads_per_core, thread_id,            \
+				       num_core_events)                        \
+	(package_id * num_entries_per_package + device_offset_in_package +       \
+		(core_id * threads_per_core + thread_id) * num_core_events)
+
+// Take cpu_index and cpu_index_to_thread_offset_map to get thread_offset,
+// and calculate the CORE event offset
+// Using for kernel and emon_output.c printing function
+// EMON_BUFFER_CORE_EVENT_OFFSET =
+//      cpu_index_to_thread_offset +  //thread offset
+//      core_event_id                 //event_offset
+#define EMON_BUFFER_CORE_EVENT_OFFSET(cpu_index_to_thread_offset,              \
+				      core_event_id)                           \
+	(cpu_index_to_thread_offset + core_event_id)
+
+// Calculate the device level to UNCORE event offset
+// Using for kernel and emon_output.c printing function
+// EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET_IN_PACKAGE =
+//      device_offset_in_package         +  //device_offset_in_package
+//      device_unit_id * num_unit_events +  //unit_offset
+//      device_event_id                     //event_offset
+#define EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET_IN_PACKAGE(                    \
+	device_offset_in_package, device_unit_id, num_unit_events,             \
+	device_event_id)                                                       \
+	(device_offset_in_package + device_unit_id * num_unit_events +           \
+		device_event_id)
+
+// Take 'device level to UNCORE event offset' and package_id,
+// calculate the UNCORE package level event offset
+// Using for emon_output.c printing function
+// EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET =
+//      package_id * num_entries_per_package +  //package_offset
+//      uncore_offset_in_package;               //offset_in_package
+#define EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(                               \
+	package_id, num_entries_per_package, uncore_offset_in_package)         \
+	(package_id * num_entries_per_package + uncore_offset_in_package)
+
+// Take 'device level to UNCORE event offset',
+// calculate the UNCORE system level event offset
+// Using for emon_output.c printing function
+// EMON_BUFFER_UNCORE_SYSTEM_EVENT_OFFSET =
+//      device_offset_in_system            +  //device_offset_in_system
+//      device_unit_id * num_system_events +  //device_unit_offset
+//      device_event_id                       //event_offset
+#define EMON_BUFFER_UNCORE_SYSTEM_EVENT_OFFSET(device_offset_in_system,        \
+					       device_unit_id,                 \
+					       num_system_events,              \
+					       device_event_id)                \
+	(device_offset_in_system + device_unit_id * num_system_events +        \
+		device_event_id)
+
+// Calculate the package level power event offset
+// Using for kernel and emon_output.c printing function
+// EMON_BUFFER_UNCORE_PACKAGE_POWER_EVENT_OFFSET =
+//      package_id * num_entries_per_package + //package offset
+//      device_offset_in_package             + //device offset
+//      package_event_offset                   //power package event offset
+#define EMON_BUFFER_UNCORE_PACKAGE_POWER_EVENT_OFFSET(                         \
+	package_id, num_entries_per_package, device_offset_in_package,         \
+	device_event_offset)                                                   \
+	(package_id * num_entries_per_package + device_offset_in_package +     \
+		device_event_offset)
+
+// Calculate the module level power event offset
+// Using for kernel and emon_output.c printing function
+// EMON_BUFFER_UNCORE_MODULE_POWER_EVENT_OFFSET =
+//      package_id * num_entries_per_package + //package offset
+//      device_offset_in_package             + //device offset
+//      num_package_events                   + //package event offset
+//      module_id * num_module_events        + //module offset
+//      module_event_offset                    //power module event offset
+#define EMON_BUFFER_UNCORE_MODULE_POWER_EVENT_OFFSET(                          \
+	package_id, num_entries_per_package, device_offset_in_package,         \
+	num_package_events, module_id, num_module_events, device_event_offset) \
+	(package_id * num_entries_per_package + device_offset_in_package +     \
+		num_package_events + module_id * num_module_events +           \
+		device_event_offset)
+
+// Calculate the package level power event offset
+// Using for kernel and emon_output.c printing function
+// EMON_BUFFER_UNCORE_THREAD_POWER_EVENT_OFFSET =
+//  package_id * num_entries_per_package                  + //package offset
+//  device_offset_in_package                              + //device offset
+//  num_package_events                                    + //package offset
+//  num_modules_per_package * num_module_events           + //module offset
+//  (core_id*threads_per_core+thread_id)*num_thread_events  + //thread offset
+//  thread_event_offset                           //power thread event offset
+#define EMON_BUFFER_UNCORE_THREAD_POWER_EVENT_OFFSET(                          \
+	package_id, num_entries_per_package, device_offset_in_package,         \
+	num_package_events, num_modules_per_package, num_module_events,        \
+	core_id, threads_per_core, thread_id, num_unit_events,                 \
+	device_event_offset)                                                   \
+	(package_id * num_entries_per_package + device_offset_in_package +     \
+		num_package_events +                                           \
+		num_modules_per_package * num_module_events +                  \
+		(core_id * threads_per_core + thread_id) * num_unit_events +   \
+		device_event_offset)
+
+/*
+ ************************************
+ *  DRIVER LOG BUFFER DECLARATIONS  *
+ ************************************
+ */
+
+#define DRV_MAX_NB_LOG_CATEGORIES 256 // Must be a multiple of 8
+#define DRV_NB_LOG_CATEGORIES 14
+#define DRV_LOG_CATEGORY_LOAD 0
+#define DRV_LOG_CATEGORY_INIT 1
+#define DRV_LOG_CATEGORY_DETECTION 2
+#define DRV_LOG_CATEGORY_ERROR 3
+#define DRV_LOG_CATEGORY_STATE_CHANGE 4
+#define DRV_LOG_CATEGORY_MARK 5
+#define DRV_LOG_CATEGORY_DEBUG 6
+#define DRV_LOG_CATEGORY_FLOW 7
+#define DRV_LOG_CATEGORY_ALLOC 8
+#define DRV_LOG_CATEGORY_INTERRUPT 9
+#define DRV_LOG_CATEGORY_TRACE 10
+#define DRV_LOG_CATEGORY_REGISTER 11
+#define DRV_LOG_CATEGORY_NOTIFICATION 12
+#define DRV_LOG_CATEGORY_WARNING 13
+
+#define LOG_VERBOSITY_UNSET 0xFF
+#define LOG_VERBOSITY_DEFAULT 0xFE
+#define LOG_VERBOSITY_NONE 0
+
+#define LOG_CHANNEL_MEMLOG 0x1
+#define LOG_CHANNEL_AUXMEMLOG 0x2
+#define LOG_CHANNEL_PRINTK 0x4
+#define LOG_CHANNEL_TRACEK 0x8
+#define LOG_CHANNEL_MOSTWHERE                                                  \
+	(LOG_CHANNEL_MEMLOG | LOG_CHANNEL_AUXMEMLOG | LOG_CHANNEL_PRINTK)
+#define LOG_CHANNEL_EVERYWHERE                                                 \
+	(LOG_CHANNEL_MEMLOG | LOG_CHANNEL_AUXMEMLOG | LOG_CHANNEL_PRINTK |     \
+	 LOG_CHANNEL_TRACEK)
+#define LOG_CHANNEL_MASK LOG_CATEGORY_VERBOSITY_EVERYWHERE
+
+#define LOG_CONTEXT_REGULAR 0x10
+#define LOG_CONTEXT_INTERRUPT 0x20
+#define LOG_CONTEXT_NOTIFICATION 0x40
+#define LOG_CONTEXT_ALL                                                        \
+	(LOG_CONTEXT_REGULAR | LOG_CONTEXT_INTERRUPT | LOG_CONTEXT_NOTIFICATION)
+#define LOG_CONTEXT_MASK LOG_CONTEXT_ALL
+#define LOG_CONTEXT_SHIFT 4
+
+#define DRV_LOG_NOTHING 0
+#define DRV_LOG_FLOW_IN 1
+#define DRV_LOG_FLOW_OUT 2
+
+/*
+ * @macro DRV_LOG_ENTRY_NODE_S
+ * @brief
+ * This structure is used to store a log message from the driver.
+ */
+
+#define DRV_LOG_MESSAGE_LENGTH 64
+#define DRV_LOG_FUNCTION_NAME_LENGTH 32
+
+typedef struct DRV_LOG_ENTRY_NODE_S DRV_LOG_ENTRY_NODE;
+typedef DRV_LOG_ENTRY_NODE * DRV_LOG_ENTRY;
+struct DRV_LOG_ENTRY_NODE_S {
+	char function_name[DRV_LOG_FUNCTION_NAME_LENGTH];
+	char message[DRV_LOG_MESSAGE_LENGTH];
+
+	U16 temporal_tag;
+	U16 integrity_tag;
+
+	U8 category;
+	U8 secondary_info; // Secondary attribute:
+		// former driver state for STATE category
+		// 'ENTER' or 'LEAVE' for FLOW and TRACE categories
+	U16 processor_id;
+	// NB: not guaranteed to be accurate (due to preemption/core migration)
+
+	U64 tsc;
+
+	U16 nb_active_interrupts; // never 100% accurate, merely indicative
+	U8 active_drv_operation; // only 100% accurate IOCTL-called functions
+	U8 driver_state;
+
+	U16 line_number; // as per the __LINE__ macro
+
+	U16 nb_active_notifications;
+
+	U64 reserved; // need padding to reach 128 bytes
+}; // this structure should be exactly 128-byte long
+
+#define DRV_LOG_ENTRY_temporal_tag(ent) ((ent)->temporal_tag)
+#define DRV_LOG_ENTRY_integrity_tag(ent) ((ent)->integrity_tag)
+#define DRV_LOG_ENTRY_category(ent) ((ent)->category)
+#define DRV_LOG_ENTRY_secondary_info(ent) ((ent)->secondary_info)
+#define DRV_LOG_ENTRY_processor_id(ent) ((ent)->processor_id)
+#define DRV_LOG_ENTRY_tsc(ent) ((ent)->tsc)
+#define DRV_LOG_ENTRY_driver_state(ent) ((ent)->driver_state)
+#define DRV_LOG_ENTRY_active_drv_operation(ent) ((ent)->active_drv_operation)
+#define DRV_LOG_ENTRY_nb_active_interrupts(ent) ((ent)->nb_active_interrupts)
+#define DRV_LOG_ENTRY_nb_active_notifications(ent)                             \
+	((ent)->nb_active_notifications)
+#define DRV_LOG_ENTRY_line_number(ent) ((ent)->line_number)
+#define DRV_LOG_ENTRY_message(ent) ((ent)->message)
+#define DRV_LOG_ENTRY_function_name(ent) ((ent)->function_name)
+
+/*
+ * @macro DRV_LOG_BUFFER_NODE_S
+ * @brief Circular buffer structure storing the latest
+ *        DRV_LOG_MAX_NB_ENTRIES driver messages
+ */
+
+#define DRV_LOG_SIGNATURE_SIZE 8 // Must be a multiple of 8
+#define DRV_LOG_SIGNATURE_0 'S'
+#define DRV_LOG_SIGNATURE_1 'e'
+#define DRV_LOG_SIGNATURE_2 'P'
+#define DRV_LOG_SIGNATURE_3 'd'
+#define DRV_LOG_SIGNATURE_4 'R'
+#define DRV_LOG_SIGNATURE_5 'v'
+#define DRV_LOG_SIGNATURE_6 '5'
+#define DRV_LOG_SIGNATURE_7 '\0'
+// The signature is "SePdRv4"; not declared as string on purpose to avoid
+// false positives when trying to identify the log buffer in a crash dump
+
+#define DRV_LOG_VERSION 1
+#define DRV_LOG_FILLER_BYTE 1
+
+#define DRV_LOG_DRIVER_VERSION_SIZE 64 // Must be a multiple of 8
+#define DRV_LOG_MAX_NB_PRI_ENTRIES 	(8192 * 2)
+// 2MB buffer [*HAS TO BE* a power of 2!] [8192 entries = 1 MB]
+#define DRV_LOG_MAX_NB_AUX_ENTRIES (8192)
+// 1MB buffer [*HAS TO BE* a power of 2!]
+#define DRV_LOG_MAX_NB_ENTRIES                                                 \
+	(DRV_LOG_MAX_NB_PRI_ENTRIES + DRV_LOG_MAX_NB_AUX_ENTRIES)
+
+typedef struct DRV_LOG_BUFFER_NODE_S DRV_LOG_BUFFER_NODE;
+typedef DRV_LOG_BUFFER_NODE * DRV_LOG_BUFFER;
+struct DRV_LOG_BUFFER_NODE_S {
+	char header_signature[DRV_LOG_SIGNATURE_SIZE];
+	// some signature to be able to locate the log even without -g; ASCII
+	// would help should we change the signature for each log's version
+	// instead of keeping it in a dedicated field?
+
+	U32 log_size; // filled with sizeof(this structure) at init.
+	U32 max_nb_pri_entries;
+	// filled with the driver's "DRV_LOG_MAX_NB_PRIM_ENTRIES" at init.
+
+	U32 max_nb_aux_entries;
+	// filled with the driver's "DRV_LOG_MAX_NB_AUX_ENTRIES" at init.
+	U32 reserved1;
+
+	U64 init_time; // primary log disambiguator
+
+	U32 disambiguator;
+	// used to differentiate the driver's version of the log when a
+	// full memory dump can contain some from userland
+	U32 log_version; // 0 at first, increase when format changes?
+
+	U32 pri_entry_index;
+	// should be incremented *atomically* as a means to (re)allocate
+	// the next primary log entry.
+	U32 aux_entry_index;
+	// should be incremented *atomically* as a means to (re)allocate
+	// the next auxiliary log entry.
+
+	char driver_version[DRV_LOG_DRIVER_VERSION_SIZE];
+
+	U8 driver_state;
+	U8 active_drv_operation;
+	U16 reserved2;
+	U32 nb_drv_operations;
+
+	U32 nb_interrupts;
+	U16 nb_active_interrupts;
+	U16 nb_active_notifications;
+
+	U32 nb_notifications;
+	U32 nb_driver_state_transitions;
+
+	U8 contiguous_physical_memory;
+	U8 reserved3;
+	U16 reserved4;
+	U32 reserved5;
+
+	U8 verbosities[DRV_MAX_NB_LOG_CATEGORIES];
+
+	DRV_LOG_ENTRY_NODE entries[DRV_LOG_MAX_NB_ENTRIES];
+
+	char footer_signature[DRV_LOG_SIGNATURE_SIZE];
+};
+
+#define DRV_LOG_BUFFER_pri_entry_index(log) ((log)->pri_entry_index)
+#define DRV_LOG_BUFFER_aux_entry_index(log) ((log)->aux_entry_index)
+#define DRV_LOG_BUFFER_header_signature(log) ((log)->header_signature)
+#define DRV_LOG_BUFFER_footer_signature(log) ((log)->footer_signature)
+#define DRV_LOG_BUFFER_log_size(log) ((log)->log_size)
+#define DRV_LOG_BUFFER_driver_version(log) ((log)->driver_version)
+#define DRV_LOG_BUFFER_driver_state(log) ((log)->driver_state)
+#define DRV_LOG_BUFFER_active_drv_operation(log) ((log)->active_drv_operation)
+#define DRV_LOG_BUFFER_nb_interrupts(log) ((log)->nb_interrupts)
+#define DRV_LOG_BUFFER_nb_active_interrupts(log) ((log)->nb_active_interrupts)
+#define DRV_LOG_BUFFER_nb_notifications(log) ((log)->nb_notifications)
+#define DRV_LOG_BUFFER_nb_active_notifications(log)                            \
+	((log)->nb_active_notifications)
+#define DRV_LOG_BUFFER_nb_driver_state_transitions(log)                        \
+	((log)->nb_driver_state_transitions)
+#define DRV_LOG_BUFFER_nb_drv_operations(log) ((log)->nb_drv_operations)
+#define DRV_LOG_BUFFER_max_nb_pri_entries(log) ((log)->max_nb_pri_entries)
+#define DRV_LOG_BUFFER_max_nb_aux_entries(log) ((log)->max_nb_aux_entries)
+#define DRV_LOG_BUFFER_init_time(log) ((log)->init_time)
+#define DRV_LOG_BUFFER_disambiguator(log) ((log)->disambiguator)
+#define DRV_LOG_BUFFER_log_version(log) ((log)->log_version)
+#define DRV_LOG_BUFFER_entries(log) ((log)->entries)
+#define DRV_LOG_BUFFER_contiguous_physical_memory(log)                         \
+	((log)->contiguous_physical_memory)
+#define DRV_LOG_BUFFER_verbosities(log) ((log)->verbosities)
+
+#define DRV_LOG_CONTROL_MAX_DATA_SIZE                                          \
+	DRV_MAX_NB_LOG_CATEGORIES // Must be a multiple of 8
+
+typedef struct DRV_LOG_CONTROL_NODE_S DRV_LOG_CONTROL_NODE;
+typedef DRV_LOG_CONTROL_NODE * DRV_LOG_CONTROL;
+
+struct DRV_LOG_CONTROL_NODE_S {
+	U32 command;
+	U32 reserved1;
+	U8 data[DRV_LOG_CONTROL_MAX_DATA_SIZE];
+	// only DRV_NB_LOG_CATEGORIES elements will be used, but let's plan for
+	// backwards compatibility if LOG_CATEGORY_UNSET, READ instead of WRITE
+
+	U64 reserved2;
+	// may later want to add support for resizing the buffer,
+	// or only log 100 first interrupts, etc.
+	U64 reserved3;
+	U64 reserved4;
+	U64 reserved5;
+};
+
+#define DRV_LOG_CONTROL_command(x) ((x)->command)
+#define DRV_LOG_CONTROL_verbosities(x) ((x)->data)
+#define DRV_LOG_CONTROL_message(x)                                             \
+	((x)->data) // Userland 'MARK' messages use the 'data' field too.
+#define DRV_LOG_CONTROL_log_size(x) (*((U32 *)((x)->data)))
+
+#define DRV_LOG_CONTROL_COMMAND_NONE 0
+#define DRV_LOG_CONTROL_COMMAND_ADJUST_VERBOSITY 1
+#define DRV_LOG_CONTROL_COMMAND_MARK 2
+#define DRV_LOG_CONTROL_COMMAND_QUERY_SIZE 3
+#define DRV_LOG_CONTROL_COMMAND_BENCHMARK 4
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_types.h b/drivers/platform/x86/sepdk/include/lwpmudrv_types.h
new file mode 100644
index 000000000000..7fe842eee890
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_types.h
@@ -0,0 +1,159 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2007-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+ ***/
+
+#ifndef _LWPMUDRV_TYPES_H_
+#define _LWPMUDRV_TYPES_H_
+
+#if defined(__cplusplus)
+extern "C" {
+#endif
+
+#if defined(BUILD_DRV_ESX)
+//SR: added size_t def
+typedef unsigned long size_t;
+typedef unsigned long ssize_t;
+#endif
+
+typedef unsigned char U8;
+typedef char S8;
+typedef short S16;
+typedef unsigned short U16;
+typedef unsigned int U32;
+typedef int S32;
+#if defined(DRV_OS_WINDOWS)
+typedef unsigned __int64 U64;
+typedef __int64 S64;
+#elif defined(DRV_OS_LINUX) || defined(DRV_OS_SOLARIS) ||                      \
+	defined(DRV_OS_MAC) || defined(DRV_OS_ANDROID) ||                      \
+	defined(DRV_OS_FREEBSD)
+typedef unsigned long long U64;
+typedef long long S64;
+typedef unsigned long ULONG;
+typedef void VOID;
+typedef void *LPVOID;
+
+#if defined(BUILD_DRV_ESX)
+//SR: added UWORD64 def
+typedef union _UWORD64 {
+	struct {
+		U32 low;
+		S32 hi;
+	} c;
+	S64 qword;
+} UWORD64, *PWORD64;
+#endif
+#else
+#error "Undefined OS"
+#endif
+
+#if defined(DRV_IA32)
+typedef S32 SIOP;
+typedef U32 UIOP;
+#elif defined(DRV_EM64T)
+typedef S64 SIOP;
+typedef U64 UIOP;
+#else
+#error "Unexpected Architecture seen"
+#endif
+
+typedef U32 DRV_BOOL;
+typedef void *PVOID;
+
+#if !defined(__DEFINE_STCHAR__)
+#define __DEFINE_STCHAR__
+#if defined(UNICODE)
+typedef wchar_t STCHAR;
+#define VTSA_T(x) L##x
+#else
+typedef char STCHAR;
+#define VTSA_T(x) x
+#endif
+#endif
+
+#if defined(DRV_OS_WINDOWS)
+#include <wchar.h>
+typedef wchar_t DRV_STCHAR;
+typedef wchar_t VTSA_CHAR;
+#else
+typedef char DRV_STCHAR;
+#endif
+
+//
+// Handy Defines
+//
+typedef U32 DRV_STATUS;
+
+#define MAX_STRING_LENGTH 1024
+#define MAXNAMELEN 256
+
+#if defined(DRV_OS_WINDOWS)
+#define UNLINK _unlink
+#define RENAME rename
+#define WCSDUP _wcsdup
+#endif
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_SOLARIS) || defined(DRV_OS_MAC) || \
+	defined(DRV_OS_ANDROID) || defined(DRV_OS_FREEBSD)
+#define UNLINK unlink
+#define RENAME rename
+#endif
+
+#if defined(DRV_OS_SOLARIS) && !defined(_KERNEL)
+//wcsdup is missing on Solaris
+#include <stdlib.h>
+#include <wchar.h>
+
+static inline wchar_t *solaris_wcsdup(const wchar_t *wc)
+{
+	wchar_t *tmp = (wchar_t *)malloc((wcslen(wc) + 1) * sizeof(wchar_t));
+
+	wcscpy(tmp, wc);
+	return tmp;
+}
+#define WCSDUP solaris_wcsdup
+#endif
+
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_FREEBSD) || defined(DRV_OS_MAC)
+#define WCSDUP wcsdup
+#endif
+
+#if !defined(_WCHAR_T_DEFINED)
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_ANDROID) || defined(DRV_OS_SOLARIS)
+#if !defined(_GNU_SOURCE)
+#define _GNU_SOURCE
+#endif
+#endif
+#endif
+
+#if (defined(DRV_OS_LINUX) || defined(DRV_OS_ANDROID)) && !defined(__KERNEL__)
+#include <wchar.h>
+typedef wchar_t VTSA_CHAR;
+#endif
+
+#if (defined(DRV_OS_MAC) || defined(DRV_OS_FREEBSD) ||                         \
+     defined(DRV_OS_SOLARIS)) &&                                               \
+	!defined(_KERNEL)
+#include <wchar.h>
+typedef wchar_t VTSA_CHAR;
+#endif
+
+#define TRUE 1
+#define FALSE 0
+
+#define ALIGN_4(x) (((x) + 3) & ~3)
+#define ALIGN_8(x) (((x) + 7) & ~7)
+#define ALIGN_16(x) (((x) + 15) & ~15)
+#define ALIGN_32(x) (((x) + 31) & ~31)
+
+#if defined(__cplusplus)
+}
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/lwpmudrv_version.h b/drivers/platform/x86/sepdk/include/lwpmudrv_version.h
new file mode 100644
index 000000000000..a2cbedd44573
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/lwpmudrv_version.h
@@ -0,0 +1,111 @@
+/****
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2010-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+****/
+/*
+ *  File  : lwpmudrv_version.h
+ */
+
+#ifndef _LWPMUDRV_VERSION_H_
+#define _LWPMUDRV_VERSION_H_
+
+#define _STRINGIFY(x) #x
+#define STRINGIFY(x) _STRINGIFY(x)
+#define _STRINGIFY_W(x) L#x
+#define STRINGIFY_W(x) _STRINGIFY_W(x)
+
+#define SEP_MAJOR_VERSION 5
+#define SEP_MINOR_VERSION 0
+#define SEP_UPDATE_VERSION 0
+#define SEP_API_VERSION SEP_UPDATE_VERSION
+#if SEP_UPDATE_VERSION > 0
+#define SEP_UPDATE_STRING " Update " STRINGIFY(SEP_UPDATE_VERSION)
+#else
+#define SEP_UPDATE_STRING ""
+#endif
+#define SEP_RELEASE_STRING ""
+
+#define EMON_MAJOR_VERSION SEP_MAJOR_VERSION
+#define EMON_MINOR_VERSION SEP_MINOR_VERSION
+#define EMON_PRODUCT_RELEASE_STRING SEP_UPDATE_VERSION
+
+#if defined(SEP_ENABLE_PRIVATE_CPUS)
+#define PRODUCT_TYPE "private"
+#define SEP_NAME "sepint"
+#define SEP_NAME_W L"sepint"
+#else
+#define PRODUCT_TYPE "public"
+#define SEP_NAME "sep"
+#define SEP_NAME_W L"sep"
+#endif
+
+#if !defined(PRODUCT_BUILDER)
+#define PRODUCT_BUILDER unknown
+#endif
+
+#define TB_FILE_EXT ".tb7"
+#define TB_FILE_EXT_W L".tb7"
+
+#define SEP_PRODUCT_NAME "Sampling Enabling Product"
+#define EMON_PRODUCT_NAME "EMON"
+
+#define PRODUCT_VERSION_DATE __DATE__ " at " __TIME__
+
+#define SEP_PRODUCT_COPYRIGHT                                                  \
+	"Copyright(C) 2007-2018 Intel Corporation. All rights reserved."
+#define EMON_PRODUCT_COPYRIGHT                                                 \
+	"Copyright(C) 1993-2018 Intel Corporation. All rights reserved."
+
+#define PRODUCT_DISCLAIMER                                                    \
+	"Warning: This computer program is protected under U.S. and \n"       \
+	"international copyright laws, and may only be used or copied in \n"  \
+	"accordance with the terms of the license agreement.  Except as \n"   \
+	"permitted by such license, no part of this computer program may  \n" \
+	"be reproduced, stored in a retrieval  system, or transmitted  \n"    \
+	"in any form or by any means without the express written consent \n"  \
+	"of Intel Corporation."
+
+#define PRODUCT_VERSION                                                        \
+	STRINGIFY(SEP_MAJOR_VERSION) "." STRINGIFY(SEP_MINOR_VERSION)
+
+#define SEP_MSG_PREFIX                                                         \
+	SEP_NAME "" STRINGIFY(SEP_MAJOR_VERSION) "_" STRINGIFY(                \
+		SEP_MINOR_VERSION) ":"
+#define SEP_VERSION_STR                                                        \
+	STRINGIFY(SEP_MAJOR_VERSION)                                           \
+	"." STRINGIFY(SEP_MINOR_VERSION) "." STRINGIFY(SEP_API_VERSION)
+
+#if defined(DRV_OS_WINDOWS)
+
+#define SEP_DRIVER_NAME SEP_NAME "drv" STRINGIFY(SEP_MAJOR_VERSION)
+#define SEP_DRIVER_NAME_W SEP_NAME_W L"drv" STRINGIFY_W(SEP_MAJOR_VERSION)
+#define SEP_DEVICE_NAME SEP_DRIVER_NAME
+
+#endif
+
+#if defined(DRV_OS_LINUX) || defined(DRV_OS_SOLARIS) ||                        \
+	defined(DRV_OS_ANDROID) || defined(DRV_OS_FREEBSD)
+
+#define SEP_DRIVER_NAME SEP_NAME "" STRINGIFY(SEP_MAJOR_VERSION)
+#define SEP_SAMPLES_NAME SEP_DRIVER_NAME "_s"
+#define SEP_UNCORE_NAME SEP_DRIVER_NAME "_u"
+#define SEP_SIDEBAND_NAME SEP_DRIVER_NAME "_b"
+#define SEP_DEVICE_NAME "/dev/" SEP_DRIVER_NAME
+
+#endif
+
+#if defined(DRV_OS_MAC)
+
+#define SEP_DRIVER_NAME SEP_NAME "" STRINGIFY(SEP_MAJOR_VERSION)
+#define SEP_SAMPLES_NAME SEP_DRIVER_NAME "_s"
+#define SEP_DEVICE_NAME SEP_DRIVER_NAME
+
+#endif
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/pax_shared.h b/drivers/platform/x86/sepdk/include/pax_shared.h
new file mode 100644
index 000000000000..a706232c9b4a
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/pax_shared.h
@@ -0,0 +1,180 @@
+/****
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+****/
+
+/*
+ *
+ *    Description:  types and definitions shared between PAX kernel
+ *                  and user modes
+ *
+ *    NOTE: alignment on page boundaries is required on 64-bit platforms!
+ *
+*/
+
+#ifndef _PAX_SHARED_H_
+#define _PAX_SHARED_H_
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+
+#define _STRINGIFY(x) #x
+#define STRINGIFY(x) _STRINGIFY(x)
+
+// PAX versioning
+
+#define PAX_MAJOR_VERSION 1 // major version
+// (increment only when PAX driver is incompatible with previous versions)
+#define PAX_MINOR_VERSION 0 // minor version
+// (increment only when new APIs added, but driver remains backwards compatible)
+#define PAX_BUGFIX_VERSION 2 // bugfix version
+// (increment only for bug fix that don't affect usermode/driver compatibility)
+
+#define PAX_VERSION_STR                                                        \
+	STRINGIFY(PAX_MAJOR_VERSION)                                           \
+	"." STRINGIFY(PAX_MINOR_VERSION) "." STRINGIFY(PAX_BUGFIX_VERSION)
+
+// PAX device name
+
+#if defined(DRV_OS_WINDOWS)
+#define PAX_NAME "sepdal"
+#define PAX_NAME_W L"sepdal"
+#else
+#define PAX_NAME "pax"
+#endif
+
+// PAX PMU reservation states
+
+#define PAX_PMU_RESERVED 1
+#define PAX_PMU_UNRESERVED 0
+
+#define PAX_GUID_UNINITIALIZED 0
+
+// PAX_IOCTL definitions
+
+#if defined(DRV_OS_WINDOWS)
+
+//
+// The name of the device as seen by the driver
+//
+#define LSTRING(x) L#x
+#define PAX_OBJECT_DEVICE_NAME L"\\Device\\sepdal" // LSTRING(PAX_NAME)
+#define PAX_OBJECT_LINK_NAME L"\\DosDevices\\sepdal" // LSTRING(PAX_NAME)
+
+#define PAX_DEVICE_NAME PAX_NAME // for CreateFile called by app
+
+#define PAX_IOCTL_DEVICE_TYPE 0xA000 // values 0-32768 reserved for Microsoft
+#define PAX_IOCTL_FUNCTION 0xA00 // values 0-2047  reserved for Microsoft
+
+//
+// Basic CTL CODE macro to reduce typographical errors
+//
+#define PAX_CTL_READ_CODE(x)                                                   \
+	CTL_CODE(PAX_IOCTL_DEVICE_TYPE, PAX_IOCTL_FUNCTION + (x),              \
+		 METHOD_BUFFERED, FILE_READ_ACCESS)
+
+#define PAX_IOCTL_INFO PAX_CTL_READ_CODE(1)
+#define PAX_IOCTL_STATUS PAX_CTL_READ_CODE(2)
+#define PAX_IOCTL_RESERVE_ALL PAX_CTL_READ_CODE(3)
+#define PAX_IOCTL_UNRESERVE PAX_CTL_READ_CODE(4)
+
+#elif defined(DRV_OS_LINUX) || defined(DRV_OS_ANDROID) ||                      \
+	defined(DRV_OS_SOLARIS)
+
+#define PAX_DEVICE_NAME "/dev/" PAX_NAME
+
+#define PAX_IOC_MAGIC 100
+#define PAX_IOCTL_INFO _IOW(PAX_IOC_MAGIC, 1, IOCTL_ARGS)
+#define PAX_IOCTL_STATUS _IOW(PAX_IOC_MAGIC, 2, IOCTL_ARGS)
+#define PAX_IOCTL_RESERVE_ALL _IO(PAX_IOC_MAGIC, 3)
+#define PAX_IOCTL_UNRESERVE _IO(PAX_IOC_MAGIC, 4)
+
+#if defined(HAVE_COMPAT_IOCTL) && defined(DRV_EM64T)
+#define PAX_IOCTL_COMPAT_INFO _IOW(PAX_IOC_MAGIC, 1, compat_uptr_t)
+#define PAX_IOCTL_COMPAT_STATUS _IOW(PAX_IOC_MAGIC, 2, compat_uptr_t)
+#define PAX_IOCTL_COMPAT_RESERVE_ALL _IO(PAX_IOC_MAGIC, 3)
+#define PAX_IOCTL_COMPAT_UNRESERVE _IO(PAX_IOC_MAGIC, 4)
+#endif
+
+#elif defined(DRV_OS_FREEBSD)
+
+#define PAX_DEVICE_NAME "/dev/" PAX_NAME
+
+#define PAX_IOC_MAGIC 100
+#define PAX_IOCTL_INFO _IOW(PAX_IOC_MAGIC, 1, IOCTL_ARGS_NODE)
+#define PAX_IOCTL_STATUS _IOW(PAX_IOC_MAGIC, 2, IOCTL_ARGS_NODE)
+#define PAX_IOCTL_RESERVE_ALL _IO(PAX_IOC_MAGIC, 3)
+#define PAX_IOCTL_UNRESERVE _IO(PAX_IOC_MAGIC, 4)
+
+#elif defined(DRV_OS_MAC)
+
+// OSX driver names are always in reverse DNS form.
+#define PAXDriverClassName com_intel_driver_PAX
+#define kPAXDriverClassName "com_intel_driver_PAX"
+#define PAX_DEVICE_NAME "com.intel.driver.PAX"
+
+// User client method dispatch selectors.
+enum { kPAXUserClientOpen,
+	kPAXUserClientClose,
+	kPAXReserveAll,
+	kPAXUnreserve,
+	kPAXGetStatus,
+	kPAXGetInfo,
+	kPAXDataIO,
+	kNumberOfMethods // Must be last
+};
+
+#else
+#warning "unknown OS in pax_shared.h"
+#endif
+
+// data for PAX_IOCTL_INFO call
+
+struct PAX_INFO_NODE_S {
+	volatile U64 managed_by; // entity managing PAX
+	volatile U32 version; // PAX version number
+	volatile U64 reserved1; // force 8-byte alignment
+	volatile U32 reserved2; // unreserved
+};
+
+typedef struct PAX_INFO_NODE_S PAX_INFO_NODE;
+typedef PAX_INFO_NODE * PAX_INFO;
+
+// data for PAX_IOCTL_STATUS call
+
+struct PAX_STATUS_NODE_S {
+	volatile U64 guid; // reservation ID (globally unique identifier)
+	volatile DRV_FILE_DESC pid; // pid of process that has the reservation
+	volatile U64 start_time; // reservation start time
+	volatile U32 is_reserved; // 1 if there is a reservation, 0 otherwise
+};
+
+typedef struct PAX_STATUS_NODE_S PAX_STATUS_NODE;
+typedef PAX_STATUS_NODE * PAX_STATUS;
+
+struct PAX_VERSION_NODE_S {
+	union {
+		U32 version;
+		struct {
+			U32 major : 8;
+			U32 minor : 8;
+			U32 bugfix : 16;
+		} s1;
+	} u1;
+};
+
+typedef struct PAX_VERSION_NODE_S PAX_VERSION_NODE;
+typedef PAX_VERSION_NODE * PAX_VERSION;
+
+#define PAX_VERSION_NODE_version(v) ((v)->u1.version)
+#define PAX_VERSION_NODE_major(v) ((v)->u1.s1.major)
+#define PAX_VERSION_NODE_minor(v) ((v)->u1.s1.minor)
+#define PAX_VERSION_NODE_bugfix(v) ((v)->u1.s1.bugfix)
+
+#endif
diff --git a/drivers/platform/x86/sepdk/include/rise_errors.h b/drivers/platform/x86/sepdk/include/rise_errors.h
new file mode 100644
index 000000000000..29fb278def7d
--- /dev/null
+++ b/drivers/platform/x86/sepdk/include/rise_errors.h
@@ -0,0 +1,326 @@
+/***
+ * -------------------------------------------------------------------------
+ *               INTEL CORPORATION PROPRIETARY INFORMATION
+ *  This software is supplied under the terms of the accompanying license
+ *  agreement or nondisclosure agreement with Intel Corporation and may not
+ *  be copied or disclosed except in accordance with the terms of that
+ *  agreement.
+ *        Copyright(C) 2004-2018 Intel Corporation.  All Rights Reserved.
+ * -------------------------------------------------------------------------
+***/
+
+#ifndef _RISE_ERRORS_H_
+#define _RISE_ERRORS_H_
+
+//
+// NOTE:
+//
+// 1) Before adding an error code, first make sure the error code doesn't
+// already exist. If it does, use that, don't create a new one just because...
+//
+// 2) When adding an error code, add it to the end of the list. Don't insert
+// error numbers in the middle of the list! For backwards compatibility,
+// we don't want the numbers changing unless we really need them
+// to for some reason (like we want to switch to negative error numbers)
+//
+// 3) Change the VT_LAST_ERROR_CODE macro to point to the (newly added)
+// last error. This is done so SW can verify the number of error codes
+// possible matches the number of error strings it has
+//
+// 4) Don't forget to update the error string table to include your
+// error code (rise.c). Since the goal is something human readable
+// you don't need to use abbreviations in there (ie. don't say "bad param",
+// say "bad parameter" or "illegal parameter passed in")
+//
+// 5) Compile and run the test_rise app (in the test_rise directory) to
+// verify things are still working
+//
+//
+
+#define VT_SUCCESS 0
+#define VT_FAILURE -1
+
+/*************************************************************/
+
+#define VT_INVALID_MAX_SAMP 1
+#define VT_INVALID_SAMP_PER_BUFF 2
+#define VT_INVALID_SAMP_INTERVAL 3
+#define VT_INVALID_PATH 4
+#define VT_TB5_IN_USE 5
+#define VT_INVALID_NUM_EVENTS 6
+#define VT_INTERNAL_ERROR 8
+#define VT_BAD_EVENT_NAME 9
+#define VT_NO_SAMP_SESSION 10
+#define VT_NO_EVENTS 11
+#define VT_MULTIPLE_RUNS 12
+#define VT_NO_SAM_PARAMS 13
+#define VT_SDB_ALREADY_EXISTS 14
+#define VT_SAMPLING_ALREADY_STARTED 15
+#define VT_TBS_NOT_SUPPORTED 16
+#define VT_INVALID_SAMPARAMS_SIZE 17
+#define VT_INVALID_EVENT_SIZE 18
+#define VT_ALREADY_PROCESSES 19
+#define VT_INVALID_EVENTS_PATH 20
+#define VT_INVALID_LICENSE 21
+
+/******************************************************/
+//SEP error codes
+
+#define VT_SAM_ERROR 22
+#define VT_SAMPLE_FILE_ALREADY_MAPPED 23
+#define VT_INVALID_SAMPLE_FILE 24
+#define VT_UNKNOWN_SECTION_NUMBER 25
+#define VT_NO_MEMORY 26
+#define VT_ENV_VAR_NOT_FOUND 27
+#define VT_SAMPLE_FILE_NOT_MAPPED 28
+#define VT_BUFFER_OVERFLOW 29
+#define VT_USER_OP_COMPLETED 30
+#define VT_BINARY_NOT_FOUND 31
+#define VT_ISM_NOT_INITIALIZED 32
+#define VT_NO_SYMBOLS 33
+#define VT_SAMPLE_FILE_MAPPING_ERROR 34
+#define VT_BUFFER_NULL 35
+#define VT_UNEXPECTED_NULL_PTR 36
+#define VT_BINARY_LOAD_FAILED 37
+#define VT_FUNCTION_NOT_FOUND_IN_BINARY 38
+#define VT_ENTRY_NOT_FOUND 39
+#define VT_SEP_SYNTAX_ERROR 40
+#define VT_SEP_OPTIONS_ERROR 41
+#define VT_BAD_EVENT_MODIFIER 42
+#define VT_INCOMPATIBLE_PARAMS 43
+#define VT_FILE_OPEN_FAILED 44
+#define VT_EARLY_EXIT 45
+#define VT_TIMEOUT_RETURN 46
+#define VT_NO_CHILD_PROCESS 47
+#define VT_DRIVER_RUNNING 48
+#define VT_DRIVER_STOPPED 49
+#define VT_MULTIPLE_RUNS_NEEDED 50
+#define VT_QUIT_IMMEDIATE 51
+#define VT_DRIVER_INIT_FAILED 52
+#define VT_NO_TB5_CREATED 53
+#define VT_NO_WRITE_PERMISSION 54
+#define VT_DSA_INIT_FAILED 55
+#define VT_INVALID_CPU_MASK 56
+#define VT_SAMP_IN_RUNNING_STATE 57
+#define VT_SAMP_IN_PAUSE_STATE 58
+#define VT_SAMP_IN_STOP_STATE 59
+#define VT_SAMP_NO_SESSION 60
+#define VT_NOT_CONFIGURED 61
+#define VT_LAUNCH_BUILD64_FAILED 62
+#define VT_BAD_PARAMETER 63
+#define VT_ISM_INIT_FAILED 64
+#define VT_INVALID_STATE_TRANS 65
+#define VT_EARLY_EXIT_N_CANCEL 66
+#define VT_EVT_MGR_NOT_INIT 67
+#define VT_ISM_SECTION_ENUM_FAILED 68
+#define VT_VG_PARSER_ERROR 69
+#define VT_MISSING_VALUE_FOR_TOKEN 70
+#define VT_EMPTY_SAMPLE_FILE_NAME 71
+#define VT_UNEXPECTED_VALUE 72
+#define VT_NOT_IMPLEMENTED 73
+#define VT_MISSING_COL_DEPNDNCIES 74
+#define VT_DEP_COL_NOT_LIB_DEFINED 75
+#define VT_COL_NOT_REG_WITH_LIB 76
+#define VT_SECTION_ALREADY_IN_USE 77
+#define VT_SECTION_NOT_EXIST 78
+#define VT_STREAM_NOT_EXIST 79
+#define VT_INVALID_STREAM 80
+#define VT_STREAM_ALREADY_IN_USE 81
+#define VT_DATA_DESC_NOT_EXIST 82
+#define VT_INVALID_ERROR_CODE 83
+#define VT_INCOMPATIBLE_VERSION 84
+#define VT_LEGACY_DATA_NOT_EXIST 85
+#define VT_INVALID_READ_START 86
+#define VT_DRIVER_OPEN_FAILED 87
+#define VT_DRIVER_IOCTL_FAILED 88
+#define VT_SAMP_FILE_CREATE_FAILED 89
+#define VT_MODULE_FILE_CREATE_FAILED 90
+#define VT_INVALID_SAMPLE_FILE_NAME 91
+#define VT_INVALID_MODULE_FILE_NAME 92
+#define VT_FORK_CHILD_PROCESS_FAILED 93
+#define VT_UNEXPECTED_MISMATCH_IN_STRING_TYPES 94
+#define VT_INCOMPLETE_TB5_ENCOUNTERED 95
+#define VT_ERR_CONVERSION_FROM_STRING_2_NUMBER 96
+#define VT_INVALID_STRING 97
+#define VT_UNSUPPORTED_DATA_SIZE 98
+#define VT_TBRW_INIT_FAILED 99
+#define VT_PLUGIN_UNLOAD 100
+#define VT_PLUGIN_ENTRY_NULL 101
+#define VT_UNKNOWN_PLUGIN 102
+#define VT_BUFFER_TOO_SMALL 103
+#define VT_CANNOT_MODIFY_COLUMN 104
+#define VT_MULT_FILTERS_NOT_ALLOWED 105
+#define VT_ADDRESS_IN_USE 106
+#define VT_NO_MORE_MMAPS 107
+#define VT_MAX_PAGES_IN_DS_EXCEEDED 108
+#define VT_INVALID_COL_TYPE_IN_GROUP_INFO 109
+#define VT_AGG_FN_ON_VARCHAR_NOT_SUPP 110
+#define VT_INVALID_ACCESS_PERMS 111
+#define VT_NO_DATA_TO_DISPLAY 112
+#define VT_TB5_IS_NOT_BOUND 113
+#define VT_MISSING_GROUP_BY_COLUMN 114
+#define VT_SMRK_MAX_STREAMS_EXCEEDED 115
+#define VT_SMRK_STREAM_NOT_CREATED 116
+#define VT_SMRK_NOT_IMPL 117
+#define VT_SMRK_TYPE_NOT_IMPL 118
+#define VT_SMRK_TYPE_ALREADY_SET 119
+#define VT_SMRK_NO_STREAM 120
+#define VT_SMRK_INVALID_STREAM_TYPE 121
+#define VT_SMRK_STREAM_NOT_FOUND 122
+#define VT_SMRK_FAIL 123
+#define VT_SECTION_NOT_READABLE 124
+#define VT_SECTION_NOT_WRITEABLE 125
+#define VT_GLOBAL_SECTION_NOT_CLOSED 126
+#define VT_STREAM_SECTION_NOT_CLOSED 127
+#define VT_STREAM_NOT_CLOSED 128
+#define VT_STREAM_NOT_BOUND 129
+#define VT_NO_COLS_SPECIFIED 130
+#define VT_NOT_ALL_SECTIONS_CLOSED 131
+#define VT_SMRK_INVALID_PTR 132
+#define VT_UNEXPECTED_BIND_MISMATCH 133
+#define VT_WIN_TIMER_ERROR 134
+#define VT_ONLY_SNGL_DEPNDT_COL_ALLWD 135
+#define VT_BAD_MODULE 136
+#define VT_INPUT_SOURCE_INFO_NOT_SET 137
+#define VT_UNSUPPORTED_TIME_GRAN 138
+#define VT_NO_SAMPLES_COLLECTED 139
+#define VT_INVALID_CPU_TYPE_VERSION 140
+#define VT_BIND_UNEXPECTED_1STMODREC 141
+#define VT_BIND_MODULES_NOT_SORTED 142
+#define VT_UNEXPECTED_NUM_CPUIDS 143
+#define VT_UNSUPPORTED_ARCH_TYPE 144
+#define VT_NO_DATA_TO_WRITE 145
+#define VT_EM_TIME_SLICE_TOO_SMALL 146
+#define VT_EM_TOO_MANY_EVENT_GROUPS 147
+#define VT_EM_ZERO_GROUPS 148
+#define VT_EM_NOT_SUPPORTED 149
+#define VT_PMU_IN_USE 150
+#define VT_TOO_MANY_INTERRUPTS 151
+#define VT_MAX_SAMPLES_REACHED 152
+#define VT_MODULE_COLLECTION_FAILED 153
+#define VT_INCOMPATIBLE_DRIVER 154
+#define VT_UNABLE_LOCATE_TRIGGER_EVENT 155
+#define VT_COMMAND_NOT_HANDLED 156
+#define VT_DRIVER_VERSION_MISMATCH 157
+#define VT_MAX_MARKERS 158
+#define VT_DRIVER_COMM_FAILED 159
+#define VT_CHIPSET_CONFIG_FAILED 160
+#define VT_BAD_DATA_BASE 161
+#define VT_PAX_SERVICE_NOT_CONNECTED 162
+#define VT_PAX_SERVICE_ERROR 163
+#define VT_PAX_PMU_RESERVE_FAILED 164
+#define VT_INVALID_CPU_INFO_TYPE 165
+#define VT_CACHE_DOESNT_EXIST 166
+#define VT_UNSUPPORTED_UNCORE_ARCH_TYPE 167
+#define VT_EXCEEDED_MAX_EVENTS 168
+#define VT_MARKER_TIMER_FAILED 169
+#define VT_PAX_PMU_UNRESERVE_FAILED 170
+#define VT_MULTIPLE_PROCESSES_FOUND 171
+#define VT_NO_SUCH_PROCESS_FOUND 172
+#define VT_PCL_NOT_ENABLED 173
+#define VT_PCL_UID_CHECK 174
+#define VT_DEL_RESULTS_DIR_FAILED 175
+#define VT_NO_VALID_EVENTS 176
+#define VT_INVALID_EVENT 177
+#define VT_EVENTS_COUNTED 178
+#define VT_EVENTS_COLLECTED 179
+#define VT_UNSUPPORTED_GFX_ARCH_TYPE 180
+#define VT_GFX_CONFIG_FAILED 181
+#define VT_UNSUPPORTED_NON_NATIVE_MODE 182
+#define VT_INVALID_DEVICE 183
+#define VT_ENV_SETUP_FAILED 184
+#define VT_RESUME_NOT_RECEIVED 185
+#define VT_UNSUPPORTED_PWR_ARCH_TYPE 186
+#define VT_PWR_CONFIG_FAILED 187
+#define VT_NMI_WATCHDOG_FOUND 188
+#define VT_NO_PMU_RESOURCES 189
+#define VT_MIC_CARD_NOT_ONLINE 190
+#define VT_FREEZE_ON_PMI_NOT_AVAIL 191
+#define VT_FLUSH_FAILED 192
+#define VT_FLUSH_SUCCESS 193
+#define VT_WRITE_ERROR 194
+#define VT_NO_SPACE 195
+#define VT_MSR_ACCESS_ERROR 196
+#define VT_PEBS_NOT_SUPPORTED 197
+#define VT_LUA_PARSE_ERROR 198
+#define VT_COMM_CONNECTION_CLOSED_BY_REMOTE 199
+#define VT_COMM_LISTEN_ERROR 200
+#define VT_COMM_BIND_ERROR 201
+#define VT_COMM_ACCEPT_ERROR 202
+#define VT_COMM_SEND_ERROR 203
+#define VT_COMM_RECV_ERROR 204
+#define VT_COMM_SOCKET_ERROR 205
+#define VT_COMM_CONNECT_ERROR 206
+#define VT_TARGET_COLLECTION_MISMATCH 207
+#define VT_INVALID_SEP_DRIVER_LOG 208
+#define VT_COMM_PROTOCOL_VERSION_MISTMATCH 209
+#define VT_SAMP_IN_UNEXPECTED_STATE 210
+#define VT_COMM_RECV_BUF_RESIZE_ERROR 211
+
+/*
+ * define error code for checking on async marker request
+ */
+#define VT_INVALID_MARKER_ID -1
+
+/*
+ * ************************************************************
+ * NOTE: after adding new error code(s), remember to also
+ *       update the following:
+ *           1) VT_LAST_ERROR_CODE below
+ *           2) viewer/sampling_utils/src/rise.c
+ *           3) collector/controller/sep_msg_catalog.xmc
+ *           4) qnx_kernel/sepdk/include/rise_errors.h
+ *
+ * ************************************************************
+ */
+
+//
+// To make error checking easier, the special VT_LAST_ERROR_CODE
+// should be set to whatever is the last error on the list above
+//
+#define VT_LAST_ERROR_CODE VT_COMM_RECV_BUF_RESIZE_ERROR
+
+//
+// Define a macro to determine success or failure. Users of this
+// error header file should use the macros instead of direct
+// checks so that we can change the error numbers in the future
+// (such as making negative numbers be an error indication and positive
+// numbers being a success with a value indication)
+//
+#define VTSA_SUCCESS(x) ((x) == VT_SUCCESS)
+#define VTSA_FAILED(x) (!VTSA_SUCCESS(x))
+
+//
+// These should be deprecated, but we'll keep them here just in case
+//
+#define SEP_IS_SUCCESS(x) VTSA_SUCCESS(x)
+#define SEP_IS_FAILED(x) VTSA_FAILED(x)
+
+/*************************************************************
+ * API Error Codes
+ *************************************************************/
+#define VTAPI_INVALID_MAX_SAMP VT_INVALID_MAX_SAMP
+#define VTAPI_INVALID_SAMP_PER_BUFF VT_INVALID_SAMP_PER_BUFF
+#define VTAPI_INVALID_SAMP_INTERVAL VT_INVALID_SAMP_INTERVAL
+#define VTAPI_INVALID_PATH VT_INVALID_PATH
+#define VTAPI_TB5_IN_USE VT_TB5_IN_USE
+#define VTAPI_INVALID_NUM_EVENTS VT_INVALID_NUM_EVENTS
+#define VTAPI_INTERNAL_ERROR VT_INTERNAL_ERROR
+#define VTAPI_BAD_EVENT_NAME VT_BAD_EVENT_NAME
+#define VTAPI_NO_SAMP_SESSION VT_NO_SAMP_SESSION
+#define VTAPI_NO_EVENTS VT_NO_EVENTS
+#define VTAPI_MULTIPLE_RUNS VT_MULTIPLE_RUNS
+#define VTAPI_NO_SAM_PARAMS VT_NO_SAM_PARAMS
+#define VTAPI_SDB_ALREADY_EXISTS VT_SDB_ALREADY_EXISTS
+#define VTAPI_SAMPLING_ALREADY_STARTED VT_SAMPLING_ALREADY_STARTED
+#define VTAPI_TBS_NOT_SUPPORTED VT_TBS_NOT_SUPPORTED
+#define VTAPI_INVALID_SAMPARAMS_SIZE VT_INVALID_SAMPARAMS_SIZE
+#define VTAPI_INVALID_EVENT_SIZE VT_INVALID_EVENT_SIZE
+#define VTAPI_ALREADY_PROCESSES VT_ALREADY_PROCESSES
+#define VTAPI_INVALID_EVENTS_PATH VT_INVALID_EVENTS_PATH
+#define VTAPI_INVALID_LICENSE VT_INVALID_LICENSE
+
+typedef int RISE_ERROR;
+typedef void *RISE_PTR;
+
+#endif
diff --git a/drivers/platform/x86/sepdk/pax/Makefile b/drivers/platform/x86/sepdk/pax/Makefile
new file mode 100755
index 000000000000..267d70eeaab5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/pax/Makefile
@@ -0,0 +1,4 @@
+ccflags-y := -I$(src)/../include -I$(src)/../inc
+
+obj-$(CONFIG_SEP_PAX) += pax.o
+
diff --git a/drivers/platform/x86/sepdk/pax/pax.c b/drivers/platform/x86/sepdk/pax/pax.c
new file mode 100755
index 000000000000..f8eebf989b0e
--- /dev/null
+++ b/drivers/platform/x86/sepdk/pax/pax.c
@@ -0,0 +1,967 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include <linux/fs.h>
+#include <linux/kobject.h>
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/version.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/delay.h>
+#if defined(CONFIG_HARDLOCKUP_DETECTOR) &&                                     \
+	LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#endif
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv.h"
+#include "lwpmudrv_ioctl.h"
+
+#include "control.h"
+#include "pax_shared.h"
+#include "pax.h"
+
+MODULE_AUTHOR("Copyright(C) 2009-2018 Intel Corporation");
+MODULE_VERSION(PAX_NAME "_" PAX_VERSION_STR);
+MODULE_LICENSE("Dual BSD/GPL");
+
+typedef struct PAX_DEV_NODE_S PAX_DEV_NODE;
+typedef PAX_DEV_NODE * PAX_DEV;
+
+struct PAX_DEV_NODE_S {
+	long buffer;
+	struct semaphore sem;
+	struct cdev cdev;
+};
+
+#define PAX_DEV_buffer(dev) ((dev)->buffer)
+#define PAX_DEV_sem(dev) ((dev)->sem)
+#define PAX_DEV_cdev(dev) ((dev)->cdev)
+
+// global variables for the PAX driver
+
+static PAX_DEV pax_control; // main control
+static dev_t pax_devnum; // the major char device number for PAX
+static PAX_VERSION_NODE pax_version; // version of PAX
+static PAX_INFO_NODE pax_info; // information on PAX
+static PAX_STATUS_NODE pax_status; // PAX reservation status
+
+static struct class *pax_class;
+
+#define NMI_WATCHDOG_PATH "/proc/sys/kernel/nmi_watchdog"
+static S8 nmi_watchdog_restore = '0';
+
+static struct proc_dir_entry *pax_version_file;
+
+static int pax_version_proc_read(struct seq_file *, void *);
+static int pax_version_proc_open(struct inode *, struct file *);
+static struct file_operations pax_version_ops = {
+	.owner = THIS_MODULE,
+	.open = pax_version_proc_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+// Print macros for kernel debugging
+
+#if defined(DEBUG)
+#define PAX_PRINT_DEBUG(fmt, args...)                                          \
+	{                                                                      \
+		printk(KERN_INFO "PAX: [DEBUG] " fmt, ##args);                 \
+	}
+#else
+#define PAX_PRINT_DEBUG(fmt, args...)                                          \
+	{                                                                      \
+		;                                                              \
+	}
+#endif
+#define PAX_PRINT(fmt, args...)                                                \
+	{                                                                      \
+		printk(KERN_INFO "PAX: " fmt, ##args);                         \
+	}
+#define PAX_PRINT_WARNING(fmt, args...)                                        \
+	{                                                                      \
+		printk(KERN_ALERT "PAX: [Warning] " fmt, ##args);              \
+	}
+#define PAX_PRINT_ERROR(fmt, args...)                                          \
+	{                                                                      \
+		printk(KERN_CRIT "PAX: [ERROR] " fmt, ##args);                 \
+	}
+
+// various other useful macros
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 25)
+#define PAX_FIND_TASK_BY_PID(pid) find_task_by_pid(pid)
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(4, 7, 0)
+#define PAX_FIND_TASK_BY_PID(pid)                                              \
+	pid_task(find_pid_ns(pid, &init_pid_ns), PIDTYPE_PID);
+#else
+#define PAX_FIND_TASK_BY_PID(pid) pid_task(find_get_pid(pid), PIDTYPE_PID);
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 18)
+#define PAX_TASKLIST_READ_LOCK() read_lock(&tasklist_lock)
+#define PAX_TASKLIST_READ_UNLOCK() read_unlock(&tasklist_lock)
+#else
+#define PAX_TASKLIST_READ_LOCK() rcu_read_lock()
+#define PAX_TASKLIST_READ_UNLOCK() rcu_read_unlock()
+#endif
+
+#if defined(CONFIG_HARDLOCKUP_DETECTOR) &&                                     \
+	LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+
+static struct task_struct *pax_Enable_NMIWatchdog_Thread;
+static struct semaphore pax_Enable_NMIWatchdog_Sem;
+static struct task_struct *pax_Disable_NMIWatchdog_Thread;
+static struct semaphore pax_Disable_NMIWatchdog_Sem;
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  S32 pax_Disable_NMIWatchdog(PVOID data)
+ *
+ * @param data - Pointer to data
+ *
+ * @return S32
+ *
+ * @brief Disable nmi watchdog
+ *
+ * <I>Special Notes</I>
+ */
+static S32 pax_Disable_NMIWatchdog(PVOID data)
+{
+	struct file *fd;
+	mm_segment_t old_fs;
+	struct cred *kcred;
+	loff_t pos = 0;
+	S8 new_val = '0';
+
+	up(&pax_Disable_NMIWatchdog_Sem);
+
+	kcred = prepare_kernel_cred(NULL);
+	if (kcred) {
+		commit_creds(kcred);
+	} else {
+		PAX_PRINT_ERROR(
+			"pax_Disable_NMIWatchdog: prepare_kernel_cred returns NULL\n");
+	}
+
+	fd = filp_open(NMI_WATCHDOG_PATH, O_RDWR, 0);
+
+	if (fd) {
+		fd->f_op->read(fd, (char __user *)&nmi_watchdog_restore, 1, &fd->f_pos);
+		PAX_PRINT_DEBUG("Existing nmi_watchdog value = %c\n",
+				nmi_watchdog_restore);
+
+		if (nmi_watchdog_restore != '0') {
+			old_fs = get_fs();
+			set_fs(KERNEL_DS);
+			fd->f_op->write(fd, (char __user *)&new_val, 1, &pos);
+			set_fs(old_fs);
+		} else {
+			PAX_PRINT_DEBUG(
+				"pax_Disable_NMIWatchdog: NMI watchdog already disabled!\n");
+		}
+
+		filp_close(fd, NULL);
+	} else {
+		PAX_PRINT_ERROR(
+			"pax_Disable_NMIWatchdog: filp_open returns NULL\n");
+	}
+
+	while (!kthread_should_stop()) {
+		schedule();
+	}
+
+	return 0;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  S32 pax_Check_NMIWatchdog(PVOID data)
+ *
+ * @param data - Pointer to data
+ *
+ * @return S32
+ *
+ * @brief Check nmi watchdog
+ *
+ * <I>Special Notes</I>
+ */
+
+#if 0
+static S32 pax_Check_NMIWatchdog(PVOID data)
+{
+	struct file *fd;
+	struct cred *kcred;
+
+	kcred = prepare_kernel_cred(NULL);
+	if (kcred) {
+		commit_creds(kcred);
+	}
+
+	fd = filp_open(NMI_WATCHDOG_PATH, O_RDWR, 0);
+
+	if (fd) {
+		fd->f_op->read(fd, &nmi_watchdog_restore, 1, &fd->f_pos);
+		PAX_PRINT_DEBUG("Checking nmi_watchdog value = %c\n",
+				nmi_watchdog_restore);
+		filp_close(fd, NULL);
+	}
+
+	do_exit(0);
+
+	return 0;
+}
+#endif
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  S32 pax_Enable_NMIWatchdog(PVOID data)
+ *
+ * @param data - Pointer to data
+ *
+ * @return S32
+ *
+ * @brief Enable nmi watchdog
+ *
+ * <I>Special Notes</I>
+ */
+static S32 pax_Enable_NMIWatchdog(PVOID data)
+{
+	struct file *fd;
+	mm_segment_t old_fs;
+	struct cred *kcred;
+	loff_t pos = 0;
+	S8 new_val = '1';
+
+	up(&pax_Enable_NMIWatchdog_Sem);
+
+	kcred = prepare_kernel_cred(NULL);
+	if (kcred) {
+		commit_creds(kcred);
+	} else {
+		PAX_PRINT_ERROR(
+			"pax_Enable_NMIWatchdog: prepare_kernel_cred returns NULL!\n");
+	}
+
+	fd = filp_open(NMI_WATCHDOG_PATH, O_WRONLY, 0);
+
+	if (fd) {
+		old_fs = get_fs();
+		set_fs(KERNEL_DS);
+		fd->f_op->write(fd, (char __user *)&new_val, 1, &pos);
+		set_fs(old_fs);
+
+		filp_close(fd, NULL);
+	} else {
+		PAX_PRINT_ERROR(
+			"pax_Enable_NMIWatchdog: filp_open returns NULL!\n");
+	}
+
+	while (!kthread_should_stop()) {
+		schedule();
+	}
+
+	return 0;
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     void pax_Init()
+ *
+ * @param  none
+ *
+ * @return none
+ *
+ * @brief  Initialize PAX system
+ *
+ * <I>Special Notes</I>
+ */
+static void pax_Init(void)
+{
+	//
+	// Initialize PAX driver version (done once at driver load time)
+	//
+
+	PAX_VERSION_NODE_major(&pax_version) = PAX_MAJOR_VERSION;
+	PAX_VERSION_NODE_minor(&pax_version) = PAX_MINOR_VERSION;
+	PAX_VERSION_NODE_bugfix(&pax_version) = PAX_BUGFIX_VERSION;
+
+	// initialize PAX_Info
+	pax_info.version = PAX_VERSION_NODE_version(&pax_version);
+	pax_info.managed_by = 1; // THIS_MODULE->name;
+
+	// initialize PAX_Status
+	pax_status.guid = PAX_GUID_UNINITIALIZED;
+	pax_status.pid = 0;
+	pax_status.start_time = 0;
+	pax_status.is_reserved = PAX_PMU_UNRESERVED;
+
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     void pax_Cleanup()
+ *
+ * @param  none
+ *
+ * @return none
+ *
+ * @brief  UnInitialize PAX system
+ *
+ * <I>Special Notes</I>
+ */
+static void pax_Cleanup(void)
+{
+	// uninitialize PAX_Info
+	pax_info.managed_by = 0;
+
+	// uninitialize PAX_Status
+	pax_status.guid = PAX_GUID_UNINITIALIZED;
+	pax_status.pid = 0;
+	pax_status.start_time = 0;
+	pax_status.is_reserved = PAX_PMU_UNRESERVED;
+
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     U32 pax_Process_Valid()
+ *
+ * @param  U32 pid - process ID
+ *
+ * @return TRUE or FALSE
+ *
+ * @brief  Check whether process with pid still exists, and if so,
+ *         whether it is still "alive".  If so, then process is
+ *         deemed valid.  Otherwise, process is deemed invalid.
+ *
+ * <I>Special Notes</I>
+ */
+static U32 pax_Process_Valid(U32 pid)
+{
+	struct task_struct *process_task;
+	U32 valid_process;
+
+	//
+	// There doesn't seem to be a way to force the process_task to continue
+	// to exist after the read_lock is released (SMP system could delete the
+	// process after lock is released on another processor), so we need to
+	// do all the work with the lock held... There is a routine on later
+	// 2.6 kernels (get_task_struct() and put_task_struct()) which seems
+	// to do what we want, but the code behind the macro calls a function
+	// that isn't EXPORT'ed so we can't use it in a device driver...
+	//
+	PAX_TASKLIST_READ_LOCK();
+	process_task = PAX_FIND_TASK_BY_PID(pax_status.pid);
+	if ((process_task == NULL) ||
+	    (process_task->exit_state == EXIT_ZOMBIE) ||
+	    (process_task->exit_state == EXIT_DEAD)) {
+		// not a valid process
+		valid_process = FALSE;
+	} else {
+		// process is "alive", so assume it is still valid ...
+		valid_process = TRUE;
+	}
+	PAX_TASKLIST_READ_UNLOCK();
+
+	return valid_process;
+}
+
+// **************************************************************************
+//
+// below are PAX Open/Read/Write device functions (appears in /proc/kallsyms)
+//
+// **************************************************************************
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     int pax_Open()
+ *
+ * @param  struct inode *inode
+ * @param  struct file  *filp
+ *
+ * @return int (TODO: check for open failure)
+ *
+ * @brief  This function is called when doing an open(/dev/pax)
+ *
+ * <I>Special Notes</I>
+ */
+static int pax_Open(struct inode *inode, struct file *filp)
+{
+	PAX_PRINT_DEBUG("open called on maj:%d, min:%d\n", imajor(inode),
+			iminor(inode));
+	filp->private_data = container_of(inode->i_cdev, PAX_DEV_NODE, cdev);
+
+	return 0;
+}
+
+// **************************************************************************
+//
+// below are PAX IOCTL function handlers
+//
+// **************************************************************************
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     OS_STATUS pax_Get_Info()
+ *
+ * @param  IOCTL_ARGS arg  - pointer to the output buffer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the PAX_IOCTL_INFO call
+ *         Returns static information related to PAX (e.g., version)
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS pax_Get_Info(IOCTL_ARGS arg)
+{
+	int error;
+
+	error = copy_to_user((void __user *)(arg->buf_usr_to_drv),
+				  &pax_info, sizeof(PAX_INFO_NODE));
+
+	if (error != 0) {
+		PAX_PRINT_ERROR(
+			"pax_Get_Info: unable to copy to user (error=%d)!\n",
+			error);
+		return OS_FAULT;
+	}
+
+	PAX_PRINT_DEBUG("pax_Get_Info: sending PAX info (%ld bytes):\n",
+			sizeof(PAX_INFO_NODE));
+	PAX_PRINT_DEBUG("pax_Get_Info:      raw_version = %u (0x%x)\n",
+			pax_info.version, pax_info.version);
+	PAX_PRINT_DEBUG("pax_Get_Info:            major = %u\n",
+			PAX_VERSION_NODE_major(&pax_version));
+	PAX_PRINT_DEBUG("pax_Get_Info:            minor = %u\n",
+			PAX_VERSION_NODE_minor(&pax_version));
+	PAX_PRINT_DEBUG("pax_Get_Info:           bugfix = %u\n",
+			PAX_VERSION_NODE_bugfix(&pax_version));
+	PAX_PRINT_DEBUG("pax_Get_Info:      managed_by = %lu\n",
+			(long unsigned int)pax_info.managed_by);
+	PAX_PRINT_DEBUG("pax_Get_Info: information sent.\n");
+
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     OS_STATUS pax_Get_Status()
+ *
+ * @param  IOCTL_ARGS arg  - pointer to the output buffer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the PAX_IOCTL_STATUS call
+ *         Returns status of the reservation (e.g., who owns)
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS pax_Get_Status(IOCTL_ARGS arg)
+{
+	int error;
+
+	error = copy_to_user((void __user *)(arg->buf_usr_to_drv),
+				  &pax_status, sizeof(PAX_STATUS_NODE));
+	if (error != 0) {
+		PAX_PRINT_ERROR(
+			"pax_Get_Status: unable to copy to user (error=%d)!\n",
+			error);
+		return OS_FAULT;
+	}
+
+	PAX_PRINT_DEBUG("pax_Get_Status: sending PAX status (%ld bytes):\n",
+			sizeof(PAX_STATUS_NODE));
+	PAX_PRINT_DEBUG("pax_Get_Status:    guid = %lu\n",
+			(long unsigned int)pax_status.guid);
+	PAX_PRINT_DEBUG("pax_Get_Status:    pid = %lu\n",
+			(long unsigned int)pax_status.pid);
+	PAX_PRINT_DEBUG("pax_Get_Status:    start_time = %lu\n",
+			(long unsigned int)pax_status.start_time);
+	PAX_PRINT_DEBUG("pax_Get_Status:    is_reserved = %u\n",
+			pax_status.is_reserved);
+	PAX_PRINT_DEBUG("pax_Get_Status: status sent.\n");
+
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     OS_STATUS pax_Unreserve()
+ *
+ * @param  none
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the PAX_IOCTL_UNRESERVE call
+ *         Returns OS_SUCCESS if PMU unreservation succeeded, otherwise failure
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS pax_Unreserve(void)
+{
+	// if no reservation is currently held, then return success
+	if (pax_status.is_reserved == PAX_PMU_UNRESERVED) {
+		PAX_PRINT_DEBUG("pax_Unreserve: currently unreserved\n");
+		return OS_SUCCESS;
+	}
+
+	// otherwise, there is a reservation ...
+	// allow the process which started the reservation to unreserve
+	// or if that process is invalid, then any other process can unreserve
+	if ((pax_status.pid == current->pid) ||
+	    (!pax_Process_Valid(pax_status.pid))) {
+		S32 reservation = -1;
+		PAX_PRINT_DEBUG(
+			"pax_Unreserve: pid %d attempting to unreserve PMU held by pid %d\n",
+			(U32)current->pid, (U32)pax_status.pid);
+
+#if !defined(DRV_ANDROID) && !defined(DRV_CHROMEOS) &&                         \
+	defined(CONFIG_HARDLOCKUP_DETECTOR) &&                                 \
+	LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+		if (nmi_watchdog_restore != '0') {
+			PAX_PRINT_DEBUG(
+				"Attempting to enable NMI watchdog...\n");
+
+			sema_init(&pax_Enable_NMIWatchdog_Sem, 0);
+
+			pax_Enable_NMIWatchdog_Thread =
+				kthread_run(&pax_Enable_NMIWatchdog, NULL,
+					    "pax_enable_nmi_watchdog");
+			if (!pax_Enable_NMIWatchdog_Thread ||
+			    pax_Enable_NMIWatchdog_Thread == ERR_PTR(-ENOMEM)) {
+				PAX_PRINT_ERROR(
+					"pax_Unreserve: could not create pax_enable_nmi_watchdog kthread.");
+			} else {
+				down(&pax_Enable_NMIWatchdog_Sem);
+				kthread_stop(pax_Enable_NMIWatchdog_Thread);
+			}
+			pax_Enable_NMIWatchdog_Thread = NULL;
+			nmi_watchdog_restore = '0';
+		}
+#endif
+
+		reservation = cmpxchg(&pax_status.is_reserved, PAX_PMU_RESERVED,
+				      PAX_PMU_UNRESERVED);
+		if (reservation < 0) {
+			// no-op ... eliminates "variable not used" compiler warning
+		}
+		PAX_PRINT_DEBUG("pax_Unreserve: reserve=%d, is_reserved=%d\n",
+				reservation, pax_status.is_reserved);
+		// unreserve but keep track of last PID/GUID that had reservation
+	}
+
+	PAX_PRINT_DEBUG("pax_Unreserve: pid %d unreserve status: %d\n",
+			current->pid, pax_status.is_reserved);
+
+	return ((pax_status.is_reserved == PAX_PMU_UNRESERVED) ? OS_SUCCESS :
+								 OS_FAULT);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     OS_STATUS pax_Reserve_All()
+ *
+ * @param  none
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the PAX_IOCTL_RESERVE_ALL call
+ *         Returns OS_SUCCESS if PMU reservation succeeded, otherwise failure
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS pax_Reserve_All(void)
+{
+	S32 reservation = -1; // previous reservation state (initially, unknown)
+
+	// check if PMU can be unreserved
+	if (pax_status.is_reserved == PAX_PMU_RESERVED) {
+		OS_STATUS unreserve_err = pax_Unreserve();
+		if (unreserve_err != OS_SUCCESS) {
+			return unreserve_err; // attempt to unreserve failed, so return error
+		}
+	}
+
+	PAX_PRINT_DEBUG("pax_Reserve_All: pid %d attempting to reserve PMU\n",
+			current->pid);
+
+	// at this point, there is no reservation, so commence race to reserve ...
+	reservation = cmpxchg(&pax_status.is_reserved, PAX_PMU_UNRESERVED,
+			      PAX_PMU_RESERVED);
+
+	// only one request to reserve will succeed, and when it does, update status
+	// information with the successful request
+	if ((reservation == PAX_PMU_UNRESERVED) &&
+	    (pax_status.is_reserved == PAX_PMU_RESERVED)) {
+		pax_status.start_time = rdtsc_ordered();
+		pax_status.pid = current->pid;
+
+#if !defined(DRV_ANDROID) && !defined(DRV_CHROMEOS) &&                         \
+	defined(CONFIG_HARDLOCKUP_DETECTOR) &&                                 \
+	LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+		sema_init(&pax_Disable_NMIWatchdog_Sem, 0);
+		pax_Disable_NMIWatchdog_Thread =
+			kthread_run(&pax_Disable_NMIWatchdog, NULL,
+				    "pax_disable_nmi_watchdog");
+		if (!pax_Disable_NMIWatchdog_Thread ||
+		    pax_Disable_NMIWatchdog_Thread == ERR_PTR(-ENOMEM)) {
+			PAX_PRINT_ERROR(
+				"pax_Reserve_All: could not create pax_disable_nmi_watchdog kthread.");
+		} else {
+			down(&pax_Disable_NMIWatchdog_Sem);
+			kthread_stop(pax_Disable_NMIWatchdog_Thread);
+		}
+		pax_Disable_NMIWatchdog_Thread = NULL;
+#endif
+
+		return OS_SUCCESS;
+	}
+
+	return OS_FAULT;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     OS_STATUS pax_Service_IOCTL()
+ *
+ * @param  inode - pointer to the device object
+ * @param  filp  - pointer to the file object
+ * @param  cmd   - ioctl value (defined in lwpmu_ioctl.h)
+ * @param  arg   - arg or arg pointer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Worker function that handles IOCTL requests from the user mode
+ *
+ * <I>Special Notes</I>
+ */
+static IOCTL_OP_TYPE pax_Service_IOCTL(IOCTL_USE_INODE struct file *filp,
+				       unsigned int cmd,
+				       IOCTL_ARGS_NODE local_args)
+{
+	int status = OS_SUCCESS;
+
+	// dispatch to appropriate PAX IOCTL function
+	switch (cmd) {
+	case PAX_IOCTL_INFO:
+		PAX_PRINT_DEBUG("PAX_IOCTL_INFO\n");
+		status = pax_Get_Info(&local_args);
+		break;
+
+	case PAX_IOCTL_STATUS:
+		PAX_PRINT_DEBUG("PAX_IOCTL_STATUS\n");
+		status = pax_Get_Status(&local_args);
+		break;
+
+	case PAX_IOCTL_RESERVE_ALL:
+		PAX_PRINT_DEBUG("PAX_IOCTL_RESERVE_ALL\n");
+		status = pax_Reserve_All();
+		break;
+
+	case PAX_IOCTL_UNRESERVE:
+		PAX_PRINT_DEBUG("PAX_IOCTL_UNRESERVE\n");
+		status = pax_Unreserve();
+		break;
+
+	default:
+		PAX_PRINT_ERROR("unknown IOCTL cmd: %d magic:%d number:%d\n",
+				cmd, _IOC_TYPE(cmd), _IOC_NR(cmd));
+		status = OS_ILLEGAL_IOCTL;
+		break;
+	}
+
+	return status;
+}
+
+static long pax_Device_Control(IOCTL_USE_INODE struct file *filp,
+			       unsigned int cmd, unsigned long arg)
+{
+	int status = OS_SUCCESS;
+	IOCTL_ARGS_NODE local_args;
+
+	memset(&local_args, 0, sizeof(IOCTL_ARGS_NODE));
+	if (arg) {
+		status = copy_from_user(&local_args, (void __user *)arg,
+					sizeof(IOCTL_ARGS_NODE));
+		if (status != OS_SUCCESS)
+			return status;
+	}
+
+	status = pax_Service_IOCTL(IOCTL_USE_INODE filp, cmd, local_args);
+	return status;
+}
+
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+static IOCTL_OP_TYPE pax_Device_Control_Compat(struct file *filp,
+					       unsigned int cmd,
+					       unsigned long arg)
+{
+	int status = OS_SUCCESS;
+	IOCTL_COMPAT_ARGS_NODE local_args_compat;
+	IOCTL_ARGS_NODE local_args;
+
+	memset(&local_args_compat, 0, sizeof(IOCTL_COMPAT_ARGS_NODE));
+	if (arg) {
+		status = copy_from_user(&local_args_compat,
+					(void __user *)arg,
+					sizeof(IOCTL_COMPAT_ARGS_NODE));
+		if (status != OS_SUCCESS)
+			return status;
+	}
+
+	local_args.len_drv_to_usr = local_args_compat.len_drv_to_usr;
+	local_args.len_usr_to_drv = local_args_compat.len_usr_to_drv;
+	local_args.buf_drv_to_usr =
+		(char *)compat_ptr(local_args_compat.buf_drv_to_usr);
+	local_args.buf_usr_to_drv =
+		(char *)compat_ptr(local_args_compat.buf_usr_to_drv);
+
+	if (cmd == PAX_IOCTL_COMPAT_INFO) {
+		cmd = PAX_IOCTL_INFO;
+	}
+	local_args.command = cmd;
+
+	status = pax_Service_IOCTL(filp, cmd, local_args);
+
+	return status;
+}
+#endif
+
+// **************************************************************************
+//
+// PAX device file operation definitions (required by kernel)
+//
+// **************************************************************************
+
+/*
+ * Structure that declares the usual file access functions
+ * First one is for pax, the control functions
+ */
+static struct file_operations pax_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = pax_Device_Control,
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+	.compat_ioctl = pax_Device_Control_Compat,
+#endif
+	.read = NULL,
+	.write = NULL,
+	.open = pax_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     int pax_Setup_Cdev()
+ *
+ * @param  dev    - pointer to the device object
+ * @param  devnum - major/minor device number
+ * @param  fops   - point to file operations struct
+ *
+ * @return int
+ *
+ * @brief  Set up functions to be handled by PAX device
+ *
+ * <I>Special Notes</I>
+ */
+static int pax_Setup_Cdev(PAX_DEV dev, struct file_operations *fops,
+			  dev_t devnum)
+{
+	cdev_init(&PAX_DEV_cdev(dev), fops);
+	PAX_DEV_cdev(dev).owner = THIS_MODULE;
+	PAX_DEV_cdev(dev).ops = fops;
+
+	return cdev_add(&PAX_DEV_cdev(dev), devnum, 1);
+}
+
+static int pax_version_proc_read(struct seq_file *file, void *v)
+{
+	seq_printf(file, "%u", PAX_VERSION_NODE_version(&pax_version));
+
+	return 0;
+}
+
+static int pax_version_proc_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, pax_version_proc_read, NULL);
+}
+
+// **************************************************************************
+//
+// Exported PAX functions (see pax.h) ; will appear under /proc/kallsyms
+//
+// **************************************************************************
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     int pax_Load()
+ *
+ * @param  none
+ *
+ * @return int
+ *
+ * @brief  Load the PAX subsystem
+ *
+ * <I>Special Notes</I>
+ */
+int pax_Load(void)
+{
+	int result;
+	struct device *pax_device;
+
+	pax_control = NULL;
+
+	PAX_PRINT_DEBUG("checking for %s interface...\n", PAX_NAME);
+
+	/* If PAX interface does not exist, create it */
+	pax_devnum = MKDEV(0, 0);
+	PAX_PRINT_DEBUG("got major device %d\n", pax_devnum);
+	/* allocate character device */
+	result = alloc_chrdev_region(&pax_devnum, 0, 1, PAX_NAME);
+	if (result < 0) {
+		PAX_PRINT_ERROR("unable to alloc chrdev_region for %s!\n",
+				PAX_NAME);
+		return result;
+	}
+
+	pax_class = class_create(THIS_MODULE, "pax");
+	if (IS_ERR(pax_class)) {
+		PAX_PRINT_ERROR("Error registering pax class\n");
+	}
+	pax_device = device_create(pax_class, NULL, pax_devnum, NULL, "pax");
+	if (pax_device == NULL) {
+		return OS_INVALID;
+	}
+
+	PAX_PRINT_DEBUG("%s major number is %d\n", PAX_NAME, MAJOR(pax_devnum));
+	/* Allocate memory for the PAX control device */
+	pax_control = (PVOID)kzalloc(sizeof(PAX_DEV_NODE), GFP_KERNEL);
+	if (!pax_control) {
+		PAX_PRINT_ERROR("Unable to allocate memory for %s device\n",
+				PAX_NAME);
+		return OS_NO_MEM;
+	}
+	// /* Initialize memory for the PAX control device */
+	// memset(pax_control, '\0', sizeof(PAX_DEV_NODE));
+	/* Register PAX file operations with the OS */
+	result = pax_Setup_Cdev(pax_control, &pax_Fops, pax_devnum);
+	if (result) {
+		PAX_PRINT_ERROR("Unable to add %s as char device (error=%d)\n",
+				PAX_NAME, result);
+		return result;
+	}
+
+	pax_Init();
+
+	pax_version_file =
+		proc_create("pax_version", 0, NULL, &pax_version_ops);
+	if (pax_version_file == NULL) {
+		SEP_PRINT_ERROR("Unalbe to create the pax_version proc file\n");
+	}
+
+	//
+	// Display driver version information
+	//
+	PAX_PRINT("PMU arbitration service v%d.%d.%d has been started.\n",
+		  PAX_VERSION_NODE_major(&pax_version),
+		  PAX_VERSION_NODE_minor(&pax_version),
+		  PAX_VERSION_NODE_bugfix(&pax_version));
+
+	return result;
+}
+
+EXPORT_SYMBOL(pax_Load);
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn     int pax_Unload()
+ *
+ * @param  none
+ *
+ * @return none
+ *
+ * @brief  Unload the PAX subsystem
+ *
+ * <I>Special Notes</I>
+ */
+void pax_Unload(void)
+{
+	// warn if unable to unreserve
+	if (pax_Unreserve() != OS_SUCCESS) {
+		PAX_PRINT_WARNING(
+			"Unloading driver with existing reservation ....");
+		PAX_PRINT_WARNING("         guid = %lu\n",
+				  (long unsigned int)pax_status.guid);
+		PAX_PRINT_WARNING("          pid = %ld\n",
+				  (long int)pax_status.pid);
+		PAX_PRINT_WARNING("   start_time = %lu\n",
+				  (long unsigned int)pax_status.start_time);
+		PAX_PRINT_WARNING("  is_reserved = %u\n",
+				  pax_status.is_reserved);
+	}
+
+	// unregister PAX device
+	unregister_chrdev(MAJOR(pax_devnum), "pax");
+	device_destroy(pax_class, pax_devnum);
+	class_destroy(pax_class);
+
+	cdev_del(&PAX_DEV_cdev(pax_control));
+	unregister_chrdev_region(pax_devnum, 1);
+	if (pax_control != NULL) {
+		kfree(pax_control);
+	}
+
+	remove_proc_entry("pax_version", NULL);
+
+	//
+	// Display driver version information
+	//
+	PAX_PRINT("PMU arbitration service v%d.%d.%d has been stopped.\n",
+		  PAX_VERSION_NODE_major(&pax_version),
+		  PAX_VERSION_NODE_minor(&pax_version),
+		  PAX_VERSION_NODE_bugfix(&pax_version));
+
+	// clean up resources used by PAX
+	pax_Cleanup();
+
+}
+
+EXPORT_SYMBOL(pax_Unload);
+
+/* Declaration of the init and exit functions */
+module_init(pax_Load);
+module_exit(pax_Unload);
diff --git a/drivers/platform/x86/sepdk/pax/pax.h b/drivers/platform/x86/sepdk/pax/pax.h
new file mode 100755
index 000000000000..b7d48f874958
--- /dev/null
+++ b/drivers/platform/x86/sepdk/pax/pax.h
@@ -0,0 +1,33 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#ifndef _PAX_H_
+#define _PAX_H_
+
+int pax_Load(void);
+
+void pax_Unload(void);
+
+#endif
diff --git a/drivers/platform/x86/sepdk/sep/Makefile b/drivers/platform/x86/sepdk/sep/Makefile
new file mode 100755
index 000000000000..405e55d53c97
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/Makefile
@@ -0,0 +1,67 @@
+ccflags-y := -I$(src)/../include -I$(src)/../inc -I$(src)/..
+ccflags-y += -DSEP_CONFIG_MODULE_LAYOUT
+# TODO: verify kaiser.h
+#ccflags-y += -DKAISER_HEADER_PRESENT
+ccflags-y += -DDRV_CPU_HOTPLUG -DDRV_USE_TASKLET_WORKAROUND
+
+asflags-y := -I$(src)/..
+
+ifdef CONFIG_SEP_PER_USER_MODE
+	ccflags-y += -DSECURE_SEP
+endif
+
+ifdef CONFIG_SEP_MINLOG_MODE
+	ccflags-y += -DDRV_MINIMAL_LOGGING
+endif
+
+ifdef CONFIG_SEP_MAXLOG_MODE
+	ccflags-y += -DDRV_MAXIMAL_LOGGING
+endif
+
+ifdef CONFIG_SEP_PRIVATE_BUILD
+	ccflags-y += -DENABLE_CPUS -DBUILD_CHIPSET -DBUILD_GFX
+endif
+
+ifdef CONFIG_SEP_ACRN
+	ccflags-y += -DDRV_SEP_ACRN_ON
+endif
+
+obj-$(CONFIG_SEP)		+= sep5.o
+
+sep5-y	:=	lwpmudrv.o        \
+		control.o         \
+		cpumon.o          \
+		eventmux.o        \
+		linuxos.o         \
+		output.o          \
+		pmi.o             \
+		sys_info.o        \
+		utility.o         \
+		valleyview_sochap.o    \
+		unc_power.o	  \
+		core2.o           \
+		perfver4.o        \
+		silvermont.o      \
+		pci.o             \
+		apic.o            \
+		pebs.o            \
+		unc_gt.o          \
+		unc_mmio.o        \
+		unc_msr.o         \
+		unc_common.o      \
+		unc_pci.o         \
+		sepdrv_p_state.o
+
+
+ifdef CONFIG_X86_64
+	sep5-y	+=	sys64.o
+endif
+
+ifdef CONFIG_X86_32
+	sep5-y	+=	sys32.o
+endif
+
+sep5-$(CONFIG_SEP_PRIVATE_BUILD)	+=	chap.o    \
+						gmch.o    \
+						gfx.o     \
+						unc_sa.o
diff --git a/drivers/platform/x86/sepdk/sep/apic.c b/drivers/platform/x86/sepdk/sep/apic.c
new file mode 100755
index 000000000000..693c526d63de
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/apic.c
@@ -0,0 +1,228 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/interrupt.h>
+#include <asm/msr.h>
+#include <asm/apic.h>
+#include <asm/io_apic.h>
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 32)
+#include <xen/xen.h>
+#endif
+#if defined(CONFIG_XEN_DOM0) && LINUX_VERSION_CODE > KERNEL_VERSION(3, 3, 0)
+#include <xen/interface/platform.h>
+#include <asm/xen/hypercall.h>
+#endif
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "apic.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "utility.h"
+
+
+#if defined(DRV_SEP_ACRN_ON)
+extern struct profiling_vm_info_list *vm_info_list;
+#else
+static DEFINE_PER_CPU(unsigned long, saved_apic_lvtpc);
+#endif
+
+/*!
+ * @fn          VOID apic_Get_APIC_ID(S32 cpu)
+ *
+ * @brief       Obtain APIC ID
+ *
+ * @param       S32 cpuid - cpu index
+ *
+ * @return      U32 APIC ID
+ */
+static VOID apic_Get_APIC_ID(S32 cpu)
+{
+	U32 apic_id = 0;
+	CPU_STATE pcpu;
+#if defined(DRV_SEP_ACRN_ON)
+	U32 i;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("CPU: %d.", cpu);
+	pcpu = &pcb[cpu];
+
+#if defined(CONFIG_XEN_DOM0) && LINUX_VERSION_CODE > KERNEL_VERSION(3, 3, 0)
+	if (xen_initial_domain()) {
+		S32 ret = 0;
+		struct xen_platform_op op = {
+			.cmd = XENPF_get_cpuinfo,
+			.interface_version = XENPF_INTERFACE_VERSION,
+			.u.pcpu_info.xen_cpuid = cpu,
+		};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 5, 0)
+		ret = HYPERVISOR_platform_op(&op);
+#else
+		ret = HYPERVISOR_dom0_op(&op);
+#endif
+		if (ret) {
+			SEP_DRV_LOG_ERROR(
+				"apic_Get_APIC_ID:Error in reading APIC ID on Xen PV");
+			apic_id = 0;
+		} else {
+			apic_id = op.u.pcpu_info.apic_id;
+		}
+	} else {
+#endif
+#ifdef CONFIG_X86_LOCAL_APIC
+		apic_id = read_apic_id();
+#endif
+#if defined(CONFIG_XEN_DOM0) && LINUX_VERSION_CODE > KERNEL_VERSION(3, 3, 0)
+	}
+#endif
+
+#if defined(DRV_SEP_ACRN_ON)
+	CPU_STATE_apic_id(pcpu) = 0;
+	if (vm_info_list == NULL) {
+		SEP_PRINT_ERROR(
+			"apic_Get_APIC_ID: Error in reading APIC ID on ACRN\n");
+	} else {
+		for (i = 0; i < vm_info_list->num_vms; i++) {
+			if (vm_info_list->vm_list[i].vm_id == 0xFFFFFFFF) {
+				CPU_STATE_apic_id(pcpu) =
+					vm_info_list->vm_list[i]
+						.cpu_map[cpu]
+						.apic_id;
+				break;
+			}
+		}
+	}
+#else
+	CPU_STATE_apic_id(pcpu) = apic_id;
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Apic_id[%d] is %d.", cpu,
+			      CPU_STATE_apic_id(pcpu));
+}
+
+/*!
+ * @fn          extern VOID APIC_Init(param)
+ *
+ * @brief       initialize the local APIC
+ *
+ * @param       int cpu_idx - The cpu to deinit
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ * This routine is expected to be called via the CONTROL_Parallel routine
+ */
+VOID APIC_Init(PVOID param)
+{
+	S32 me;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	if (param == NULL) {
+		preempt_disable();
+		me = CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		me = *(S32 *)param;
+	}
+
+	apic_Get_APIC_ID(me);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          extern VOID APIC_Install_Interrupt_Handler(param)
+ *
+ * @brief       Install the interrupt handler
+ *
+ * @param       int param - The linear address of the Local APIC
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ * The linear address is necessary if the LAPIC is used.  If X2APIC is
+ * used the linear address is not necessary.
+ */
+VOID APIC_Install_Interrupt_Handler(PVOID param)
+{
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+#if !defined(DRV_SEP_ACRN_ON)
+	per_cpu(saved_apic_lvtpc, CONTROL_THIS_CPU()) = apic_read(APIC_LVTPC);
+	apic_write(APIC_LVTPC, APIC_DM_NMI);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          extern VOID APIC_Enable_PMI(void)
+ *
+ * @brief       Enable the PMU interrupt
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+VOID APIC_Enable_Pmi(VOID)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+#if !defined(DRV_SEP_ACRN_ON)
+	apic_write(APIC_LVTPC, APIC_DM_NMI);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          extern VOID APIC_Restore_LVTPC(void)
+ *
+ * @brief       Restore APIC LVTPC value
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+VOID APIC_Restore_LVTPC(PVOID param)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+#if !defined(DRV_SEP_ACRN_ON)
+	apic_write(APIC_LVTPC, per_cpu(saved_apic_lvtpc, CONTROL_THIS_CPU()));
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/chap.c b/drivers/platform/x86/sepdk/sep/chap.c
new file mode 100755
index 000000000000..434e9aeb658e
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/chap.c
@@ -0,0 +1,474 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include <linux/version.h>
+#include <linux/percpu.h>
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "lwpmudrv_chipset.h"
+#include "inc/lwpmudrv.h"
+#include "inc/control.h"
+#include "inc/ecb_iterators.h"
+#include "inc/utility.h"
+#include "inc/chap.h"
+
+extern DRV_CONFIG drv_cfg;
+extern CHIPSET_CONFIG pma;
+extern CPU_STATE pcb;
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static U32 chap_Init_Chipset(void)
+ *
+ * @brief       Chipset PMU initialization
+ *
+ * @param       None
+ *
+ * @return      VT_SUCCESS if successful, otherwise error
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+static U32 chap_Init_Chipset(void)
+{
+	U32 i;
+	CHIPSET_SEGMENT mch_chipset_seg = &CHIPSET_CONFIG_mch(pma);
+	CHIPSET_SEGMENT ich_chipset_seg = &CHIPSET_CONFIG_ich(pma);
+	CHIPSET_SEGMENT noa_chipset_seg = &CHIPSET_CONFIG_noa(pma);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	SEP_DRV_LOG_TRACE("Initializing chipset ...");
+
+	if (DRV_CONFIG_enable_chipset(drv_cfg)) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			pcb[i].chipset_count_init = TRUE;
+		}
+		if ((CHIPSET_CONFIG_mch_chipset(pma)) &&
+		(CHIPSET_SEGMENT_virtual_address(mch_chipset_seg) == 0)) {
+			// Map virtual address of PCI CHAP interface
+			CHIPSET_SEGMENT_virtual_address(
+				mch_chipset_seg) =
+				(U64)(UIOP)ioremap_nocache(
+					CHIPSET_SEGMENT_physical_address(
+						mch_chipset_seg),
+					CHIPSET_SEGMENT_size(
+						mch_chipset_seg));
+		}
+
+		if ((CHIPSET_CONFIG_ich_chipset(pma)) &&
+		(CHIPSET_SEGMENT_virtual_address(ich_chipset_seg) == 0)) {
+			// Map the virtual address of PCI CHAP interface
+			CHIPSET_SEGMENT_virtual_address(
+				ich_chipset_seg) =
+				(U64)(UIOP)ioremap_nocache(
+					CHIPSET_SEGMENT_physical_address(
+						ich_chipset_seg),
+					CHIPSET_SEGMENT_size(
+						ich_chipset_seg));
+		}
+
+		// Here we map the MMIO registers for the Gen X processors.
+		if ((CHIPSET_CONFIG_noa_chipset(pma)) &&
+		(CHIPSET_SEGMENT_virtual_address(noa_chipset_seg) == 0)) {
+			// Map the virtual address of PCI CHAP interface
+			CHIPSET_SEGMENT_virtual_address(
+				noa_chipset_seg) =
+				(U64)(UIOP)ioremap_nocache(
+					CHIPSET_SEGMENT_physical_address(
+						noa_chipset_seg),
+					CHIPSET_SEGMENT_size(
+						noa_chipset_seg));
+		}
+
+		//
+		// always collect processor events
+		//
+		CHIPSET_CONFIG_processor(pma) = 1;
+	} else {
+		CHIPSET_CONFIG_processor(pma) = 0;
+	}
+	SEP_DRV_LOG_TRACE("Initializing chipset done.");
+
+	SEP_DRV_LOG_TRACE_OUT("");
+	return VT_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static U32 chap_Start_Chipset(void)
+ * @param       None
+ * @return      VT_SUCCESS if successful, otherwise error
+ * @brief       Start collection on the Chipset PMU
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+static VOID chap_Start_Chipset(void)
+{
+	U32 i;
+	CHAP_INTERFACE chap;
+	CHIPSET_SEGMENT mch_chipset_seg = &CHIPSET_CONFIG_mch(pma);
+	CHIPSET_SEGMENT ich_chipset_seg = &CHIPSET_CONFIG_ich(pma);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	//
+	// reset and start chipset counters
+	//
+	SEP_DRV_LOG_TRACE("Starting chipset counters...\n");
+	if (pma) {
+		chap = (CHAP_INTERFACE)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			mch_chipset_seg);
+		if (chap != NULL) {
+			for (i = 0;
+			i < CHIPSET_SEGMENT_total_events(mch_chipset_seg);
+			i++) {
+				CHAP_INTERFACE_command_register(&chap[i]) =
+					0x00040000; // Reset to zero
+				CHAP_INTERFACE_command_register(&chap[i]) =
+					0x00010000; // Restart
+			}
+		}
+
+		chap = (CHAP_INTERFACE)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			ich_chipset_seg);
+		if (chap != NULL) {
+			for (i = 0;
+			i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+			i++) {
+				CHAP_INTERFACE_command_register(&chap[i]) =
+					0x00040000; // Reset to zero
+				CHAP_INTERFACE_command_register(&chap[i]) =
+					0x00010000; // Restart
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE("Starting chipset counters done.\n");
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static U32 chap_Read_Counters(PVOID param)
+ *
+ * @brief       Read the CHAP counter data
+ *
+ * @param       PVOID param - address of the buffer to write into
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+static VOID chap_Read_Counters(PVOID param)
+{
+	U64 *data;
+	CHAP_INTERFACE chap;
+	U32 mch_cpu;
+	int i, data_index;
+	U64 tmp_data;
+	U64 *mch_data;
+	U64 *ich_data;
+	U64 *mmio_data;
+	U64 *mmio;
+	U32 this_cpu;
+	CHIPSET_SEGMENT mch_chipset_seg = &CHIPSET_CONFIG_mch(pma);
+	CHIPSET_SEGMENT ich_chipset_seg = &CHIPSET_CONFIG_ich(pma);
+	CHIPSET_SEGMENT noa_chipset_seg = &CHIPSET_CONFIG_noa(pma);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	data = param;
+	data_index = 0;
+
+	// Save the Motherboard time.  This is universal time for this
+	// system.  This is the only 64-bit timer so we save it first so
+	// always aligned on 64-bit boundary that way.
+
+	if (CHIPSET_CONFIG_mch_chipset(pma)) {
+		mch_data = data + data_index;
+		// Save the MCH counters.
+		chap = (CHAP_INTERFACE)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			mch_chipset_seg);
+		for (i = CHIPSET_SEGMENT_start_register(mch_chipset_seg);
+		     i < CHIPSET_SEGMENT_total_events(mch_chipset_seg); i++) {
+			CHAP_INTERFACE_command_register(&chap[i]) =
+				0x00020000; // Sample
+		}
+
+		// The StartingReadRegister is only used for special event
+		// configs that use CHAP counters to trigger events in other
+		// CHAP counters.  This is an unusual request but useful in
+		// getting the number of lit subspans - implying a count of the
+		// number of triangles.  I am not sure it will be used
+		// elsewhere.  We cannot read some of the counters because it
+		// will invalidate their configuration to trigger other CHAP
+		// counters.  Yuk!
+		data_index += CHIPSET_SEGMENT_start_register(mch_chipset_seg);
+		for (i = CHIPSET_SEGMENT_start_register(mch_chipset_seg);
+		     i < CHIPSET_SEGMENT_total_events(mch_chipset_seg); i++) {
+			data[data_index++] =
+				CHAP_INTERFACE_data_register(&chap[i]);
+		}
+
+		// Initialize the counters on the first interrupt
+		if (pcb[this_cpu].chipset_count_init == TRUE) {
+			for (i = 0;
+			     i < CHIPSET_SEGMENT_total_events(mch_chipset_seg);
+			     i++) {
+				pcb[this_cpu].last_mch_count[i] = mch_data[i];
+			}
+		}
+
+		// Now compute the delta!
+		// NOTE: Special modification to accomodate Gen 4 work - count
+		// everything since last interrupt - regardless of cpu!  This
+		// way there is only one count of the Gen 4 counters.
+		//
+		mch_cpu = CHIPSET_CONFIG_host_proc_run(pma) ? this_cpu : 0;
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(mch_chipset_seg);
+		     i++) {
+			tmp_data = mch_data[i];
+			if (mch_data[i] < pcb[mch_cpu].last_mch_count[i]) {
+				mch_data[i] = mch_data[i] + (U32)(-1) -
+						pcb[mch_cpu].last_mch_count[i];
+			} else {
+				mch_data[i] = mch_data[i] -
+						pcb[mch_cpu].last_mch_count[i];
+			}
+			pcb[mch_cpu].last_mch_count[i] = tmp_data;
+		}
+	}
+
+	if (CHIPSET_CONFIG_ich_chipset(pma)) {
+		// Save the ICH counters.
+		ich_data = data + data_index;
+		chap = (CHAP_INTERFACE)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			ich_chipset_seg);
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+			i++) {
+			CHAP_INTERFACE_command_register(&chap[i]) =
+				0x00020000; // Sample
+		}
+
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+			i++) {
+			data[data_index++] =
+				CHAP_INTERFACE_data_register(&chap[i]);
+		}
+
+		// Initialize the counters on the first interrupt
+		if (pcb[this_cpu].chipset_count_init == TRUE) {
+			for (i = 0;
+			i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+			i++) {
+
+				pcb[this_cpu].last_ich_count[i] = ich_data[i];
+			}
+		}
+
+		// Now compute the delta!
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+		     i++) {
+			tmp_data = ich_data[i];
+			if (ich_data[i] < pcb[this_cpu].last_ich_count[i]) {
+				ich_data[i] = ich_data[i] + (U32)(-1) -
+					pcb[this_cpu].last_ich_count[i];
+			} else {
+				ich_data[i] = ich_data[i] -
+					pcb[this_cpu].last_ich_count[i];
+			}
+			pcb[this_cpu].last_ich_count[i] = tmp_data;
+		}
+	}
+
+	if (CHIPSET_CONFIG_noa_chipset(pma)) {
+		// Save the MMIO counters.
+		mmio_data = data + data_index;
+		mmio = (U64 *)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			noa_chipset_seg);
+
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(noa_chipset_seg);
+		     i++) {
+			data[data_index++] =
+				mmio[i * 2 + 2244]; // 64-bit quantity
+		}
+
+		// Initialize the counters on the first interrupt
+		if (pcb[this_cpu].chipset_count_init == TRUE) {
+			for (i = 0;
+			     i < CHIPSET_SEGMENT_total_events(noa_chipset_seg);
+			     i++) {
+				pcb[this_cpu].last_mmio_count[i] = mmio_data[i];
+			}
+		}
+
+		// Now compute the delta!
+		for (i = 0; i < CHIPSET_SEGMENT_total_events(noa_chipset_seg);
+		     i++) {
+			tmp_data = mmio_data[i];
+			if (mmio_data[i] < pcb[this_cpu].last_mmio_count[i]) {
+				mmio_data[i] = mmio_data[i] + (U32)(-1) -
+					pcb[this_cpu].last_mmio_count[i];
+			} else {
+				mmio_data[i] = mmio_data[i] -
+					pcb[this_cpu].last_mmio_count[i];
+			}
+			pcb[this_cpu].last_mmio_count[i] = tmp_data;
+		}
+	}
+
+	pcb[this_cpu].chipset_count_init = FALSE;
+
+	FOR_EACH_DATA_REG(pecb, i)
+	{
+		data[data_index++] = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i), (U64)0);
+	}
+	END_FOR_EACH_DATA_REG;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static VOID chap_Stop_Chipset(void)
+ *
+ * @brief       Stop the Chipset PMU
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+static VOID chap_Stop_Chipset(void)
+{
+	U32 i;
+	CHAP_INTERFACE chap;
+	CHIPSET_SEGMENT mch_chipset_seg = &CHIPSET_CONFIG_mch(pma);
+	CHIPSET_SEGMENT ich_chipset_seg = &CHIPSET_CONFIG_ich(pma);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	//
+	// reset and start chipset counters
+	//
+	SEP_DRV_LOG_TRACE("Stopping chipset counters...");
+
+	if (pma == NULL) {
+		return;
+	}
+
+	if (CHIPSET_CONFIG_mch_chipset(pma)) {
+		chap = (CHAP_INTERFACE)(UIOP)
+			CHIPSET_SEGMENT_virtual_address(mch_chipset_seg);
+		if (chap != NULL) {
+			for (i = 0;
+			i < CHIPSET_SEGMENT_total_events(mch_chipset_seg);
+			i++) {
+				CHAP_INTERFACE_command_register(&chap[i])
+					= 0x00000000; // Stop
+				CHAP_INTERFACE_command_register(&chap[i])
+					= 0x00040000; // Reset to Zero
+			}
+		}
+	}
+
+	if (CHIPSET_CONFIG_ich_chipset(pma)) {
+		chap = (CHAP_INTERFACE)(UIOP)
+			CHIPSET_SEGMENT_virtual_address(
+				ich_chipset_seg);
+		if (chap != NULL) {
+			for (i = 0;
+			i < CHIPSET_SEGMENT_total_events(ich_chipset_seg);
+			i++) {
+				CHAP_INTERFACE_command_register(&chap[i])
+					= 0x00000000; // Stop
+				CHAP_INTERFACE_command_register(&chap[i])
+					= 0x00040000; // Reset to Zero
+			}
+		}
+	}
+
+	if (CHIPSET_CONFIG_mch_chipset(pma) &&
+		CHIPSET_SEGMENT_virtual_address(mch_chipset_seg)) {
+
+		iounmap((void __iomem *)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			mch_chipset_seg));
+		CHIPSET_SEGMENT_virtual_address(mch_chipset_seg) = 0;
+	}
+
+	if (CHIPSET_CONFIG_ich_chipset(pma) &&
+		CHIPSET_SEGMENT_virtual_address(ich_chipset_seg)) {
+
+		iounmap((void __iomem *)(UIOP)CHIPSET_SEGMENT_virtual_address(
+			ich_chipset_seg));
+		CHIPSET_SEGMENT_virtual_address(ich_chipset_seg) = 0;
+	}
+	CONTROL_Free_Memory(pma);
+	pma = NULL;
+
+	SEP_DRV_LOG_TRACE("Stopped chipset counters.");
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static VOID chap_Fini_Chipset(void)
+ *
+ * @brief       Finish routine on a per-logical-core basis
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *             <NONE>
+ */
+static VOID chap_Fini_Chipset(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+	SEP_DRV_LOG_TRACE_OUT("Empty function.");
+}
+
+CS_DISPATCH_NODE chap_dispatch = {
+	.init_chipset = chap_Init_Chipset,
+	.start_chipset = chap_Start_Chipset,
+	.read_counters = chap_Read_Counters,
+	.stop_chipset = chap_Stop_Chipset,
+	.fini_chipset = chap_Fini_Chipset,
+	.Trigger_Read = NULL
+};
diff --git a/drivers/platform/x86/sepdk/sep/control.c b/drivers/platform/x86/sepdk/sep/control.c
new file mode 100755
index 000000000000..474de2c3e578
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/control.c
@@ -0,0 +1,896 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/mm.h>
+#include <linux/mempool.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+
+#include "lwpmudrv_types.h"
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "utility.h"
+#include <linux/sched.h>
+#include <linux/kallsyms.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
+#define SMP_CALL_FUNCTION(func, ctx, retry, wait)                              \
+	smp_call_function((func), (ctx), (wait))
+#define SMP_CALL_FUNCTION_SINGLE(cpuid, func, ctx, retry, wait)                \
+	smp_call_function_single((cpuid), (func), (ctx), (wait))
+#define ON_EACH_CPU(func, ctx, retry, wait) on_each_cpu((func), (ctx), (wait))
+#else
+#define SMP_CALL_FUNCTION(func, ctx, retry, wait)                              \
+	smp_call_function((func), (ctx), (retry), (wait))
+#define SMP_CALL_FUNCTION_SINGLE(cpuid, func, ctx, retry, wait)                \
+	smp_call_function_single((cpuid), (func), (ctx), (retry), (wait))
+#define ON_EACH_CPU(func, ctx, retry, wait)                                    \
+	on_each_cpu((func), (ctx), (retry), (wait))
+#endif
+
+#if defined(DRV_SEP_ACRN_ON)
+void (*local_vfree_atomic)(const void *addr) = NULL;
+#endif
+
+/*
+ */
+GLOBAL_STATE_NODE driver_state;
+MSR_DATA msr_data;
+static MEM_TRACKER mem_tr_head; // start of the mem tracker list
+static MEM_TRACKER mem_tr_tail; // end of mem tracker list
+static spinlock_t mem_tr_lock; // spinlock for mem tracker list
+static unsigned long flags;
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID CONTROL_Invoke_Cpu (func, ctx, arg)
+ *
+ * @brief    Set up a DPC call and insert it into the queue
+ *
+ * @param    IN cpu_idx  - the core id to dispatch this function to
+ *           IN func     - function to be invoked by the specified core(s)
+ *           IN ctx      - pointer to the parameter block for each function
+ *                         invocation
+ *
+ * @return   None
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+VOID CONTROL_Invoke_Cpu(int cpu_idx, VOID (*func)(PVOID), PVOID ctx)
+{
+	SEP_DRV_LOG_TRACE_IN("CPU: %d, function: %p, ctx: %p.", cpu_idx, func,
+			     ctx);
+	SMP_CALL_FUNCTION_SINGLE(cpu_idx, func, ctx, 0, 1);
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_Service(func, ctx, blocking, exclude)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function invocation
+ * @param    blocking - Wait for invoked function to complete
+ * @param    exclude  - exclude the current core from executing the code
+ *
+ * @returns  None
+ *
+ * @brief    Service routine to handle all kinds of parallel invoke on all CPU calls
+ *
+ * <I>Special Notes:</I>
+ *           Invoke the function provided in parallel in either a blocking or
+ *           non-blocking mode.  The current core may be excluded if desired.
+ *           NOTE - Do not call this function directly from source code.
+ *           Use the aliases CONTROL_Invoke_Parallel(), CONTROL_Invoke_Parallel_NB(),
+ *           or CONTROL_Invoke_Parallel_XS().
+ *
+ */
+VOID CONTROL_Invoke_Parallel_Service(VOID (*func)(PVOID), PVOID ctx,
+					    int blocking, int exclude)
+{
+	SEP_DRV_LOG_TRACE_IN("Fn: %p, ctx: %p, block: %d, excl: %d.",
+			     func, ctx, blocking, exclude);
+
+	GLOBAL_STATE_cpu_count(driver_state) = 0;
+	GLOBAL_STATE_dpc_count(driver_state) = 0;
+
+	if (GLOBAL_STATE_num_cpus(driver_state) == 1) {
+		if (!exclude) {
+			func(ctx);
+		}
+		SEP_DRV_LOG_TRACE_OUT("");
+		return;
+	}
+	if (!exclude) {
+		ON_EACH_CPU(func, ctx, 0, blocking);
+		SEP_DRV_LOG_TRACE_OUT("");
+		return;
+	}
+
+	preempt_disable();
+	SMP_CALL_FUNCTION(func, ctx, 0, blocking);
+	preempt_enable();
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID control_Memory_Tracker_Delete_Node(mem_tr)
+ *
+ * @param    IN mem_tr    - memory tracker node to delete
+ *
+ * @returns  None
+ *
+ * @brief    Delete specified node in the memory tracker
+ *
+ * <I>Special Notes:</I>
+ *           Assumes mem_tr_lock is already held while calling this function!
+ */
+static VOID control_Memory_Tracker_Delete_Node(MEM_TRACKER mem_tr)
+{
+	MEM_TRACKER prev_tr = NULL;
+	MEM_TRACKER next_tr = NULL;
+	U32 size = 0;
+
+	SEP_DRV_LOG_ALLOC_IN("");
+
+	if (!mem_tr) {
+		SEP_DRV_LOG_ALLOC_OUT("mem_tr is NULL!");
+		return;
+	}
+	size = MEM_TRACKER_max_size(mem_tr) * sizeof(MEM_EL_NODE);
+	// update the linked list
+	prev_tr = MEM_TRACKER_prev(mem_tr);
+	next_tr = MEM_TRACKER_next(mem_tr);
+	if (prev_tr) {
+		MEM_TRACKER_next(prev_tr) = next_tr;
+	}
+	if (next_tr) {
+		MEM_TRACKER_prev(next_tr) = prev_tr;
+	}
+
+	// free the allocated mem_el array (if any)
+	if (MEM_TRACKER_mem(mem_tr)) {
+		if (MEM_TRACKER_array_vmalloc(mem_tr)) {
+			vfree(MEM_TRACKER_mem(mem_tr));
+		} else {
+			if (size < MAX_KMALLOC_SIZE) {
+				kfree(MEM_TRACKER_mem(mem_tr));
+			} else {
+				free_pages(
+					(unsigned long)MEM_TRACKER_mem(mem_tr),
+					get_order(size));
+			}
+		}
+	}
+
+	// free the mem_tracker node
+	if (MEM_TRACKER_node_vmalloc(mem_tr)) {
+		vfree(mem_tr);
+	} else {
+		kfree(mem_tr);
+	}
+	SEP_DRV_LOG_ALLOC_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID control_Memory_Tracker_Create_Node(void)
+ *
+ * @param    None    - size of the memory to allocate
+ *
+ * @returns  OS_SUCCESS if successful, otherwise error
+ *
+ * @brief    Initialize the memory tracker
+ *
+ * <I>Special Notes:</I>
+ *           Assumes mem_tr_lock is already held while calling this function!
+ *
+ *           Since this function can be called within either GFP_KERNEL or
+ *           GFP_ATOMIC contexts, the most restrictive allocation is used
+ *           (viz., GFP_ATOMIC).
+ */
+static U32 control_Memory_Tracker_Create_Node(void)
+{
+	U32 size = MEM_EL_MAX_ARRAY_SIZE * sizeof(MEM_EL_NODE);
+	PVOID location = NULL;
+	MEM_TRACKER mem_tr = NULL;
+
+	SEP_DRV_LOG_ALLOC_IN("");
+
+	// create a mem tracker node
+	mem_tr = (MEM_TRACKER)kmalloc(sizeof(MEM_TRACKER_NODE), GFP_ATOMIC);
+	if (!mem_tr) {
+		mem_tr = (MEM_TRACKER)vmalloc(sizeof(MEM_TRACKER_NODE));
+		if (mem_tr) {
+			MEM_TRACKER_node_vmalloc(mem_tr) = 1;
+		} else {
+			SEP_DRV_LOG_ERROR_ALLOC_OUT(
+				"Failed to allocate mem tracker node.");
+			return OS_FAULT;
+		}
+	} else {
+		MEM_TRACKER_node_vmalloc(mem_tr) = 0;
+	}
+	SEP_DRV_LOG_TRACE("Node %p, vmalloc %d.", mem_tr,
+			  MEM_TRACKER_node_vmalloc(mem_tr));
+
+	// create an initial array of mem_el's inside the mem tracker node
+	MEM_TRACKER_array_vmalloc(mem_tr) = 0;
+	if (size < MAX_KMALLOC_SIZE) {
+		location = (PVOID)kmalloc(size, GFP_ATOMIC);
+		SEP_DRV_LOG_ALLOC("Allocated small memory (0x%p, %d).",
+				  location, (S32)size);
+	} else {
+		location = (PVOID)__get_free_pages(GFP_ATOMIC, get_order(size));
+		SEP_DRV_LOG_ALLOC("Allocated large memory (0x%p, %d).",
+				  location, (S32)size);
+	}
+	if (!location) {
+		location = (PVOID)vmalloc(size);
+		if (location) {
+			MEM_TRACKER_array_vmalloc(mem_tr) = 1;
+			SEP_DRV_LOG_ALLOC(
+				"Allocated memory (vmalloc) (0x%p, %d).",
+				location, (S32)size);
+		} else {
+			if (MEM_TRACKER_node_vmalloc(mem_tr)) {
+				vfree(mem_tr);
+			} else {
+				kfree(mem_tr);
+			}
+			SEP_DRV_LOG_ERROR_ALLOC_OUT(
+				"Failed to allocate mem_el array... deleting node.");
+			return OS_FAULT;
+		}
+	}
+
+	// initialize new mem tracker node
+	MEM_TRACKER_mem(mem_tr) = location;
+	MEM_TRACKER_prev(mem_tr) = NULL;
+	MEM_TRACKER_next(mem_tr) = NULL;
+
+	// initialize mem_tracker's mem_el array
+	MEM_TRACKER_max_size(mem_tr) = MEM_EL_MAX_ARRAY_SIZE;
+	MEM_TRACKER_elements(mem_tr) = 0;
+	memset(MEM_TRACKER_mem(mem_tr), 0, size);
+
+	// update the linked list
+	if (!mem_tr_head) {
+		mem_tr_head = mem_tr;
+	} else {
+		MEM_TRACKER_prev(mem_tr) = mem_tr_tail;
+		MEM_TRACKER_next(mem_tr_tail) = mem_tr;
+	}
+	mem_tr_tail = mem_tr;
+
+	SEP_DRV_LOG_ALLOC_OUT("Allocated node=0x%p, max_elements=%d, size=%d.",
+			MEM_TRACKER_mem(mem_tr_tail),
+			MEM_EL_MAX_ARRAY_SIZE, size);
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID control_Memory_Tracker_Add(location, size, vmalloc_flag)
+ *
+ * @param    IN location     - memory location
+ * @param    IN size         - size of the memory to allocate
+ * @param    IN vmalloc_flag - flag that indicates if the allocation was done with vmalloc
+ *
+ * @returns  None
+ *
+ * @brief    Keep track of allocated memory with memory tracker
+ *
+ * <I>Special Notes:</I>
+ *           Starting from first mem_tracker node, the algorithm
+ *           finds the first "hole" in the mem_tracker list and
+ *           tracks the memory allocation there.
+ */
+static U32 control_Memory_Tracker_Add(PVOID location, ssize_t size,
+				      DRV_BOOL vmalloc_flag)
+{
+	S32 i, n;
+	U32 status;
+	DRV_BOOL found;
+	MEM_TRACKER mem_tr;
+
+	SEP_DRV_LOG_ALLOC_IN("Location: %p, size: %u, flag: %u.", location,
+			     (U32)size, vmalloc_flag);
+
+	spin_lock_irqsave(&mem_tr_lock, flags);
+
+	// check if there is space in ANY of mem_tracker's nodes for the memory item
+	mem_tr = mem_tr_head;
+	found = FALSE;
+	status = OS_SUCCESS;
+	i = n = 0;
+	while (mem_tr && (!found)) {
+		if (MEM_TRACKER_elements(mem_tr) <
+		    MEM_TRACKER_max_size(mem_tr)) {
+			for (i = 0; i < MEM_TRACKER_max_size(mem_tr); i++) {
+				if (!MEM_TRACKER_mem_address(mem_tr, i)) {
+					SEP_DRV_LOG_ALLOC(
+						"Found index %d of %d available.",
+						i,
+						MEM_TRACKER_max_size(mem_tr) -
+							1);
+					n = i;
+					found = TRUE;
+					break;
+				}
+			}
+		}
+		if (!found) {
+			mem_tr = MEM_TRACKER_next(mem_tr);
+		}
+	}
+
+	if (!found) {
+		// extend into (i.e., create new) mem_tracker node ...
+		status = control_Memory_Tracker_Create_Node();
+		if (status != OS_SUCCESS) {
+			SEP_DRV_LOG_ERROR("Unable to create mem tracker node.");
+			goto finish_add;
+		}
+		// use mem tracker tail node and first available entry in mem_el array
+		mem_tr = mem_tr_tail;
+		n = 0;
+	}
+
+	// we now have a location in mem tracker to keep track of the memory item
+	MEM_TRACKER_mem_address(mem_tr, n) = location;
+	MEM_TRACKER_mem_size(mem_tr, n) = size;
+	MEM_TRACKER_mem_vmalloc(mem_tr, n) = vmalloc_flag;
+	MEM_TRACKER_elements(mem_tr)++;
+	SEP_DRV_LOG_ALLOC("Tracking (0x%p, %d) in node %d of %d.", location,
+			  (S32)size, n, MEM_TRACKER_max_size(mem_tr) - 1);
+
+finish_add:
+	spin_unlock_irqrestore(&mem_tr_lock, flags);
+
+	SEP_DRV_LOG_ALLOC_OUT("Result: %u.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Init(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Initializes Memory Tracker
+ *
+ * <I>Special Notes:</I>
+ *           This should only be called when the driver is being loaded.
+ */
+VOID CONTROL_Memory_Tracker_Init(void)
+{
+	SEP_DRV_LOG_ALLOC_IN("Initializing mem tracker.");
+
+	mem_tr_head = NULL;
+	mem_tr_tail = NULL;
+
+	spin_lock_init(&mem_tr_lock);
+
+	SEP_DRV_LOG_ALLOC_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Free(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Frees memory used by Memory Tracker
+ *
+ * <I>Special Notes:</I>
+ *           This should only be called when the driver is being unloaded.
+ */
+VOID CONTROL_Memory_Tracker_Free(void)
+{
+	S32 i;
+	MEM_TRACKER temp;
+
+	SEP_DRV_LOG_ALLOC_IN("Destroying mem tracker.");
+
+	spin_lock_irqsave(&mem_tr_lock, flags);
+
+	// check for any memory that was not freed, and free it
+	while (mem_tr_head) {
+		if (MEM_TRACKER_elements(mem_tr_head)) {
+			for (i = 0; i < MEM_TRACKER_max_size(mem_tr_head);
+			     i++) {
+				if (MEM_TRACKER_mem_address(mem_tr_head, i)) {
+					SEP_DRV_LOG_WARNING(
+						"Index %d of %d, not freed (0x%p, %d) ... freeing now.",
+						i,
+						MEM_TRACKER_max_size(
+							mem_tr_head) -
+							1,
+						MEM_TRACKER_mem_address(
+							mem_tr_head, i),
+						MEM_TRACKER_mem_size(
+							mem_tr_head, i));
+
+					if (MEM_TRACKER_mem_vmalloc(mem_tr_head,
+								    i)) {
+						vfree(MEM_TRACKER_mem_address(
+							mem_tr_head, i));
+					} else {
+						free_pages(
+							(unsigned long)
+								MEM_TRACKER_mem_address(
+									mem_tr_head,
+									i),
+							get_order(MEM_TRACKER_mem_size(
+								mem_tr_head,
+								i)));
+					}
+					MEM_TRACKER_mem_address(mem_tr_head,
+								i) = NULL;
+					MEM_TRACKER_mem_size(mem_tr_head, i) =
+						0;
+					MEM_TRACKER_mem_vmalloc(mem_tr_head,
+								i) = 0;
+				}
+			}
+		}
+		temp = mem_tr_head;
+		mem_tr_head = MEM_TRACKER_next(mem_tr_head);
+		control_Memory_Tracker_Delete_Node(temp);
+	}
+
+	mem_tr_tail = NULL;
+
+	spin_unlock_irqrestore(&mem_tr_lock, flags);
+
+	SEP_DRV_LOG_ALLOC_OUT("Mem tracker destruction complete.");
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID CONTROL_Memory_Tracker_Compaction(void)
+ *
+ * @param    None
+ *
+ * @returns  None
+ *
+ * @brief    Compacts the memory allocator if holes are detected
+ *
+ * <I>Special Notes:</I>
+ *           The algorithm compacts mem_tracker nodes such that
+ *           node entries are full starting from mem_tr_head
+ *           up until the first empty node is detected, after
+ *           which nodes up to mem_tr_tail will be empty.
+ *           At end of collection (or at other safe sync point),
+ *           we reclaim/compact space used by mem tracker.
+ */
+VOID CONTROL_Memory_Tracker_Compaction(void)
+{
+	S32 i, j, n, m, c, d;
+	DRV_BOOL found, overlap;
+	MEM_TRACKER mem_tr1, mem_tr2, empty_tr;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	spin_lock_irqsave(&mem_tr_lock, flags);
+
+	mem_tr1 = mem_tr_head;
+
+	i = j = n = c = d = 0;
+
+	/*
+	 * step1: free up the track node which does not contain any elements.
+	 */
+	while (mem_tr1) {
+		SEP_DRV_LOG_ALLOC("Node %p, index %d, elememts %d.", mem_tr1, n,
+				  MEM_TRACKER_elements(mem_tr1));
+		if (MEM_TRACKER_elements(mem_tr1)) {
+			mem_tr1 = MEM_TRACKER_next(mem_tr1);
+		} else {
+			empty_tr = mem_tr1;
+			mem_tr1 = MEM_TRACKER_next(mem_tr1);
+			if (empty_tr == mem_tr_head) {
+				mem_tr_head = mem_tr1;
+			}
+			if (empty_tr == mem_tr_tail) {
+				mem_tr_tail = MEM_TRACKER_prev(empty_tr);
+			}
+			control_Memory_Tracker_Delete_Node(empty_tr);
+			d++;
+			SEP_DRV_LOG_ALLOC("Delete node %p.", mem_tr1);
+		}
+	}
+
+	mem_tr1 = mem_tr_head;
+	mem_tr2 = mem_tr_tail;
+
+	/*
+	 * there is no need to compact if memory tracker was never used, or only have one track node
+	 */
+	overlap = (mem_tr1 == mem_tr2);
+	if (!mem_tr1 || !mem_tr2 || overlap) {
+		goto finish_compact;
+	}
+
+	/*
+	 * step2: there are more than 2 track node.
+	 *        starting from head node, find an empty element slot in a node
+	 *        if there is no empty slot or the node is tail, the compact is done.
+	 *        find an element in tail node, and move it to the empty slot fount below.
+	 *        if tail node is empty after moving, free it up.
+	 *        repeat until only one node.
+	 */
+	m = MEM_TRACKER_max_size(mem_tr2) - 1;
+	while (!overlap) {
+		// find an empty node
+		found = FALSE;
+		while (!found && !overlap && mem_tr1) {
+			SEP_DRV_LOG_TRACE(
+				"Looking at mem_tr1 0x%p, index=%d, elements %d.",
+				mem_tr1, n, MEM_TRACKER_elements(mem_tr1));
+			if (MEM_TRACKER_elements(mem_tr1) <
+			    MEM_TRACKER_max_size(mem_tr1)) {
+				for (i = n; i < MEM_TRACKER_max_size(mem_tr1);
+				     i++) {
+					if (!MEM_TRACKER_mem_address(mem_tr1,
+								     i)) {
+						SEP_DRV_LOG_TRACE(
+							"Found index %d of %d empty.",
+							i,
+							MEM_TRACKER_max_size(
+								mem_tr1) -
+								1);
+						found = TRUE;
+						break; // tentative
+					}
+				}
+			}
+
+			// if no overlap and an empty node was not found, then advance to next node
+			if (!found) {
+				mem_tr1 = MEM_TRACKER_next(mem_tr1);
+				// check for overlap
+				overlap = (mem_tr1 == mem_tr2);
+				n = 0;
+			}
+		}
+		// all nodes going in forward direction are full, so exit
+		if (!found || overlap || !mem_tr1) {
+			goto finish_compact;
+		}
+
+		// find a non-empty node
+		found = FALSE;
+		while (!found && !overlap && mem_tr2) {
+			SEP_DRV_LOG_ALLOC(
+				"Looking at mem_tr2 0x%p, index=%d, elements %d.",
+				mem_tr2, m, MEM_TRACKER_elements(mem_tr2));
+			if (MEM_TRACKER_elements(mem_tr2)) {
+				for (j = m; j >= 0; j--) {
+					if (MEM_TRACKER_mem_address(mem_tr2,
+								    j)) {
+						SEP_DRV_LOG_ALLOC(
+							"Found index %d of %d non-empty.",
+							j,
+							MEM_TRACKER_max_size(
+								mem_tr2) -
+								1);
+						found = TRUE;
+						// Any reason why we are not 'breaking' here?
+					}
+				}
+			}
+
+			// if no overlap and no non-empty node was found, then retreat to prev node
+			if (!found) {
+				empty_tr = mem_tr2; // keep track of empty node
+				mem_tr2 = MEM_TRACKER_prev(mem_tr2);
+				m = MEM_TRACKER_max_size(mem_tr2) - 1;
+				mem_tr_tail = mem_tr2; // keep track of new tail
+				// reclaim empty mem_tracker node
+				control_Memory_Tracker_Delete_Node(empty_tr);
+				// keep track of number of node deletions performed
+				d++;
+				// check for overlap
+				overlap = (mem_tr1 == mem_tr2);
+			}
+		}
+		// all nodes going in reverse direction are empty, so exit
+		if (!found || overlap || !mem_tr2) {
+			goto finish_compact;
+		}
+
+		// swap empty node with non-empty node so that "holes" get bubbled towards the end of list
+		MEM_TRACKER_mem_address(mem_tr1, i) =
+			MEM_TRACKER_mem_address(mem_tr2, j);
+		MEM_TRACKER_mem_size(mem_tr1, i) =
+			MEM_TRACKER_mem_size(mem_tr2, j);
+		MEM_TRACKER_mem_vmalloc(mem_tr1, i) =
+			MEM_TRACKER_mem_vmalloc(mem_tr2, j);
+		MEM_TRACKER_elements(mem_tr1)++;
+
+		MEM_TRACKER_mem_address(mem_tr2, j) = NULL;
+		MEM_TRACKER_mem_size(mem_tr2, j) = 0;
+		MEM_TRACKER_mem_vmalloc(mem_tr2, j) = FALSE;
+		MEM_TRACKER_elements(mem_tr2)--;
+
+		SEP_DRV_LOG_ALLOC(
+			"Node <%p, elemts %d, index %d> moved to <%p, elemts %d, index %d>.",
+			mem_tr2, MEM_TRACKER_elements(mem_tr2), j, mem_tr1,
+			MEM_TRACKER_elements(mem_tr1), i);
+
+		// keep track of number of memory compactions performed
+		c++;
+
+		// start new search starting from next element in mem_tr1
+		n = i + 1;
+
+		// start new search starting from prev element in mem_tr2
+		m = j - 1;
+	}
+
+finish_compact:
+	spin_unlock_irqrestore(&mem_tr_lock, flags);
+
+	SEP_DRV_LOG_FLOW_OUT(
+		"Number of elements compacted = %d, nodes deleted = %d.", c, d);
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn PVOID CONTROL_Allocate_Memory(size)
+ *
+ * @param    IN size     - size of the memory to allocate
+ *
+ * @returns  char*       - pointer to the allocated memory block
+ *
+ * @brief    Allocate and zero memory
+ *
+ * <I>Special Notes:</I>
+ *           Allocate memory in the GFP_KERNEL pool.
+ *
+ *           Use this if memory is to be allocated within a context where
+ *           the allocator can block the allocation (e.g., by putting
+ *           the caller to sleep) while it tries to free up memory to
+ *           satisfy the request.  Otherwise, if the allocation must
+ *           occur atomically (e.g., caller cannot sleep), then use
+ *           CONTROL_Allocate_KMemory instead.
+ */
+PVOID CONTROL_Allocate_Memory(size_t size)
+{
+	U32 status;
+	PVOID location = NULL;
+
+	SEP_DRV_LOG_ALLOC_IN("Attempting to allocate %d bytes.", (S32)size);
+
+	if (size <= 0) {
+		SEP_DRV_LOG_WARNING_ALLOC_OUT(
+			"Cannot allocate a number of bytes <= 0.");
+		return NULL;
+	}
+
+	// determine whether to use mem_tracker or not
+	if (size < MAX_KMALLOC_SIZE) {
+		location = (PVOID)kmalloc(size, GFP_KERNEL);
+		SEP_DRV_LOG_ALLOC("Allocated small memory (0x%p, %d)", location,
+				  (S32)size);
+	}
+	if (!location) {
+		location = (PVOID)vmalloc(size);
+		if (location) {
+			status = control_Memory_Tracker_Add(location, size,
+							    TRUE);
+			SEP_DRV_LOG_ALLOC("Allocated large memory (0x%p, %d)",
+					  location, (S32)size);
+			if (status != OS_SUCCESS) {
+				// failed to track in mem_tracker, so free up memory and return NULL
+				SEP_DRV_LOG_ERROR(
+					"Allocated %db; failed to track w/ MEM_TRACKER. Freeing...",
+					(S32)size);
+				vfree(location);
+				location = NULL;
+			}
+		}
+	}
+
+	if (!location) {
+		SEP_DRV_LOG_ERROR("Failed to allocated %db.", (S32)size);
+	} else {
+		memset(location, 0, size);
+	}
+
+	SEP_DRV_LOG_ALLOC_OUT("Returning %p.", location);
+	return location;
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn PVOID CONTROL_Allocate_KMemory(size)
+ *
+ * @param    IN size     - size of the memory to allocate
+ *
+ * @returns  char*       - pointer to the allocated memory block
+ *
+ * @brief    Allocate and zero memory
+ *
+ * <I>Special Notes:</I>
+ *           Allocate memory in the GFP_ATOMIC pool.
+ *
+ *           Use this if memory is to be allocated within a context where
+ *           the allocator cannot block the allocation (e.g., by putting
+ *           the caller to sleep) as it tries to free up memory to
+ *           satisfy the request.  Examples include interrupt handlers,
+ *           process context code holding locks, etc.
+ */
+PVOID CONTROL_Allocate_KMemory(size_t size)
+{
+	U32 status;
+	PVOID location;
+
+	SEP_DRV_LOG_ALLOC_IN("Attempting to allocate %d bytes.", (S32)size);
+
+	if (size <= 0) {
+		SEP_DRV_LOG_ALLOC_OUT(
+			"Cannot allocate a number of bytes <= 0.");
+		return NULL;
+	}
+
+	if (size < MAX_KMALLOC_SIZE) {
+		location = (PVOID)kmalloc(size, GFP_ATOMIC);
+		SEP_DRV_LOG_ALLOC("Allocated small memory (0x%p, %d)", location,
+				  (S32)size);
+	} else {
+		location = (PVOID)__get_free_pages(GFP_ATOMIC, get_order(size));
+		if (location) {
+			status = control_Memory_Tracker_Add(location, size,
+							    FALSE);
+			SEP_DRV_LOG_ALLOC("Allocated large memory (0x%p, %d)",
+					  location, (S32)size);
+			if (status != OS_SUCCESS) {
+				// failed to track in mem_tracker, so free up memory and return NULL
+				SEP_DRV_LOG_ERROR(
+					"Allocated %db; failed to track w/ MEM_TRACKER. Freeing...",
+					(S32)size);
+				free_pages((unsigned long)location,
+					   get_order(size));
+				location = NULL;
+			}
+		}
+	}
+
+	if (!location) {
+		SEP_DRV_LOG_ERROR("Failed to allocated %db.", (S32)size);
+	} else {
+		memset(location, 0, size);
+	}
+
+	SEP_DRV_LOG_ALLOC_OUT("Returning %p.", location);
+	return location;
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn PVOID CONTROL_Free_Memory(location)
+ *
+ * @param    IN location  - size of the memory to allocate
+ *
+ * @returns  pointer to the allocated memory block
+ *
+ * @brief    Frees the memory block
+ *
+ * <I>Special Notes:</I>
+ *           Does not try to free memory if fed with a NULL pointer
+ *           Expected usage:
+ *               ptr = CONTROL_Free_Memory(ptr);
+ *           Does not do compaction ... can have "holes" in
+ *           mem_tracker list after this operation.
+ */
+PVOID CONTROL_Free_Memory(PVOID location)
+{
+	S32 i;
+	DRV_BOOL found;
+	MEM_TRACKER mem_tr;
+
+	SEP_DRV_LOG_ALLOC_IN("Attempting to free %p.", location);
+
+	if (!location) {
+		SEP_DRV_LOG_ALLOC_OUT("Cannot free NULL.");
+		return NULL;
+	}
+
+#if defined(DRV_SEP_ACRN_ON)
+	if (!local_vfree_atomic) {
+		local_vfree_atomic = (PVOID)UTILITY_Find_Symbol("vfree_atomic");
+		if (!local_vfree_atomic) {
+			SEP_PRINT_ERROR("Could not find 'vfree_atomic'!\n");
+		}
+	}
+#endif
+	spin_lock_irqsave(&mem_tr_lock, flags);
+
+	// scan through mem_tracker nodes for matching entry (if any)
+	mem_tr = mem_tr_head;
+	found = FALSE;
+	while (mem_tr) {
+		for (i = 0; i < MEM_TRACKER_max_size(mem_tr); i++) {
+			if (location == MEM_TRACKER_mem_address(mem_tr, i)) {
+				SEP_DRV_LOG_ALLOC(
+					"Freeing large memory location 0x%p",
+					location);
+				found = TRUE;
+				if (MEM_TRACKER_mem_vmalloc(mem_tr, i)) {
+#if defined(DRV_SEP_ACRN_ON)
+					if (unlikely(in_atomic() &&
+						     local_vfree_atomic)) {
+						local_vfree_atomic(location);
+					} else {
+#endif
+						vfree(location);
+					}
+
+#if defined(DRV_SEP_ACRN_ON)
+				}
+#endif
+				else {
+					free_pages(
+						(unsigned long)location,
+						get_order(MEM_TRACKER_mem_size(
+							mem_tr, i)));
+				}
+				MEM_TRACKER_mem_address(mem_tr, i) = NULL;
+				MEM_TRACKER_mem_size(mem_tr, i) = 0;
+				MEM_TRACKER_mem_vmalloc(mem_tr, i) = 0;
+				MEM_TRACKER_elements(mem_tr)--;
+				goto finish_free;
+			}
+		}
+		mem_tr = MEM_TRACKER_next(mem_tr);
+	}
+
+finish_free:
+	spin_unlock_irqrestore(&mem_tr_lock, flags);
+
+	// must have been of smaller than the size limit for mem tracker nodes
+	if (!found) {
+		SEP_DRV_LOG_ALLOC("Freeing small memory location 0x%p",
+				  location);
+		kfree(location);
+	}
+
+	SEP_DRV_LOG_ALLOC_OUT("Success. Returning NULL.");
+	return NULL;
+}
diff --git a/drivers/platform/x86/sepdk/sep/core2.c b/drivers/platform/x86/sepdk/sep/core2.c
new file mode 100755
index 000000000000..a56ad28cd097
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/core2.c
@@ -0,0 +1,2137 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "lwpmudrv.h"
+#include "utility.h"
+#include "control.h"
+#include "output.h"
+#include "core2.h"
+#include "ecb_iterators.h"
+#include "pebs.h"
+#include "apic.h"
+
+#if !defined(DRV_ANDROID)
+#include "jkt_unc_ha.h"
+#include "jkt_unc_qpill.h"
+#include "pci.h"
+#endif
+
+extern EVENT_CONFIG global_ec;
+extern U64 *read_counter_info;
+extern LBR lbr;
+extern DRV_CONFIG drv_cfg;
+extern DEV_CONFIG pcfg;
+extern PWR pwr;
+extern U64 *interrupt_counts;
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+
+#if !defined(DRV_ANDROID)
+static U32 direct2core_data_saved;
+static U32 bl_bypass_data_saved;
+#endif
+
+static U32 restore_reg_addr[3];
+
+typedef struct SADDR_S {
+	S64 addr : CORE2_LBR_DATA_BITS;
+} SADDR;
+
+#define SADDR_addr(x) ((x).addr)
+#define MSR_ENERGY_MULTIPLIER 0x606 // Energy Multiplier MSR
+
+#if !defined(DRV_ANDROID)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Disable_Direct2core(ECB)
+ *
+ * @param    pecb     ECB of group being scheduled
+ *
+ * @return   None     No return needed
+ *
+ * @brief    program the QPILL and HA register for disabling of direct2core
+ *
+ * <I>Special Notes</I>
+ */
+static VOID core2_Disable_Direct2core(ECB pecb)
+{
+	U32 busno = 0;
+	U32 dev_idx = 0;
+	U32 base_idx = 0;
+	U32 device_id = 0;
+	U32 value = 0;
+	U32 vendor_id = 0;
+	U32 core2_qpill_dev_no[2] = { 8, 9 };
+	U32 this_cpu;
+
+	SEP_DRV_LOG_TRACE_IN("PECB: %p.", pecb);
+
+	this_cpu = CONTROL_THIS_CPU();
+
+	// Discover the bus # for HA
+	for (busno = 0; busno < MAX_BUSNO; busno++) {
+		value = PCI_Read_U32(busno, JKTUNC_HA_DEVICE_NO,
+				     JKTUNC_HA_D2C_FUNC_NO, 0);
+		vendor_id = value & VENDOR_ID_MASK;
+		device_id = (value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT;
+
+		if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+			continue;
+		}
+		if (device_id != JKTUNC_HA_D2C_DID) {
+			continue;
+		}
+		value = 0;
+		// now program at the offset
+		value = PCI_Read_U32(busno, JKTUNC_HA_DEVICE_NO,
+				     JKTUNC_HA_D2C_FUNC_NO,
+				     JKTUNC_HA_D2C_OFFSET);
+		restore_ha_direct2core[this_cpu][busno] = 0;
+		restore_ha_direct2core[this_cpu][busno] = value;
+	}
+	for (busno = 0; busno < MAX_BUSNO; busno++) {
+		value = PCI_Read_U32(busno, JKTUNC_HA_DEVICE_NO,
+				     JKTUNC_HA_D2C_FUNC_NO, 0);
+		vendor_id = value & VENDOR_ID_MASK;
+		device_id = (value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT;
+
+		if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+			continue;
+		}
+		if (device_id != JKTUNC_HA_D2C_DID) {
+			continue;
+		}
+
+		// now program at the offset
+		value = PCI_Read_U32(busno, JKTUNC_HA_DEVICE_NO,
+				     JKTUNC_HA_D2C_FUNC_NO,
+				     JKTUNC_HA_D2C_OFFSET);
+		value |= value | JKTUNC_HA_D2C_BITMASK;
+		PCI_Write_U32(busno, JKTUNC_HA_DEVICE_NO, JKTUNC_HA_D2C_FUNC_NO,
+			      JKTUNC_HA_D2C_OFFSET, value);
+	}
+
+	// Discover the bus # for QPI
+	for (dev_idx = 0; dev_idx < 2; dev_idx++) {
+		base_idx = dev_idx * MAX_BUSNO;
+		for (busno = 0; busno < MAX_BUSNO; busno++) {
+			value = PCI_Read_U32(busno, core2_qpill_dev_no[dev_idx],
+					     JKTUNC_QPILL_D2C_FUNC_NO, 0);
+			vendor_id = value & VENDOR_ID_MASK;
+			device_id =
+				(value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT;
+
+			if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+				continue;
+			}
+			if ((device_id != JKTUNC_QPILL0_D2C_DID) &&
+			    (device_id != JKTUNC_QPILL1_D2C_DID)) {
+				continue;
+			}
+			// now program at the corresponding offset
+			value = PCI_Read_U32(busno, core2_qpill_dev_no[dev_idx],
+					     JKTUNC_QPILL_D2C_FUNC_NO,
+					     JKTUNC_QPILL_D2C_OFFSET);
+			restore_qpi_direct2core[this_cpu][base_idx + busno] = 0;
+			restore_qpi_direct2core[this_cpu][base_idx + busno] =
+				value;
+		}
+	}
+	for (dev_idx = 0; dev_idx < 2; dev_idx++) {
+		for (busno = 0; busno < MAX_BUSNO; busno++) {
+			value = PCI_Read_U32(busno, core2_qpill_dev_no[dev_idx],
+					     JKTUNC_QPILL_D2C_FUNC_NO, 0);
+			vendor_id = value & VENDOR_ID_MASK;
+			device_id =
+				(value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT;
+
+			if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+				continue;
+			}
+			if ((device_id != JKTUNC_QPILL0_D2C_DID) &&
+			    (device_id != JKTUNC_QPILL1_D2C_DID)) {
+				continue;
+			}
+			// now program at the corresponding offset
+			value = PCI_Read_U32(busno, core2_qpill_dev_no[dev_idx],
+					     JKTUNC_QPILL_D2C_FUNC_NO,
+					     JKTUNC_QPILL_D2C_OFFSET);
+			value |= value | JKTUNC_QPILL_D2C_BITMASK;
+			PCI_Write_U32(busno, core2_qpill_dev_no[dev_idx],
+				      JKTUNC_QPILL_D2C_FUNC_NO,
+				      JKTUNC_QPILL_D2C_OFFSET, value);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Disable_BL_Bypass(ECB)
+ *
+ * @param    pecb     ECB of group being scheduled
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Disable the BL Bypass
+ *
+ * <I>Special Notes</I>
+ */
+static VOID core2_Disable_BL_Bypass(ECB pecb)
+{
+	U64 value;
+	U32 this_cpu;
+
+	SEP_DRV_LOG_TRACE_IN("PECB: %p.", pecb);
+
+	this_cpu = CONTROL_THIS_CPU();
+
+	value = SYS_Read_MSR(CORE2UNC_DISABLE_BL_BYPASS_MSR);
+	restore_bl_bypass[this_cpu] = 0;
+	restore_bl_bypass[this_cpu] = value;
+	value |= CORE2UNC_BLBYPASS_BITMASK;
+	SYS_Write_MSR(CORE2UNC_DISABLE_BL_BYPASS_MSR, value);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Write_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initial set up of the PMU registers
+ *
+ * <I>Special Notes</I>
+ *         Initial write of PMU registers.
+ *         Walk through the enties and write the value of the register accordingly.
+ *         Assumption:  For CCCR registers the enable bit is set to value 0.
+ *         When current_group = 0, then this is the first time this routine is called,
+ *         initialize the locks and set up EM tables.
+ */
+static VOID core2_Write_PMU(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	EVENT_CONFIG ec;
+	DISPATCH dispatch;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	if (CPU_STATE_current_group(pcpu) == 0) {
+		if (EVENT_CONFIG_mode(ec) != EM_DISABLED) {
+			U32 index;
+			U32 st_index;
+			U32 j;
+
+			/* Save all the initialization values away into an array for Event Multiplexing. */
+			for (j = 0; j < EVENT_CONFIG_num_groups(ec); j++) {
+				CPU_STATE_current_group(pcpu) = j;
+				st_index = CPU_STATE_current_group(pcpu) *
+					   EVENT_CONFIG_max_gp_events(ec);
+				FOR_EACH_REG_CORE_OPERATION(
+					pecb, i, PMU_OPERATION_DATA_GP)
+				{
+					index = st_index + i -
+						ECB_operations_register_start(
+							pecb,
+							PMU_OPERATION_DATA_GP);
+					CPU_STATE_em_tables(pcpu)[index] =
+						ECB_entries_reg_value(pecb, i);
+				}
+				END_FOR_EACH_REG_CORE_OPERATION;
+			}
+			/* Reset the current group to the very first one. */
+			CPU_STATE_current_group(pcpu) =
+				this_cpu % EVENT_CONFIG_num_groups(ec);
+		}
+	}
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		/*
+		 * Writing the GLOBAL Control register enables the PMU to start counting.
+		 * So write 0 into the register to prevent any counting from starting.
+		 */
+		if (i == ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+		/*
+		 *  PEBS is enabled for this collection session
+		 */
+		if (DRV_SETUP_INFO_pebs_accessible(&req_drv_setup_info) &&
+		    i == ECB_SECTION_REG_INDEX(pecb, PEBS_ENABLE_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS) &&
+		    ECB_entries_reg_value(pecb, i)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+#if defined(MYDEBUG)
+		{
+			U64 val = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE(
+				"Register 0x%x: wrvalue 0x%llx, rdvalue 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				ECB_entries_reg_value(pecb, i), val);
+		}
+#endif
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Disable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Zero out the global control register.  This automatically disables the PMU counters.
+ *
+ */
+static VOID core2_Disable_PMU(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	if (GET_DRIVER_STATE() != DRV_STATE_RUNNING) {
+		SEP_DRV_LOG_TRACE("Driver state is not RUNNING.");
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+		if (DEV_CONFIG_pebs_mode(pcfg)) {
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, PEBS_ENABLE_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				0LL);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Enable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Set the enable bit for all the Control registers
+ *
+ */
+static VOID core2_Enable_PMU(PVOID param)
+{
+	/*
+	 * Get the value from the event block
+	 *   0 == location of the global control reg for this block.
+	 *   Generalize this location awareness when possible
+	 */
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	if (KVM_guest_mode) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+	if (GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		APIC_Enable_Pmi();
+		if (CPU_STATE_reset_mask(pcpu)) {
+			SEP_DRV_LOG_TRACE("Overflow reset mask %llx.",
+					  CPU_STATE_reset_mask(pcpu));
+			// Reinitialize the global overflow control register
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			CPU_STATE_reset_mask(pcpu) = 0LL;
+		}
+		if (CPU_STATE_group_swap(pcpu)) {
+			CPU_STATE_group_swap(pcpu) = 0;
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			if (DEV_CONFIG_pebs_mode(pcfg)) {
+				SYS_Write_MSR(
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					ECB_entries_reg_value(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)));
+			}
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+#if defined(MYDEBUG)
+			{
+				U64 val;
+				val = SYS_Read_MSR(ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+				SEP_DRV_LOG_TRACE(
+					"Write reg 0x%x--- read 0x%llx.",
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							GLOBAL_CTRL_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					val);
+			}
+#endif
+		}
+	}
+	SEP_DRV_LOG_TRACE("Reenabled PMU with value 0x%llx.",
+			  ECB_entries_reg_value(pecb, 0));
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void corei7_Enable_PMU_2(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Set the enable bit for all the Control registers
+ *
+ */
+static VOID corei7_Enable_PMU_2(PVOID param)
+{
+	/*
+	 * Get the value from the event block
+	 *   0 == location of the global control reg for this block.
+	 */
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U64 pebs_val = 0;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	if (KVM_guest_mode) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+	if (GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		APIC_Enable_Pmi();
+		if (CPU_STATE_group_swap(pcpu)) {
+			CPU_STATE_group_swap(pcpu) = 0;
+			if (DEV_CONFIG_pebs_mode(pcfg)) {
+				pebs_val = SYS_Read_MSR(ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, PEBS_ENABLE_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+				if (ECB_entries_reg_value(
+					    pecb,
+					    ECB_SECTION_REG_INDEX(
+						    pecb, PEBS_ENABLE_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)) !=
+				    0) {
+					SYS_Write_MSR(
+						ECB_entries_reg_id(
+							pecb,
+							ECB_SECTION_REG_INDEX(
+								pecb,
+								PEBS_ENABLE_REG_INDEX,
+								PMU_OPERATION_GLOBAL_REGS)),
+						ECB_entries_reg_value(
+							pecb,
+							ECB_SECTION_REG_INDEX(
+								pecb,
+								PEBS_ENABLE_REG_INDEX,
+								PMU_OPERATION_GLOBAL_REGS)));
+				} else if (pebs_val != 0) {
+					SYS_Write_MSR(
+						ECB_entries_reg_id(
+							pecb,
+							ECB_SECTION_REG_INDEX(
+								pecb,
+								PEBS_ENABLE_REG_INDEX,
+								PMU_OPERATION_GLOBAL_REGS)),
+						0LL);
+				}
+			}
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+#if defined(MYDEBUG)
+			SEP_DRV_LOG_TRACE("Reenabled PMU with value 0x%llx.",
+					  ECB_entries_reg_value(pecb, 0));
+#endif
+		}
+		if (CPU_STATE_reset_mask(pcpu)) {
+#if defined(MYDEBUG)
+			SEP_DRV_LOG_TRACE("Overflow reset mask %llx.",
+					  CPU_STATE_reset_mask(pcpu));
+#endif
+			// Reinitialize the global overflow control register
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			CPU_STATE_reset_mask(pcpu) = 0LL;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Read_PMU_Data(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read all the data MSR's into a buffer.  Called by the interrupt handler.
+ *
+ */
+static void core2_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("PMU control_data 0x%p, buffer 0x%p.",
+			  LWPMU_DEVICE_PMU_register_data(&devices[dev_idx]),
+			  buffer);
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		j = EMON_BUFFER_CORE_EVENT_OFFSET(
+			EMON_BUFFER_DRIVER_HELPER_core_index_to_thread_offset_map(
+				emon_buffer_driver_helper)[this_cpu],
+			ECB_entries_core_event_id(pecb, i));
+
+		buffer[j] = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u, event_id=%u", j,
+				  buffer[j], this_cpu,
+				  ECB_entries_core_event_id(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Check_Overflow_Errata(pecb, index, overflow_status)
+ *
+ * @param  pecb:            The current event control block
+ * @param  index:           index of the register to process
+ * @param  overflow_status: current overflow mask
+ *
+ * @return Updated Event mask of the overflowed registers.
+ *
+ * @brief  Go through the overflow errata for the architecture and set the mask
+ *
+ * <I>Special Notes</I>
+ *         fixed_counter1 on some architectures gets interfered by
+ *         other event counts.  Overcome this problem by reading the
+ *         counter value and resetting the overflow mask.
+ *
+ */
+static U64 core2_Check_Overflow_Errata(ECB pecb, U32 index, U64 overflow_status)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (DRV_CONFIG_num_events(drv_cfg) == 1) {
+		SEP_DRV_LOG_TRACE_OUT("Res: %llu. (num_events = 1)",
+				      overflow_status);
+		return overflow_status;
+	}
+	if (ECB_entries_reg_id(pecb, index) == IA32_FIXED_CTR1 &&
+	    (overflow_status & 0x200000000LL) == 0LL) {
+		U64 val = SYS_Read_MSR(IA32_FIXED_CTR1);
+		val &= ECB_entries_max_bits(pecb, index);
+		if (val < ECB_entries_reg_value(pecb, index)) {
+			overflow_status |= 0x200000000LL;
+			SEP_DRV_LOG_TRACE(
+				"Reset -- clk count %llx, status %llx.", val,
+				overflow_status);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llu.", overflow_status);
+	return overflow_status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Check_Overflow(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static void core2_Check_Overflow(DRV_MASKS masks)
+{
+	U32 index;
+	U64 overflow_status = 0;
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+	DISPATCH dispatch;
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	bd = &cpu_buf[this_cpu];
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+
+	if (dispatch->check_overflow_gp_errata) {
+		overflow_status = dispatch->check_overflow_gp_errata(
+			pecb, &overflow_status_clr);
+	}
+	SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+			if (dispatch->check_overflow_errata) {
+				overflow_status =
+					dispatch->check_overflow_errata(
+						pecb, i, overflow_status);
+			}
+		} else if (ECB_entries_is_gp_reg_get(pecb, i)) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      ECB_entries_reg_value(pecb, i));
+
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (ECB_entries_fixed_reg_get(pecb, i)) {
+				CPU_STATE_p_state_counting(pcpu) = 1;
+			}
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_uncore_get(pecb, i)) {
+				DRV_EVENT_MASK_uncore_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_branch_evt_get(pecb, i)) {
+				DRV_EVENT_MASK_branch(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	// Reinitialize the global overflow control register
+	SYS_Write_MSR(ECB_entries_reg_id(
+			      pecb, ECB_SECTION_REG_INDEX(
+					    pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS)),
+		      overflow_status_clr);
+
+	SEP_DRV_LOG_TRACE("Check Overflow completed %d.", this_cpu);
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Swap_Group(restart)
+ *
+ * @param    restart    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Perform the mechanics of swapping the event groups for event mux operations
+ *
+ * <I>Special Notes</I>
+ *         Swap function for event multiplexing.
+ *         Freeze the counting.
+ *         Swap the groups.
+ *         Enable the counting.
+ *         Reset the event trigger count
+ *
+ */
+static VOID core2_Swap_Group(DRV_BOOL restart)
+{
+	U32 index;
+	U32 next_group;
+	U32 st_index;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+
+	st_index =
+		CPU_STATE_current_group(pcpu) * EVENT_CONFIG_max_gp_events(ec);
+	next_group = (CPU_STATE_current_group(pcpu) + 1);
+	if (next_group >= EVENT_CONFIG_num_groups(ec)) {
+		next_group = 0;
+	}
+
+	SEP_DRV_LOG_TRACE("current group : 0x%x.",
+			  CPU_STATE_current_group(pcpu));
+	SEP_DRV_LOG_TRACE("next group : 0x%x.", next_group);
+
+	// Save the counters for the current group
+	if (!DRV_CONFIG_event_based_counts(drv_cfg)) {
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			CPU_STATE_em_tables(pcpu)[index] =
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE("Saved value for reg 0x%x : 0x%llx.",
+					  ECB_entries_reg_id(pecb, i),
+					  CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	CPU_STATE_current_group(pcpu) = next_group;
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	// First write the GP control registers (eventsel)
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_CTRL_GP)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DRV_CONFIG_event_based_counts(drv_cfg)) {
+		// In EBC mode, reset the counts for all events except for trigger event
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+		{
+			if (ECB_entries_event_id_index(pecb, i) !=
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			}
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	} else {
+		// Then write the gp count registers
+		st_index = CPU_STATE_current_group(pcpu) *
+			   EVENT_CONFIG_max_gp_events(ec);
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      CPU_STATE_em_tables(pcpu)[index]);
+			SEP_DRV_LOG_TRACE(
+				"Restore value for reg 0x%x : 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_OCR)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	/*
+	 *  reset the em factor when a group is swapped
+	 */
+	CPU_STATE_trigger_count(pcpu) = EVENT_CONFIG_em_factor(ec);
+
+	/*
+	 * The enable routine needs to rewrite the control registers
+	 */
+	CPU_STATE_reset_mask(pcpu) = 0LL;
+	CPU_STATE_group_swap(pcpu) = 1;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Initialize(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initialize the PMU setting up for collection
+ *
+ * <I>Special Notes</I>
+ *         Saves the relevant PMU state (minimal set of MSRs required
+ *         to avoid conflicts with other Linux tools, such as Oprofile).
+ *         This function should be called in parallel across all CPUs
+ *         prior to the start of sampling, before PMU state is changed.
+ *
+ */
+static VOID core2_Initialize(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 i = 0;
+	ECB pecb = NULL;
+	U32 cur_grp;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	pcpu = &pcb[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	CPU_STATE_pmu_state(pcpu) = pmu_state + (this_cpu * 3);
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to save PMU state on CPU %d.", this_cpu);
+		return;
+	}
+
+	restore_reg_addr[0] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, DEBUG_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[1] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[2] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, FIXED_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	// save the original PMU state on this CPU (NOTE: must only be called ONCE per collection)
+	CPU_STATE_pmu_state(pcpu)[0] = SYS_Read_MSR(restore_reg_addr[0]);
+	CPU_STATE_pmu_state(pcpu)[1] = SYS_Read_MSR(restore_reg_addr[1]);
+	CPU_STATE_pmu_state(pcpu)[2] = SYS_Read_MSR(restore_reg_addr[2]);
+
+	if (DRV_CONFIG_ds_area_available(drv_cfg) &&
+	    DEV_CONFIG_pebs_mode(pcfg)) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, PEBS_ENABLE_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+
+	SEP_DRV_LOG_TRACE("Saving PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[0]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[1]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[2]);
+
+#if !defined(DRV_ANDROID)
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Not socket master.");
+		return;
+	}
+
+	direct2core_data_saved = 0;
+	bl_bypass_data_saved = 0;
+	cur_grp = CPU_STATE_current_group(pcpu);
+
+	if (restore_ha_direct2core && restore_qpi_direct2core) {
+		for (i = 0; i < GLOBAL_STATE_num_em_groups(driver_state); i++) {
+			pecb = LWPMU_DEVICE_PMU_register_data(
+				&devices[dev_idx])[i];
+			if (pecb && (ECB_flags(pecb) & ECB_direct2core_bit)) {
+				core2_Disable_Direct2core(
+					LWPMU_DEVICE_PMU_register_data(
+						&devices[dev_idx])[cur_grp]);
+				direct2core_data_saved = 1;
+				break;
+			}
+		}
+	}
+	if (restore_bl_bypass) {
+		for (i = 0; i < GLOBAL_STATE_num_em_groups(driver_state); i++) {
+			pecb = LWPMU_DEVICE_PMU_register_data(
+				&devices[dev_idx])[i];
+			if (pecb && (ECB_flags(pecb) & ECB_bl_bypass_bit)) {
+				core2_Disable_BL_Bypass(
+					LWPMU_DEVICE_PMU_register_data(
+						&devices[dev_idx])[cur_grp]);
+				bl_bypass_data_saved = 1;
+				break;
+			}
+		}
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Destroy(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Reset the PMU setting up after collection
+ *
+ * <I>Special Notes</I>
+ *         Restores the previously saved PMU state done in core2_Initialize.
+ *         This function should be called in parallel across all CPUs
+ *         after sampling collection ends/terminates.
+ *
+ */
+static VOID core2_Destroy(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	pcpu = &pcb[this_cpu];
+
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to restore PMU state on CPU %d.", this_cpu);
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("Clearing PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0x0.");
+
+	// Tentative code below (trying to avoid race conditions with the NMI watchdog). Should be evaluated in the coming few days. (2018/05/21)
+	SYS_Write_MSR(restore_reg_addr[0], 0);
+	SYS_Write_MSR(restore_reg_addr[1], 0);
+	SYS_Write_MSR(restore_reg_addr[2], 0);
+
+	CPU_STATE_pmu_state(pcpu) = NULL;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn core2_Read_LBRs(buffer)
+ *
+ * @param   IN buffer - pointer to the buffer to write the data into
+ * @return  Last branch source IP address
+ *
+ * @brief   Read all the LBR registers into the buffer provided and return
+ *
+ */
+static U64 core2_Read_LBRs(VOID *buffer, PVOID data)
+{
+	U32 i, count = 0;
+	U64 *lbr_buf = NULL;
+	U64 value = 0;
+	U64 tos_ip_addr = 0;
+	U64 tos_ptr = 0;
+	SADDR saddr;
+	U32 this_cpu;
+	U32 dev_idx;
+	LBR lbr;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	lbr = LWPMU_DEVICE_lbr(&devices[dev_idx]);
+
+	if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+		lbr_buf = (U64 *)buffer;
+	}
+
+	for (i = 0; i < LBR_num_entries(lbr); i++) {
+		value = SYS_Read_MSR(LBR_entries_reg_id(lbr, i));
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			*lbr_buf = value;
+		}
+		SEP_DRV_LOG_TRACE("core2_Read_LBRs %u, 0x%llx.", i, value);
+		if (i == 0) {
+			tos_ptr = value;
+		} else {
+			if (LBR_entries_etype(lbr, i) ==
+			    LBR_ENTRY_FROM_IP) { // LBR from register
+				if (tos_ptr == count) {
+					SADDR_addr(saddr) =
+						value & CORE2_LBR_BITMASK;
+					tos_ip_addr = (U64)SADDR_addr(
+						saddr); // Add signed extension
+					SEP_DRV_LOG_TRACE(
+						"Tos_ip_addr %llu, 0x%llx.",
+						tos_ptr, value);
+				}
+				count++;
+			}
+		}
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			lbr_buf++;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llu.", tos_ip_addr);
+	return tos_ip_addr;
+}
+
+/*
+ * @fn corei7_Read_LBRs(buffer)
+ *
+ * @param   IN buffer - pointer to the buffer to write the data into
+ * @return  Last branch source IP address
+ *
+ * @brief   Read all the LBR registers into the buffer provided and return
+ *
+ */
+static U64 corei7_Read_LBRs(VOID *buffer, PVOID data)
+{
+	U32 i, count = 0;
+	U64 *lbr_buf = NULL;
+	U64 value = 0;
+	U64 tos_ip_addr = 0;
+	U64 tos_ptr = 0;
+	SADDR saddr;
+	U32 pairs = 0;
+	U32 this_cpu;
+	U32 dev_idx;
+	LBR lbr;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	lbr = LWPMU_DEVICE_lbr(&devices[dev_idx]);
+
+	if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+		lbr_buf = (U64 *)buffer;
+	}
+
+	if (LBR_num_entries(lbr) > 0) {
+		pairs = (LBR_num_entries(lbr) - 1) / 2;
+	}
+	for (i = 0; i < LBR_num_entries(lbr); i++) {
+		value = SYS_Read_MSR(LBR_entries_reg_id(lbr, i));
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			*lbr_buf = value;
+		}
+		if (DEV_CONFIG_collect_callstacks(pcfg)) {
+			if ((LBR_entries_etype(lbr, i) == LBR_ENTRY_FROM_IP &&
+			     i > tos_ptr + 1) ||
+			    (LBR_entries_etype(lbr, i) == LBR_ENTRY_TO_IP &&
+			     i > tos_ptr + pairs + 1)) {
+				if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+					*lbr_buf = 0x0ULL;
+					lbr_buf++;
+				}
+				continue;
+			}
+		}
+#if defined(DRV_SEP_ACRN_ON)
+		if (DEV_CONFIG_collect_callstacks(pcfg)) {
+			if ((LBR_entries_etype(lbr, i) == LBR_ENTRY_FROM_IP &&
+			     i > tos_ptr + 1) ||
+			    (LBR_entries_etype(lbr, i) == LBR_ENTRY_TO_IP &&
+			     i > tos_ptr + pairs + 1)) {
+				if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+					*lbr_buf = 0x0ULL;
+					lbr_buf++;
+				}
+				continue;
+			}
+		}
+#endif
+		SEP_DRV_LOG_TRACE("I: %u, value: 0x%llx.", i, value);
+		if (i == 0) {
+			tos_ptr = value;
+		} else {
+			if (LBR_entries_etype(lbr, i) ==
+			    LBR_ENTRY_FROM_IP) { // LBR from register
+				if (tos_ptr == count) {
+					SADDR_addr(saddr) =
+						value & CORE2_LBR_BITMASK;
+					tos_ip_addr = (U64)SADDR_addr(
+						saddr); // Add signed extension
+					SEP_DRV_LOG_TRACE(
+						"tos_ip_addr %llu, 0x%llx.",
+						tos_ptr, value);
+				}
+				count++;
+			}
+		}
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			lbr_buf++;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llu.", tos_ip_addr);
+	return tos_ip_addr;
+}
+
+static VOID core2_Clean_Up(VOID *param)
+{
+#if !defined(DRV_ANDROID)
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 busno = 0;
+	U32 dev_idx = 0;
+	U32 base_idx = 0;
+	U32 device_id = 0;
+	U32 value = 0;
+	U32 vendor_id = 0;
+	U32 core2_qpill_dev_no[2] = { 8, 9 };
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+#if !defined(DRV_ANDROID)
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+#endif
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		if (ECB_entries_clean_up_get(pecb, i)) {
+			SEP_DRV_LOG_TRACE("clean up set --- RegId --- %x.",
+					  ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+#if !defined(DRV_ANDROID)
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Not socket master.");
+		return;
+	}
+
+	if (restore_ha_direct2core && restore_qpi_direct2core &&
+	    direct2core_data_saved) {
+		// Discover the bus # for HA
+		for (busno = 0; busno < MAX_BUSNO; busno++) {
+			value = PCI_Read_U32(busno, JKTUNC_HA_DEVICE_NO,
+					     JKTUNC_HA_D2C_FUNC_NO, 0);
+			vendor_id = value & VENDOR_ID_MASK;
+			device_id =
+				(value & DEVICE_ID_MASK) >> DEVICE_ID_BITSHIFT;
+
+			if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+				continue;
+			}
+			if (device_id != JKTUNC_HA_D2C_DID) {
+				continue;
+			}
+
+			// now program at the offset
+			PCI_Write_U32(busno, JKTUNC_HA_DEVICE_NO,
+				      JKTUNC_HA_D2C_FUNC_NO,
+				      JKTUNC_HA_D2C_OFFSET,
+				      restore_ha_direct2core[this_cpu][busno]);
+		}
+
+		// Discover the bus # for QPI
+		for (dev_idx = 0; dev_idx < 2; dev_idx++) {
+			base_idx = dev_idx * MAX_BUSNO;
+			for (busno = 0; busno < MAX_BUSNO; busno++) {
+				value = PCI_Read_U32(
+					busno, core2_qpill_dev_no[dev_idx],
+					JKTUNC_QPILL_D2C_FUNC_NO, 0);
+				vendor_id = value & VENDOR_ID_MASK;
+				device_id = (value & DEVICE_ID_MASK) >>
+					    DEVICE_ID_BITSHIFT;
+
+				if (vendor_id != DRV_IS_PCI_VENDOR_ID_INTEL) {
+					continue;
+				}
+				if ((device_id != JKTUNC_QPILL0_D2C_DID) &&
+				    (device_id != JKTUNC_QPILL1_D2C_DID)) {
+					continue;
+				}
+				// now program at the corresponding offset
+				PCI_Write_U32(busno,
+					      core2_qpill_dev_no[dev_idx],
+					      JKTUNC_QPILL_D2C_FUNC_NO,
+					      JKTUNC_QPILL_D2C_OFFSET,
+					      restore_qpi_direct2core[this_cpu]
+								     [base_idx +
+								      busno]);
+			}
+		}
+	}
+	if (restore_bl_bypass && bl_bypass_data_saved) {
+		SYS_Write_MSR(CORE2UNC_DISABLE_BL_BYPASS_MSR,
+			      restore_bl_bypass[this_cpu]);
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+static VOID corei7_Errata_Fix(void)
+{
+	U32 this_cpu = CONTROL_THIS_CPU();
+	CPU_STATE pcpu = &pcb[this_cpu];
+	ECB(pecb) = NULL;
+	U32 dev_idx, cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, PEBS_ENABLE_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_HW_ERRATA)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+static VOID corei7_Errata_Fix_2(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_HW_ERRATA)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Check_Overflow_Htoff_Mode(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static void core2_Check_Overflow_Htoff_Mode(DRV_MASKS masks)
+{
+	U32 index;
+	U64 value = 0;
+	U64 overflow_status = 0;
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	U32 cur_grp;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+	ECB pecb;
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	bd = &cpu_buf[this_cpu];
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	SEP_DRV_LOG_TRACE("");
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+	SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+
+	if (dispatch->check_overflow_gp_errata) {
+		overflow_status = dispatch->check_overflow_gp_errata(
+			pecb, &overflow_status_clr);
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+		} else if (ECB_entries_is_gp_reg_get(pecb, i) &&
+			   ECB_entries_reg_value(pecb, i) != 0) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+			if (i >= (ECB_operations_register_start(
+					  pecb, PMU_OPERATION_DATA_GP) +
+				  4) &&
+			    i <= (ECB_operations_register_start(
+					  pecb, PMU_OPERATION_DATA_GP) +
+				  7)) {
+				value = SYS_Read_MSR(
+					ECB_entries_reg_id(pecb, i));
+				if (value > 0 && value <= 0x100000000LL) {
+					overflow_status |= ((U64)1 << index);
+				}
+			}
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      ECB_entries_reg_value(pecb, i));
+
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (ECB_entries_fixed_reg_get(pecb, i)) {
+				CPU_STATE_p_state_counting(pcpu) = 1;
+			}
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_branch_evt_get(pecb, i)) {
+				DRV_EVENT_MASK_branch(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	// Reinitialize the global overflow control register
+	SYS_Write_MSR(ECB_entries_reg_id(
+			      pecb, ECB_SECTION_REG_INDEX(
+					    pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS)),
+		      overflow_status_clr);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void core2_Read_Power(buffer)
+ *
+ * @param    buffer   - pointer to the buffer to write the data into
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Read all the power MSRs into the buffer provided and return.
+ *
+ */
+static VOID corei7_Read_Power(VOID *buffer)
+{
+	U32 i;
+	U64 *pwr_buf = (U64 *)buffer;
+	U32 this_cpu;
+	U32 dev_idx;
+	PWR pwr;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pwr = LWPMU_DEVICE_pwr(&devices[dev_idx]);
+
+	for (i = 0; i < PWR_num_entries(pwr); i++) {
+		*pwr_buf = SYS_Read_MSR(PWR_entries_reg_id(pwr, i));
+		pwr_buf++;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn core2_Read_Counts(param, id)
+ *
+ * @param    param      The read thread node to process
+ * @param    id         The event id for the which the sample is generated
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read CPU event based counts for the events with reg value=0 and store into the buffer param;
+ *
+ */
+static VOID core2_Read_Counts(PVOID param, U32 id)
+{
+	U64 *data;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 event_id = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (DEV_CONFIG_ebc_group_id_offset(pcfg)) {
+		// Write GroupID
+		data = (U64 *)((S8 *)param +
+			       DEV_CONFIG_ebc_group_id_offset(pcfg));
+		*data = CPU_STATE_current_group(pcpu) + 1;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_counter_event_offset(pecb, i) == 0) {
+			continue;
+		}
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, i));
+		event_id = ECB_entries_event_id_index(pecb, i);
+		if (event_id == id) {
+			*data = ~(ECB_entries_reg_value(pecb, i) - 1) &
+				ECB_entries_max_bits(pecb, i);
+			;
+		} else {
+			*data = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DRV_CONFIG_enable_p_state(drv_cfg)) {
+		CPU_STATE_p_state_counting(pcpu) = 0;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn corei7_Check_Overflow_Errata(pecb)
+ *
+ * @param pecb:            The current event control block
+ * @param overflow_status: current overflow mask
+ *
+ * @return   Updated Event mask of the overflowed registers.
+ *
+ * @brief    There is a bug where highly correlated precise events do
+ *           not raise an indication on overflows in Core i7 and SNB.
+ */
+static U64 corei7_Check_Overflow_Errata(ECB pecb__, U64 *overflow_status_clr)
+{
+	U64 index = 0, value = 0, overflow_status = 0;
+
+	SEP_DRV_LOG_TRACE_IN("PECB: %p, overflow_status_clr: %p.", pecb__,
+			     overflow_status_clr);
+
+	overflow_status = *overflow_status_clr;
+
+	if (DRV_CONFIG_num_events(drv_cfg) == 1) {
+		SEP_DRV_LOG_TRACE_OUT("Res = %llu (num_events = 1).",
+				      overflow_status);
+		return overflow_status;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_reg_value(pecb, i) == 0) {
+			continue;
+		}
+		if (ECB_entries_is_gp_reg_get(pecb, i)) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+			value = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			if (value > 0LL && value <= 0x100000000LL) {
+				overflow_status |= ((U64)1 << index);
+				*overflow_status_clr |= ((U64)1 << index);
+				SEP_DRV_LOG_TRACE("Counter 0x%x value 0x%llx.",
+						  ECB_entries_reg_id(pecb, i),
+						  value);
+			}
+			continue;
+		}
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+			if (!(overflow_status & ((U64)1 << index))) {
+				value = SYS_Read_MSR(
+					ECB_entries_reg_id(pecb, i));
+				if (ECB_entries_reg_id(pecb, i) ==
+				    ECB_entries_reg_id(
+					    pecb,
+					    ECB_SECTION_REG_INDEX(
+						    pecb, 0,
+						    PMU_OPERATION_CHECK_OVERFLOW_GP_ERRATA))) {
+					if (!(value > 0LL &&
+					      value <= 0x1000000LL) &&
+					    (*overflow_status_clr &
+					     ((U64)1 << index))) {
+						//Clear it only for overflow_status so that we do not create sample records
+						//Please do not remove the check for MSR index
+						overflow_status =
+							overflow_status &
+							~((U64)1 << index);
+						continue;
+					}
+				}
+				if (value > 0LL && value <= 0x100000000LL) {
+					overflow_status |= ((U64)1 << index);
+					*overflow_status_clr |=
+						((U64)1 << index);
+					SEP_DRV_LOG_TRACE(
+						"counter 0x%x value 0x%llx\n",
+						ECB_entries_reg_id(pecb, i),
+						value);
+				}
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("Res = %llu.", overflow_status);
+	return overflow_status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 corei7_Read_Platform_Info
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param       void
+ *
+ * @return      value read from the register
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID corei7_Platform_Info(PVOID data)
+{
+	DRV_PLATFORM_INFO platform_data = (DRV_PLATFORM_INFO)data;
+	U64 value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Data: %p.", data);
+
+	if (!platform_data) {
+		SEP_DRV_LOG_TRACE_OUT("Platform_data is NULL!");
+		return;
+	}
+
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) = 0;
+
+#define IA32_MSR_PLATFORM_INFO 0xCE
+	value = SYS_Read_MSR(IA32_MSR_PLATFORM_INFO);
+
+	DRV_PLATFORM_INFO_info(platform_data) = value;
+	DRV_PLATFORM_INFO_ddr_freq_index(platform_data) = 0;
+#undef IA32_MSR_PLATFORM_INFO
+#define IA32_MSR_MISC_ENABLE 0x1A4
+	DRV_PLATFORM_INFO_misc_valid(platform_data) = 1;
+	value = SYS_Read_MSR(IA32_MSR_MISC_ENABLE);
+	DRV_PLATFORM_INFO_misc_info(platform_data) = value;
+#undef IA32_MSR_MISC_ENABLE
+	SEP_DRV_LOG_TRACE("Read from MSR_ENERGY_MULTIPLIER reg is %llu.",
+			  SYS_Read_MSR(MSR_ENERGY_MULTIPLIER));
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) =
+		(U32)(SYS_Read_MSR(MSR_ENERGY_MULTIPLIER) & 0x00001F00) >> 8;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 corei7_Platform_Info_Nehalem
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param       void
+ *
+ * @return      value read from the register
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID corei7_Platform_Info_Nehalem(PVOID data)
+{
+	DRV_PLATFORM_INFO platform_data = (DRV_PLATFORM_INFO)data;
+	U64 value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Data: %p.", data);
+
+	if (!platform_data) {
+		SEP_DRV_LOG_TRACE_OUT("Platform_data is NULL!");
+		return;
+	}
+
+#define IA32_MSR_PLATFORM_INFO 0xCE
+	value = SYS_Read_MSR(IA32_MSR_PLATFORM_INFO);
+
+	DRV_PLATFORM_INFO_info(platform_data) = value;
+	DRV_PLATFORM_INFO_ddr_freq_index(platform_data) = 0;
+#undef IA32_MSR_PLATFORM_INFO
+#define IA32_MSR_MISC_ENABLE 0x1A4
+	DRV_PLATFORM_INFO_misc_valid(platform_data) = 1;
+	value = SYS_Read_MSR(IA32_MSR_MISC_ENABLE);
+	DRV_PLATFORM_INFO_misc_info(platform_data) = value;
+#undef IA32_MSR_MISC_ENABLE
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) = 0;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+DISPATCH_NODE core2_dispatch = { .init = core2_Initialize,
+				 .fini = core2_Destroy,
+				 .write = core2_Write_PMU,
+				 .freeze = core2_Disable_PMU,
+				 .restart = core2_Enable_PMU,
+				 .read_data = core2_Read_PMU_Data,
+				 .check_overflow = core2_Check_Overflow,
+				 .swap_group = core2_Swap_Group,
+				 .read_lbrs = core2_Read_LBRs,
+				 .cleanup = core2_Clean_Up,
+				 .hw_errata = NULL,
+				 .read_power = NULL,
+				 .check_overflow_errata =
+					 core2_Check_Overflow_Errata,
+				 .read_counts = core2_Read_Counts,
+				 .check_overflow_gp_errata = NULL,
+				 .read_ro = NULL,
+				 .platform_info = NULL,
+				 .trigger_read = NULL,
+				 .scan_for_uncore = NULL,
+				 .read_metrics = NULL };
+
+DISPATCH_NODE corei7_dispatch = { .init = core2_Initialize,
+				  .fini = core2_Destroy,
+				  .write = core2_Write_PMU,
+				  .freeze = core2_Disable_PMU,
+				  .restart = core2_Enable_PMU,
+				  .read_data = core2_Read_PMU_Data,
+				  .check_overflow = core2_Check_Overflow,
+				  .swap_group = core2_Swap_Group,
+				  .read_lbrs = corei7_Read_LBRs,
+				  .cleanup = core2_Clean_Up,
+				  .hw_errata = corei7_Errata_Fix,
+				  .read_power = corei7_Read_Power,
+				  .check_overflow_errata = NULL,
+				  .read_counts = core2_Read_Counts,
+				  .check_overflow_gp_errata =
+					  corei7_Check_Overflow_Errata,
+				  .read_ro = NULL,
+				  .platform_info = corei7_Platform_Info,
+				  .trigger_read = NULL,
+				  .scan_for_uncore = NULL,
+				  .read_metrics = NULL };
+
+DISPATCH_NODE corei7_dispatch_2 = { .init = core2_Initialize,
+				    .fini = core2_Destroy,
+				    .write = core2_Write_PMU,
+				    .freeze = core2_Disable_PMU,
+				    .restart = corei7_Enable_PMU_2,
+				    .read_data = core2_Read_PMU_Data,
+				    .check_overflow = core2_Check_Overflow,
+				    .swap_group = core2_Swap_Group,
+				    .read_lbrs = corei7_Read_LBRs,
+				    .cleanup = core2_Clean_Up,
+				    .hw_errata = corei7_Errata_Fix_2,
+				    .read_power = corei7_Read_Power,
+				    .check_overflow_errata = NULL,
+				    .read_counts = core2_Read_Counts,
+				    .check_overflow_gp_errata =
+					    corei7_Check_Overflow_Errata,
+				    .read_ro = NULL,
+				    .platform_info = corei7_Platform_Info,
+				    .trigger_read = NULL,
+				    .scan_for_uncore = NULL,
+				    .read_metrics = NULL };
+
+DISPATCH_NODE corei7_dispatch_nehalem = {
+	.init = core2_Initialize,
+	.fini = core2_Destroy,
+	.write = core2_Write_PMU,
+	.freeze = core2_Disable_PMU,
+	.restart = core2_Enable_PMU,
+	.read_data = core2_Read_PMU_Data,
+	.check_overflow = core2_Check_Overflow,
+	.swap_group = core2_Swap_Group,
+	.read_lbrs = corei7_Read_LBRs,
+	.cleanup = core2_Clean_Up,
+	.hw_errata = corei7_Errata_Fix,
+	.read_power = corei7_Read_Power,
+	.check_overflow_errata = NULL,
+	.read_counts = core2_Read_Counts,
+	.check_overflow_gp_errata = corei7_Check_Overflow_Errata,
+	.read_ro = NULL,
+	.platform_info = corei7_Platform_Info_Nehalem,
+	.trigger_read = NULL,
+	.scan_for_uncore = NULL,
+	.read_metrics = NULL
+};
+
+DISPATCH_NODE corei7_dispatch_htoff_mode = {
+	.init = core2_Initialize,
+	.fini = core2_Destroy,
+	.write = core2_Write_PMU,
+	.freeze = core2_Disable_PMU,
+	.restart = core2_Enable_PMU,
+	.read_data = core2_Read_PMU_Data,
+	.check_overflow = core2_Check_Overflow_Htoff_Mode,
+	.swap_group = core2_Swap_Group,
+	.read_lbrs = corei7_Read_LBRs,
+	.cleanup = core2_Clean_Up,
+	.hw_errata = corei7_Errata_Fix,
+	.read_power = corei7_Read_Power,
+	.check_overflow_errata = NULL,
+	.read_counts = core2_Read_Counts,
+	.check_overflow_gp_errata = corei7_Check_Overflow_Errata,
+	.read_ro = NULL,
+	.platform_info = corei7_Platform_Info,
+	.trigger_read = NULL,
+	.scan_for_uncore = NULL,
+	.read_metrics = NULL
+};
+
+DISPATCH_NODE corei7_dispatch_htoff_mode_2 = {
+	.init = core2_Initialize,
+	.fini = core2_Destroy,
+	.write = core2_Write_PMU,
+	.freeze = core2_Disable_PMU,
+	.restart = corei7_Enable_PMU_2,
+	.read_data = core2_Read_PMU_Data,
+	.check_overflow = core2_Check_Overflow_Htoff_Mode,
+	.swap_group = core2_Swap_Group,
+	.read_lbrs = corei7_Read_LBRs,
+	.cleanup = core2_Clean_Up,
+	.hw_errata = corei7_Errata_Fix_2,
+	.read_power = corei7_Read_Power,
+	.check_overflow_errata = NULL,
+	.read_counts = core2_Read_Counts,
+	.check_overflow_gp_errata = corei7_Check_Overflow_Errata,
+	.read_ro = NULL,
+	.platform_info = corei7_Platform_Info,
+	.trigger_read = NULL,
+	.scan_for_uncore = NULL,
+	.read_metrics = NULL
+};
diff --git a/drivers/platform/x86/sepdk/sep/cpumon.c b/drivers/platform/x86/sepdk/sep/cpumon.c
new file mode 100755
index 000000000000..ac8ade14f106
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/cpumon.c
@@ -0,0 +1,357 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+/*
+ *  CVS_Id="$Id$"
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/interrupt.h>
+#if defined(DRV_EM64T)
+#include <asm/desc.h>
+#endif
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "apic.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "utility.h"
+#include "cpumon.h"
+#include "pmi.h"
+#include "sys_info.h"
+
+#include <linux/ptrace.h>
+#include <asm/nmi.h>
+
+#if !defined(DRV_SEP_ACRN_ON)
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 2, 0))
+#include <linux/notifier.h>
+static int cpumon_NMI_Handler(unsigned int cmd, struct pt_regs *regs)
+{
+	U32 captured_state = GET_DRIVER_STATE();
+
+	if (DRIVER_STATE_IN(captured_state, STATE_BIT_RUNNING |
+						    STATE_BIT_PAUSING |
+						    STATE_BIT_PREPARE_STOP |
+						    STATE_BIT_TERMINATING)) {
+		if (captured_state != DRV_STATE_TERMINATING) {
+			PMI_Interrupt_Handler(regs);
+		}
+		return NMI_HANDLED;
+	} else {
+		return NMI_DONE;
+	}
+}
+
+#define EBS_NMI_CALLBACK cpumon_NMI_Handler
+
+#else
+#include <linux/kdebug.h>
+static int cpumon_NMI_Handler(struct notifier_block *self, unsigned long val,
+			      void *data)
+{
+	struct die_args *args = (struct die_args *)data;
+	U32 captured_state = GET_DRIVER_STATE();
+
+	if (args) {
+		switch (val) {
+		case DIE_NMI:
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38))
+		case DIE_NMI_IPI:
+#endif
+			if (DRIVER_STATE_IN(captured_state,
+					    STATE_BIT_RUNNING |
+						    STATE_BIT_PAUSING |
+						    STATE_BIT_PREPARE_STOP |
+						    STATE_BIT_TERMINATING)) {
+				if (captured_state != DRV_STATE_TERMINATING) {
+					PMI_Interrupt_Handler(args->regs);
+				}
+				return NOTIFY_STOP;
+			}
+		}
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block cpumon_notifier = { .notifier_call =
+							 cpumon_NMI_Handler,
+						 .next = NULL,
+#if (LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38))
+						 .priority = 2
+#else
+		.priority = NMI_LOCAL_LOW_PRIOR,
+#endif
+};
+#endif
+#endif
+
+static volatile S32 cpuhook_installed;
+
+/*
+ * CPU Monitoring Functionality
+ */
+
+/*
+ * General per-processor initialization
+ */
+#if defined(DRV_CPU_HOTPLUG)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       DRV_BOOL CPUMON_is_Online_Allowed()
+ *
+ * @param    None
+ *
+ * @return   DRV_BOOL TRUE if cpu is allowed to go Online, else FALSE
+ *
+ * @brief    Checks if the cpu is allowed to go online during the
+ * @brief    current driver state
+ *
+ */
+DRV_BOOL CPUMON_is_Online_Allowed(void)
+{
+	DRV_BOOL is_allowed = FALSE;
+#if !defined(DRV_SEP_ACRN_ON)
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	switch (cur_driver_state) {
+	case DRV_STATE_IDLE:
+	case DRV_STATE_PAUSED:
+	case DRV_STATE_RUNNING:
+	case DRV_STATE_PAUSING:
+		is_allowed = TRUE;
+		break;
+	default:
+		SEP_DRV_LOG_TRACE(
+			"CPU is prohibited to online in driver state %d.",
+			cur_driver_state);
+		break;
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", is_allowed);
+	return is_allowed;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       DRV_BOOL CPUMON_is_Offline_Allowed()
+ *
+ * @param    None
+ *
+ * @return   DRV_BOOL TRUE if cpu is allowed to go Offline, else FALSE
+ *
+ * @brief    Checks if the cpu is allowed to go offline during the
+ * @brief    current driver state
+ *
+ */
+DRV_BOOL CPUMON_is_Offline_Allowed(void)
+{
+	DRV_BOOL is_allowed = FALSE;
+#if !defined(DRV_SEP_ACRN_ON)
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	switch (cur_driver_state) {
+	case DRV_STATE_PAUSED:
+	case DRV_STATE_RUNNING:
+	case DRV_STATE_PAUSING:
+		is_allowed = TRUE;
+		break;
+	default:
+		SEP_DRV_LOG_TRACE(
+			"CPU is prohibited to offline in driver state %d.",
+			cur_driver_state);
+		break;
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", is_allowed);
+	return is_allowed;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID CPUMON_Online_Cpu(
+ *               PVOID param)
+ *
+ * @param    PVOID parm
+ *
+ * @return   None
+ *
+ * @brief    Sets a cpu online, initialize APIC on it,
+ * @brief    Build the sys_info for this cpu
+ *
+ */
+VOID CPUMON_Online_Cpu(PVOID param)
+{
+	S32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy parm: %p.", parm);
+
+	if (param == NULL) {
+		preempt_disable();
+		this_cpu = CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		this_cpu = *(S32 *)param;
+	}
+	pcpu = &pcb[this_cpu];
+	if (pcpu == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT("Unable to set CPU %d online!",
+					      this_cpu);
+		return;
+	}
+	SEP_DRV_LOG_INIT("Setting CPU %d online, PCPU = %p.", this_cpu, pcpu);
+	CPU_STATE_offlined(pcpu) = FALSE;
+	CPU_STATE_accept_interrupt(pcpu) = 1;
+	CPU_STATE_initial_mask(pcpu) = 1;
+	CPU_STATE_group_swap(pcpu) = 1;
+	APIC_Init(NULL);
+	APIC_Install_Interrupt_Handler(NULL);
+
+	SYS_INFO_Build_Cpu(NULL);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID CPUMON_Offline_Cpu(
+ *               PVOID param)
+ *
+ * @param    PVOID parm
+ *
+ * @return   None
+ *
+ * @brief    Sets a cpu offline
+ *
+ */
+VOID CPUMON_Offline_Cpu(PVOID param)
+{
+	S32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy parm: %p.", parm);
+
+	if (param == NULL) {
+		preempt_disable();
+		this_cpu = CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		this_cpu = *(S32 *)param;
+	}
+	pcpu = &pcb[this_cpu];
+
+	if (pcpu == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT("Unable to set CPU %d offline.",
+					      this_cpu);
+		return;
+	}
+	SEP_DRV_LOG_INIT("Setting CPU %d offline.", this_cpu);
+	CPU_STATE_offlined(pcpu) = TRUE;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void CPUMON_Install_Cpuhooks(void)
+ *
+ * @param    None
+ *
+ * @return   None     No return needed
+ *
+ * @brief  set up the interrupt handler (on a per-processor basis)
+ * @brief  Initialize the APIC in two phases (current CPU, then others)
+ *
+ */
+VOID CPUMON_Install_Cpuhooks(void)
+{
+#if !defined(DRV_SEP_ACRN_ON)
+	S32 me = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (cpuhook_installed) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT("Cpuhook already installed.");
+		return;
+	}
+
+	CONTROL_Invoke_Parallel(APIC_Init, NULL);
+	CONTROL_Invoke_Parallel(APIC_Install_Interrupt_Handler,
+				(PVOID)(size_t)me);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 2, 0))
+	register_nmi_handler(NMI_LOCAL, EBS_NMI_CALLBACK, 0, "sep_pmi");
+#else
+	register_die_notifier(&cpumon_notifier);
+#endif
+
+	cpuhook_installed = 1;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void CPUMON_Remove_Cpuhools(void)
+ *
+ * @param    None
+ *
+ * @return   None     No return needed
+ *
+ * @brief  De-Initialize the APIC in phases
+ * @brief  clean up the interrupt handler (on a per-processor basis)
+ *
+ */
+VOID CPUMON_Remove_Cpuhooks(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(APIC_Restore_LVTPC, NULL);
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3, 2, 0))
+	unregister_nmi_handler(NMI_LOCAL, "sep_pmi");
+#else
+	unregister_die_notifier(&cpumon_notifier);
+#endif
+
+	cpuhook_installed = 0;
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/eventmux.c b/drivers/platform/x86/sepdk/sep/eventmux.c
new file mode 100755
index 000000000000..1d8099dc674a
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/eventmux.c
@@ -0,0 +1,446 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/jiffies.h>
+#include <linux/time.h>
+#include <linux/percpu.h>
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "utility.h"
+#include "eventmux.h"
+
+static PVOID em_tables;
+static size_t em_tables_size;
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Allocate_Groups (
+ *                         VOID  *params
+ *                        )
+ *
+ * @brief       Allocate memory need to support event multiplexing
+ *
+ * @param       params - pointer to a S32 that holds the size of buffer to allocate
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              Allocate the memory needed to save different group counters
+ *              Called via the parallel control mechanism
+ */
+static VOID eventmux_Allocate_Groups(PVOID params)
+{
+	U32 this_cpu;
+	CPU_STATE cpu_state;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	cpu_state = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	preempt_enable();
+
+	if (EVENT_CONFIG_mode(ec) == EM_DISABLED ||
+	    EVENT_CONFIG_num_groups(ec) == 1) {
+		return;
+	}
+
+	CPU_STATE_em_tables(cpu_state) =
+		em_tables + CPU_STATE_em_table_offset(cpu_state);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Deallocate_Groups (
+ *                         VOID  *params
+ *                        )
+ *
+ * @brief       Free the scratch memory need to support event multiplexing
+ *
+ * @param       params - pointer to NULL
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              Free the memory needed to save different group counters
+ *              Called via the parallel control mechanism
+ */
+static VOID eventmux_Deallocate_Groups(PVOID params)
+{
+	U32 this_cpu;
+	CPU_STATE cpu_state;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	cpu_state = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	preempt_enable();
+
+	if (EVENT_CONFIG_mode(ec) == EM_DISABLED ||
+	    EVENT_CONFIG_num_groups(ec) == 1) {
+		return;
+	}
+
+	CPU_STATE_em_tables(cpu_state) = NULL;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Timer_Callback_Thread (
+ *                        )
+ *
+ * @brief       Stop all the timer threads and terminate them
+ *
+ * @param       none
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              timer routine - The event multiplexing happens here.
+ */
+static VOID eventmux_Timer_Callback_Thread(
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	struct timer_list *tl
+#else
+	unsigned long arg
+#endif
+)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	SEP_DRV_LOG_TRACE_IN("");
+#else
+	SEP_DRV_LOG_TRACE_IN("Arg: %u.", (U32)arg);
+#endif
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	preempt_enable();
+
+	if (CPU_STATE_em_tables(pcpu) == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Em_tables is NULL!");
+		return;
+	}
+
+	dispatch->swap_group(TRUE);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	mod_timer(CPU_STATE_em_timer(pcpu),
+		  jiffies + CPU_STATE_em_timer_delay(pcpu));
+#else
+	CPU_STATE_em_timer(pcpu)->expires = jiffies + arg;
+	add_timer(CPU_STATE_em_timer(pcpu));
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Prepare_Timer_Threads (
+ *                         VOID
+ *                        )
+ *
+ * @brief       Stop all the timer threads and terminate them
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              Set up the timer threads to prepare for event multiplexing.
+ *              Do not start the threads as yet
+ */
+static VOID eventmux_Prepare_Timer_Threads(PVOID arg)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// initialize and set up the timer for all cpus
+	// Do not start the timer as yet.
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	preempt_enable();
+
+	if (EVENT_CONFIG_mode(ec) != EM_TIMER_BASED) {
+		return;
+	}
+
+	CPU_STATE_em_timer(pcpu) = (struct timer_list *)CONTROL_Allocate_Memory(
+		sizeof(struct timer_list));
+
+	if (CPU_STATE_em_timer(pcpu) == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Pcpu = NULL!");
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Cancel_Timers (
+ *                         VOID
+ *                        )
+ *
+ * @brief       Stop all the timer threads and terminate them
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              Cancel all the timer threads that have been started
+ */
+static VOID eventmux_Cancel_Timers(void)
+{
+	CPU_STATE pcpu;
+	S32 i;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	/*
+	 *  Cancel the timer for all active CPUs
+	 */
+	for (i = 0; i < GLOBAL_STATE_active_cpus(driver_state); i++) {
+		pcpu = &pcb[i];
+		dev_idx = core_to_dev_map[i];
+		ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+		if (EVENT_CONFIG_mode(ec) != EM_TIMER_BASED) {
+			continue;
+		}
+		del_timer_sync(CPU_STATE_em_timer(pcpu));
+		CPU_STATE_em_timer(pcpu) =
+			(struct timer_list *)CONTROL_Free_Memory(
+				CPU_STATE_em_timer(pcpu));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID eventmux_Start_Timers (
+ *                         long unsigned arg
+ *                        )
+ *
+ * @brief       Start the timer on a single cpu
+ *
+ * @param       delay   interval time in jiffies
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              start the timer on a single cpu
+ *              Call from each cpu to get cpu affinity for Timer_Callback_Thread
+ */
+static VOID eventmux_Start_Timers(PVOID arg)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+	unsigned long delay;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	preempt_enable();
+
+	if (EVENT_CONFIG_mode(ec) != EM_TIMER_BASED ||
+	    EVENT_CONFIG_num_groups(ec) == 1) {
+		return;
+	}
+
+	/*
+	 * notice we want to use group 0's time slice for the initial timer
+	 */
+	delay = msecs_to_jiffies(EVENT_CONFIG_em_factor(ec));
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	CPU_STATE_em_timer_delay(pcpu) = delay;
+	timer_setup(CPU_STATE_em_timer(pcpu), eventmux_Timer_Callback_Thread,
+		    0);
+	mod_timer(CPU_STATE_em_timer(pcpu),
+		  jiffies + CPU_STATE_em_timer_delay(pcpu));
+#else
+	init_timer(CPU_STATE_em_timer(pcpu));
+	CPU_STATE_em_timer(pcpu)->function = eventmux_Timer_Callback_Thread;
+	CPU_STATE_em_timer(pcpu)->data = delay;
+	CPU_STATE_em_timer(pcpu)->expires = jiffies + delay;
+	add_timer(CPU_STATE_em_timer(pcpu));
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID EVENTMUX_Start (
+ *                         VOID
+ *                        )
+ *
+ * @brief       Start the timers and enable all the threads
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              if event multiplexing has been enabled, set up the time slices and
+ *              start the timer threads for all the timers
+ */
+VOID EVENTMUX_Start(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	/*
+	 * Start the timer for all cpus
+	 */
+	CONTROL_Invoke_Parallel(eventmux_Start_Timers, NULL);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID EVENTMUX_Initialize (
+ *                         VOID
+ *                        )
+ *
+ * @brief       Initialize the event multiplexing module
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              if event multiplexing has been enabled,
+ *              then allocate the memory needed to save and restore all the counter data
+ *              set up the timers needed, but do not start them
+ */
+VOID EVENTMUX_Initialize(void)
+{
+	S32 size_of_vector;
+	S32 cpu_num;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		pcpu = &pcb[cpu_num];
+		dev_idx = core_to_dev_map[cpu_num];
+		ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+		if (EVENT_CONFIG_mode(ec) == EM_DISABLED ||
+		    EVENT_CONFIG_num_groups(ec) == 1) {
+			continue;
+		}
+		size_of_vector = EVENT_CONFIG_num_groups(ec) *
+				 EVENT_CONFIG_max_gp_events(ec) * sizeof(S64);
+		CPU_STATE_em_table_offset(pcpu) = em_tables_size;
+		em_tables_size += size_of_vector;
+	}
+
+	if (em_tables_size) {
+		em_tables = CONTROL_Allocate_Memory(em_tables_size);
+	}
+	CONTROL_Invoke_Parallel(eventmux_Allocate_Groups, NULL);
+
+	CONTROL_Invoke_Parallel(eventmux_Prepare_Timer_Threads,
+				(VOID *)(size_t)0);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID EVENTMUX_Destroy (
+ *                         VOID
+ *                        )
+ *
+ * @brief       Clean up the event multiplexing threads
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              if event multiplexing has been enabled, then stop and cancel all the timers
+ *              free up all the memory that is associated with EM
+ */
+VOID EVENTMUX_Destroy(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	eventmux_Cancel_Timers();
+
+	if (em_tables) {
+		em_tables = CONTROL_Free_Memory(em_tables);
+		em_tables_size = 0;
+	}
+	CONTROL_Invoke_Parallel(eventmux_Deallocate_Groups, (VOID *)(size_t)0);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/gfx.c b/drivers/platform/x86/sepdk/sep/gfx.c
new file mode 100755
index 000000000000..38342f6740c4
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/gfx.c
@@ -0,0 +1,261 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include <asm/page.h>
+#include <asm/io.h>
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_gfx.h"
+#include "lwpmudrv.h"
+#include "inc/pci.h"
+#include "gfx.h"
+#include "utility.h"
+
+static char *gfx_virtual_addr;
+static SEP_MMIO_NODE gfx_map;
+static U32 gfx_code = GFX_CTRL_DISABLE;
+static U32 gfx_counter[GFX_NUM_COUNTERS];
+static U32 gfx_overflow[GFX_NUM_COUNTERS];
+
+/*!
+ * @fn     OS_STATUS GFX_Read
+ *
+ * @brief  Reads the counters into the buffer provided for the purpose
+ *
+ * @param  buffer  - buffer to read the counts into
+ *
+ * @return STATUS_SUCCESS if read succeeded, otherwise error
+ *
+ * @note
+ */
+OS_STATUS GFX_Read(S8 *buffer)
+{
+	U64 *samp = (U64 *)buffer;
+	U32 i;
+	U32 val;
+	char *reg_addr;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	// GFX counting was not specified
+	if (gfx_virtual_addr == NULL || gfx_code == GFX_CTRL_DISABLE) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"OS_INVALID (!gfx_virtual_addr || gfx_code==GFX_CTRL_DISABLE)");
+		return OS_INVALID;
+	}
+
+	// check for sampling buffer
+	if (!samp) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (!samp).");
+		return OS_INVALID;
+	}
+
+	// set the GFX register address
+	reg_addr = gfx_virtual_addr + GFX_PERF_REG;
+
+	// for all counters - save the information to the sampling stream
+	for (i = 0; i < GFX_NUM_COUNTERS; i++) {
+		// read the ith GFX event count
+		reg_addr += 4;
+		val = *(U32 *)(reg_addr);
+#if defined(GFX_COMPUTE_DELTAS)
+		// if the current count is bigger than the previous one,
+		// then the counter overflowed
+		// so make sure the delta gets adjusted to account for it
+		if (val < gfx_counter[i]) {
+			samp[i] = val + (GFX_CTR_OVF_VAL - gfx_counter[i]);
+		} else {
+			samp[i] = val - gfx_counter[i];
+		}
+#else
+		// just keep track of raw count for this counter
+		// if the current count is bigger than the previous one,
+		// then the counter overflowed
+		if (val < gfx_counter[i]) {
+			gfx_overflow[i]++;
+		}
+		samp[i] = val + (U64)gfx_overflow[i] * GFX_CTR_OVF_VAL;
+#endif
+		// save the current count
+		gfx_counter[i] = val;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/*!
+ * @fn     OS_STATUS GFX_Set_Event_Code
+ *
+ * @brief  Programs the Graphics PMU with the right event code
+ *
+ * @param  arg - buffer containing graphics event code
+ *
+ * @return STATUS_SUCCESS if success, otherwise error
+ *
+ * @note
+ */
+OS_STATUS GFX_Set_Event_Code(IOCTL_ARGS arg)
+{
+	U32 i;
+	char *reg_addr;
+	U32 reg_value;
+
+	SEP_DRV_LOG_FLOW_IN("Arg: %p.", arg);
+
+	// extract the graphics event code from usermode
+	if (get_user(gfx_code, (int __user *)arg->buf_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"OS_FAULT (Unable to obtain gfx_code from usermode!).");
+		return OS_FAULT;
+	}
+	SEP_DRV_LOG_TRACE("Got gfx_code=0x%x.", gfx_code);
+
+	// memory map the address to GFX counters, if not already done
+	if (gfx_virtual_addr == NULL) {
+		PCI_Map_Memory(&gfx_map, GFX_BASE_ADDRESS + GFX_BASE_NEW_OFFSET,
+			       PAGE_SIZE);
+		gfx_virtual_addr =
+			(char *)(UIOP)SEP_MMIO_NODE_virtual_address(&gfx_map);
+	}
+
+	// initialize the GFX counts
+	for (i = 0; i < GFX_NUM_COUNTERS; i++) {
+		gfx_counter[i] = 0;
+		gfx_overflow[i] = 0;
+		// only used if storing raw counts
+		// (i.e., GFX_COMPUTE_DELTAS is undefined)
+	}
+
+	// get current GFX event code
+	if (gfx_virtual_addr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"OS_INVALID (Invalid gfx_virtual_addr=0x%p!).",
+			gfx_virtual_addr);
+		return OS_INVALID;
+	}
+
+	reg_addr = gfx_virtual_addr + GFX_PERF_REG;
+	reg_value = *(U32 *)(reg_addr);
+	SEP_DRV_LOG_TRACE("Read reg_value=0x%x from reg_addr=0x%p.", reg_value,
+			reg_addr);
+
+	/* Update the GFX counter group */
+	// write the GFX counter group with reset = 1 for all counters
+	reg_value = (gfx_code | GFX_REG_CTR_CTRL);
+	*(U32 *)(reg_addr) = reg_value;
+	SEP_DRV_LOG_TRACE("Wrote reg_value=0x%x to reg_addr=0x%p.", reg_value,
+			reg_addr);
+
+	SEP_DRV_LOG_FLOW_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/*!
+ * @fn     OS_STATUS GFX_Start
+ *
+ * @brief  Starts the count of the Graphics PMU
+ *
+ * @param  NONE
+ *
+ * @return OS_SUCCESS if success, otherwise error
+ *
+ * @note
+ */
+OS_STATUS GFX_Start(void)
+{
+	U32 reg_value;
+	char *reg_addr;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// GFX counting was not specified
+	if (gfx_virtual_addr == NULL || gfx_code == GFX_CTRL_DISABLE) {
+		SEP_DRV_LOG_ERROR(
+			"Invalid gfx_virtual_addr=0x%p or gfx_code=0x%x.",
+			gfx_virtual_addr, gfx_code);
+		SEP_DRV_LOG_TRACE_OUT("OS_INVALID.");
+		return OS_INVALID;
+	}
+
+	// turn on GFX counters as per event code
+	reg_addr = gfx_virtual_addr + GFX_PERF_REG;
+	*(U32 *)(reg_addr) = gfx_code;
+
+	// verify event code was written properly
+	reg_value = *(U32 *)reg_addr;
+	if (reg_value != gfx_code) {
+		SEP_DRV_LOG_ERROR("Got register value 0x%x, expected 0x%x.",
+				  reg_value, gfx_code);
+		SEP_DRV_LOG_TRACE_OUT("OS_INVALID.");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/*!
+ * @fn     OS_STATUS GFX_Stop
+ *
+ * @brief  Stops the count of the Graphics PMU
+ *
+ * @param  NONE
+ *
+ * @return OS_SUCCESS if success, otherwise error
+ *
+ * @note
+ */
+OS_STATUS GFX_Stop(void)
+{
+	char *reg_addr;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// GFX counting was not specified
+	if (gfx_virtual_addr == NULL || gfx_code == GFX_CTRL_DISABLE) {
+		SEP_DRV_LOG_ERROR(
+			"Invalid gfx_virtual_addr=0x%p or gfx_code=0x%x.",
+			gfx_virtual_addr, gfx_code);
+		SEP_DRV_LOG_TRACE_OUT("OS_INVALID.");
+		return OS_INVALID;
+	}
+
+	// turn off GFX counters
+	reg_addr = gfx_virtual_addr + GFX_PERF_REG;
+	*(U32 *)(reg_addr) = GFX_CTRL_DISABLE;
+
+	// unmap the memory mapped virtual address
+	PCI_Unmap_Memory(&gfx_map);
+	gfx_virtual_addr = NULL;
+
+	// reset the GFX global variables
+	gfx_code = GFX_CTRL_DISABLE;
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
diff --git a/drivers/platform/x86/sepdk/sep/gmch.c b/drivers/platform/x86/sepdk/sep/gmch.c
new file mode 100755
index 000000000000..41b9ee8b67a5
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/gmch.c
@@ -0,0 +1,505 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include <linux/version.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/pci.h>
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+
+#if defined(PCI_HELPERS_API)
+#include <asm/intel_scu_ipc.h>
+#include <asm/intel-mid.h>
+#endif
+
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "lwpmudrv_chipset.h"
+#include "inc/lwpmudrv.h"
+#include "inc/control.h"
+#include "inc/utility.h"
+#include "inc/gmch.h"
+#include "inc/pci.h"
+
+// global variables for determining which register offsets to use
+static U32 gmch_register_read; // value=0 indicates invalid read register
+static U32 gmch_register_write; // value=0 indicates invalid write register
+static U32 number_of_events;
+
+//global variable for reading GMCH counter values
+static U64 *gmch_current_data;
+static U64 *gmch_to_read_data;
+
+// global variable for tracking number of overflows per GMCH counter
+static U32 gmch_overflow[MAX_CHIPSET_COUNTERS];
+static U64 last_gmch_count[MAX_CHIPSET_COUNTERS];
+
+extern DRV_CONFIG drv_cfg;
+extern CHIPSET_CONFIG pma;
+extern CPU_STATE pcb;
+
+/*
+ * @fn        gmch_PCI_Read32(address)
+ *
+ * @brief     Read the 32bit value specified by the address
+ *
+ * @return    the read value
+ *
+ */
+#if defined(PCI_HELPERS_API)
+#define gmch_PCI_Read32 intel_mid_msgbus_read32_raw
+#else
+static U32 gmch_PCI_Read32(unsigned long address)
+{
+	U32 read_value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Address: %lx.", address);
+
+	PCI_Write_U32(0, 0, 0, GMCH_MSG_CTRL_REG, (U32)address);
+	read_value = PCI_Read_U32(0, 0, 0, GMCH_MSG_DATA_REG);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %x.", read_value);
+	return read_value;
+}
+#endif
+
+/*
+ * @fn        gmch_PCI_Write32(address, data)
+ *
+ * @brief     Write the 32bit value into the address specified
+ *
+ * @return    None
+ *
+ */
+#if defined(PCI_HELPERS_API)
+#define gmch_PCI_Write32 intel_mid_msgbus_write32_raw
+#else
+static void gmch_PCI_Write32(unsigned long address, unsigned long data)
+{
+	SEP_DRV_LOG_TRACE_IN("Address: %lx, data: %lx.", address, data);
+
+	PCI_Write_U32(0, 0, 0, GMCH_MSG_DATA_REG, data);
+	PCI_Write_U32(0, 0, 0, GMCH_MSG_CTRL_REG, address);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/*
+ * @fn        gmch_Check_Enabled()
+ *
+ * @brief     Read GMCH PMON capabilities
+ *
+ * @param     None
+ *
+ * @return    GMCH enable bits
+ *
+ */
+static ULONG gmch_Check_Enabled(void)
+{
+	ULONG enabled_value;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	enabled_value =
+		gmch_PCI_Read32(GMCH_PMON_CAPABILITIES + gmch_register_read);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %lx.", enabled_value);
+	return enabled_value;
+}
+
+/*
+ * @fn        gmch_Init_Chipset()
+ *
+ * @brief     Initialize GMCH Counters.  See note below.
+ *
+ * @param     None
+ *
+ * @note      This function must be called BEFORE any other function
+ *            in this file!
+ *
+ * @return    VT_SUCCESS if successful, error otherwise
+ *
+ */
+static U32 gmch_Init_Chipset(void)
+{
+	int i;
+	CHIPSET_SEGMENT cs;
+	CHIPSET_SEGMENT gmch_chipset_seg;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cs = &CHIPSET_CONFIG_gmch(pma);
+	gmch_chipset_seg = &CHIPSET_CONFIG_gmch(pma);
+
+	// configure read/write registers offsets according to usermode setting
+	if (cs) {
+		gmch_register_read = CHIPSET_SEGMENT_read_register(cs);
+		gmch_register_write = CHIPSET_SEGMENT_write_register(cs);
+		;
+	}
+	if (gmch_register_read == 0 || gmch_register_write == 0) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"VT_CHIPSET_CONFIG_FAILED(Invalid GMCH read/write registers!)");
+		return VT_CHIPSET_CONFIG_FAILED;
+	}
+
+	number_of_events = CHIPSET_SEGMENT_total_events(gmch_chipset_seg);
+	SEP_DRV_LOG_INIT("Number of chipset events %d.", number_of_events);
+
+	// Allocate memory for reading GMCH counter values + the group id
+	gmch_current_data =
+		CONTROL_Allocate_Memory((number_of_events + 1) * sizeof(U64));
+	if (!gmch_current_data) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_NO_MEM (!gmch_current_data).");
+		return OS_NO_MEM;
+	}
+	gmch_to_read_data =
+		CONTROL_Allocate_Memory((number_of_events + 1) * sizeof(U64));
+	if (!gmch_to_read_data) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_NO_MEM (!gmch_to_read_data).");
+		return OS_NO_MEM;
+	}
+
+	if (!DRV_CONFIG_enable_chipset(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"VT_SUCCESS (!DRV_CONFIG_enable_chipset(drv_cfg)).");
+		return VT_SUCCESS;
+	}
+
+	if (!CHIPSET_CONFIG_gmch_chipset(pma)) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"VT_SUCCESS (!CHIPSET_CONFIG_gmch_chipset(drv_cfg)).");
+		return VT_SUCCESS;
+	}
+	// initialize the GMCH per-counter overflow numbers
+	for (i = 0; i < MAX_CHIPSET_COUNTERS; i++) {
+		gmch_overflow[i] = 0;
+		last_gmch_count[i] = 0;
+	}
+
+	// disable fixed and GP counters
+	gmch_PCI_Write32(GMCH_PMON_GLOBAL_CTRL + gmch_register_write,
+			 0x00000000);
+	// clear fixed counter filter
+	gmch_PCI_Write32(GMCH_PMON_FIXED_CTR_CTRL + gmch_register_write,
+			 0x00000000);
+
+	SEP_DRV_LOG_TRACE_OUT("VT_SUCCESS.");
+	return VT_SUCCESS;
+}
+
+/*
+ * @fn        gmch_Start_Counters()
+ *
+ * @brief     Start the GMCH Counters.
+ *
+ * @param     None
+ *
+ * @return    None
+ *
+ */
+static VOID gmch_Start_Counters(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// reset and start chipset counters
+	if (pma == NULL) {
+		SEP_DRV_LOG_ERROR("gmch_Start_Counters: ERROR pma=NULL.");
+	}
+
+	// enable fixed and GP counters
+	gmch_PCI_Write32(GMCH_PMON_GLOBAL_CTRL + gmch_register_write,
+			 0x0001000F);
+	// enable fixed counter filter
+	gmch_PCI_Write32(GMCH_PMON_FIXED_CTR_CTRL + gmch_register_write,
+			 0x00000001);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn        gmch_Trigger_Read()
+ *
+ * @brief     Read the GMCH counters through PCI Config space
+ *
+ * @return    None
+ *
+ */
+static VOID gmch_Trigger_Read(void)
+{
+	U64 *data;
+	int i, data_index;
+	U64 val;
+	U64 *gmch_data;
+	U32 counter_data_low;
+	U32 counter_data_high;
+	U64 counter_data;
+	U64 cmd_register_low_read;
+	U64 cmd_register_high_read;
+	U32 gp_counter_index = 0;
+	U64 overflow;
+	U32 cur_driver_state;
+
+	CHIPSET_SEGMENT gmch_chipset_seg;
+	CHIPSET_EVENT chipset_events;
+	U64 *temp;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (!IS_COLLECTING_STATE(cur_driver_state)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Invalid driver state!");
+		return;
+	}
+
+	if (pma == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("pma is NULL!");
+		return;
+	}
+
+	if (gmch_current_data == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("gmch_current_data is NULL!");
+		return;
+	}
+
+	if (CHIPSET_CONFIG_gmch_chipset(pma) == 0) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"CHIPSET_CONFIG_gmch_chipset(pma) is NULL!");
+		return;
+	}
+
+	data = gmch_current_data;
+	data_index = 0;
+
+	preempt_disable();
+	SYS_Local_Irq_Disable();
+	gmch_chipset_seg = &CHIPSET_CONFIG_gmch(pma);
+	chipset_events = CHIPSET_SEGMENT_events(gmch_chipset_seg);
+
+	// Write GroupID
+	data[data_index] = 1;
+	// Increment the data index as the event id starts from zero
+	data_index++;
+
+	// GMCH data will be written as gmch_data[0], gmch_data[1], ...
+	gmch_data = data + data_index;
+
+	// read the GMCH counters and add them into the sample record
+
+	// iterate through GMCH counters configured to collect on events
+	for (i = 0; i < CHIPSET_SEGMENT_total_events(gmch_chipset_seg); i++) {
+		U32 event_id = CHIPSET_EVENT_event_id(&chipset_events[i]);
+		// read count for fixed GMCH counter event
+		if (event_id == 0) {
+			cmd_register_low_read =
+				GMCH_PMON_FIXED_CTR0 + gmch_register_read;
+			data[data_index++] =
+				(U64)gmch_PCI_Read32(cmd_register_low_read);
+			overflow = GMCH_PMON_FIXED_CTR_OVF_VAL;
+		} else {
+			// read count for general GMCH counter event
+			switch (gp_counter_index) {
+			case 0:
+			default:
+				cmd_register_low_read = GMCH_PMON_GP_CTR0_L +
+							gmch_register_read;
+				cmd_register_high_read = GMCH_PMON_GP_CTR0_H +
+							 gmch_register_read;
+				break;
+
+			case 1:
+				cmd_register_low_read = GMCH_PMON_GP_CTR1_L +
+							gmch_register_read;
+				cmd_register_high_read = GMCH_PMON_GP_CTR1_H +
+							 gmch_register_read;
+				break;
+
+			case 2:
+				cmd_register_low_read = GMCH_PMON_GP_CTR2_L +
+							gmch_register_read;
+				cmd_register_high_read = GMCH_PMON_GP_CTR2_H +
+							 gmch_register_read;
+				break;
+
+			case 3:
+				cmd_register_low_read = GMCH_PMON_GP_CTR3_L +
+							gmch_register_read;
+				cmd_register_high_read = GMCH_PMON_GP_CTR3_H +
+							 gmch_register_read;
+				break;
+			}
+			counter_data_low =
+				gmch_PCI_Read32(cmd_register_low_read);
+			counter_data_high =
+				gmch_PCI_Read32(cmd_register_high_read);
+			counter_data = (U64)counter_data_high;
+			data[data_index++] =
+				(counter_data << 32) + counter_data_low;
+			overflow = GMCH_PMON_GP_CTR_OVF_VAL;
+			gp_counter_index++;
+		}
+
+		/* Compute the running count of the event. */
+		gmch_data[i] &= overflow;
+		val = gmch_data[i];
+		if (gmch_data[i] < last_gmch_count[i]) {
+			gmch_overflow[i]++;
+		}
+		gmch_data[i] = gmch_data[i] + gmch_overflow[i] * overflow;
+		last_gmch_count[i] = val;
+	}
+
+	temp = gmch_to_read_data;
+	gmch_to_read_data = gmch_current_data;
+	gmch_current_data = temp;
+	SYS_Local_Irq_Enable();
+	preempt_enable();
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn        gmch_Read_Counters()
+ *
+ * @brief     Copy the GMCH data to the sampling data stream.
+ *
+ * @param     param - pointer to data stream where samples are to be written
+ *
+ * @return    None
+ *
+ */
+static VOID gmch_Read_Counters(PVOID param)
+{
+	U64 *data;
+	int i;
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (!IS_COLLECTING_STATE(cur_driver_state)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Invalid driver state!");
+		return;
+	}
+
+	if (pma == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("pma is NULL!");
+		return;
+	}
+
+	if (param == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("param is NULL!");
+		return;
+	}
+
+	if (gmch_to_read_data == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("gmch_to_read_data is NULL!");
+		return;
+	}
+
+	/*
+	 * Account for the group id that is placed at start of chipset array
+	 * Number of data elements to be transferred is number_of_events + 1.
+	 */
+	data = param;
+	for (i = 0; i < number_of_events + 1; i++) {
+		data[i] = gmch_to_read_data[i];
+		SEP_DRV_LOG_TRACE(
+			"Interrupt gmch read counters data %d is: 0x%llx.", i,
+			data[i]);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn        gmch_Stop_Counters()
+ *
+ * @brief     Stop the GMCH counters
+ *
+ * @param     None
+ *
+ * @return    None
+ *
+ */
+static VOID gmch_Stop_Counters(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// stop and reset the chipset counters
+	number_of_events = 0;
+	if (pma == NULL) {
+		SEP_DRV_LOG_ERROR("gmch_Stop_Counters: pma=NULL.");
+	}
+
+	// disable fixed and GP counters
+	gmch_PCI_Write32(GMCH_PMON_GLOBAL_CTRL + gmch_register_write,
+			 0x00000000);
+	gmch_PCI_Write32(GMCH_PMON_FIXED_CTR_CTRL + gmch_register_write,
+			 0x00000000);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn        gmch_Fini_Chipset()
+ *
+ * @brief     Reset GMCH to state where it can be used again.
+ *            Called at cleanup phase.
+ *
+ * @param     None
+ *
+ * @return    None
+ *
+ */
+static VOID gmch_Fini_Chipset(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!gmch_Check_Enabled()) {
+		SEP_DRV_LOG_WARNING("GMCH is not enabled!");
+	}
+
+	gmch_current_data = CONTROL_Free_Memory(gmch_current_data);
+	gmch_to_read_data = CONTROL_Free_Memory(gmch_to_read_data);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+//
+// Initialize the GMCH chipset dispatch table
+//
+
+CS_DISPATCH_NODE gmch_dispatch = { .init_chipset = gmch_Init_Chipset,
+				   .start_chipset = gmch_Start_Counters,
+				   .read_counters = gmch_Read_Counters,
+				   .stop_chipset = gmch_Stop_Counters,
+				   .fini_chipset = gmch_Fini_Chipset,
+				   .Trigger_Read = gmch_Trigger_Read };
diff --git a/drivers/platform/x86/sepdk/sep/linuxos.c b/drivers/platform/x86/sepdk/sep/linuxos.c
new file mode 100755
index 000000000000..08da10e614d8
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/linuxos.c
@@ -0,0 +1,1477 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+
+#include <linux/module.h>
+#include <linux/notifier.h>
+#include <linux/profile.h>
+#include <linux/sched.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 11, 0)
+#include <linux/sched/mm.h>
+#include <linux/sched/signal.h>
+#else
+#include <linux/sched.h>
+#endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+#include <linux/cpuhotplug.h>
+#endif
+#include <linux/fs.h>
+#include <linux/cpu.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 28)
+#include <trace/events/sched.h>
+#endif
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "inc/lwpmudrv.h"
+#include "inc/control.h"
+#include "inc/utility.h"
+#include "inc/cpumon.h"
+#include "inc/output.h"
+#include "inc/pebs.h"
+
+#include "inc/linuxos.h"
+#include "inc/apic.h"
+
+#include <asm/apic.h>
+#include <asm/io_apic.h>
+
+
+extern DRV_BOOL multi_pebs_enabled;
+extern DRV_BOOL sched_switch_enabled;
+extern uid_t uid;
+extern volatile pid_t control_pid;
+static volatile S32 hooks_installed;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0)
+static struct tracepoint *tp_sched_switch;
+#endif
+
+#define HOOK_FREE 0
+#define HOOK_UNINSTALL -10000
+static atomic_t hook_state = ATOMIC_INIT(HOOK_UNINSTALL);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0) && defined(DRV_CPU_HOTPLUG)
+static enum cpuhp_state cpuhp_sepdrv_state;
+#endif
+extern wait_queue_head_t wait_exit;
+
+static PVOID local_tasklist_lock;
+
+#define MY_TASK PROFILE_TASK_EXIT
+#define MY_UNMAP PROFILE_MUNMAP
+#ifdef CONFIG_X86_64
+#define MR_SEG_NUM 0
+#else
+#define MR_SEG_NUM 2
+#endif
+
+#if !defined(KERNEL_IMAGE_SIZE)
+#define KERNEL_IMAGE_SIZE (512 * 1024 * 1024)
+#endif
+
+#if defined(DRV_IA32)
+static U16 linuxos_Get_Exec_Mode(struct task_struct *p)
+{
+	return (unsigned short)MODE_32BIT;
+}
+#endif
+
+#if defined(DRV_EM64T)
+static U16 linuxos_Get_Exec_Mode(struct task_struct *p)
+{
+	SEP_DRV_LOG_TRACE_IN("P: %p.", p);
+
+	if (!p) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("MODE_UNKNOWN (p is NULL!).");
+		return MODE_UNKNOWN;
+	}
+
+	if (test_tsk_thread_flag(p, TIF_IA32)) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"Res: %u (test_tsk_thread_flag TIF_IA32).",
+			(U16)(unsigned short)MODE_32BIT);
+		return (unsigned short)MODE_32BIT;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U16)(unsigned short)MODE_64BIT);
+	return (unsigned short)MODE_64BIT;
+}
+#endif
+
+static S32 linuxos_Load_Image_Notify_Routine(char *name, U64 base, U32 size,
+					     U64 page_offset, U32 pid,
+					     U32 parent_pid, U32 options,
+					     unsigned short mode,
+					     S32 load_event, U32 segment_num,
+					     U32 kernel_modules, U32 osid)
+{
+	char *raw_path;
+	ModuleRecord *mra;
+	char buf[sizeof(ModuleRecord) + MAXNAMELEN + 32];
+	U64 tsc_read;
+	S32 local_load_event = (load_event == -1) ? 0 : load_event;
+	U64 page_offset_shift;
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(load_event == 1,
+					  "Name: '%s', pid: %d.", name, pid);
+
+	mra = (ModuleRecord *)buf;
+	memset(mra, '\0', sizeof(buf));
+	raw_path = (char *)mra + sizeof(ModuleRecord);
+
+	page_offset_shift = page_offset << PAGE_SHIFT;
+	MR_page_offset_Set(mra, page_offset_shift);
+	MODULE_RECORD_segment_type(mra) = mode;
+	MODULE_RECORD_load_addr64(mra) = (U64)(size_t)base;
+	MODULE_RECORD_length64(mra) = size;
+	MODULE_RECORD_tsc_used(mra) = 1;
+	MODULE_RECORD_first_module_rec_in_process(mra) =
+		options & LOPTS_1ST_MODREC;
+	MODULE_RECORD_segment_number(mra) = segment_num;
+	MODULE_RECORD_exe(mra) = (LOPTS_EXE & options) ? 1 : 0;
+	MODULE_RECORD_global_module_tb5(mra) =
+		(options & LOPTS_GLOBAL_MODULE) ? 1 : 0;
+	MODULE_RECORD_global_module(mra) =
+		(options & LOPTS_GLOBAL_MODULE) ? 1 : 0;
+	MODULE_RECORD_processed(mra) = 0;
+	MODULE_RECORD_parent_pid(mra) = parent_pid;
+	MODULE_RECORD_osid(mra) = osid;
+	MODULE_RECORD_pid_rec_index(mra) = pid;
+
+	if (kernel_modules) {
+		MODULE_RECORD_tsc(mra) = 0;
+		MR_unloadTscSet(mra, (U64)(0xffffffffffffffffLL));
+	} else {
+		UTILITY_Read_TSC(&tsc_read);
+		preempt_disable();
+		tsc_read -= TSC_SKEW(CONTROL_THIS_CPU());
+		preempt_enable();
+
+		if (local_load_event) {
+			MR_unloadTscSet(mra, tsc_read);
+		} else {
+			MR_unloadTscSet(mra, (U64)(-1));
+		}
+	}
+
+	MODULE_RECORD_pid_rec_index_raw(mra) = 1; // raw pid
+#if defined(DEBUG)
+	if (total_loads_init) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE(
+			load_event == 1,
+			"Setting pid_rec_index_raw pid 0x%x %s.", pid, name);
+	}
+#endif
+
+	strncpy(raw_path, name, MAXNAMELEN);
+	raw_path[MAXNAMELEN] = 0;
+	MODULE_RECORD_path_length(mra) = (U16)strlen(raw_path) + 1;
+	MODULE_RECORD_rec_length(mra) = (U16)ALIGN_8(
+		sizeof(ModuleRecord) + MODULE_RECORD_path_length(mra));
+
+#if defined(DRV_IA32)
+	MODULE_RECORD_selector(mra) = (pid == 0) ? __KERNEL_CS : __USER_CS;
+#endif
+#if defined(DRV_EM64T)
+	if (mode == MODE_64BIT) {
+		MODULE_RECORD_selector(mra) =
+			(pid == 0) ? __KERNEL_CS : __USER_CS;
+	} else if (mode == MODE_32BIT) {
+		MODULE_RECORD_selector(mra) =
+			(pid == 0) ? __KERNEL32_CS : __USER32_CS;
+	}
+#endif
+
+	OUTPUT_Module_Fill((PVOID)mra, MODULE_RECORD_rec_length(mra),
+			   load_event == 1);
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(load_event == 1, "OS_SUCCESS");
+	return OS_SUCCESS;
+}
+
+#ifdef DRV_MM_EXE_FILE_PRESENT
+static DRV_BOOL linuxos_Equal_VM_Exe_File(struct vm_area_struct *vma)
+{
+	S8 name_vm_file[MAXNAMELEN];
+	S8 name_exe_file[MAXNAMELEN];
+	S8 *pname_vm_file = NULL;
+	S8 *pname_exe_file = NULL;
+	DRV_BOOL res;
+
+	SEP_DRV_LOG_TRACE_IN("FMA: %p.", vma);
+
+	if (vma == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("FALSE (!vma).");
+		return FALSE;
+	}
+
+	if (vma->vm_file == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("FALSE (!vma->vm_file).");
+		return FALSE;
+	}
+
+	if (vma->vm_mm->exe_file == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("FALSE (!vma->vm_mm->exe_file).");
+		return FALSE;
+	}
+
+	pname_vm_file = D_PATH(vma->vm_file,
+			name_vm_file, MAXNAMELEN);
+	pname_exe_file = D_PATH(vma->vm_mm->exe_file,
+			name_exe_file, MAXNAMELEN);
+	res = strcmp(pname_vm_file, pname_exe_file) == 0;
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", res);
+	return res;
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          linuxos_Map_Kernel_Modules (void)
+ *
+ * @brief       Obtain kernel module details from modules list
+ *              and map the details to the module record.
+ *
+ * @return      S32       VT_SUCCESS on success
+ */
+static S32 linuxos_Map_Kernel_Modules(void)
+{
+	struct module *current_module;
+	struct list_head *modules;
+	U16 exec_mode;
+	unsigned long long addr;
+	unsigned long long size;
+#if defined(CONFIG_RANDOMIZE_BASE)
+	unsigned long dyn_addr = 0;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+#if defined(CONFIG_MODULES)
+	mutex_lock(&module_mutex);
+
+#if defined(DRV_EM64T)
+	addr = (unsigned long)__START_KERNEL_map;
+	exec_mode = MODE_64BIT;
+#elif defined(DRV_IA32)
+	addr = (unsigned long)PAGE_OFFSET;
+	exec_mode = MODE_32BIT;
+#else
+	exec_mode = MODE_UNKNOWN;
+#endif
+
+	SEP_DRV_LOG_TRACE(
+		"     kernel module            address           size");
+	SEP_DRV_LOG_TRACE(
+		"  -------------------    ------------------    -------");
+
+	addr += (CONFIG_PHYSICAL_START + (CONFIG_PHYSICAL_ALIGN - 1)) &
+		~(CONFIG_PHYSICAL_ALIGN - 1);
+	size = (unsigned long)KERNEL_IMAGE_SIZE -
+	       ((CONFIG_PHYSICAL_START + (CONFIG_PHYSICAL_ALIGN - 1)) &
+		~(CONFIG_PHYSICAL_ALIGN - 1)) -
+	       1;
+
+#if defined(CONFIG_RANDOMIZE_BASE)
+	if (!dyn_addr) {
+		dyn_addr = (unsigned long)UTILITY_Find_Symbol("_text");
+		if (!dyn_addr) {
+			dyn_addr = (unsigned long)UTILITY_Find_Symbol("_stext");
+		}
+
+		if (dyn_addr && dyn_addr > addr) {
+			dyn_addr &= ~(PAGE_SIZE - 1);
+			size -= (dyn_addr - addr);
+			addr = dyn_addr;
+		} else {
+			SEP_DRV_LOG_WARNING_TRACE_OUT(
+				"Could not find the kernel start address!");
+		}
+	}
+#endif
+
+	linuxos_Load_Image_Notify_Routine(
+		"vmlinux", addr, size, 0, 0, 0,
+		LOPTS_1ST_MODREC | LOPTS_GLOBAL_MODULE | LOPTS_EXE, exec_mode,
+		-1, MR_SEG_NUM, 1, OS_ID_NATIVE);
+
+	SEP_DRV_LOG_TRACE("kmodule: %20s    0x%llx    0x%llx.", "vmlinux", addr,
+			  size);
+
+#if defined(DRV_SEP_ACRN_ON)
+	linuxos_Load_Image_Notify_Routine(
+		"VMM", 0x0, (U32)0xffffffffffffffffLL, 0, 0, 0,
+		LOPTS_1ST_MODREC | LOPTS_GLOBAL_MODULE | LOPTS_EXE, exec_mode,
+		-1, MR_SEG_NUM, 1, OS_ID_ACORN);
+#endif
+
+	for (modules = (struct list_head *)(THIS_MODULE->list.prev);
+	     (unsigned long)modules > MODULES_VADDR; modules = modules->prev)
+		;
+	list_for_each_entry (current_module, modules, list) {
+		char *name = current_module->name;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 5, 0) ||                           \
+	defined(SEP_CONFIG_MODULE_LAYOUT)
+		addr = (unsigned long)current_module->core_layout.base;
+		size = current_module->core_layout.size;
+#else
+		addr = (unsigned long)current_module->module_core;
+		size = current_module->core_size;
+#endif
+
+		if (module_is_live(current_module)) {
+			SEP_DRV_LOG_TRACE("kmodule: %20s    0x%llx    0x%llx.",
+					  name, addr, size);
+			linuxos_Load_Image_Notify_Routine(
+				name, addr, size, 0, 0, 0, LOPTS_GLOBAL_MODULE,
+				exec_mode, -1, 0, 1, OS_ID_NATIVE);
+		}
+	}
+
+	mutex_unlock(&module_mutex);
+#endif
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS");
+	return OS_SUCCESS;
+}
+
+//
+// Register the module for a process.  The task_struct and mm
+// should be locked if necessary to make sure they don't change while we're
+// iterating...
+// Used as a service routine
+//
+static S32 linuxos_VMA_For_Process(struct task_struct *p,
+				   struct vm_area_struct *vma, S32 load_event,
+				   U32 *first)
+{
+	U32 options = 0;
+	S8 name[MAXNAMELEN];
+	S8 *pname = NULL;
+	U32 ppid = 0;
+	U16 exec_mode;
+	U64 page_offset = 0;
+
+#if defined(DRV_ANDROID)
+	char andr_app[TASK_COMM_LEN];
+#endif
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(
+		load_event == 1, "P = %p, vma = %p, load_event: %d, first: %p.",
+		p, vma, load_event, first);
+
+	if (p == NULL) {
+		SEP_DRV_LOG_NOTIFICATION_ERROR(load_event == 1,
+					       "Skipped p=NULL.");
+		SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(load_event == 1,
+						   "OS_SUCCESS (!p).");
+		return OS_SUCCESS;
+	}
+
+	if (vma->vm_file)
+		pname = D_PATH(vma->vm_file, name, MAXNAMELEN);
+
+	page_offset = vma->vm_pgoff;
+
+	if (!IS_ERR(pname) && pname != NULL) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE(load_event == 1,
+					       "enum: %s, %d, %lx, %lx %llu.",
+					       pname, p->pid, vma->vm_start,
+					       (vma->vm_end - vma->vm_start),
+					       page_offset);
+
+		// if the VM_EXECUTABLE flag is set then this is the module
+		// that is being used to name the module
+		if (DRV_VM_MOD_EXECUTABLE(vma)) {
+			options |= LOPTS_EXE;
+#if defined(DRV_ANDROID)
+			if (!strcmp(pname, "/system/bin/app_process") ||
+			    !strcmp(pname, "/system/bin/app_process32") ||
+			    !strcmp(pname, "/system/bin/app_process64")) {
+				memset(andr_app, '\0', TASK_COMM_LEN);
+				strncpy(andr_app, p->comm, TASK_COMM_LEN);
+				pname = andr_app;
+			}
+#endif
+		}
+		// mark the first of the bunch...
+		if (*first == 1) {
+			options |= LOPTS_1ST_MODREC;
+			*first = 0;
+		}
+	}
+#if defined(DRV_ALLOW_VDSO)
+	else if ((vma->vm_mm  != NULL) &&
+		 vma->vm_start == (long)vma->vm_mm->context.vdso) {
+		pname = "[vdso]";
+	}
+#endif
+#if defined(DRV_ALLOW_SYSCALL)
+	else if (vma->vm_start == VSYSCALL_START) {
+		pname = "[vsyscall]";
+	}
+#endif
+
+	if (pname != NULL) {
+		options = 0;
+		if (DRV_VM_MOD_EXECUTABLE(vma)) {
+			options |= LOPTS_EXE;
+		}
+
+		if (p && p->parent) {
+			ppid = p->parent->tgid;
+		}
+		exec_mode = linuxos_Get_Exec_Mode(p);
+		// record this module
+		linuxos_Load_Image_Notify_Routine(
+			pname, vma->vm_start, (vma->vm_end - vma->vm_start),
+			page_offset, p->pid, ppid, options, exec_mode,
+			load_event, 1, 0, OS_ID_NATIVE);
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(load_event == 1, "OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+//
+// Common loop to enumerate all modules for a process.  The task_struct and mm
+// should be locked if necessary to make sure they don't change while we're
+// iterating...
+//
+static S32 linuxos_Enum_Modules_For_Process(struct task_struct *p,
+					    struct mm_struct *mm,
+					    S32 load_event)
+{
+	struct vm_area_struct *mmap;
+	U32 first = 1;
+
+#if defined(SECURE_SEP)
+	uid_t l_uid;
+#endif
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(load_event == 1,
+					  "P: %p, mm: %p, load_event: %d.", p,
+					  mm, load_event);
+
+#if defined(SECURE_SEP)
+	l_uid = DRV_GET_UID(p);
+	/*
+	 * Check for:  same uid, or root uid
+	 */
+	if (l_uid != uid && l_uid != 0) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(
+			load_event == 1,
+			"OS_SUCCESS (secure_sep && l_uid != uid && l_uid != 0).");
+		return OS_SUCCESS;
+	}
+#endif
+	for (mmap = mm->mmap; mmap; mmap = mmap->vm_next) {
+		/* We have 3 distinct conditions here.
+		 * 1) Is the page executable?
+		 * 2) Is is a part of the vdso area?
+		 * 3) Is it the vsyscall area?
+		 */
+		if (((mmap->vm_flags & VM_EXEC) && mmap->vm_file &&
+		     mmap->vm_file->DRV_F_DENTRY)
+#if defined(DRV_ALLOW_VDSO)
+		    || (mmap->vm_mm &&
+			mmap->vm_start == (long)mmap->vm_mm->context.vdso)
+#endif
+#if defined(DRV_ALLOW_VSYSCALL)
+		    || (mmap->vm_start == VSYSCALL_START)
+#endif
+		) {
+
+			linuxos_VMA_For_Process(p, mmap, load_event, &first);
+		}
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(load_event == 1, "OS_SUCCESS");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static int linuxos_Exec_Unmap_Notify(
+ *                  struct notifier_block  *self,
+ *                  unsigned long           val,
+ *                  VOID                   *data)
+ *
+ * @brief       this function is called whenever a task exits
+ *
+ * @param       self IN  - not used
+ *              val  IN  - not used
+ *              data IN  - this is cast in the mm_struct of the task that is call unmap
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ * This notification is called from do_munmap(mm/mmap.c). This is called when ever
+ * a module is loaded or unloaded. It looks like it is called right after a module is
+ * loaded or before its unloaded (if using dlopen, dlclose).
+ * However it is not called when a process is exiting instead exit_mmap is called
+ * (resulting in an EXIT_MMAP notification).
+ */
+static int linuxos_Exec_Unmap_Notify(struct notifier_block *self,
+				     unsigned long val, PVOID data)
+{
+	struct mm_struct *mm;
+	struct vm_area_struct *mmap = NULL;
+	U32 first = 1;
+	U32 cur_driver_state;
+
+#if defined(SECURE_SEP)
+	uid_t l_uid;
+#endif
+
+	SEP_DRV_LOG_NOTIFICATION_IN("Self: %p, val: %lu, data: %p.", self, val,
+				    data);
+	SEP_DRV_LOG_NOTIFICATION_TRACE(SEP_IN_NOTIFICATION,
+				       "enter: unmap: hook_state %d.",
+				       atomic_read(&hook_state));
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+#if defined(SECURE_SEP)
+	l_uid = DRV_GET_UID(current);
+	/*
+	 * Check for:  same uid, or root uid
+	 */
+	if (l_uid != uid && l_uid != 0) {
+		SEP_DRV_LOG_NOTIFICATION_OUT(
+			"Returns 0 (secure_sep && l_uid != uid && l_uid != 0).");
+		return 0;
+	}
+#endif
+
+	if (!IS_COLLECTING_STATE(cur_driver_state)) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Early exit (driver state).");
+		return 0;
+	}
+	if (!atomic_add_negative(1, &hook_state)) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE(SEP_IN_NOTIFICATION,
+					       "unmap: hook_state %d.",
+					       atomic_read(&hook_state));
+		mm = get_task_mm(current);
+		if (mm) {
+			UTILITY_down_read_mm(mm);
+			mmap = FIND_VMA(mm, data);
+			if (mmap && mmap->vm_file &&
+			    (mmap->vm_flags & VM_EXEC)) {
+				linuxos_VMA_For_Process(current, mmap, TRUE,
+							&first);
+			}
+			UTILITY_up_read_mm(mm);
+			mmput(mm);
+		}
+	}
+	atomic_dec(&hook_state);
+	SEP_DRV_LOG_NOTIFICATION_TRACE(SEP_IN_NOTIFICATION,
+				       "exit: unmap done: hook_state %d.",
+				       atomic_read(&hook_state));
+
+	SEP_DRV_LOG_NOTIFICATION_OUT("Returns 0.");
+	return 0;
+}
+
+#if defined(DRV_CPU_HOTPLUG)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID linuxos_Handle_Online_cpu(
+ *               PVOID param)
+ *
+ * @param    PVOID param
+ *
+ * @return   None
+ *
+ * @brief    Callback function to set the cpu online
+ * @brief    and begin collection on it
+ */
+static VOID linuxos_Handle_Online_cpu(PVOID param)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(SEP_IN_NOTIFICATION,
+					  "Dummy param: %p.", param);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	preempt_enable();
+	CPUMON_Online_Cpu((PVOID)&this_cpu);
+	if (CPU_STATE_pmu_state(&pcb[this_cpu]) == NULL) {
+		if (dispatch && dispatch->init) {
+			dispatch->init(NULL);
+		}
+	}
+	if (dispatch && dispatch->write) {
+		dispatch->write(NULL);
+	}
+	CPU_STATE_group_swap(&pcb[this_cpu]) = 1;
+	if (GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		// possible race conditions with notifications.
+		// cleanup should wait until all notifications are done,
+		// and new notifications should not proceed
+		if (dispatch && dispatch->restart) {
+			dispatch->restart(NULL);
+		}
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(SEP_IN_NOTIFICATION, "");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID linuxos_Handle_Offline_cpu(
+ *               PVOID param)
+ *
+ * @param    PVOID param
+ *
+ * @return   None
+ *
+ * @brief    Callback function to set the cpu offline
+ * @brief    and stop collection on it
+ */
+static VOID linuxos_Handle_Offline_cpu(PVOID param)
+{
+	U32 this_cpu;
+	U32 apic_lvterr;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(SEP_IN_NOTIFICATION,
+					  "Dummy param: %p.", param);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	preempt_enable();
+	CPUMON_Offline_Cpu((PVOID)&this_cpu);
+	if (dispatch && dispatch->freeze) {
+		dispatch->freeze(NULL);
+	}
+	apic_lvterr = apic_read(APIC_LVTERR);
+	apic_write(APIC_LVTERR, apic_lvterr | APIC_LVT_MASKED);
+	APIC_Restore_LVTPC(NULL);
+	apic_write(APIC_LVTERR, apic_lvterr);
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(SEP_IN_NOTIFICATION, "");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       int linuxos_online_cpu(
+ *               unsigned int cpu)
+ *
+ * @param    unsigned int cpu
+ *
+ * @return   None
+ *
+ * @brief    Invokes appropriate call back function when CPU is online
+ */
+static int linuxos_online_cpu(unsigned int cpu)
+{
+	SEP_DRV_LOG_NOTIFICATION_IN("Cpu %d coming online.", cpu);
+
+	if (CPUMON_is_Online_Allowed()) {
+		CONTROL_Invoke_Cpu(cpu, linuxos_Handle_Online_cpu, NULL);
+		SEP_DRV_LOG_NOTIFICATION_OUT("Cpu %d came online.", cpu);
+		return 0;
+	} else {
+		SEP_DRV_LOG_WARNING_NOTIFICATION_OUT(
+			"Cpu %d is not allowed to come online!", cpu);
+		return 0;
+	}
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       int linuxos_offline_cpu(
+ *               unsigned int cpu)
+ *
+ * @param    unsigned int cpu
+ *
+ * @return   None
+ *
+ * @brief    Invokes appropriate call back function when CPU is offline
+ */
+static int linuxos_offline_cpu(unsigned int cpu)
+{
+	SEP_DRV_LOG_NOTIFICATION_IN("Cpu %d going offline.", cpu);
+
+	if (CPUMON_is_Offline_Allowed()) {
+		CONTROL_Invoke_Cpu(cpu, linuxos_Handle_Offline_cpu, NULL);
+		SEP_DRV_LOG_NOTIFICATION_OUT("Cpu %d went offline.", cpu);
+		return 0;
+	} else {
+		SEP_DRV_LOG_WARNING_NOTIFICATION_OUT(
+			"Cpu %d is not allowed to go offline!", cpu);
+		return 0;
+	}
+}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       OS_STATUS linuxos_Hotplug_Notifier(
+ *               struct notifier_block *block, unsigned long action, void *pcpu)
+ *
+ * @param    struct notifier_block *block - notifier block
+ *           unsigned long action - notifier action
+ *           void *pcpu - per cpu pcb
+ *
+ * @return   NOTIFY_OK, if successful
+ *
+ * @brief    Hotplug Notifier function that handles various cpu states
+ * @brief    and invokes respective callback functions
+ */
+static OS_STATUS linuxos_Hotplug_Notifier(struct notifier_block *block,
+					  unsigned long action, void *pcpu)
+{
+	U32 cpu = (unsigned int)(unsigned long)pcpu;
+
+	SEP_DRV_LOG_NOTIFICATION_IN(
+		"Cpu: %u, action: %u.", cpu,
+		action); // nb: will overcount number of pending notifications
+	// when using this routine
+
+	switch (action & ~CPU_TASKS_FROZEN) {
+	case CPU_DOWN_FAILED:
+		SEP_DRV_LOG_ERROR("SEP cpu %d offline failed!", cpu);
+	case CPU_ONLINE:
+		linuxos_online_cpu(cpu);
+		break;
+	case CPU_DOWN_PREPARE:
+		linuxos_offline_cpu(cpu);
+		break;
+	default:
+		SEP_DRV_LOG_WARNING(
+			"DEFAULT: cpu %d unhandled action value is %d.", cpu,
+			action);
+		break;
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_OUT("");
+	return NOTIFY_OK;
+}
+
+static struct notifier_block cpu_hotplug_notifier = {
+	.notifier_call = &linuxos_Hotplug_Notifier,
+};
+#endif
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID LINUXOS_Register_Hotplug(
+ *               VOID)
+ *
+ * @param    None
+ *
+ * @return   None
+ *
+ * @brief    Registers the Hotplug Notifier
+ */
+VOID LINUXOS_Register_Hotplug(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	S32 err;
+
+	SEP_DRV_LOG_INIT_IN(
+		"Kernel version >= 4.10.0: using direct notifications.");
+
+	err = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN, "ia64/sep5:online",
+					linuxos_online_cpu,
+					linuxos_offline_cpu);
+	cpuhp_sepdrv_state = (int)err;
+#else
+	SEP_DRV_LOG_INIT_IN("Kernel version < 4.10.0: using notification hub.");
+	register_cpu_notifier(&cpu_hotplug_notifier);
+#endif
+	SEP_DRV_LOG_INIT_OUT("Hotplug notifier registered.");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID LINUXOS_Unregister_Hotplug(
+ *               VOID)
+ *
+ * @param    None
+ *
+ * @return   None
+ *
+ * @brief    Unregisters the Hotplug Notifier
+ */
+VOID LINUXOS_Unregister_Hotplug(void)
+{
+	SEP_DRV_LOG_INIT_IN("Unregistering hotplug notifier.");
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 10, 0)
+	cpuhp_remove_state_nocalls(cpuhp_sepdrv_state);
+#else
+	unregister_cpu_notifier(&cpu_hotplug_notifier);
+#endif
+	SEP_DRV_LOG_INIT_OUT("Hotplug notifier unregistered.");
+}
+#endif
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          OS_STATUS LINUXOS_Enum_Process_Modules(DRV_BOOL at_end)
+ *
+ * @brief       gather all the process modules that are present.
+ *
+ * @param       at_end - the collection happens at the end of the sampling run
+ *
+ * @return      OS_SUCCESS
+ *
+ * <I>Special Notes:</I>
+ *              This routine gathers all the process modules that are present
+ *              in the system at this time.  If at_end is set to be TRUE, then
+ *              act as if all the modules are being unloaded.
+ *
+ */
+OS_STATUS LINUXOS_Enum_Process_Modules(DRV_BOOL at_end)
+{
+	int n = 0;
+	struct task_struct *p;
+
+	SEP_DRV_LOG_TRACE_IN("At_end: %u.", at_end);
+	SEP_DRV_LOG_TRACE("Begin tasks.");
+
+	if (GET_DRIVER_STATE() == DRV_STATE_TERMINATING) {
+		SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS (TERMINATING).");
+		return OS_SUCCESS;
+	}
+
+	if (!local_tasklist_lock) {
+		local_tasklist_lock =
+			(PVOID)(UIOP)UTILITY_Find_Symbol("tasklist_lock");
+		if (!local_tasklist_lock) {
+			SEP_DRV_LOG_WARNING("Could not find tasklist_lock.");
+		}
+	}
+
+	// In some machines the tasklist_lock symbol does not exist.
+	// For temporary solution we skip the lock if there is no tasklist_lock
+	if (local_tasklist_lock) {
+#if defined(                                                                   \
+	DEFINE_QRWLOCK) // assuming that if DEFINE_QRWLOCK is defined, then tasklist_lock was defined using it
+		qread_lock(local_tasklist_lock);
+#else
+		read_lock(local_tasklist_lock);
+#endif
+	}
+
+	FOR_EACH_TASK(p)
+	{
+		struct mm_struct *mm;
+
+		SEP_DRV_LOG_TRACE("Looking at task %d.", n);
+		/*
+		 *  Call driver notification routine for each module
+		 *  that is mapped into the process created by the fork
+		 */
+		p->comm[TASK_COMM_LEN - 1] = 0;
+		// making sure there is a trailing 0
+		mm = get_task_mm(p);
+
+		if (!mm) {
+			SEP_DRV_LOG_TRACE(
+				"Skipped (p->mm=NULL). P=0x%p, pid=%d, p->comm=%s.",
+				p, p->pid, p->comm);
+			linuxos_Load_Image_Notify_Routine(
+				p->comm, 0, 0, 0, p->pid,
+				(p->parent) ? p->parent->tgid : 0,
+				LOPTS_EXE | LOPTS_1ST_MODREC,
+				linuxos_Get_Exec_Mode(p),
+				2, // '2' to trigger 'if (load_event)' conditions
+				1, 0, OS_ID_NATIVE);
+			continue;
+		}
+
+		UTILITY_down_read_mm(mm);
+		linuxos_Enum_Modules_For_Process(p, mm, at_end ? -1 : 0);
+		UTILITY_up_read_mm(mm);
+		mmput(mm);
+		n++;
+	}
+
+	if (local_tasklist_lock) {
+#if defined(DEFINE_QRWLOCK)
+		qread_unlock(local_tasklist_lock);
+#else
+		read_unlock(local_tasklist_lock);
+#endif
+	}
+
+	SEP_DRV_LOG_TRACE("Enum_Process_Modules done with %d tasks.", n);
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          static int linuxos_Exit_Task_Notify(struct notifier_block * self,
+ *                  unsigned long val, PVOID data)
+ * @brief       this function is called whenever a task exits
+ *
+ * @param       self IN  - not used
+ *              val IN  - not used
+ *              data IN  - this is cast into the task_struct of the exiting task
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ * this function is called whenever a task exits.  It is called right before
+ * the virtual memory areas are freed.  We just enumerate through all the modules
+ * of the task and set the unload sample count and the load event flag to 1 to
+ * indicate this is a module unload
+ */
+static int linuxos_Exit_Task_Notify(struct notifier_block *self,
+				    unsigned long val, PVOID data)
+{
+	struct task_struct *p = (struct task_struct *)data;
+	int status = OS_SUCCESS;
+	U32 cur_driver_state;
+	struct mm_struct *mm;
+
+	SEP_DRV_LOG_NOTIFICATION_IN("Self: %p, val: %lu, data: %p.", self, val,
+				    data);
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (cur_driver_state == DRV_STATE_UNINITIALIZED ||
+	    cur_driver_state == DRV_STATE_TERMINATING) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Early exit (driver state).");
+		return status;
+	}
+	SEP_DRV_LOG_TRACE("Pid = %d tgid = %d.", p->pid, p->tgid);
+	if (p->pid == control_pid) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE(
+			SEP_IN_NOTIFICATION,
+			"The collector task has been terminated via an uncatchable signal.");
+		SEP_DRV_LOG_NOTIFICATION_WARNING(SEP_IN_NOTIFICATION,
+						 "Sep was killed!");
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		wake_up_interruptible(&wait_exit);
+
+		SEP_DRV_LOG_NOTIFICATION_OUT("Res = %u (pid == control_pid).",
+					     status);
+		return status;
+	}
+
+	if (cur_driver_state != DRV_STATE_IDLE &&
+	    !IS_COLLECTING_STATE(cur_driver_state)) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Res = %u (stopping collection).",
+					     status);
+		return status;
+	}
+
+	mm = get_task_mm(p);
+	if (!mm) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Res = %u (!p->mm).", status);
+		return status;
+	}
+	UTILITY_down_read_mm(mm);
+	if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+		if (!atomic_add_negative(1, &hook_state)) {
+			linuxos_Enum_Modules_For_Process(p, mm, 1);
+		}
+		atomic_dec(&hook_state);
+	}
+	UTILITY_up_read_mm(mm);
+	mmput(mm);
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE(SEP_IN_NOTIFICATION, "Hook_state %d.",
+				       atomic_read(&hook_state));
+
+	SEP_DRV_LOG_NOTIFICATION_OUT("Res = %u.", status);
+	return status;
+}
+
+/*
+ *  The notifier block.  All the static entries have been defined at this point
+ */
+static struct notifier_block linuxos_exec_unmap_nb = {
+	.notifier_call = linuxos_Exec_Unmap_Notify,
+};
+
+static struct notifier_block linuxos_exit_task_nb = {
+	.notifier_call = linuxos_Exit_Task_Notify,
+};
+
+#if defined(CONFIG_TRACEPOINTS)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void capture_sched_switch(VOID *)
+ * @brief       capture current pid/tid on all cpus
+ *
+ * @param       p IN - not used
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+static void capture_sched_switch(void *p)
+{
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	SIDEBAND_INFO sideband_info;
+	U64 tsc;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	UTILITY_Read_TSC(&tsc);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+
+	bd = &cpu_sideband_buf[this_cpu];
+	if (bd == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Bd is NULL!");
+		return;
+	}
+
+	sideband_info = (SIDEBAND_INFO)OUTPUT_Reserve_Buffer_Space(
+		bd, sizeof(SIDEBAND_INFO_NODE), FALSE, !SEP_IN_NOTIFICATION,
+		(S32)this_cpu);
+	if (sideband_info == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Sideband_info is NULL!");
+		return;
+	}
+
+	SIDEBAND_INFO_pid(sideband_info) = current->tgid;
+	SIDEBAND_INFO_tid(sideband_info) = current->pid;
+	SIDEBAND_INFO_tsc(sideband_info) = tsc;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 28)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void record_pebs_process_info(...)
+ * @brief       record all sched switch pid/tid info
+ *
+ * @param       ignore IN - not used
+ *              from   IN
+ *              to     IN
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+static void record_pebs_process_info(void *ignore, bool preempt,
+				     struct task_struct *from,
+				     struct task_struct *to)
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 35)
+static void record_pebs_process_info(void *ignore, struct task_struct *from,
+				     struct task_struct *to)
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 28)
+static void record_pebs_process_info(struct rq *ignore,
+				     struct task_struct *from,
+				     struct task_struct *to)
+#endif
+{
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	SIDEBAND_INFO sideband_info;
+	U64 tsc;
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_NOTIFICATION_IN("From: %p, to: %p.", from, to);
+
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (cur_driver_state != DRV_STATE_IDLE &&
+	    !IS_COLLECTING_STATE(cur_driver_state)) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Early exit (driver state).");
+		return;
+	}
+
+	UTILITY_Read_TSC(&tsc);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE(SEP_IN_NOTIFICATION,
+				       "[OUT<%d:%d:%s>-IN<%d:%d:%s>].",
+				       from->tgid, from->pid, from->comm,
+				       to->tgid, to->pid, to->comm);
+
+	bd = &cpu_sideband_buf[this_cpu];
+	if (bd == NULL) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Early exit (!bd).");
+		return;
+	}
+
+#if (LINUX_VERSION_CODE >= KERNEL_VERSION(4, 13, 0)) &&                        \
+	(LINUX_VERSION_CODE < KERNEL_VERSION(4, 14, 0))
+	sideband_info = (SIDEBAND_INFO)OUTPUT_Reserve_Buffer_Space(
+		bd, sizeof(SIDEBAND_INFO_NODE), TRUE, SEP_IN_NOTIFICATION,
+		(S32)this_cpu);
+#else
+	sideband_info = (SIDEBAND_INFO)OUTPUT_Reserve_Buffer_Space(
+		bd, sizeof(SIDEBAND_INFO_NODE), FALSE, SEP_IN_NOTIFICATION,
+		(S32)this_cpu);
+#endif
+
+	if (sideband_info == NULL) {
+		SEP_DRV_LOG_NOTIFICATION_OUT("Early exit (!sideband_info).");
+		return;
+	}
+
+	SIDEBAND_INFO_pid(sideband_info) = to->tgid;
+	SIDEBAND_INFO_tid(sideband_info) = to->pid;
+	SIDEBAND_INFO_tsc(sideband_info) = tsc;
+
+	SEP_DRV_LOG_NOTIFICATION_OUT("");
+}
+#endif
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void find_sched_switch_tracepoint
+ * @brief       find trace poing for sched_switch
+ *
+ * @param       tp    pass in by system
+ *              param pointer of trace point
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+static void find_sched_switch_tracepoint(struct tracepoint *tp, VOID *param)
+{
+	struct tracepoint **ptp = (struct tracepoint **)param;
+
+	SEP_DRV_LOG_TRACE_IN("Tp: %p, param: %p.", tp, param);
+
+	if (tp && ptp) {
+		SEP_DRV_LOG_TRACE("trace point name: %s.", tp->name);
+		if (!strcmp(tp->name, "sched_switch")) {
+			SEP_DRV_LOG_TRACE(
+				"Found trace point for sched_switch.");
+			*ptp = tp;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          int install_sched_switch_callback(void)
+ * @brief       registers sched_switch callbacks for PEBS sideband
+ *
+ * @param       none
+ *
+ * @return      0 success else error number
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+static int install_sched_switch_callback(void)
+{
+	int err = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+	SEP_DRV_LOG_INIT("Installing PEBS linux OS Hooks.");
+
+#if defined(CONFIG_TRACEPOINTS)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0)
+	if (!tp_sched_switch) {
+		for_each_kernel_tracepoint(&find_sched_switch_tracepoint,
+					   &tp_sched_switch);
+	}
+	if (!tp_sched_switch) {
+		err = -EIO;
+		SEP_DRV_LOG_INIT(
+			"Please check Linux is built w/ CONFIG_CONTEXT_SWITCH_TRACER.");
+	} else {
+		err = tracepoint_probe_register(
+			tp_sched_switch, (void *)record_pebs_process_info,
+			NULL);
+	}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 35)
+	err = register_trace_sched_switch(record_pebs_process_info, NULL);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 28)
+	err = register_trace_sched_switch(record_pebs_process_info);
+#else
+	SEP_DRV_LOG_INIT(
+		"Please use Linux kernel version >= 2.6.28 to use multiple pebs.");
+	err = -1;
+#endif
+	CONTROL_Invoke_Parallel(capture_sched_switch, NULL);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %d.", err);
+	return err;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID LINUXOS_Install_Hooks(void)
+ * @brief       registers the profiling callbacks
+ *
+ * @param       none
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+VOID LINUXOS_Install_Hooks(void)
+{
+	int err = 0;
+	int err2 = 0;
+
+	SEP_DRV_LOG_INIT_IN("Installing Linux OS Hooks.");
+
+	if (hooks_installed == 1) {
+		SEP_DRV_LOG_INIT_OUT("The OS Hooks are already installed.");
+		return;
+	}
+
+	linuxos_Map_Kernel_Modules();
+
+	err = profile_event_register(MY_UNMAP, &linuxos_exec_unmap_nb);
+	err2 = profile_event_register(MY_TASK, &linuxos_exit_task_nb);
+	if (err || err2) {
+		if (err == OS_NO_SYSCALL) {
+			SEP_DRV_LOG_WARNING(
+				"This kernel does not implement kernel profiling hooks...");
+			SEP_DRV_LOG_WARNING(
+				"...task termination and image unloads will not be tracked...");
+			SEP_DRV_LOG_WARNING("...during sampling session!");
+		}
+	}
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		err = install_sched_switch_callback();
+		if (err) {
+			SEP_DRV_LOG_WARNING(
+				"Failed to install sched_switch callback for multiple pebs.");
+		}
+	}
+
+	hooks_installed = 1;
+	atomic_set(&hook_state, HOOK_FREE);
+
+	SEP_DRV_LOG_INIT_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          int uninstall_sched_switch_callback(void)
+ * @brief       unregisters sched_switch callbacks for PEBS sideband
+ *
+ * @param       none
+ *
+ * @return      0 success else error number
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+static int uninstall_sched_switch_callback(void)
+{
+	int err = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+	SEP_DRV_LOG_INIT("Uninstalling PEBS Linux OS Hooks.");
+
+#if defined(CONFIG_TRACEPOINTS)
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0)
+	if (!tp_sched_switch) {
+		err = -EIO;
+		SEP_DRV_LOG_INIT(
+			"Please check Linux is built w/ CONFIG_CONTEXT_SWITCH_TRACER.");
+	} else {
+		err = tracepoint_probe_unregister(
+			tp_sched_switch, (void *)record_pebs_process_info,
+			NULL);
+	}
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 35)
+	err = unregister_trace_sched_switch(record_pebs_process_info, NULL);
+#elif LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 28)
+	err = unregister_trace_sched_switch(record_pebs_process_info);
+#else
+	SEP_DRV_LOG_INIT(
+		"Please use Linux kernel version >= 2.6.28 to use multiple pebs.");
+	err = -1;
+#endif
+	CONTROL_Invoke_Parallel(capture_sched_switch, NULL);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %d.", err);
+	return err;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID LINUXOS_Uninstall_Hooks(void)
+ * @brief       unregisters the profiling callbacks
+ *
+ * @param       none
+ *
+ * @return
+ *
+ * <I>Special Notes:</I>
+ *
+ * None
+ */
+VOID LINUXOS_Uninstall_Hooks(void)
+{
+	int err = 0;
+	int value = 0;
+	int tries = 10;
+
+	SEP_DRV_LOG_INIT_IN("Uninstalling Linux OS Hooks.");
+
+	if (hooks_installed == 0) {
+		SEP_DRV_LOG_INIT_OUT("Hooks are not installed!");
+		return;
+	}
+
+	hooks_installed = 0;
+	profile_event_unregister(MY_UNMAP, &linuxos_exec_unmap_nb);
+	profile_event_unregister(MY_TASK, &linuxos_exit_task_nb);
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		err = uninstall_sched_switch_callback();
+		if (err) {
+			SEP_DRV_LOG_WARNING(
+				"Failed to uninstall sched_switch callback for multiple pebs.");
+		}
+	}
+
+	value = atomic_cmpxchg(&hook_state, HOOK_FREE, HOOK_UNINSTALL);
+	if ((value == HOOK_FREE) ||
+	    (value == HOOK_UNINSTALL)) { // already in free or uninstall state
+		SEP_DRV_LOG_INIT_OUT(
+			"Uninstall hook done (already in state %d).", value);
+		return;
+	}
+	atomic_add(HOOK_UNINSTALL, &hook_state);
+	while (tries) {
+		SYS_IO_Delay();
+		SYS_IO_Delay();
+		value = atomic_read(&hook_state);
+		if (value == HOOK_UNINSTALL) {
+			break;
+		}
+		tries--;
+	}
+
+	SEP_DRV_LOG_INIT_OUT("Done -- state %d, tries %d.", value, tries);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          DRV_BOOL LINUXOS_Check_KVM_Guest_Process()
+ *
+ * @brief       check the presence of kvm guest process
+ *
+ * @param       none
+ *
+ * @return      TRUE if the kvm guest process is running, FALSE if not
+ */
+DRV_BOOL LINUXOS_Check_KVM_Guest_Process(void)
+{
+	struct task_struct *p;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!local_tasklist_lock) {
+		local_tasklist_lock =
+			(PVOID)(UIOP)UTILITY_Find_Symbol("tasklist_lock");
+		if (!local_tasklist_lock) {
+			SEP_DRV_LOG_WARNING("Could not find tasklist_lock.");
+		}
+	}
+
+	// In some machines the tasklist_lock symbol does not exist.
+	// For temporary solution we skip the lock if there is no tasklist_lock
+	if (local_tasklist_lock) {
+#if defined(DEFINE_QRWLOCK)
+		qread_lock(local_tasklist_lock);
+#else
+		read_lock(local_tasklist_lock);
+#endif
+	}
+
+	FOR_EACH_TASK(p)
+	{
+		// if (p == NULL) {
+		// 	continue;
+		// }
+
+		p->comm[TASK_COMM_LEN - 1] =
+			0; // making sure there is a trailing 0
+
+		if (!strncmp(p->comm, "qemu-kvm", 8)) {
+			if (local_tasklist_lock) {
+#if defined(DEFINE_QRWLOCK)
+				qread_unlock(local_tasklist_lock);
+#else
+				read_unlock(local_tasklist_lock);
+#endif
+			}
+
+			SEP_DRV_LOG_INIT_TRACE_OUT("TRUE (found qemu-kvm!).");
+			return TRUE;
+		}
+	}
+
+	if (local_tasklist_lock) {
+#if defined(DEFINE_QRWLOCK)
+		qread_unlock(local_tasklist_lock);
+#else
+		read_unlock(local_tasklist_lock);
+#endif
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("FALSE");
+	return FALSE;
+}
diff --git a/drivers/platform/x86/sepdk/sep/lwpmudrv.c b/drivers/platform/x86/sepdk/sep/lwpmudrv.c
new file mode 100755
index 000000000000..f13552c20774
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/lwpmudrv.c
@@ -0,0 +1,7537 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_version.h"
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/errno.h>
+#include <linux/types.h>
+#include <asm/page.h>
+#include <linux/cdev.h>
+#include <linux/proc_fs.h>
+#include <linux/fcntl.h>
+#include <linux/device.h>
+#include <linux/ptrace.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 11, 0)
+#include <linux/sched/clock.h>
+#else
+#include <linux/sched.h>
+#endif
+#include <linux/syscalls.h>
+#include <asm/unistd.h>
+#include <linux/compat.h>
+#include <linux/vmalloc.h>
+#include <linux/kthread.h>
+#if defined(CONFIG_HYPERVISOR_GUEST)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 34)
+#include <asm/hypervisor.h>
+#endif
+#endif
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 32)
+#include <xen/xen.h>
+#endif
+
+#if defined(CONFIG_XEN_HAVE_VPMU)
+#include <asm/xen/hypercall.h>
+#include <asm/xen/page.h>
+#include <xen/interface/xenpmu.h>
+#endif
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_ioctl.h"
+#include "lwpmudrv_struct.h"
+#include "inc/ecb_iterators.h"
+#include "inc/unc_common.h"
+
+#if defined(BUILD_GFX)
+#include "gfx.h"
+#endif
+
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#endif
+#include "pci.h"
+
+#include "apic.h"
+#include "cpumon.h"
+#include "lwpmudrv.h"
+#include "utility.h"
+#include "control.h"
+#include "core2.h"
+#include "pmi.h"
+
+#include "output.h"
+#include "linuxos.h"
+#include "sys_info.h"
+#include "eventmux.h"
+#include "pebs.h"
+
+MODULE_AUTHOR("Copyright(C) 2007-2018 Intel Corporation");
+MODULE_VERSION(SEP_NAME "_" SEP_VERSION_STR);
+MODULE_LICENSE("Dual BSD/GPL");
+
+static struct task_struct *abnormal_handler;
+#if defined(DRV_SEP_ACRN_ON)
+static struct task_struct *acrn_buffer_handler[MAX_NR_PCPUS] = { NULL };
+#endif
+
+typedef struct LWPMU_DEV_NODE_S LWPMU_DEV_NODE;
+typedef LWPMU_DEV_NODE * LWPMU_DEV;
+
+struct LWPMU_DEV_NODE_S {
+	long buffer;
+	struct semaphore sem;
+	struct cdev cdev;
+};
+
+#define LWPMU_DEV_buffer(dev) ((dev)->buffer)
+#define LWPMU_DEV_sem(dev) ((dev)->sem)
+#define LWPMU_DEV_cdev(dev) ((dev)->cdev)
+
+/* Global variables of the driver */
+static SEP_VERSION_NODE drv_version;
+U64 *read_counter_info;
+U64 *prev_counter_data;
+static U64 prev_counter_size;
+VOID **desc_data;
+U64 total_ram;
+U32 output_buffer_size = OUTPUT_LARGE_BUFFER;
+U32 saved_buffer_size;
+static S32 desc_count;
+uid_t uid;
+DRV_CONFIG drv_cfg;
+static DEV_CONFIG cur_pcfg;
+volatile pid_t control_pid;
+U64 *interrupt_counts;
+static LWPMU_DEV lwpmu_control;
+static LWPMU_DEV lwmod_control;
+static LWPMU_DEV lwsamp_control;
+static LWPMU_DEV lwsampunc_control;
+static LWPMU_DEV lwsideband_control;
+EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+
+/* needed for multiple devices (core/uncore) */
+U32 num_devices;
+static U32 num_core_devs;
+U32 cur_device;
+LWPMU_DEVICE devices;
+static U32 uncore_em_factor;
+static unsigned long unc_timer_interval;
+static struct timer_list *unc_read_timer;
+static S32 max_groups_unc;
+DRV_BOOL multi_pebs_enabled = FALSE;
+DRV_BOOL unc_buf_init = FALSE;
+DRV_BOOL NMI_mode = TRUE;
+DRV_BOOL KVM_guest_mode = FALSE;
+DRV_SETUP_INFO_NODE req_drv_setup_info;
+
+/* needed for target agent support */
+U32 osid = OS_ID_NATIVE;
+DRV_BOOL sched_switch_enabled = FALSE;
+
+#if defined(DRV_SEP_ACRN_ON)
+struct profiling_vm_info_list *vm_info_list;
+shared_buf_t **samp_buf_per_cpu;
+#endif
+
+#define UNCORE_EM_GROUP_SWAP_FACTOR 100
+#define PMU_DEVICES 2 // pmu, mod
+
+extern U32 *cpu_built_sysinfo;
+
+#define DRV_DEVICE_DELIMITER "!"
+
+#if defined(DRV_USE_UNLOCKED_IOCTL)
+static struct mutex ioctl_lock;
+#endif
+
+#if defined(BUILD_CHIPSET)
+CHIPSET_CONFIG pma;
+CS_DISPATCH cs_dispatch;
+#endif
+static S8 *cpu_mask_bits;
+
+/*
+ *  Global data: Buffer control structure
+ */
+BUFFER_DESC cpu_buf;
+BUFFER_DESC unc_buf;
+BUFFER_DESC module_buf;
+BUFFER_DESC cpu_sideband_buf;
+
+static dev_t lwpmu_DevNum; /* the major and minor parts for SEP3 base */
+static dev_t lwsamp_DevNum; /* the major and minor parts for SEP3 percpu */
+static dev_t lwsampunc_DevNum;
+/* the major and minor parts for SEP3 per package */
+static dev_t lwsideband_DevNum;
+
+static struct class *pmu_class;
+
+//extern volatile int config_done;
+
+CPU_STATE pcb;
+static size_t pcb_size;
+U32 *core_to_package_map;
+U32 *core_to_phys_core_map;
+U32 *core_to_thread_map;
+U32 *core_to_dev_map;
+U32 *threads_per_core;
+U32 num_packages;
+U64 *pmu_state;
+U64 *cpu_tsc;
+static U64 *prev_cpu_tsc;
+static U64 *diff_cpu_tsc;
+U64 *restore_bl_bypass;
+U32 **restore_ha_direct2core;
+U32 **restore_qpi_direct2core;
+U32 *occupied_core_ids;
+UNCORE_TOPOLOGY_INFO_NODE uncore_topology;
+PLATFORM_TOPOLOGY_PROG_NODE platform_topology_prog_node;
+static PLATFORM_TOPOLOGY_PROG_NODE req_platform_topology_prog_node;
+
+#if !defined(DRV_SEP_ACRN_ON)
+static U8 *prev_set_CR4;
+#endif
+
+wait_queue_head_t wait_exit;
+
+// extern OS_STATUS SOCPERF_Switch_Group3 (void);
+
+#if !defined(DRV_USE_UNLOCKED_IOCTL)
+#define MUTEX_INIT(lock)
+#define MUTEX_LOCK(lock)
+#define MUTEX_UNLOCK(lock)
+#else
+#define MUTEX_INIT(lock) mutex_init(&(lock));
+#define MUTEX_LOCK(lock) mutex_lock(&(lock))
+#define MUTEX_UNLOCK(lock) mutex_unlock(&(lock))
+#endif
+
+#if defined(CONFIG_XEN_HAVE_VPMU)
+typedef struct xen_pmu_params xen_pmu_params_t;
+typedef struct xen_pmu_data xen_pmu_data_t;
+
+static DEFINE_PER_CPU(xen_pmu_data_t *, sep_xenpmu_shared);
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  void lwpmudrv_PWR_Info(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief Make a copy of the Power control information that has been passed in.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_PWR_Info(IOCTL_ARGS arg)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+	if (DEV_CONFIG_power_capture(cur_pcfg) == FALSE) {
+		SEP_DRV_LOG_WARNING_FLOW_OUT(
+			"'Success' (Power capture is disabled!).");
+		return OS_SUCCESS;
+	}
+
+	// make sure size of incoming arg is correct
+	if ((arg->len_usr_to_drv != sizeof(PWR_NODE)) ||
+	    (arg->buf_usr_to_drv == NULL)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"OS_FAULT (PWR capture has not been configured!).");
+		return OS_FAULT;
+	}
+
+	//
+	// First things first: Make a copy of the data for global use.
+	//
+	LWPMU_DEVICE_pwr(&devices[cur_device]) =
+		CONTROL_Allocate_Memory((int)arg->len_usr_to_drv);
+	if (!LWPMU_DEVICE_pwr(&devices[cur_device])) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(LWPMU_DEVICE_pwr(&devices[cur_device]),
+			   (void __user *)arg->buf_usr_to_drv, arg->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/*
+ * @fn void lwpmudrv_Allocate_Restore_Buffer
+ *
+ * @param
+ *
+ * @return   OS_STATUS
+ *
+ * @brief    allocate buffer space to save/restore the data (for JKT, QPILL and HA register) before collection
+ */
+static OS_STATUS lwpmudrv_Allocate_Restore_Buffer(void)
+{
+	int i = 0;
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!restore_ha_direct2core) {
+		restore_ha_direct2core = CONTROL_Allocate_Memory(
+			GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32 *));
+		if (!restore_ha_direct2core) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Memory allocation failure for restore_ha_direct2core!");
+			return OS_NO_MEM;
+		}
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_ha_direct2core[i] = CONTROL_Allocate_Memory(
+				MAX_BUSNO * sizeof(U32));
+		}
+	}
+	if (!restore_qpi_direct2core) {
+		restore_qpi_direct2core = CONTROL_Allocate_Memory(
+			GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32 *));
+		if (!restore_qpi_direct2core) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Memory allocation failure for restore_qpi_direct2core!");
+			return OS_NO_MEM;
+		}
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_qpi_direct2core[i] = CONTROL_Allocate_Memory(
+				2 * MAX_BUSNO * sizeof(U32));
+		}
+	}
+	if (!restore_bl_bypass) {
+		restore_bl_bypass = CONTROL_Allocate_Memory(
+			GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64));
+		if (!restore_bl_bypass) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Memory allocation failure for restore_bl_bypass!");
+			return OS_NO_MEM;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/*
+ * @fn void lwpmudrv_Allocate_Uncore_Buffer
+ *
+ * @param
+ *
+ * @return   OS_STATUS
+ *
+ * @brief    allocate buffer space for writing/reading uncore data
+ */
+static OS_STATUS lwpmudrv_Allocate_Uncore_Buffer(void)
+{
+	U32 i, j, k, l;
+	U32 max_entries = 0;
+	U32 num_entries;
+	ECB ecb;
+
+	SEP_DRV_LOG_TRACE_IN(
+		""); // this function is not checking memory allocations properly
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		if (!LWPMU_DEVICE_pcfg(&devices[i])) {
+			continue;
+		}
+		LWPMU_DEVICE_acc_value(&devices[i]) =
+			CONTROL_Allocate_Memory(num_packages * sizeof(U64 **));
+		LWPMU_DEVICE_prev_value(&devices[i]) =
+			CONTROL_Allocate_Memory(num_packages * sizeof(U64 *));
+		for (j = 0; j < num_packages; j++) {
+			// Allocate memory and zero out accumulator array (one per group)
+			LWPMU_DEVICE_acc_value(&devices[i])[j] =
+				CONTROL_Allocate_Memory(
+					LWPMU_DEVICE_em_groups_count(
+						&devices[i]) *
+					sizeof(U64 *));
+			for (k = 0;
+			     k < LWPMU_DEVICE_em_groups_count(&devices[i]);
+			     k++) {
+				ecb = LWPMU_DEVICE_PMU_register_data(
+					&devices[i])[k];
+				num_entries =
+					ECB_num_events(ecb) *
+					LWPMU_DEVICE_num_units(&devices[i]);
+				LWPMU_DEVICE_acc_value(&devices[i])[j][k] =
+					CONTROL_Allocate_Memory(num_entries *
+								sizeof(U64));
+				for (l = 0; l < num_entries; l++) {
+					LWPMU_DEVICE_acc_value(
+						&devices[i])[j][k][l] = 0LL;
+				}
+				if (max_entries < num_entries) {
+					max_entries = num_entries;
+				}
+			}
+			// Allocate memory and zero out prev_value array (one across groups)
+			LWPMU_DEVICE_prev_value(&devices[i])[j] =
+				CONTROL_Allocate_Memory(max_entries *
+							sizeof(U64));
+			for (k = 0; k < max_entries; k++) {
+				LWPMU_DEVICE_prev_value(&devices[i])[j][k] =
+					0LL;
+			}
+		}
+		max_entries = 0;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/*
+ * @fn void lwpmudrv_Free_Uncore_Buffer
+ *
+ * @param
+ *
+ * @return   OS_STATUS
+ *
+ * @brief    Free uncore data buffers
+ */
+static OS_STATUS lwpmudrv_Free_Uncore_Buffer(U32 i)
+{
+	U32 j, k;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (LWPMU_DEVICE_prev_value(&devices[i])) {
+		for (j = 0; j < num_packages; j++) {
+			LWPMU_DEVICE_prev_value(&devices[i])[j] =
+				CONTROL_Free_Memory(LWPMU_DEVICE_prev_value(
+					&devices[i])[j]);
+		}
+		LWPMU_DEVICE_prev_value(&devices[i]) = CONTROL_Free_Memory(
+			LWPMU_DEVICE_prev_value(&devices[i]));
+	}
+	if (LWPMU_DEVICE_acc_value(&devices[i])) {
+		for (j = 0; j < num_packages; j++) {
+			if (LWPMU_DEVICE_acc_value(&devices[i])[j]) {
+				for (k = 0; k < LWPMU_DEVICE_em_groups_count(
+							&devices[i]);
+				     k++) {
+					LWPMU_DEVICE_acc_value(
+						&devices[i])[j][k] =
+						CONTROL_Free_Memory(
+							LWPMU_DEVICE_acc_value(
+								&devices[i])[j]
+									    [k]);
+				}
+				LWPMU_DEVICE_acc_value(&devices[i])[j] =
+					CONTROL_Free_Memory(
+						LWPMU_DEVICE_acc_value(
+							&devices[i])[j]);
+			}
+		}
+		LWPMU_DEVICE_acc_value(&devices[i]) = CONTROL_Free_Memory(
+			LWPMU_DEVICE_acc_value(&devices[i]));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/*
+ * @fn void lwpmudrv_Free_Restore_Buffer
+ *
+ * @param
+ *
+ * @return   OS_STATUS
+ *
+ * @brief    allocate buffer space to save/restore the data (for JKT, QPILL and HA register) before collection
+ */
+static OS_STATUS lwpmudrv_Free_Restore_Buffer(void)
+{
+	U32 i = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (restore_ha_direct2core) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_ha_direct2core[i] =
+				CONTROL_Free_Memory(restore_ha_direct2core[i]);
+		}
+		restore_ha_direct2core =
+			CONTROL_Free_Memory(restore_ha_direct2core);
+	}
+	if (restore_qpi_direct2core) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_qpi_direct2core[i] =
+				CONTROL_Free_Memory(restore_qpi_direct2core[i]);
+		}
+		restore_qpi_direct2core =
+			CONTROL_Free_Memory(restore_qpi_direct2core);
+	}
+	if (restore_bl_bypass) {
+		restore_bl_bypass = CONTROL_Free_Memory(restore_bl_bypass);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Initialize_State(void)
+ *
+ * @param none
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Allocates the memory needed at load time.  Initializes all the
+ * @brief  necessary state variables with the default values.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Initialize_State(void)
+{
+	S32 i, max_cpu_id = 0;
+
+	SEP_DRV_LOG_INIT_IN("");
+
+	for_each_possible_cpu (i) {
+		if (cpu_present(i)) {
+			if (i > max_cpu_id) {
+				max_cpu_id = i;
+			}
+		}
+	}
+	max_cpu_id++;
+
+	/*
+	 *  Machine Initializations
+	 *  Abstract this information away into a separate entry point
+	 *
+	 *  Question:  Should we allow for the use of Hot-cpu
+	 *    add/subtract functionality while the driver is executing?
+	 */
+	if (max_cpu_id > num_present_cpus()) {
+		GLOBAL_STATE_num_cpus(driver_state) = max_cpu_id;
+	} else {
+		GLOBAL_STATE_num_cpus(driver_state) = num_present_cpus();
+	}
+	GLOBAL_STATE_active_cpus(driver_state) = num_online_cpus();
+	GLOBAL_STATE_cpu_count(driver_state) = 0;
+	GLOBAL_STATE_dpc_count(driver_state) = 0;
+	GLOBAL_STATE_num_em_groups(driver_state) = 0;
+	CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_UNINITIALIZED);
+
+	SEP_DRV_LOG_INIT_OUT("Success: num_cpus=%d, active_cpus=%d.",
+			     GLOBAL_STATE_num_cpus(driver_state),
+			     GLOBAL_STATE_active_cpus(driver_state));
+	return OS_SUCCESS;
+}
+
+#if !defined(CONFIG_PREEMPT_COUNT) && !defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static void lwpmudrv_Fill_TSC_Info (PVOID param)
+ *
+ * @param param - pointer the buffer to fill in.
+ *
+ * @return none
+ *
+ * @brief  Read the TSC and write into the correct array slot.
+ *
+ * <I>Special Notes</I>
+ */
+atomic_t read_now;
+static wait_queue_head_t read_tsc_now;
+static VOID lwpmudrv_Fill_TSC_Info(PVOID param)
+{
+	U32 this_cpu;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	//
+	// Wait until all CPU's are ready to proceed
+	// This will serve as a synchronization point to compute tsc skews.
+	//
+
+	if (atomic_read(&read_now) >= 1) {
+		if (atomic_dec_and_test(&read_now) == FALSE) {
+			wait_event_interruptible(read_tsc_now,
+						 (atomic_read(&read_now) >= 1));
+		}
+	} else {
+		wake_up_interruptible_all(&read_tsc_now);
+	}
+	UTILITY_Read_TSC(&cpu_tsc[this_cpu]);
+	SEP_DRV_LOG_TRACE("This cpu %d --- tsc --- 0x%llx.", this_cpu,
+			  cpu_tsc[this_cpu]);
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+}
+#endif
+
+/*********************************************************************
+ *  Internal Driver functions
+ *     Should be called only from the lwpmudrv_DeviceControl routine
+ *********************************************************************/
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Dump_Tracer(const char *)
+ *
+ * @param Name of the tracer
+ *
+ * @return void
+ *
+ * @brief  Function that handles the generation of markers into the ftrace stream
+ *
+ * <I>Special Notes</I>
+ */
+static void lwpmudrv_Dump_Tracer(const char *name, U64 tsc)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+	if (tsc == 0) {
+		preempt_disable();
+		UTILITY_Read_TSC(&tsc);
+		tsc -= TSC_SKEW(CONTROL_THIS_CPU());
+		preempt_enable();
+	}
+	SEP_DRV_LOG_TRACE_OUT("Success");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Version(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_VERSION call.
+ * @brief  Returns the version number of the kernel mode sampling.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Version(IOCTL_ARGS arg)
+{
+	OS_STATUS status;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	// Check if enough space is provided for collecting the data
+	if ((arg->len_drv_to_usr != sizeof(U32)) ||
+	    (arg->buf_drv_to_usr == NULL)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	status = put_user(SEP_VERSION_NODE_sep_version(&drv_version),
+			  (U32 __user *)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Reserve(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief
+ * @brief  Local function that handles the LWPMU_IOCTL_RESERVE call.
+ * @brief  Sets the state to RESERVED if possible.  Returns BUSY if unable
+ * @brief  to reserve the PMU.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Reserve(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	// Check if enough space is provided for collecting the data
+	if ((arg->len_drv_to_usr != sizeof(S32)) ||
+	    (arg->buf_drv_to_usr == NULL)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	status = put_user(!CHANGE_DRIVER_STATE(STATE_BIT_UNINITIALIZED,
+					       DRV_STATE_RESERVED),
+			  (int __user*)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+#if !defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Finish_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Finalize PMU after collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Finish_Op(PVOID param)
+{
+	U32 this_cpu = CONTROL_THIS_CPU();
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	DISPATCH dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->fini != NULL) {
+		dispatch->fini(&dev_idx);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static VOID lwpmudrv_Clean_Up(DRV_BOOL)
+ *
+ * @param  DRV_BOOL finish - Flag to call finish
+ *
+ * @return VOID
+ *
+ * @brief  Cleans up the memory allocation.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Clean_Up(DRV_BOOL finish)
+{
+	U32 i;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		drv_cfg = CONTROL_Free_Memory(drv_cfg);
+		goto signal_end;
+	}
+
+	if (devices) {
+		U32 id;
+		U32 num_groups = 0;
+		EVENT_CONFIG ec;
+		DISPATCH dispatch_unc = NULL;
+
+		for (id = 0; id < num_devices; id++) {
+			if (LWPMU_DEVICE_pcfg(&devices[id])) {
+				if (LWPMU_DEVICE_device_type(&devices[id]) ==
+				    DEVICE_INFO_UNCORE) {
+					dispatch_unc = LWPMU_DEVICE_dispatch(
+						&devices[id]);
+					if (dispatch_unc &&
+					    dispatch_unc->fini) {
+						SEP_DRV_LOG_TRACE(
+							"LWP: calling UNC Init.");
+						dispatch_unc->fini(
+							(PVOID *)&id);
+					}
+					lwpmudrv_Free_Uncore_Buffer(id);
+				} else if (finish) {
+#if !defined(DRV_SEP_ACRN_ON)
+					CONTROL_Invoke_Parallel(
+						lwpmudrv_Finish_Op, NULL);
+#endif
+				}
+			}
+
+			if (LWPMU_DEVICE_PMU_register_data(&devices[id])) {
+				ec = LWPMU_DEVICE_ec(&devices[id]);
+				if (LWPMU_DEVICE_device_type(&devices[id]) ==
+				    DEVICE_INFO_CORE) {
+					num_groups =
+						EVENT_CONFIG_num_groups(ec);
+				} else {
+					num_groups =
+						EVENT_CONFIG_num_groups_unc(ec);
+				}
+				for (i = 0; i < num_groups; i++) {
+					LWPMU_DEVICE_PMU_register_data(
+						&devices[id])[i] =
+						CONTROL_Free_Memory(
+							LWPMU_DEVICE_PMU_register_data(
+								&devices[id])[i]);
+				}
+				LWPMU_DEVICE_PMU_register_data(&devices[id]) =
+					CONTROL_Free_Memory(
+						LWPMU_DEVICE_PMU_register_data(
+							&devices[id]));
+			}
+			LWPMU_DEVICE_pcfg(&devices[id]) = CONTROL_Free_Memory(
+				LWPMU_DEVICE_pcfg(&devices[id]));
+			LWPMU_DEVICE_ec(&devices[id]) = CONTROL_Free_Memory(
+				LWPMU_DEVICE_ec(&devices[id]));
+			if (LWPMU_DEVICE_lbr(&devices[id])) {
+				LWPMU_DEVICE_lbr(&devices[id]) =
+					CONTROL_Free_Memory(
+						LWPMU_DEVICE_lbr(&devices[id]));
+			}
+			if (LWPMU_DEVICE_pwr(&devices[id])) {
+				LWPMU_DEVICE_pwr(&devices[id]) =
+					CONTROL_Free_Memory(
+						LWPMU_DEVICE_pwr(&devices[id]));
+			}
+			if (LWPMU_DEVICE_cur_group(&devices[id])) {
+				LWPMU_DEVICE_cur_group(&devices[id]) =
+					CONTROL_Free_Memory(
+						LWPMU_DEVICE_cur_group(
+							&devices[id]));
+			}
+		}
+		devices = CONTROL_Free_Memory(devices);
+	}
+
+	if (desc_data) {
+		for (i = 0; i < GLOBAL_STATE_num_descriptors(driver_state);
+		     i++) {
+			desc_data[i] = CONTROL_Free_Memory(desc_data[i]);
+		}
+		desc_data = CONTROL_Free_Memory(desc_data);
+	}
+
+	if (restore_bl_bypass) {
+		restore_bl_bypass = CONTROL_Free_Memory(restore_bl_bypass);
+	}
+
+	if (restore_qpi_direct2core) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_qpi_direct2core[i] =
+				CONTROL_Free_Memory(restore_qpi_direct2core[i]);
+		}
+		restore_qpi_direct2core =
+			CONTROL_Free_Memory(restore_qpi_direct2core);
+	}
+
+	if (restore_ha_direct2core) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			restore_ha_direct2core[i] =
+				CONTROL_Free_Memory(restore_ha_direct2core[i]);
+		}
+		restore_ha_direct2core =
+			CONTROL_Free_Memory(restore_ha_direct2core);
+	}
+
+	drv_cfg = CONTROL_Free_Memory(drv_cfg);
+	pmu_state = CONTROL_Free_Memory(pmu_state);
+	cpu_mask_bits = CONTROL_Free_Memory(cpu_mask_bits);
+	core_to_dev_map = CONTROL_Free_Memory(core_to_dev_map);
+#if defined(BUILD_CHIPSET)
+	pma = CONTROL_Free_Memory(pma);
+#endif
+
+signal_end:
+	GLOBAL_STATE_num_em_groups(driver_state) = 0;
+	GLOBAL_STATE_num_descriptors(driver_state) = 0;
+	num_devices = 0;
+	num_core_devs = 0;
+	max_groups_unc = 0;
+	control_pid = 0;
+	unc_buf_init = FALSE;
+
+	OUTPUT_Cleanup();
+	memset(pcb, 0, pcb_size);
+
+	SEP_DRV_LOG_FLOW_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static NTSTATUS lwpmudrv_Initialize_Driver (PVOID buf_usr_to_drv, size_t len_usr_to_drv)
+ *
+ * @param  buf_usr_to_drv   - pointer to the input buffer
+ * @param  len_usr_to_drv   - size of the input buffer
+ *
+ * @return NTSTATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_INIT_DRIVER call.
+ * @brief  Sets up the interrupt handler.
+ * @brief  Set up the output buffers/files needed to make the driver
+ * @brief  operational.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Initialize_Driver(PVOID buf_usr_to_drv,
+					size_t len_usr_to_drv)
+{
+	S32 cpu_num;
+	int status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	if (!CHANGE_DRIVER_STATE(STATE_BIT_RESERVED, DRV_STATE_IDLE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Unexpected driver state!");
+		return OS_FAULT;
+	}
+
+	interrupt_counts = NULL;
+	pmu_state = NULL;
+
+	drv_cfg = CONTROL_Allocate_Memory(len_usr_to_drv);
+	if (!drv_cfg) {
+		status = OS_NO_MEM;
+		SEP_DRV_LOG_ERROR("Memory allocation failure for drv_cfg!");
+		goto clean_return;
+	}
+
+	if (copy_from_user(drv_cfg, (void __user *)buf_usr_to_drv,
+			len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR("Memory copy failure for drv_cfg!");
+		status = OS_FAULT;
+		goto clean_return;
+	}
+
+	if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+#if (defined(DRV_EM64T))
+		if (output_buffer_size == OUTPUT_LARGE_BUFFER) {
+			output_buffer_size = OUTPUT_CP_BUFFER;
+		}
+#endif
+		interrupt_counts = CONTROL_Allocate_Memory(
+			GLOBAL_STATE_num_cpus(driver_state) *
+			DRV_CONFIG_num_events(drv_cfg) * sizeof(U64));
+		if (interrupt_counts == NULL) {
+			SEP_DRV_LOG_ERROR(
+				"Memory allocation failure for interrupt_counts!");
+			status = OS_NO_MEM;
+			goto clean_return;
+		}
+	} else if (output_buffer_size == OUTPUT_CP_BUFFER) {
+		output_buffer_size = OUTPUT_LARGE_BUFFER;
+	}
+
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		SEP_DRV_LOG_FLOW_OUT("Success, using PCL.");
+		return OS_SUCCESS;
+	}
+
+	pmu_state = CONTROL_Allocate_KMemory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64) * 3);
+	if (!pmu_state) {
+		SEP_DRV_LOG_ERROR("Memory allocation failure for pmu_state!");
+		status = OS_NO_MEM;
+		goto clean_return;
+	}
+	uncore_em_factor = 0;
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		CPU_STATE_accept_interrupt(&pcb[cpu_num]) = 1;
+		CPU_STATE_initial_mask(&pcb[cpu_num]) = 1;
+		CPU_STATE_group_swap(&pcb[cpu_num]) = 1;
+		CPU_STATE_reset_mask(&pcb[cpu_num]) = 0;
+		CPU_STATE_num_samples(&pcb[cpu_num]) = 0;
+		CPU_STATE_last_p_state_valid(&pcb[cpu_num]) = FALSE;
+#if defined(DRV_CPU_HOTPLUG)
+		CPU_STATE_offlined(&pcb[cpu_num]) = TRUE;
+#else
+		CPU_STATE_offlined(&pcb[cpu_num]) = FALSE;
+#endif
+		CPU_STATE_nmi_handled(&pcb[cpu_num]) = 0;
+	}
+
+	DRV_CONFIG_seed_name(drv_cfg) = NULL;
+	DRV_CONFIG_seed_name_len(drv_cfg) = 0;
+
+	SEP_DRV_LOG_TRACE("Config : size = %d.", DRV_CONFIG_size(drv_cfg));
+	SEP_DRV_LOG_TRACE("Config : counting_mode = %d.",
+			  DRV_CONFIG_counting_mode(drv_cfg));
+
+	control_pid = current->pid;
+	SEP_DRV_LOG_TRACE("Control PID = %d.", control_pid);
+
+	if (core_to_dev_map == NULL) {
+		core_to_dev_map = CONTROL_Allocate_Memory(
+			GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	}
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		if (cpu_buf == NULL) {
+			cpu_buf = CONTROL_Allocate_Memory(
+				GLOBAL_STATE_num_cpus(driver_state) *
+				sizeof(BUFFER_DESC_NODE));
+			if (!cpu_buf) {
+				SEP_DRV_LOG_ERROR(
+					"Memory allocation failure for cpu_buf!");
+				status = OS_NO_MEM;
+				goto clean_return;
+			}
+		}
+
+		if (module_buf == NULL) {
+			module_buf = CONTROL_Allocate_Memory(
+				sizeof(BUFFER_DESC_NODE));
+			if (!module_buf) {
+				status = OS_NO_MEM;
+				goto clean_return;
+			}
+		}
+
+#if defined(CONFIG_TRACEPOINTS)
+		multi_pebs_enabled = (DRV_CONFIG_multi_pebs_enabled(drv_cfg) &&
+				      (DRV_SETUP_INFO_page_table_isolation(
+					       &req_drv_setup_info) ==
+				       DRV_SETUP_INFO_PTI_DISABLED));
+#endif
+		if (multi_pebs_enabled || sched_switch_enabled) {
+			if (cpu_sideband_buf == NULL) {
+				cpu_sideband_buf = CONTROL_Allocate_Memory(
+					GLOBAL_STATE_num_cpus(driver_state) *
+					sizeof(BUFFER_DESC_NODE));
+				if (!cpu_sideband_buf) {
+					SEP_DRV_LOG_ERROR(
+						"Memory allocation failure for cpu_sideband_buf!");
+					status = OS_NO_MEM;
+					goto clean_return;
+				}
+			}
+		}
+
+#if defined(DRV_SEP_ACRN_ON)
+		if (samp_buf_per_cpu == NULL) {
+			samp_buf_per_cpu =
+				(shared_buf_t **)CONTROL_Allocate_Memory(
+					GLOBAL_STATE_num_cpus(driver_state) *
+					sizeof(shared_buf_t *));
+			if (!samp_buf_per_cpu) {
+				SEP_PRINT_ERROR(
+					"lwpmudrv_Initialize: unable to allocate memory for samp_buf_per_cpu\n");
+				goto clean_return;
+			}
+		}
+
+		for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+		     cpu_num++) {
+			samp_buf_per_cpu[cpu_num] = sbuf_allocate(
+				TRACE_ELEMENT_NUM, TRACE_ELEMENT_SIZE);
+			if (!samp_buf_per_cpu[cpu_num]) {
+				pr_err("Failed to allocate sampbuf on cpu%d\n",
+				       cpu_num);
+				goto clean_return;
+			}
+
+			status = sbuf_share_setup(cpu_num, ACRN_SEP,
+						  samp_buf_per_cpu[cpu_num]);
+			if (status < 0) {
+				status = OS_FAULT;
+				pr_err("Failed to set up sampbuf on cpu%d\n",
+				       cpu_num);
+				goto clean_return;
+			}
+		}
+#endif
+
+		/*
+	 * Allocate the output and control buffers for each CPU in the system
+	 * Allocate and set up the temp output files for each CPU in the system
+	 * Allocate and set up the temp outout file for detailing the Modules in the system
+	 */
+		status = OUTPUT_Initialize();
+		if (status != OS_SUCCESS) {
+			SEP_DRV_LOG_ERROR("OUTPUT_Initialize failed!");
+			goto clean_return;
+		}
+
+		/*
+	 * Program the APIC and set up the interrupt handler
+	 */
+#if !defined(DRV_SEP_ACRN_ON)
+		CPUMON_Install_Cpuhooks();
+#endif
+		SEP_DRV_LOG_TRACE("Finished Installing cpu hooks.");
+#if defined(DRV_CPU_HOTPLUG)
+		for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+		     cpu_num++) {
+			if (cpu_built_sysinfo &&
+			    cpu_built_sysinfo[cpu_num] == 0) {
+				cpu_tsc[cpu_num] = cpu_tsc[0];
+				CONTROL_Invoke_Cpu(cpu_num, SYS_INFO_Build_Cpu,
+						   NULL);
+			}
+		}
+#endif
+
+#if defined(DRV_EM64T)
+		SYS_Get_GDT_Base((PVOID *)&gdt_desc);
+#endif
+		SEP_DRV_LOG_TRACE("About to install module notification.");
+		LINUXOS_Install_Hooks();
+	}
+
+clean_return:
+	if (status != OS_SUCCESS) {
+		drv_cfg = CONTROL_Free_Memory(drv_cfg);
+		interrupt_counts = CONTROL_Free_Memory(interrupt_counts);
+		pmu_state = CONTROL_Free_Memory(pmu_state);
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Initialize (PVOID buf_usr_to_drv, size_t len_usr_to_drv)
+ *
+ * @param  buf_usr_to_drv  - pointer to the input buffer
+ * @param  len_usr_to_drv  - size of the input buffer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_INIT call.
+ * @brief  Sets up the interrupt handler.
+ * @brief  Set up the output buffers/files needed to make the driver
+ * @brief  operational.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Initialize(PVOID buf_usr_to_drv,
+				     size_t len_usr_to_drv)
+{
+	int status = OS_SUCCESS;
+	S32 cpu_num;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	if (cur_device >= num_devices) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"No more devices to allocate! Wrong lwpmudrv_Init_Num_Devices.");
+		return OS_FAULT;
+	}
+
+	/*
+	 *   Program State Initializations
+	 */
+	LWPMU_DEVICE_pcfg(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(len_usr_to_drv);
+	if (!LWPMU_DEVICE_pcfg(&devices[cur_device])) {
+		status = OS_NO_MEM;
+		SEP_DRV_LOG_ERROR("Memory allocation failure for pcfg!");
+		goto clean_return;
+	}
+
+	if (copy_from_user(LWPMU_DEVICE_pcfg(&devices[cur_device]),
+			   (void __user *)buf_usr_to_drv, len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR("Memory copy failure for pcfg!");
+		status = OS_FAULT;
+		goto clean_return;
+	}
+	cur_pcfg = (DEV_CONFIG)LWPMU_DEVICE_pcfg(&devices[cur_device]);
+
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		SEP_DRV_LOG_FLOW_OUT("Success, using PCL.");
+		return OS_SUCCESS;
+	}
+
+	LWPMU_DEVICE_dispatch(&devices[cur_device]) =
+		UTILITY_Configure_CPU(DEV_CONFIG_dispatch_id(cur_pcfg));
+	if (LWPMU_DEVICE_dispatch(&devices[cur_device]) == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Dispatch pointer is NULL!");
+		status = OS_INVALID;
+		goto clean_return;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		status = PEBS_Initialize(cur_device);
+		if (status != OS_SUCCESS) {
+			SEP_DRV_LOG_ERROR("PEBS_Initialize failed!");
+			goto clean_return;
+		}
+	}
+#endif
+
+	/* Create core to device ID map */
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		if (CPU_STATE_core_type(&pcb[cpu_num]) ==
+		    DEV_CONFIG_core_type(cur_pcfg)) {
+			core_to_dev_map[cpu_num] = cur_device;
+		}
+	}
+	num_core_devs++; //New core device
+	LWPMU_DEVICE_device_type(&devices[cur_device]) = DEVICE_INFO_CORE;
+
+clean_return:
+	if (status != OS_SUCCESS) {
+		// release all memory allocated in this function:
+		lwpmudrv_Clean_Up(FALSE);
+#if !defined(DRV_SEP_ACRN_ON)
+		PEBS_Destroy();
+#endif
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Initialize_Num_Devices(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief
+ * @brief  Local function that handles the LWPMU_IOCTL_INIT_NUM_DEV call.
+ * @brief  Init # uncore devices.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Initialize_Num_Devices(IOCTL_ARGS arg)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	// Check if enough space is provided for collecting the data
+	if ((arg->len_usr_to_drv != sizeof(U32)) ||
+	    (arg->buf_usr_to_drv == NULL)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	if (copy_from_user(&num_devices, (void __user *)arg->buf_usr_to_drv,
+			   arg->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure");
+		return OS_FAULT;
+	}
+	/*
+	 *   Allocate memory for number of devices
+	 */
+	if (num_devices != 0) {
+		devices = CONTROL_Allocate_Memory(num_devices *
+						  sizeof(LWPMU_DEVICE_NODE));
+		if (!devices) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Unable to allocate memory for devices!");
+			return OS_NO_MEM;
+		}
+	}
+	cur_device = 0;
+
+	SEP_DRV_LOG_FLOW_OUT("Success: num_devices=%d, devices=0x%p.",
+			     num_devices, devices);
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Initialize_UNC(PVOID buf_usr_to_drv, U32 len_usr_to_drv)
+ *
+ * @param  buf_usr_to_drv   - pointer to the input buffer
+ * @param  len_usr_to_drv   - size of the input buffer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_INIT call.
+ * @brief  Sets up the interrupt handler.
+ * @brief  Set up the output buffers/files needed to make the driver
+ * @brief  operational.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Initialize_UNC(PVOID buf_usr_to_drv,
+					 U32 len_usr_to_drv)
+{
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 i;
+	int status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: current state is not IDLE.");
+		return OS_IN_PROGRESS;
+	}
+
+	if (!devices) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("No devices allocated!");
+		return OS_INVALID;
+	}
+
+	/*
+	 *   Program State Initializations:
+	 *   Foreach device, copy over pcfg and configure dispatch table
+	 */
+	if (cur_device >= num_devices) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"No more devices to allocate! Wrong lwpmudrv_Init_Num_Devices.");
+		return OS_FAULT;
+	}
+	if (buf_usr_to_drv == NULL) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+	if (len_usr_to_drv != sizeof(DEV_UNC_CONFIG_NODE)) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Got len_usr_to_drv=%d, expecting size=%d",
+			len_usr_to_drv, (int)sizeof(DEV_UNC_CONFIG_NODE));
+		return OS_FAULT;
+	}
+	// allocate memory
+	LWPMU_DEVICE_pcfg(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(sizeof(DEV_UNC_CONFIG_NODE));
+	// copy over pcfg
+	if (copy_from_user(LWPMU_DEVICE_pcfg(&devices[cur_device]),
+			   (void __user *)buf_usr_to_drv, len_usr_to_drv)) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Failed to copy from user!");
+		return OS_FAULT;
+	}
+	// configure dispatch from dispatch_id
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[cur_device]);
+	if (!pcfg_unc) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid pcfg_unc.");
+		return OS_INVALID;
+	}
+
+	LWPMU_DEVICE_dispatch(&devices[cur_device]) =
+		UTILITY_Configure_CPU(DEV_UNC_CONFIG_dispatch_id(pcfg_unc));
+	if (LWPMU_DEVICE_dispatch(&devices[cur_device]) == NULL) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Unable to configure CPU!");
+		return OS_FAULT;
+	}
+
+	LWPMU_DEVICE_cur_group(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(num_packages * sizeof(S32));
+	if (LWPMU_DEVICE_cur_group(&devices[cur_device]) == NULL) {
+		CHANGE_DRIVER_STATE(STATE_BIT_ANY, DRV_STATE_TERMINATING);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Cur_grp allocation failed for device %u!", cur_device);
+		return OS_NO_MEM;
+	}
+	for (i = 0; i < num_packages; i++) {
+		LWPMU_DEVICE_cur_group(&devices[cur_device])[i] = 0;
+	}
+
+	LWPMU_DEVICE_em_groups_count(&devices[cur_device]) = 0;
+	LWPMU_DEVICE_num_units(&devices[cur_device]) = 0;
+	LWPMU_DEVICE_device_type(&devices[cur_device]) = DEVICE_INFO_UNCORE;
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		if (unc_buf == NULL) {
+			unc_buf = CONTROL_Allocate_Memory(
+				num_packages * sizeof(BUFFER_DESC_NODE));
+			if (!unc_buf) {
+				CHANGE_DRIVER_STATE(STATE_BIT_ANY,
+						    DRV_STATE_TERMINATING);
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"Memory allocation failure.");
+				return OS_NO_MEM;
+			}
+		}
+
+		if (!unc_buf_init) {
+			status = OUTPUT_Initialize_UNC();
+			if (status != OS_SUCCESS) {
+				CHANGE_DRIVER_STATE(STATE_BIT_ANY,
+						    DRV_STATE_TERMINATING);
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"OUTPUT_Initialize failed!");
+				return status;
+			}
+			unc_buf_init = TRUE;
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("unc dispatch id = %d.",
+			     DEV_UNC_CONFIG_dispatch_id(pcfg_unc));
+
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Terminate(void)
+ *
+ * @param  none
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the DRV_OPERATION_TERMINATE call.
+ * @brief  Cleans up the interrupt handler and resets the PMU state.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Terminate(void)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() == DRV_STATE_UNINITIALIZED) {
+		SEP_DRV_LOG_FLOW_OUT("Success (already uninitialized).");
+		return OS_SUCCESS;
+	}
+
+	if (!CHANGE_DRIVER_STATE(STATE_BIT_STOPPED | STATE_BIT_TERMINATING,
+				 DRV_STATE_UNINITIALIZED)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Unexpected state!");
+		return OS_FAULT;
+	}
+
+	if (drv_cfg && DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		LINUXOS_Uninstall_Hooks();
+	}
+
+	lwpmudrv_Clean_Up(TRUE);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Switch_To_Next_Group(param)
+ *
+ * @param  none
+ *
+ * @return none
+ *
+ * @brief  Switch to the next event group for both core and uncore.
+ * @brief  This function assumes an active collection is frozen
+ * @brief  or no collection is active.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Switch_To_Next_Group(void)
+{
+	S32 cpuid;
+	U32 i, j;
+	CPU_STATE pcpu;
+	EVENT_CONFIG ec;
+	DEV_UNC_CONFIG pcfg_unc;
+	DISPATCH dispatch_unc;
+	ECB pecb_unc = NULL;
+	U32 cur_grp = 0;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	for (cpuid = 0; cpuid < GLOBAL_STATE_num_cpus(driver_state); cpuid++) {
+		pcpu = &pcb[cpuid];
+		ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(
+			&devices[core_to_dev_map[cpuid]]);
+		CPU_STATE_current_group(pcpu)++;
+		// make the event group list circular
+		CPU_STATE_current_group(pcpu) %= EVENT_CONFIG_num_groups(ec);
+	}
+
+	if (num_devices) {
+		for (i = num_core_devs; i < num_devices; i++) {
+			pcfg_unc = LWPMU_DEVICE_pcfg(&devices[i]);
+			dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+			if (LWPMU_DEVICE_em_groups_count(&devices[i]) > 1) {
+				if (pcb && pcfg_unc && dispatch_unc &&
+				    DRV_CONFIG_emon_mode(drv_cfg)) {
+					for (j = 0; j < num_packages; j++) {
+						cur_grp =
+							LWPMU_DEVICE_cur_group(
+								&devices[i])[j];
+						pecb_unc =
+							LWPMU_DEVICE_PMU_register_data(
+								&devices[i])
+								[cur_grp];
+						LWPMU_DEVICE_cur_group(
+							&devices[i])[j]++;
+						if (CPU_STATE_current_group(
+							    &pcb[0]) == 0) {
+							LWPMU_DEVICE_cur_group(
+								&devices[i])[j] =
+								0;
+						}
+						LWPMU_DEVICE_cur_group(
+							&devices[i])[j] %=
+							LWPMU_DEVICE_em_groups_count(
+								&devices[i]);
+					}
+					SEP_DRV_LOG_TRACE(
+						"Swap Group to %d for device %d.",
+						cur_grp, i);
+					if (pecb_unc &&
+					    ECB_device_type(pecb_unc) ==
+						    DEVICE_UNC_SOCPERF) {
+						// SOCPERF_Switch_Group3();
+					}
+				}
+			}
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwmpudrv_Get_Driver_State(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_GET_Driver_State call.
+ * @brief  Returns the current driver state.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Driver_State(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// Check if enough space is provided for collecting the data
+	if ((arg->len_drv_to_usr != sizeof(U32)) ||
+	    (arg->buf_drv_to_usr == NULL)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Invalid arguments!");
+		return OS_FAULT;
+	}
+
+	status = put_user(GET_DRIVER_STATE(), (U32 __user*)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: %d.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Pause_Uncore(void)
+ *
+ * @param - 1 if switching group, 0 otherwise
+ *
+ * @return OS_STATUS
+ *
+ * @brief Pause the uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Pause_Uncore(PVOID param)
+{
+	U32 i;
+	U32 switch_grp;
+	DEV_UNC_CONFIG pcfg_unc = NULL;
+	DISPATCH dispatch_unc = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	switch_grp = *((U32 *)param);
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+		dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+
+		if (pcfg_unc && dispatch_unc && dispatch_unc->freeze) {
+			SEP_DRV_LOG_TRACE("LWP: calling UNC Pause.");
+			if (switch_grp) {
+				if (LWPMU_DEVICE_em_groups_count(&devices[i]) >
+				    1) {
+					dispatch_unc->freeze(&i);
+				}
+			} else {
+				dispatch_unc->freeze(&i);
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#if !defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Pause_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Pause the core/uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Pause_Op(PVOID param)
+{
+	U32 dev_idx;
+	DISPATCH dispatch;
+	U32 switch_grp = 0;
+	U32 this_cpu = CONTROL_THIS_CPU();
+
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->freeze != NULL &&
+	    DRV_CONFIG_use_pcl(drv_cfg) == FALSE) {
+		dispatch->freeze(param);
+	}
+
+	lwpmudrv_Pause_Uncore((PVOID)&switch_grp);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Pause(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Pause the collection
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Pause(void)
+{
+	int i;
+	int done = FALSE;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!pcb || !drv_cfg) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Pcb or drv_cfg pointer is NULL!");
+		return OS_INVALID;
+	}
+
+	if (CHANGE_DRIVER_STATE(STATE_BIT_RUNNING, DRV_STATE_PAUSING)) {
+		if (DRV_CONFIG_use_pcl(drv_cfg) == FALSE) {
+			for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state);
+			     i++) {
+				CPU_STATE_accept_interrupt(&pcb[i]) = 0;
+			}
+			while (!done) {
+				done = TRUE;
+				for (i = 0;
+				     i < GLOBAL_STATE_num_cpus(driver_state);
+				     i++) {
+					if (atomic_read(&CPU_STATE_in_interrupt(
+						    &pcb[i]))) {
+						done = FALSE;
+					}
+				}
+			}
+		}
+#if !defined(DRV_SEP_ACRN_ON)
+		CONTROL_Invoke_Parallel(lwpmudrv_Pause_Op, NULL);
+#endif
+		/*
+	 * This means that the PAUSE state has been reached.
+	 */
+		CHANGE_DRIVER_STATE(STATE_BIT_PAUSING, DRV_STATE_PAUSED);
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Resume_Uncore(void)
+ *
+ * @param - 1 if switching group, 0 otherwise
+ *
+ * @return OS_STATUS
+ *
+ * @brief Resume the uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Resume_Uncore(PVOID param)
+{
+	U32 i;
+	U32 switch_grp;
+	DEV_UNC_CONFIG pcfg_unc = NULL;
+	DISPATCH dispatch_unc = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	switch_grp = *((U32 *)param);
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+		dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+
+		if (pcfg_unc && dispatch_unc && dispatch_unc->restart) {
+			SEP_DRV_LOG_TRACE("LWP: calling UNC Resume.");
+			if (switch_grp) {
+				if (LWPMU_DEVICE_em_groups_count(&devices[i]) >
+				    1) {
+					dispatch_unc->restart(&i);
+				}
+			} else {
+				dispatch_unc->restart(&i);
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#if !defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Resume_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Resume the core/uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Resume_Op(PVOID param)
+{
+	U32 this_cpu = CONTROL_THIS_CPU();
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	DISPATCH dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	U32 switch_grp = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->restart != NULL &&
+	    DRV_CONFIG_use_pcl(drv_cfg) == FALSE) {
+		dispatch->restart((VOID *)(size_t)0);
+	}
+
+	lwpmudrv_Resume_Uncore((PVOID)&switch_grp);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Resume(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Resume the collection
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Resume(void)
+{
+	int i;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!pcb || !drv_cfg) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Pcb or drv_cfg pointer is NULL!");
+		return OS_INVALID;
+	}
+
+	/*
+	 * If we are in the process of pausing sampling, wait until the pause has been
+	 * completed.  Then start the Resume process.
+	 */
+	while (GET_DRIVER_STATE() == DRV_STATE_PAUSING) {
+		/*
+	 *  This delay probably needs to be expanded a little bit more for large systems.
+	 *  For now, it is probably sufficient.
+	 */
+		SYS_IO_Delay();
+		SYS_IO_Delay();
+	}
+
+	if (CHANGE_DRIVER_STATE(STATE_BIT_PAUSED, DRV_STATE_RUNNING)) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			if (cpu_mask_bits) {
+				CPU_STATE_accept_interrupt(&pcb[i]) =
+					cpu_mask_bits[i] ? 1 : 0;
+				CPU_STATE_group_swap(&pcb[i]) = 1;
+			} else {
+				CPU_STATE_accept_interrupt(&pcb[i]) = 1;
+				CPU_STATE_group_swap(&pcb[i]) = 1;
+			}
+		}
+#if !defined(DRV_SEP_ACRN_ON)
+		CONTROL_Invoke_Parallel(lwpmudrv_Resume_Op, NULL);
+#endif
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Write_Uncore(void)
+ *
+ * @param - 1 if switching group, 0 otherwise
+ *
+ * @return OS_STATUS
+ *
+ * @brief Program the uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Write_Uncore(PVOID param)
+{
+	U32 i;
+	U32 switch_grp;
+	DEV_UNC_CONFIG pcfg_unc = NULL;
+	DISPATCH dispatch_unc = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	switch_grp = *((U32 *)param);
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+		dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+
+		if (pcfg_unc && dispatch_unc && dispatch_unc->write) {
+			SEP_DRV_LOG_TRACE("LWP: calling UNC Write.");
+			if (switch_grp) {
+				if (LWPMU_DEVICE_em_groups_count(&devices[i]) >
+				    1) {
+					dispatch_unc->write(&i);
+				}
+			} else {
+				dispatch_unc->write(&i);
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Write_Op(void)
+ *
+ * @param - Do operation for Core only
+ *
+ * @return OS_STATUS
+ *
+ * @brief Program the core/uncore collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Write_Op(PVOID param)
+{
+	U32 this_cpu = CONTROL_THIS_CPU();
+
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	DISPATCH dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	U32 switch_grp = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->write != NULL) {
+		dispatch->write((VOID *)(size_t)0);
+	}
+
+	if (param == NULL) {
+		lwpmudrv_Write_Uncore((PVOID)&switch_grp);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Switch_Group(void)
+ *
+ * @param none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Switch the current group that is being collected.
+ *
+ * <I>Special Notes</I>
+ *     This routine is called from the user mode code to handle the multiple group
+ *     situation.  4 distinct steps are taken:
+ *     Step 1: Pause the sampling
+ *     Step 2: Increment the current group count
+ *     Step 3: Write the new group to the PMU
+ *     Step 4: Resume sampling
+ */
+static OS_STATUS lwpmudrv_Switch_Group(void)
+{
+	S32 idx;
+	CPU_STATE pcpu;
+	EVENT_CONFIG ec;
+	OS_STATUS status = OS_SUCCESS;
+	U32 current_state = GET_DRIVER_STATE();
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!pcb || !drv_cfg) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Pcb or drv_cfg pointer is NULL!");
+		return OS_INVALID;
+	}
+
+	if (current_state != DRV_STATE_RUNNING &&
+	    current_state != DRV_STATE_PAUSED) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Return value: %d (invalid driver state!).", status);
+		return status;
+	}
+
+	status = lwpmudrv_Pause();
+
+	for (idx = 0; idx < GLOBAL_STATE_num_cpus(driver_state); idx++) {
+		pcpu = &pcb[idx];
+		ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(
+			&devices[core_to_dev_map[idx]]);
+		CPU_STATE_current_group(pcpu)++;
+		// make the event group list circular
+		CPU_STATE_current_group(pcpu) %= EVENT_CONFIG_num_groups(ec);
+	}
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Write_Op,
+				(VOID *)(size_t)CONTROL_THIS_CPU());
+#else
+	lwpmudrv_Write_Op((VOID *)(size_t)CONTROL_THIS_CPU());
+#endif
+	if (drv_cfg && DRV_CONFIG_start_paused(drv_cfg) == FALSE) {
+		lwpmudrv_Resume();
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Trigger_Read_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Read uncore data
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Trigger_Read_Op(PVOID param)
+{
+	DEV_UNC_CONFIG pcfg_unc = NULL;
+	DISPATCH dispatch_unc = NULL;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_num;
+	U64 tsc;
+	BUFFER_DESC bd;
+	EVENT_DESC evt_desc;
+	U32 cur_grp;
+	ECB pecb;
+	U32 sample_size = 0;
+	U32 offset = 0;
+	PVOID buf;
+	UncoreSampleRecordPC *psamp;
+	U32 i;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_num = core_to_package_map[this_cpu];
+
+	if (!DRIVER_STATE_IN(GET_DRIVER_STATE(),
+			     STATE_BIT_RUNNING | STATE_BIT_PAUSED)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("State is not RUNNING or PAUSED!");
+		return;
+	}
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Not socket master.");
+		return;
+	}
+
+	UTILITY_Read_TSC(&tsc);
+	bd = &unc_buf[package_num];
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+		if (pcfg_unc) {
+			cur_grp = LWPMU_DEVICE_cur_group(
+				&devices[i])[package_num];
+			pecb = LWPMU_DEVICE_PMU_register_data(
+				&devices[i])[cur_grp];
+			evt_desc = desc_data[ECB_descriptor_id(pecb)];
+			sample_size += EVENT_DESC_sample_size(evt_desc);
+		}
+	}
+
+	buf = OUTPUT_Reserve_Buffer_Space(bd, sample_size, FALSE,
+					  !SEP_IN_NOTIFICATION, -1);
+
+	if (buf) {
+		for (i = num_core_devs; i < num_devices; i++) {
+			pcfg_unc =
+				(DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+			dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+			if (pcfg_unc && dispatch_unc &&
+			    dispatch_unc->trigger_read) {
+				cur_grp = LWPMU_DEVICE_cur_group(
+					&devices[i])[package_num];
+				pecb = LWPMU_DEVICE_PMU_register_data(
+					&devices[i])[cur_grp];
+				evt_desc = desc_data[ECB_descriptor_id(pecb)];
+
+				psamp = (UncoreSampleRecordPC *)(((S8 *)buf) +
+								 offset);
+				UNCORE_SAMPLE_RECORD_descriptor_id(psamp) =
+					ECB_descriptor_id(pecb);
+				UNCORE_SAMPLE_RECORD_tsc(psamp) = tsc;
+				UNCORE_SAMPLE_RECORD_uncore_valid(psamp) = 1;
+				UNCORE_SAMPLE_RECORD_cpu_num(psamp) =
+					(U16)this_cpu;
+				UNCORE_SAMPLE_RECORD_pkg_num(psamp) =
+					(U16)package_num;
+
+				dispatch_unc->trigger_read(psamp, i);
+				offset += EVENT_DESC_sample_size(evt_desc);
+			}
+		}
+	} else {
+		SEP_DRV_LOG_WARNING(
+			"Buffer space reservation failed; some samples will be dropped.");
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Uncore_Switch_Group(void)
+ *
+ * @param none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Switch the current group that is being collected.
+ *
+ * <I>Special Notes</I>
+ *     This routine is called from the user mode code to handle the multiple group
+ *     situation.  4 distinct steps are taken:
+ *     Step 1: Pause the sampling
+ *     Step 2: Increment the current group count
+ *     Step 3: Write the new group to the PMU
+ *     Step 4: Resume sampling
+ */
+static OS_STATUS lwpmudrv_Uncore_Switch_Group(void)
+{
+	OS_STATUS status = OS_SUCCESS;
+	U32 current_state = GET_DRIVER_STATE();
+	U32 i = 0;
+	U32 j, k;
+	DEV_UNC_CONFIG pcfg_unc;
+	DISPATCH dispatch_unc;
+	ECB ecb_unc;
+	U32 cur_grp;
+	U32 num_units;
+	U32 switch_grp = 1;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!devices || !drv_cfg) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Devices or drv_cfg pointer is NULL!");
+		return OS_INVALID;
+	}
+
+	if (current_state != DRV_STATE_RUNNING &&
+	    current_state != DRV_STATE_PAUSED) {
+		SEP_DRV_LOG_FLOW_OUT("Driver state is not RUNNING or PAUSED!");
+		return OS_INVALID;
+	}
+
+	if (max_groups_unc > 1) {
+		CONTROL_Invoke_Parallel(lwpmudrv_Pause_Uncore,
+					(PVOID)&switch_grp);
+		for (i = num_core_devs; i < num_devices; i++) {
+			pcfg_unc = LWPMU_DEVICE_pcfg(&devices[i]);
+			dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+			num_units = LWPMU_DEVICE_num_units(&devices[i]);
+			if (!pcfg_unc || !dispatch_unc) {
+				continue;
+			}
+			if (LWPMU_DEVICE_em_groups_count(&devices[i]) > 1) {
+				for (j = 0; j < num_packages; j++) {
+					cur_grp = LWPMU_DEVICE_cur_group(
+						&devices[i])[j];
+					ecb_unc =
+						LWPMU_DEVICE_PMU_register_data(
+							&devices[i])[cur_grp];
+					// Switch group
+					LWPMU_DEVICE_cur_group(
+						&devices[i])[j]++;
+					LWPMU_DEVICE_cur_group(
+						&devices[i])[j] %=
+						LWPMU_DEVICE_em_groups_count(
+							&devices[i]);
+					if (ecb_unc &&
+					    (ECB_device_type(ecb_unc) ==
+					     DEVICE_UNC_SOCPERF) &&
+					    (j == 0)) {
+						// SOCPERF_Switch_Group3();
+					}
+					// Post group switch
+					cur_grp = LWPMU_DEVICE_cur_group(
+						&devices[i])[j];
+					ecb_unc =
+						LWPMU_DEVICE_PMU_register_data(
+							&devices[i])[cur_grp];
+					for (k = 0;
+					     k < (ECB_num_events(ecb_unc) *
+						  num_units);
+					     k++) {
+						LWPMU_DEVICE_prev_value(
+							&devices[i])[j][k] =
+							0LL; //zero out prev_value for new collection
+					}
+				}
+			}
+		}
+		CONTROL_Invoke_Parallel(lwpmudrv_Write_Uncore,
+					(PVOID)&switch_grp);
+		CONTROL_Invoke_Parallel(lwpmudrv_Resume_Uncore,
+					(PVOID)&switch_grp);
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static VOID lwpmudrv_Trigger_Read(void)
+ *
+ * @param - none
+ *
+ * @return - OS_STATUS
+ *
+ * @brief Read the Counter Data.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Trigger_Read(
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	struct timer_list *tl
+#else
+	unsigned long arg
+#endif
+)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_RUNNING) {
+		SEP_DRV_LOG_TRACE_OUT("Success: driver state is not RUNNING");
+		return;
+	}
+#if defined(BUILD_CHIPSET)
+	if (cs_dispatch && cs_dispatch->Trigger_Read) {
+		cs_dispatch->Trigger_Read();
+	}
+#endif
+
+	if (drv_cfg && DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		SEP_DRV_LOG_TRACE_OUT("Success: Using PCL");
+		return;
+	}
+
+	CONTROL_Invoke_Parallel(lwpmudrv_Trigger_Read_Op, NULL);
+
+	uncore_em_factor++;
+	if (uncore_em_factor == DRV_CONFIG_unc_em_factor(drv_cfg)) {
+		SEP_DRV_LOG_TRACE("Switching Uncore Group...");
+		lwpmudrv_Uncore_Switch_Group();
+		uncore_em_factor = 0;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	mod_timer(unc_read_timer, jiffies + unc_timer_interval);
+#else
+	unc_read_timer->expires = jiffies + unc_timer_interval;
+	add_timer(unc_read_timer);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("Success.");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwmudrv_Read_Specific_TSC (PVOID param)
+ *
+ * @param param - pointer to the result
+ *
+ * @return none
+ *
+ * @brief  Read the tsc value in the current processor and
+ * @brief  write the result into param.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Read_Specific_TSC(PVOID param)
+{
+	U32 this_cpu;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	if (this_cpu == 0) {
+		UTILITY_Read_TSC((U64 *)param);
+	}
+	preempt_enable();
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID lwpmudrv_Uncore_Stop_Timer (void)
+ *
+ * @brief       Stop the uncore read timer
+ *
+ * @param       none
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID lwpmudrv_Uncore_Stop_Timer(void)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (unc_read_timer == NULL) {
+		return;
+	}
+
+	del_timer_sync(unc_read_timer);
+	unc_read_timer = CONTROL_Free_Memory(unc_read_timer);
+
+	SEP_DRV_LOG_FLOW_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          OS_STATUS lwpmudrv_Uncore_Start_Timer (void)
+ *
+ * @brief       Start the uncore read timer
+ *
+ * @param       none
+ *
+ * @return      OS_STATUS
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID lwpmudrv_Uncore_Start_Timer(void)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	unc_timer_interval =
+		msecs_to_jiffies(DRV_CONFIG_unc_timer_interval(drv_cfg));
+	unc_read_timer = CONTROL_Allocate_Memory(sizeof(struct timer_list));
+	if (unc_read_timer == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for unc_read_timer!");
+		return;
+	}
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 15, 0)
+	timer_setup(unc_read_timer, lwpmudrv_Trigger_Read, 0);
+	mod_timer(unc_read_timer, jiffies + unc_timer_interval);
+#else
+	init_timer(unc_read_timer);
+	unc_read_timer->function = lwpmudrv_Trigger_Read;
+	unc_read_timer->expires = jiffies + unc_timer_interval;
+	add_timer(unc_read_timer);
+#endif
+
+	SEP_DRV_LOG_FLOW_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Init_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Initialize PMU before collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Init_Op(PVOID param)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->init != NULL) {
+		dispatch->init(&dev_idx);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Init_PMU(void)
+ *
+ * @param - none
+ *
+ * @return - OS_STATUS
+ *
+ * @brief Initialize the PMU and the driver state in preparation for data collection.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Init_PMU(IOCTL_ARGS args)
+{
+	DEV_UNC_CONFIG pcfg_unc = NULL;
+	DISPATCH dispatch_unc = NULL;
+	EVENT_CONFIG ec;
+	U32 i;
+	U32 emon_buffer_size = 0;
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->len_usr_to_drv == 0 || args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	if (copy_from_user(&emon_buffer_size, (void __user *)args->buf_usr_to_drv,
+			   sizeof(U32))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure");
+		return OS_FAULT;
+	}
+	prev_counter_size = emon_buffer_size;
+
+	if (!drv_cfg) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("drv_cfg not set!");
+		return OS_FAULT;
+	}
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		SEP_DRV_LOG_FLOW_OUT("Success: using PCL.");
+		return OS_SUCCESS;
+	}
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Discarded: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(
+			&devices[core_to_dev_map[i]]);
+		CPU_STATE_trigger_count(&pcb[i]) = EVENT_CONFIG_em_factor(ec);
+		CPU_STATE_trigger_event_num(&pcb[i]) =
+			EVENT_CONFIG_em_event_num(ec);
+	}
+
+	// set cur_device's total groups to max groups of all devices
+	max_groups_unc = 0;
+	for (i = num_core_devs; i < num_devices; i++) {
+		if (max_groups_unc <
+		    LWPMU_DEVICE_em_groups_count(&devices[i])) {
+			max_groups_unc =
+				LWPMU_DEVICE_em_groups_count(&devices[i]);
+		}
+	}
+	// now go back and up total groups for all devices
+	if (DRV_CONFIG_emon_mode(drv_cfg) == TRUE) {
+		for (i = num_core_devs; i < num_devices; i++) {
+			if (LWPMU_DEVICE_em_groups_count(&devices[i]) <
+			    max_groups_unc) {
+				LWPMU_DEVICE_em_groups_count(&devices[i]) =
+					max_groups_unc;
+			}
+		}
+	}
+
+	// allocate save/restore space before program the PMU
+	lwpmudrv_Allocate_Restore_Buffer();
+
+	// allocate uncore read buffers for SEP
+	if (unc_buf_init && !DRV_CONFIG_emon_mode(drv_cfg)) {
+		lwpmudrv_Allocate_Uncore_Buffer();
+	}
+
+	// must be done after pcb is created and before PMU is first written to
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Init_Op, NULL);
+#else
+	lwpmudrv_Init_Op(NULL);
+#endif
+
+	for (i = num_core_devs; i < num_devices; i++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[i]);
+		dispatch_unc = LWPMU_DEVICE_dispatch(&devices[i]);
+		if (pcfg_unc && dispatch_unc && dispatch_unc->init) {
+			dispatch_unc->init((VOID *)&i);
+		}
+	}
+
+	// Allocate PEBS buffers
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		PEBS_Allocate();
+	}
+
+	//
+	// Transfer the data into the PMU registers
+	//
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Write_Op, NULL);
+#else
+	lwpmudrv_Write_Op(NULL);
+#endif
+
+	SEP_DRV_LOG_TRACE("IOCTL_Init_PMU - finished initial Write.");
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == TRUE ||
+	    DRV_CONFIG_emon_mode(drv_cfg) == TRUE) {
+		if (!read_counter_info) {
+			read_counter_info =
+				CONTROL_Allocate_Memory(emon_buffer_size);
+			if (!read_counter_info) {
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"Memory allocation failure!");
+				return OS_NO_MEM;
+			}
+		}
+		if (!prev_counter_data) {
+			prev_counter_data =
+				CONTROL_Allocate_Memory(emon_buffer_size);
+			if (!prev_counter_data) {
+				read_counter_info =
+					CONTROL_Free_Memory(read_counter_info);
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"Memory allocation failure!");
+				return OS_NO_MEM;
+			}
+		}
+		if (!emon_buffer_driver_helper) {
+			// allocate size = size of EMON_BUFFER_DRIVER_HELPER_NODE + the number of entries in core_index_to_thread_offset_map, which is num of cpu
+			emon_buffer_driver_helper = CONTROL_Allocate_Memory(
+				sizeof(EMON_BUFFER_DRIVER_HELPER_NODE) +
+				sizeof(U32) *
+					GLOBAL_STATE_num_cpus(driver_state));
+			if (!emon_buffer_driver_helper) {
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"Memory allocation failure!");
+				return OS_NO_MEM;
+			}
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Read_MSR(pvoid param)
+ *
+ * @param param - pointer to the buffer to store the MSR counts
+ *
+ * @return none
+ *
+ * @brief
+ * @brief  Read the U64 value at address in buf_drv_to_usr and
+ * @brief  write the result into buf_usr_to_drv.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Read_MSR(PVOID param)
+{
+	S32 cpu_idx;
+	MSR_DATA this_node;
+#if !defined(DRV_SEP_ACRN_ON)
+	S64 reg_num;
+#else
+	struct profiling_msr_ops_list *msr_list;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (param == NULL) {
+		preempt_disable();
+		cpu_idx = (S32)CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		cpu_idx = *(S32 *)param;
+	}
+#if !defined(DRV_SEP_ACRN_ON)
+	this_node = &msr_data[cpu_idx];
+	reg_num = MSR_DATA_addr(this_node);
+
+	if (reg_num != 0) {
+		MSR_DATA_value(this_node) =
+			(U64)SYS_Read_MSR((U32)MSR_DATA_addr(this_node));
+	}
+#else
+	msr_list = (struct profiling_msr_ops_list *)CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) *
+		sizeof(struct profiling_msr_ops_list));
+	memset(msr_list, 0,
+	       GLOBAL_STATE_num_cpus(driver_state) *
+		       sizeof(struct profiling_msr_ops_list));
+	for (cpu_idx = 0; cpu_idx < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_idx++) {
+		this_node = &msr_data[cpu_idx];
+		msr_list[cpu_idx].collector_id = COLLECTOR_SEP;
+		msr_list[cpu_idx].entries[0].msr_id = MSR_DATA_addr(this_node);
+		msr_list[cpu_idx].entries[0].op_type = MSR_OP_READ;
+		msr_list[cpu_idx].entries[0].value = 0LL;
+		msr_list[cpu_idx].num_entries = 1;
+		msr_list[cpu_idx].msr_op_state = MSR_OP_REQUESTED;
+	}
+
+	BUG_ON(!virt_addr_valid(msr_list));
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_MSR_OPS,
+			virt_to_phys(msr_list));
+
+	for (cpu_idx = 0; cpu_idx < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_idx++) {
+		this_node = &msr_data[cpu_idx];
+		MSR_DATA_value(this_node) = msr_list[cpu_idx].entries[0].value;
+	}
+
+	msr_list = CONTROL_Free_Memory(msr_list);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Read_MSR_All_Cores(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read the U64 value at address into buf_drv_to_usr and write
+ * @brief  the result into buf_usr_to_drv.
+ * @brief  Returns OS_SUCCESS if the read across all cores succeed,
+ * @brief  otherwise OS_FAULT.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Read_MSR_All_Cores(IOCTL_ARGS arg)
+{
+	U64 *val;
+	S32 reg_num;
+	S32 i;
+	MSR_DATA node;
+#if defined(DRV_SEP_ACRN_ON)
+	S32 this_cpu = 0;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if ((arg->len_usr_to_drv != sizeof(U32)) ||
+	    (arg->buf_usr_to_drv == NULL) || (arg->buf_drv_to_usr == NULL)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_FAULT;
+	}
+
+	val = (U64 *)arg->buf_drv_to_usr;
+	if (val == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("NULL buf_usr_to_drv!");
+		return OS_FAULT;
+	}
+
+	if (copy_from_user(&reg_num, (void __user *)arg->buf_usr_to_drv, sizeof(U32))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	msr_data = CONTROL_Allocate_Memory(GLOBAL_STATE_num_cpus(driver_state) *
+					   sizeof(MSR_DATA_NODE));
+	if (!msr_data) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure!");
+		return OS_NO_MEM;
+	}
+
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		node = &msr_data[i];
+		MSR_DATA_addr(node) = reg_num;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Read_MSR, NULL);
+#else
+	lwpmudrv_Read_MSR(&this_cpu);
+#endif
+
+	/* copy values to arg array? */
+	if (arg->len_drv_to_usr < GLOBAL_STATE_num_cpus(driver_state)) {
+		msr_data = CONTROL_Free_Memory(msr_data);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Not enough memory allocated in output buffer!");
+		return OS_FAULT;
+	}
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		node = &msr_data[i];
+		if (copy_to_user((void __user *)&val[i], (U64 *)&MSR_DATA_value(node),
+				 sizeof(U64))) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+			return OS_FAULT;
+		}
+	}
+
+	msr_data = CONTROL_Free_Memory(msr_data);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Write_MSR(pvoid iaram)
+ *
+ * @param param - pointer to array containing the MSR address and the value to be written
+ *
+ * @return none
+ *
+ * @brief
+ * @brief  Read the U64 value at address in buf_drv_to_usr and
+ * @brief  write the result into buf_usr_to_drv.
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Write_MSR(PVOID param)
+{
+	S32 cpu_idx;
+	MSR_DATA this_node;
+#if !defined(DRV_SEP_ACRN_ON)
+	U32 reg_num;
+	U64 val;
+#else
+	struct profiling_msr_ops_list *msr_list;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (param == NULL) {
+		preempt_disable();
+		cpu_idx = (S32)CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		cpu_idx = *(S32 *)param;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	this_node = &msr_data[cpu_idx];
+	reg_num = (U32)MSR_DATA_addr(this_node);
+	val = (U64)MSR_DATA_value(this_node);
+	// don't attempt to write MSR 0
+	if (reg_num == 0) {
+		preempt_enable();
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Error: tried to write MSR 0!");
+		return;
+	}
+
+	SYS_Write_MSR(reg_num, val);
+	preempt_enable();
+
+#else
+	msr_list = (struct profiling_msr_ops_list *)CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) *
+		sizeof(struct profiling_msr_ops_list));
+	memset(msr_list, 0,
+	       GLOBAL_STATE_num_cpus(driver_state) *
+		       sizeof(struct profiling_msr_ops_list));
+	for (cpu_idx = 0; cpu_idx < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_idx++) {
+		this_node = &msr_data[cpu_idx];
+		msr_list[cpu_idx].collector_id = COLLECTOR_SEP;
+		msr_list[cpu_idx].entries[0].msr_id = MSR_DATA_addr(this_node);
+		msr_list[cpu_idx].entries[0].op_type = MSR_OP_WRITE;
+		msr_list[cpu_idx].entries[0].value = MSR_DATA_value(this_node);
+		msr_list[cpu_idx].num_entries = 1;
+		msr_list[cpu_idx].msr_op_state = MSR_OP_REQUESTED;
+	}
+
+	BUG_ON(!virt_addr_valid(msr_list));
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_MSR_OPS,
+			virt_to_phys(msr_list));
+
+	msr_list = CONTROL_Free_Memory(msr_list);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Write_MSR_All_Cores(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read the U64 value at address into buf_usr_to_drv and write
+ * @brief  the result into buf_usr_to_drv.
+ * @brief  Returns OS_SUCCESS if the write across all cores succeed,
+ * @brief  otherwise OS_FAULT.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Write_MSR_All_Cores(IOCTL_ARGS arg)
+{
+	EVENT_REG_NODE buf;
+	EVENT_REG buf_usr_to_drv = &buf;
+	U32 reg_num;
+	U64 val;
+	S32 i;
+	MSR_DATA node;
+#if defined(DRV_SEP_ACRN_ON)
+	S32 this_cpu = 0;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_usr_to_drv < sizeof(EVENT_REG_NODE) ||
+	    arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_FAULT;
+	}
+
+	if (copy_from_user(buf_usr_to_drv, (void __user *)arg->buf_usr_to_drv,
+			   sizeof(EVENT_REG_NODE))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+	reg_num = (U32)EVENT_REG_reg_id(buf_usr_to_drv, 0);
+	val = (U64)EVENT_REG_reg_value(buf_usr_to_drv, 0);
+
+	msr_data = CONTROL_Allocate_Memory(GLOBAL_STATE_num_cpus(driver_state) *
+					   sizeof(MSR_DATA_NODE));
+	if (!msr_data) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure");
+		return OS_NO_MEM;
+	}
+
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		node = &msr_data[i];
+		MSR_DATA_addr(node) = reg_num;
+		MSR_DATA_value(node) = val;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Write_MSR, NULL);
+#else
+	lwpmudrv_Write_MSR(&this_cpu);
+#endif
+
+	msr_data = CONTROL_Free_Memory(msr_data);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Read_Data_Op(PVOID param)
+ *
+ * @param param   - dummy
+ *
+ * @return void
+ *
+ * @brief  Read all the core/uncore data counters at one shot
+ *
+ * <I>Special Notes</I>
+ */
+static void lwpmudrv_Read_Data_Op(VOID *param)
+{
+	U32 this_cpu;
+	DISPATCH dispatch;
+	U32 dev_idx;
+	DEV_UNC_CONFIG pcfg_unc;
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (devices == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Devices is null!");
+		return;
+	}
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	if (dispatch != NULL && dispatch->read_data != NULL) {
+		dispatch->read_data(param);
+	}
+	for (dev_idx = num_core_devs; dev_idx < num_devices; dev_idx++) {
+		pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+		if (pcfg_unc == NULL) {
+			continue;
+		}
+		if (!(DRV_CONFIG_emon_mode(drv_cfg) ||
+		      DRV_CONFIG_counting_mode(drv_cfg))) {
+			continue;
+		}
+		dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+		if (dispatch == NULL) {
+			continue;
+		}
+		if (dispatch->read_data == NULL) {
+			continue;
+		}
+		dispatch->read_data((VOID *)&dev_idx);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Read_MSRs(IOCTL_ARG arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read all the programmed data counters and accumulate them
+ * @brief  into a single buffer.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Read_MSRs(IOCTL_ARGS arg)
+{
+#if defined(DRV_SEP_ACRN_ON)
+	S32 this_cpu = 0;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr == 0 || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_SUCCESS;
+	}
+	//
+	// Transfer the data in the PMU registers to the output buffer
+	//
+	if (!read_counter_info) {
+		read_counter_info =
+			CONTROL_Allocate_Memory(arg->len_drv_to_usr);
+		if (!read_counter_info) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure");
+			return OS_NO_MEM;
+		}
+	}
+	if (!prev_counter_data) {
+		prev_counter_data =
+			CONTROL_Allocate_Memory(arg->len_drv_to_usr);
+		if (!prev_counter_data) {
+			read_counter_info =
+				CONTROL_Free_Memory(read_counter_info);
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure");
+			return OS_NO_MEM;
+		}
+	}
+	memset(read_counter_info, 0, arg->len_drv_to_usr);
+
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Read_Data_Op, NULL);
+#else
+	lwpmudrv_Read_Data_Op(&this_cpu);
+#endif
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, read_counter_info,
+			 arg->len_drv_to_usr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+#if defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Read_Metrics_Op(PVOID param)
+ *
+ * @param param   - dummy
+ *
+ * @return void
+ *
+ * @brief  Read metrics register IA32_PERF_METRICS to collect topdown metrics
+ *         from PMU
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Read_Metrics_Op(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 offset;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	DISPATCH dispatch;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	// The pmu metric will be append after core event at thread level (basically treat them as extra core events).
+	// Move the pointer to the end of the core event for this cpu index.
+	offset = EMON_BUFFER_CORE_EVENT_OFFSET(
+		EMON_BUFFER_DRIVER_HELPER_core_index_to_thread_offset_map(
+			emon_buffer_driver_helper)[this_cpu],
+		EMON_BUFFER_DRIVER_HELPER_core_num_events(
+			emon_buffer_driver_helper));
+
+	if (!DEV_CONFIG_enable_perf_metrics(pcfg) ||
+	    !DEV_CONFIG_emon_perf_metrics_offset(pcfg) ||
+	    (CPU_STATE_current_group(pcpu) != 0)) {
+		return;
+	}
+
+	if (dispatch != NULL && dispatch->read_metrics != NULL) {
+		dispatch->read_metrics(read_counter_info + offset);
+		SEP_DRV_LOG_TRACE("Data read = %llu.",
+				  *(read_counter_info + offset));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Read_Metrics(IOCTL_ARGS args)
+ *
+ * @param args   - pointer to IOCTL_ARGS_NODE structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read metrics register on all cpus and accumulate them into the output
+ *         buffer
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Read_Metrics(IOCTL_ARGS args)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 offset;
+	U64 *p_buffer;
+	DEV_CONFIG pcfg;
+	U32 idx;
+
+	SEP_DRV_LOG_FLOW_IN("Args: %p.", args);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	p_buffer = (U64 *)(args->buf_drv_to_usr);
+
+	if (args->len_drv_to_usr == 0 || args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_FLOW_OUT("Invalid parameters!");
+		return OS_SUCCESS;
+	}
+
+	if (!read_counter_info) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Read_counter_info is NULL!");
+		return OS_FAULT;
+	}
+
+	CONTROL_Invoke_Parallel(lwpmudrv_Read_Metrics_Op, NULL);
+	for (idx = 0; idx < num_core_devs; idx++) {
+		pcfg = LWPMU_DEVICE_pcfg(&devices[idx]);
+		offset = DEV_CONFIG_emon_perf_metrics_offset(pcfg);
+		if (!DEV_CONFIG_enable_perf_metrics(pcfg) || !offset ||
+		    (CPU_STATE_current_group(pcpu) != 0)) {
+			continue;
+		}
+		p_buffer += offset;
+		if (copy_to_user((void __user *)p_buffer, read_counter_info + offset,
+				 (sizeof(U64) * num_packages *
+				  GLOBAL_STATE_num_cpus(driver_state) *
+				  DEV_CONFIG_num_perf_metrics(pcfg)))) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Failed copy_to_user for read_counter_info!");
+			return OS_FAULT;
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success.");
+	return OS_SUCCESS;
+}
+
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Read_Counters_And_Switch_Group(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read / Store the counters and switch to the next valid group.
+ *
+ * <I>Special Notes</I>
+ *     This routine is called from the user mode code to handle the multiple group
+ *     situation.  10 distinct steps are taken:
+ *     Step 1:  Save the previous cpu's tsc
+ *     Step 2:  Read the current cpu's tsc
+ *     Step 3:  Pause the counting PMUs
+ *     Step 4:  Calculate the difference between the current and previous cpu's tsc
+ *     Step 5:  Save original buffer ptr and copy cpu's tsc into the output buffer
+ *              Increment the buffer position by number of CPU
+ *     Step 6:  Read the currently programmed data PMUs and copy the data into the output buffer
+ *              Restore the original buffer ptr.
+ *     Step 7:  Write the new group to the PMU
+ *     Step 8:  Write the new group to the PMU
+ *     Step 9:  Read the current cpu's tsc for next collection (so read MSRs time not included in report)
+ *     Step 10: Resume the counting PMUs
+ */
+static OS_STATUS lwpmudrv_Read_Counters_And_Switch_Group(IOCTL_ARGS arg)
+{
+	U64 *p_buffer = NULL;
+	char *orig_r_buf_ptr = NULL;
+	U64 orig_r_buf_len = 0;
+	OS_STATUS status = OS_SUCCESS;
+	DRV_BOOL enter_in_pause_state = 0;
+	U32 i = 0;
+#if !defined(CONFIG_PREEMPT_COUNT)
+	U64 *tmp = NULL;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->buf_drv_to_usr == NULL || arg->len_drv_to_usr == 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	if (!DRIVER_STATE_IN(GET_DRIVER_STATE(),
+			     STATE_BIT_RUNNING | STATE_BIT_PAUSED)) {
+		SEP_DRV_LOG_FLOW_OUT(
+			"'Success'/error: driver state is not RUNNING or PAUSED!");
+		return OS_SUCCESS;
+	}
+
+	if (GET_DRIVER_STATE() == DRV_STATE_PAUSED) {
+		enter_in_pause_state = 1;
+	}
+
+	// step 1
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		// swap cpu_tsc and prev_cpu_tsc, so that cpu_tsc is saved in prev_cpu_tsc.
+		tmp = prev_cpu_tsc;
+		prev_cpu_tsc = cpu_tsc;
+		cpu_tsc = tmp;
+	} else
+#endif
+		prev_cpu_tsc[0] = cpu_tsc[0];
+
+		// step 2
+		// if per_cpu_tsc is not defined, read cpu0's tsc and save in var cpu_tsc[0]
+		// if per_cpu_tsc is defined, read all cpu's tsc and save in var cpu_tsc by lwpmudrv_Fill_TSC_Info
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+		init_waitqueue_head(&read_tsc_now);
+		CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info,
+					(PVOID)(size_t)0);
+	} else
+#endif
+		CONTROL_Invoke_Cpu(0, lwpmudrv_Read_Specific_TSC, &cpu_tsc[0]);
+
+	// step 3
+	// Counters should be frozen right after time stamped.
+	if (!enter_in_pause_state) {
+		status = lwpmudrv_Pause();
+	}
+
+	// step 4
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+#if !defined(CONFIG_PREEMPT_COUNT)
+			diff_cpu_tsc[i] = cpu_tsc[i] - prev_cpu_tsc[i];
+#else
+			// if CONFIG_PREEMPT_COUNT is defined, means lwpmudrv_Fill_TSC_Info can not be run.
+			// return all cpu's tsc difference with cpu0's tsc difference instead
+			diff_cpu_tsc[i] = cpu_tsc[0] - prev_cpu_tsc[0];
+#endif
+		}
+	} else {
+		diff_cpu_tsc[0] = cpu_tsc[0] - prev_cpu_tsc[0];
+	}
+
+	// step 5
+	orig_r_buf_ptr = arg->buf_drv_to_usr;
+	orig_r_buf_len = arg->len_drv_to_usr;
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, diff_cpu_tsc,
+			 GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	p_buffer = (U64 *)(arg->buf_drv_to_usr);
+	p_buffer += GLOBAL_STATE_num_cpus(driver_state);
+	arg->buf_drv_to_usr = (char *)p_buffer;
+	arg->len_drv_to_usr -=
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64);
+
+	// step 6
+	status = lwpmudrv_Read_MSRs(arg);
+
+#if defined(DRV_SEP_ACRN_ON)
+	status = lwpmudrv_Read_Metrics(arg);
+#endif
+	arg->buf_drv_to_usr = orig_r_buf_ptr;
+	arg->len_drv_to_usr = orig_r_buf_len;
+
+	// step 7
+	// for each processor, increment its current group number
+	lwpmudrv_Switch_To_Next_Group();
+
+	// step 8
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Write_Op, NULL);
+#else
+	lwpmudrv_Write_Op(NULL);
+#endif
+
+	// step 9
+	// if per_cpu_tsc is defined, read all cpu's tsc and save in cpu_tsc for next run
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+		init_waitqueue_head(&read_tsc_now);
+		CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info,
+					(PVOID)(size_t)0);
+	} else
+#endif
+		CONTROL_Invoke_Cpu(0, lwpmudrv_Read_Specific_TSC, &cpu_tsc[0]);
+
+	// step 10
+	if (!enter_in_pause_state) {
+		status = lwpmudrv_Resume();
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/*
+ * @fn  static OS_STATUS lwpmudrv_Read_And_Reset_Counters(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read the current value of the counters, and reset them all to 0.
+ *
+ * <I>Special Notes</I>
+ *     This routine is called from the user mode code to handle the multiple group
+ *     situation. 9 distinct steps are taken:
+ *     Step 1: Save the previous cpu's tsc
+ *     Step 2: Read the current cpu's tsc
+ *     Step 3: Pause the counting PMUs
+ *     Step 4: Calculate the difference between the current and previous cpu's tsc
+ *     Step 5: Save original buffer ptr and copy cpu's tsc into the output buffer
+ *             Increment the buffer position by number of CPU
+ *     Step 6: Read the currently programmed data PMUs and copy the data into the output buffer
+ *             Restore the original buffer ptr.
+ *     Step 7: Write the new group to the PMU
+ *     Step 8: Read the current cpu's tsc for next collection (so read MSRs time not included in report)
+ *     Step 9: Resume the counting PMUs
+ */
+static OS_STATUS lwpmudrv_Read_And_Reset_Counters(IOCTL_ARGS arg)
+{
+	U64 *p_buffer = NULL;
+	char *orig_r_buf_ptr = NULL;
+	U64 orig_r_buf_len = 0;
+	OS_STATUS status = OS_SUCCESS;
+	DRV_BOOL enter_in_pause_state = 0;
+	U32 i = 0;
+#if !defined(CONFIG_PREEMPT_COUNT)
+	U64 *tmp = NULL;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->buf_drv_to_usr == NULL || arg->len_drv_to_usr == 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_FAULT;
+	}
+
+	if (!DRIVER_STATE_IN(GET_DRIVER_STATE(),
+			     STATE_BIT_RUNNING | STATE_BIT_PAUSED)) {
+		SEP_DRV_LOG_FLOW_OUT(
+			"'Success'/error: driver state is not RUNNING or PAUSED!");
+		return OS_SUCCESS;
+	}
+
+	if (GET_DRIVER_STATE() == DRV_STATE_PAUSED) {
+		enter_in_pause_state = 1;
+	}
+
+	// step 1
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		// swap cpu_tsc and prev_cpu_tsc, so that cpu_tsc is saved in prev_cpu_tsc.
+		tmp = prev_cpu_tsc;
+		prev_cpu_tsc = cpu_tsc;
+		cpu_tsc = tmp;
+	} else
+#endif
+		prev_cpu_tsc[0] = cpu_tsc[0];
+
+		// step 2
+		// if per_cpu_tsc is not defined, read cpu0's tsc into var cpu_tsc[0]
+		// if per_cpu_tsc is defined, read all cpu's tsc into var cpu_tsc by lwpmudrv_Fill_TSC_Info
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+		init_waitqueue_head(&read_tsc_now);
+		CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info,
+					(PVOID)(size_t)0);
+	} else
+#endif
+		CONTROL_Invoke_Cpu(0, lwpmudrv_Read_Specific_TSC, &cpu_tsc[0]);
+
+	// step 3
+	// Counters should be frozen right after time stamped.
+	if (!enter_in_pause_state) {
+		status = lwpmudrv_Pause();
+		if (status != OS_INVALID) {
+			return status;
+		}
+	}
+
+	// step 4
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+#if !defined(CONFIG_PREEMPT_COUNT)
+			diff_cpu_tsc[i] = cpu_tsc[i] - prev_cpu_tsc[i];
+#else
+			// if CONFIG_PREEMPT_COUNT is defined, means lwpmudrv_Fill_TSC_Info can not be run.
+			// return all cpu's tsc difference with cpu0's tsc difference instead
+			diff_cpu_tsc[i] = cpu_tsc[0] - prev_cpu_tsc[0];
+#endif
+		}
+	} else {
+		diff_cpu_tsc[0] = cpu_tsc[0] - prev_cpu_tsc[0];
+	}
+
+	// step 5
+	orig_r_buf_ptr = arg->buf_drv_to_usr;
+	orig_r_buf_len = arg->len_drv_to_usr;
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, diff_cpu_tsc,
+			 GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64))) {
+		return OS_FAULT;
+	}
+
+	p_buffer = (U64 *)(arg->buf_drv_to_usr);
+	p_buffer += GLOBAL_STATE_num_cpus(driver_state);
+	arg->buf_drv_to_usr = (char *)p_buffer;
+	arg->len_drv_to_usr -=
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64);
+
+	// step 6
+	status = lwpmudrv_Read_MSRs(arg);
+#if defined(DRV_SEP_ACRN_ON)
+	status = lwpmudrv_Read_Metrics(arg);
+#endif
+	arg->buf_drv_to_usr = orig_r_buf_ptr;
+	arg->len_drv_to_usr = orig_r_buf_len;
+
+	// step 7
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Write_Op, NULL);
+#else
+	lwpmudrv_Write_Op(NULL);
+#endif
+
+	// step 8
+	// if per_cpu_tsc is defined, read all cpu's tsc and save in cpu_tsc for next run
+#if !defined(CONFIG_PREEMPT_COUNT)
+	if (DRV_CONFIG_per_cpu_tsc(drv_cfg)) {
+		atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+		init_waitqueue_head(&read_tsc_now);
+		CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info,
+					(PVOID)(size_t)0);
+	} else
+#endif
+		CONTROL_Invoke_Cpu(0, lwpmudrv_Read_Specific_TSC, &cpu_tsc[0]);
+
+	// step 9
+	if (!enter_in_pause_state) {
+		status = lwpmudrv_Resume();
+		if (status !=  OS_SUCCESS)
+			return status;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Set_Num_EM_Groups(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief Configure the event multiplexing group.
+ *
+ * <I>Special Notes</I>
+ *     None
+ */
+static OS_STATUS lwpmudrv_Set_EM_Config(IOCTL_ARGS arg)
+{
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: Driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (arg->buf_usr_to_drv == NULL ||
+	    arg->len_usr_to_drv != sizeof(EVENT_CONFIG_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	LWPMU_DEVICE_ec(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(sizeof(EVENT_CONFIG_NODE));
+	if (!LWPMU_DEVICE_ec(&devices[cur_device])) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure for ec!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(LWPMU_DEVICE_ec(&devices[cur_device]),
+			   (void __user *)arg->buf_usr_to_drv, sizeof(EVENT_CONFIG_NODE))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure (event config)!");
+		return OS_FAULT;
+	}
+
+	ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(&devices[cur_device]);
+	LWPMU_DEVICE_PMU_register_data(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(EVENT_CONFIG_num_groups(ec) *
+					sizeof(VOID *));
+	if (!LWPMU_DEVICE_PMU_register_data(&devices[cur_device])) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for PMU_register_data!");
+		return OS_NO_MEM;
+	}
+
+	EVENTMUX_Initialize();
+
+	SEP_DRV_LOG_FLOW_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Set_EM_Config_UNC(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Set the number of em groups in the global state node.
+ * @brief  Also, copy the EVENT_CONFIG struct that has been passed in,
+ * @brief  into a global location for now.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Set_EM_Config_UNC(IOCTL_ARGS arg)
+{
+	EVENT_CONFIG ec;
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	// allocate memory
+	LWPMU_DEVICE_ec(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(sizeof(EVENT_CONFIG_NODE));
+	if (copy_from_user(LWPMU_DEVICE_ec(&devices[cur_device]),
+			   (void __user *)arg->buf_usr_to_drv, arg->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for LWPMU_device_ec!");
+		return OS_FAULT;
+	}
+	// configure num_groups from ec of the specific device
+	ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(&devices[cur_device]);
+	SEP_DRV_LOG_TRACE("Num Groups UNCORE: %d.",
+			  EVENT_CONFIG_num_groups_unc(ec));
+	LWPMU_DEVICE_PMU_register_data(&devices[cur_device]) =
+		CONTROL_Allocate_Memory(EVENT_CONFIG_num_groups_unc(ec) *
+					sizeof(VOID *));
+	if (!LWPMU_DEVICE_PMU_register_data(&devices[cur_device])) {
+		LWPMU_DEVICE_ec(&devices[cur_device]) = CONTROL_Free_Memory(
+			LWPMU_DEVICE_ec(&devices[cur_device]));
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for LWPMU_DEVICE_PMU_register_data");
+		return OS_NO_MEM;
+	}
+	LWPMU_DEVICE_em_groups_count(&devices[cur_device]) = 0;
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Configure_events(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Copies one group of events into kernel space at
+ * @brief  PMU_register_data[em_groups_count].
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Configure_Events(IOCTL_ARGS arg)
+{
+	U32 group_id;
+	ECB ecb;
+	U32 em_groups_count;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	ec = (EVENT_CONFIG)LWPMU_DEVICE_ec(&devices[cur_device]);
+	em_groups_count = LWPMU_DEVICE_em_groups_count(&devices[cur_device]);
+
+	if (em_groups_count >= EVENT_CONFIG_num_groups(ec)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: EM groups number exceeded initial configuration!");
+		return OS_INVALID;
+	}
+	if (arg->buf_usr_to_drv == NULL ||
+	    arg->len_usr_to_drv < sizeof(ECB_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	ecb = CONTROL_Allocate_Memory(arg->len_usr_to_drv);
+	if (!ecb) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for ecb!");
+		return OS_NO_MEM;
+	}
+	if (copy_from_user(ecb, (void __user *)arg->buf_usr_to_drv, arg->len_usr_to_drv)) {
+		CONTROL_Free_Memory(ecb);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure for ecb data!");
+		return OS_FAULT;
+	}
+	group_id = ECB_group_id(ecb);
+
+	if (group_id >= EVENT_CONFIG_num_groups(ec)) {
+		CONTROL_Free_Memory(ecb);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Group_id is larger than total number of groups!");
+		return OS_INVALID;
+	}
+
+	LWPMU_DEVICE_PMU_register_data(&devices[cur_device])[group_id] = ecb;
+	LWPMU_DEVICE_em_groups_count(&devices[cur_device]) = group_id + 1;
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Configure_events_UNC(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Make a copy of the uncore registers that need to be programmed
+ * @brief  for the next event set used for event multiplexing
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Configure_Events_UNC(IOCTL_ARGS arg)
+{
+	VOID **PMU_register_data_unc;
+	S32 em_groups_count_unc;
+	ECB ecb;
+	EVENT_CONFIG ec_unc;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 group_id = 0;
+	ECB in_ecb = NULL;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	em_groups_count_unc =
+		LWPMU_DEVICE_em_groups_count(&devices[cur_device]);
+	PMU_register_data_unc =
+		LWPMU_DEVICE_PMU_register_data(&devices[cur_device]);
+	ec_unc = LWPMU_DEVICE_ec(&devices[cur_device]);
+	pcfg_unc = LWPMU_DEVICE_pcfg(&devices[cur_device]);
+
+	if (pcfg_unc == NULL || ec_unc == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Pcfg_unc or ec_unc NULL!");
+		return OS_INVALID;
+	}
+
+	if (em_groups_count_unc >= (S32)EVENT_CONFIG_num_groups_unc(ec_unc)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Uncore EM groups number exceeded initial configuration!");
+		return OS_INVALID;
+	}
+	if (arg->buf_usr_to_drv == NULL ||
+	    arg->len_usr_to_drv < sizeof(ECB_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	in_ecb = CONTROL_Allocate_Memory(arg->len_usr_to_drv);
+	if (!in_ecb) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for uncore ecb!");
+		return OS_NO_MEM;
+	}
+	if (copy_from_user(in_ecb, (void __user *)arg->buf_usr_to_drv, arg->len_usr_to_drv)) {
+		CONTROL_Free_Memory(in_ecb);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for uncore ecb data!");
+		return OS_FAULT;
+	}
+	group_id = ECB_group_id(in_ecb);
+
+	if (group_id >= EVENT_CONFIG_num_groups_unc(ec_unc)) {
+		CONTROL_Free_Memory(in_ecb);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Group_id is larger than total number of groups!");
+		return OS_INVALID;
+	}
+
+	PMU_register_data_unc[group_id] = in_ecb;
+	// at this point, we know the number of uncore events for this device,
+	// so allocate the results buffer per thread for uncore only for SEP event based uncore counting
+	ecb = PMU_register_data_unc[group_id];
+	if (ecb == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Encountered NULL ECB!");
+		return OS_INVALID;
+	}
+	LWPMU_DEVICE_num_events(&devices[cur_device]) = ECB_num_events(ecb);
+	LWPMU_DEVICE_em_groups_count(&devices[cur_device]) = group_id + 1;
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Set_Sample_Descriptors(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Set the number of descriptor groups in the global state node.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Set_Sample_Descriptors(IOCTL_ARGS arg)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+	if (arg->len_usr_to_drv != sizeof(U32) || arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (Unknown size of Sample Descriptors!).");
+		return OS_INVALID;
+	}
+
+	desc_count = 0;
+	if (copy_from_user(&GLOBAL_STATE_num_descriptors(driver_state),
+			(void __user *)arg->buf_usr_to_drv, sizeof(U32))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure");
+		return OS_FAULT;
+	}
+
+	desc_data = CONTROL_Allocate_Memory(
+		(size_t)GLOBAL_STATE_num_descriptors(driver_state) *
+		sizeof(VOID *));
+	if (desc_data == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for desc_data!");
+		return OS_NO_MEM;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Configure_Descriptors(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ * @return OS_STATUS
+ *
+ * @brief Make a copy of the descriptors that need to be read in order
+ * @brief to configure a sample record.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Configure_Descriptors(IOCTL_ARGS arg)
+{
+	U32 uncopied;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (desc_count >= GLOBAL_STATE_num_descriptors(driver_state)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Descriptor groups number exceeded initial configuration!");
+		return OS_INVALID;
+	}
+
+	if (arg->len_usr_to_drv == 0 || arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arg value!");
+		return OS_INVALID;
+	}
+	if (desc_data == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("NULL desc_data!");
+		return OS_INVALID;
+	}
+	//
+	// First things first: Make a copy of the data for global use.
+	//
+	desc_data[desc_count] = CONTROL_Allocate_Memory(arg->len_usr_to_drv);
+	uncopied = copy_from_user(desc_data[desc_count], (void __user *)arg->buf_usr_to_drv,
+				  arg->len_usr_to_drv);
+	if (uncopied > 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Unable to copy desc_data from user!");
+		return OS_NO_MEM;
+	}
+	SEP_DRV_LOG_TRACE("Added descriptor # %d.", desc_count);
+	desc_count++;
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_LBR_Info(IOCTL_ARGS arg)
+ *
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ * @return OS_STATUS
+ *
+ * @brief Make a copy of the LBR information that is passed in.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_LBR_Info(IOCTL_ARGS arg)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (cur_pcfg == NULL || DEV_CONFIG_collect_lbrs(cur_pcfg) == FALSE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"LBR capture has not been configured!");
+		return OS_INVALID;
+	}
+
+	if (arg->len_usr_to_drv == 0 || arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_INVALID;
+	}
+
+	//
+	// First things first: Make a copy of the data for global use.
+	//
+
+	LWPMU_DEVICE_lbr(&devices[cur_device]) =
+		CONTROL_Allocate_Memory((int)arg->len_usr_to_drv);
+	if (!LWPMU_DEVICE_lbr(&devices[cur_device])) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Memory allocation failure for lbr!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(LWPMU_DEVICE_lbr(&devices[cur_device]),
+			   (void __user *)arg->buf_usr_to_drv, arg->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for lbr struct!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+#if !defined(DRV_SEP_ACRN_ON)
+#define CR4_PCE 0x00000100 //Performance-monitoring counter enable RDPMC
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Set_CR4_PCE_Bit(PVOID param)
+ *
+ * @param param - dummy parameter
+ *
+ * @return NONE
+ *
+ * @brief Set CR4's PCE bit on the logical processor
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Set_CR4_PCE_Bit(PVOID param)
+{
+	U32 this_cpu;
+#if defined(DRV_IA32)
+	U32 prev_CR4_value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// remember if RDPMC bit previously set
+	// and then enabled it
+	__asm__("movl %%cr4, %%eax\n\t"
+		"movl %%eax, %0\n\t"
+		"orl  %1, %%eax\n\t"
+		"movl %%eax, %%cr4\n\t"
+		: "=irg"(prev_CR4_value)
+		: "irg"(CR4_PCE)
+		: "eax");
+#endif
+#if defined(DRV_EM64T)
+	U64 prev_CR4_value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// remember if RDPMC bit previously set
+	// and then enabled it
+	__asm__("movq %%cr4, %%rax\n\t"
+		"movq %%rax, %0\n\t"
+		"orq  %1, %%rax\n\t"
+		"movq %%rax, %%cr4"
+		: "=irg"(prev_CR4_value)
+		: "irg"(CR4_PCE)
+		: "rax");
+#endif
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+
+	// if bit RDPMC bit was set before,
+	// set flag for when we clear it
+	if (prev_CR4_value & CR4_PCE) {
+		prev_set_CR4[this_cpu] = 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void lwpmudrv_Clear_CR4_PCE_Bit(PVOID param)
+ *
+ * @param param - dummy parameter
+ *
+ * @return NONE
+ *
+ * @brief ClearSet CR4's PCE bit on the logical processor
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Clear_CR4_PCE_Bit(PVOID param)
+{
+	U32 this_cpu;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+
+	// only clear the CR4 bit if it wasn't set
+	// before we started
+	if (prev_set_CR4 && !prev_set_CR4[this_cpu]) {
+#if defined(DRV_IA32)
+		__asm__("movl %%cr4, %%eax\n\t"
+			"andl %0, %%eax\n\t"
+			"movl %%eax, %%cr4\n"
+			:
+			: "irg"(~CR4_PCE)
+			: "eax");
+#endif
+#if defined(DRV_EM64T)
+		__asm__("movq %%cr4, %%rax\n\t"
+			"andq %0, %%rax\n\t"
+			"movq %%rax, %%cr4\n"
+			:
+			: "irg"(~CR4_PCE)
+			: "rax");
+#endif
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Start(void)
+ *
+ * @param none
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the LWPMU_IOCTL_START call.
+ * @brief  Set up the OS hooks for process/thread/load notifications.
+ * @brief  Write the initial set of MSRs.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Start(void)
+{
+	OS_STATUS status = OS_SUCCESS;
+#if !defined(CONFIG_PREEMPT_COUNT) && !defined(DRV_SEP_ACRN_ON)
+	U32 cpu_num;
+#endif
+#if defined(DRV_SEP_ACRN_ON)
+	struct profiling_control *control = NULL;
+	S32 i;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!CHANGE_DRIVER_STATE(STATE_BIT_IDLE, DRV_STATE_RUNNING)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (drv_cfg == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("NULL drv_cfg!");
+		return OS_INVALID;
+	}
+
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		if (DRV_CONFIG_start_paused(drv_cfg)) {
+			CHANGE_DRIVER_STATE(STATE_BIT_RUNNING,
+					    DRV_STATE_PAUSED);
+		}
+		SEP_DRV_LOG_FLOW_OUT("[PCL enabled] Early return value: %d",
+				     status);
+		return status;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	prev_set_CR4 = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U8));
+	CONTROL_Invoke_Parallel(lwpmudrv_Set_CR4_PCE_Bit, (PVOID)(size_t)0);
+#endif
+
+#if !defined(CONFIG_PREEMPT_COUNT) && !defined(DRV_SEP_ACRN_ON)
+	atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+	init_waitqueue_head(&read_tsc_now);
+	CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info, (PVOID)(size_t)0);
+#endif
+
+#if !defined(CONFIG_PREEMPT_COUNT) && !defined(DRV_SEP_ACRN_ON)
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		if (CPU_STATE_offlined(&pcb[cpu_num])) {
+			cpu_tsc[cpu_num] = cpu_tsc[0];
+		}
+	}
+#else
+	UTILITY_Read_TSC(&cpu_tsc[0]);
+#endif
+
+	if (DRV_CONFIG_start_paused(drv_cfg)) {
+		CHANGE_DRIVER_STATE(STATE_BIT_RUNNING, DRV_STATE_PAUSED);
+	} else {
+#if !defined(DRV_SEP_ACRN_ON)
+		CONTROL_Invoke_Parallel(lwpmudrv_Resume_Op, NULL);
+#else
+		control = (struct profiling_control *)CONTROL_Allocate_Memory(
+			sizeof(struct profiling_control));
+		if (control == NULL) {
+			SEP_PRINT_ERROR(
+				"lwpmudrv_Start: Unable to allocate memory\n");
+			return OS_NO_MEM;
+		}
+		memset(control, 0, sizeof(struct profiling_control));
+
+		BUG_ON(!virt_addr_valid(control));
+		control->collector_id = COLLECTOR_SEP;
+
+		acrn_hypercall2(HC_PROFILING_OPS, PROFILING_GET_CONTROL_SWITCH,
+				virt_to_phys(control));
+
+		SEP_PRINT_DEBUG("ACRN profiling collection running 0x%llx\n",
+				control->switches);
+
+		if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+			control->switches |= (1 << CORE_PMU_SAMPLING) |
+					     (1 << VM_SWITCH_TRACING);
+			if (DEV_CONFIG_collect_lbrs(cur_pcfg)) {
+				control->switches |= (1 << LBR_PMU_SAMPLING);
+			}
+		} else {
+			control->switches |= (1 << CORE_PMU_COUNTING);
+		}
+
+		acrn_hypercall2(HC_PROFILING_OPS, PROFILING_SET_CONTROL_SWITCH,
+				virt_to_phys(control));
+		control = CONTROL_Free_Memory(control);
+
+		if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+			char kthread_name[MAXNAMELEN];
+			for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state);
+			     i++) {
+				snprintf(kthread_name, MAXNAMELEN, "%s_%d",
+					 "SEPDRV_BUFFER_HANDLER", i);
+				acrn_buffer_handler[i] =
+					kthread_create(PMI_Buffer_Handler,
+						       (VOID *)(size_t)i,
+						       kthread_name);
+				if (acrn_buffer_handler[i]) {
+					wake_up_process(acrn_buffer_handler[i]);
+				}
+			}
+			SEP_PRINT_DEBUG(
+				"lwpmudrv_Prepare_Stop: flushed all the remaining buffer\n");
+		}
+#endif
+
+#if defined(BUILD_CHIPSET)
+		if (DRV_CONFIG_enable_chipset(drv_cfg) && cs_dispatch != NULL &&
+		    cs_dispatch->start_chipset != NULL) {
+			cs_dispatch->start_chipset();
+		}
+#endif
+
+		EVENTMUX_Start();
+		lwpmudrv_Dump_Tracer("start", 0);
+
+#if defined(BUILD_GFX)
+		SEP_DRV_LOG_TRACE("Enable_gfx=%d.",
+				  (int)DRV_CONFIG_enable_gfx(drv_cfg));
+		if (DRV_CONFIG_enable_gfx(drv_cfg)) {
+			GFX_Start();
+		}
+#endif
+		if (unc_buf_init) {
+			lwpmudrv_Uncore_Start_Timer();
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static OS_STATUS lwpmudrv_Cleanup_Op(void)
+ *
+ * @param - none
+ *
+ * @return OS_STATUS
+ *
+ * @brief Clean up registers after collection
+ *
+ * <I>Special Notes</I>
+ */
+static VOID lwpmudrv_Cleanup_Op(PVOID param)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (dispatch != NULL && dispatch->cleanup != NULL) {
+		dispatch->cleanup(&dev_idx);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn lwpmudrv_Prepare_Stop();
+ *
+ * @param        NONE
+ * @return       OS_STATUS
+ *
+ * @brief  Local function that handles the DRV_OPERATION_STOP call.
+ * @brief  Cleans up the interrupt handler.
+ */
+static OS_STATUS lwpmudrv_Prepare_Stop(void)
+{
+	S32 i;
+	S32 done = FALSE;
+	S32 cpu_num;
+#if defined(DRV_SEP_ACRN_ON)
+	struct profiling_control *control = NULL;
+#endif
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+		if (!CHANGE_DRIVER_STATE(STATE_BIT_RUNNING | STATE_BIT_PAUSED,
+					 DRV_STATE_PREPARE_STOP)) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Unexpected driver state.");
+			return OS_INVALID;
+		}
+	} else {
+		SEP_DRV_LOG_WARNING("Abnormal termination path.");
+	}
+
+	if (drv_cfg == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("drv_cfg is NULL!");
+		return OS_INVALID;
+	}
+
+	if (DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		SEP_DRV_LOG_FLOW_OUT("Success: using PCL");
+		return OS_SUCCESS;
+	}
+
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		CPU_STATE_accept_interrupt(&pcb[i]) = 0;
+	}
+	while (!done) {
+		done = TRUE;
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			if (atomic_read(&CPU_STATE_in_interrupt(&pcb[i]))) {
+				done = FALSE;
+			}
+		}
+	}
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Pause_Op, NULL);
+#else
+	control = (struct profiling_control *)CONTROL_Allocate_Memory(
+		sizeof(struct profiling_control));
+	if (control == NULL) {
+		SEP_PRINT_ERROR("lwpmudrv_Start: Unable to allocate memory\n");
+		return OS_NO_MEM;
+	}
+	memset(control, 0, sizeof(struct profiling_control));
+
+	BUG_ON(!virt_addr_valid(control));
+	control->collector_id = COLLECTOR_SEP;
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_GET_CONTROL_SWITCH,
+			virt_to_phys(control));
+
+	SEP_PRINT_DEBUG("ACRN profiling collection running 0x%llx\n",
+			control->switches);
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		control->switches &=
+			~((1 << CORE_PMU_SAMPLING) | (1 << VM_SWITCH_TRACING));
+	} else {
+		control->switches &= ~(1 << CORE_PMU_COUNTING);
+	}
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_SET_CONTROL_SWITCH,
+			virt_to_phys(control));
+	control = CONTROL_Free_Memory(control);
+#endif
+
+	SEP_DRV_LOG_TRACE("Outside of all interrupts.");
+
+#if defined(BUILD_CHIPSET)
+	if (DRV_CONFIG_enable_chipset(drv_cfg) && cs_dispatch != NULL &&
+	    cs_dispatch->stop_chipset != NULL) {
+		cs_dispatch->stop_chipset();
+	}
+#endif
+
+#if defined(BUILD_GFX)
+	SEP_DRV_LOG_TRACE("Enable_gfx=%d.",
+			  (int)DRV_CONFIG_enable_gfx(drv_cfg));
+	if (DRV_CONFIG_enable_gfx(drv_cfg)) {
+		GFX_Stop();
+	}
+#endif
+
+	if (unc_buf_init) {
+		lwpmudrv_Uncore_Stop_Timer();
+	}
+
+	if (drv_cfg == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("drv_cfg is NULL!");
+		return OS_INVALID;
+	}
+
+	/*
+	 * Clean up all the control registers
+	 */
+#if !defined(DRV_SEP_ACRN_ON)
+	CONTROL_Invoke_Parallel(lwpmudrv_Cleanup_Op, (VOID *)NULL);
+#else
+	lwpmudrv_Cleanup_Op(NULL);
+#endif
+
+	SEP_DRV_LOG_TRACE("Cleanup finished.");
+	lwpmudrv_Free_Restore_Buffer();
+
+#if !defined(DRV_SEP_ACRN_ON)
+	if (prev_set_CR4) {
+		CONTROL_Invoke_Parallel(lwpmudrv_Clear_CR4_PCE_Bit,
+					(VOID *)(size_t)0);
+		prev_set_CR4 = CONTROL_Free_Memory(prev_set_CR4);
+	}
+#endif
+
+#if defined(BUILD_CHIPSET)
+	if (DRV_CONFIG_enable_chipset(drv_cfg) && cs_dispatch &&
+	    cs_dispatch->fini_chipset) {
+		cs_dispatch->fini_chipset();
+	}
+#endif
+
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		SEP_DRV_LOG_TRACE(
+			"# of PMU interrupts via NMI triggered on cpu%d: %u.",
+			cpu_num, CPU_STATE_nmi_handled(&pcb[cpu_num]));
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success.");
+	return OS_SUCCESS;
+}
+
+/*
+ * @fn lwpmudrv_Finish_Stop();
+ *
+ * @param  NONE
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the DRV_OPERATION_STOP call.
+ * @brief  Cleans up the interrupt handler.
+ */
+static OS_STATUS lwpmudrv_Finish_Stop(void)
+{
+	OS_STATUS status = OS_SUCCESS;
+	S32 idx, cpu;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+		if (!CHANGE_DRIVER_STATE(STATE_BIT_PREPARE_STOP,
+					 DRV_STATE_STOPPED)) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Unexpected driver state!");
+			return OS_FAULT;
+		}
+	} else {
+		SEP_DRV_LOG_WARNING("Abnormal termination path.");
+	}
+
+	if (drv_cfg == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("drv_cfg is NULL!");
+		return OS_INVALID;
+	}
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+#if !defined(DRV_SEP_ACRN_ON)
+			CONTROL_Invoke_Parallel(PEBS_Flush_Buffer, NULL);
+#endif
+			/*
+		 *  Make sure that the module buffers are not deallocated and that the module flush
+		 *  thread has not been terminated.
+		 */
+			if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+				status = LINUXOS_Enum_Process_Modules(TRUE);
+			}
+			OUTPUT_Flush();
+		}
+		/*
+	 * Clean up the interrupt handler via the IDT
+	 */
+#if !defined(DRV_SEP_ACRN_ON)
+		CPUMON_Remove_Cpuhooks();
+		PEBS_Destroy();
+#else
+		for (cpu = 0; cpu < GLOBAL_STATE_num_cpus(driver_state);
+		     cpu++) {
+			sbuf_share_setup(cpu, ACRN_SEP, NULL);
+			sbuf_free(samp_buf_per_cpu[cpu]);
+		}
+		samp_buf_per_cpu = CONTROL_Free_Memory(samp_buf_per_cpu);
+#endif
+		EVENTMUX_Destroy();
+	}
+
+	if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+		if (interrupt_counts) {
+			for (cpu = 0; cpu < GLOBAL_STATE_num_cpus(driver_state);
+			     cpu++) {
+				for (idx = 0;
+				     idx < DRV_CONFIG_num_events(drv_cfg);
+				     idx++) {
+					SEP_DRV_LOG_TRACE(
+						"Interrupt count: CPU %d, event %d = %lld.",
+						cpu, idx,
+						interrupt_counts
+							[cpu * DRV_CONFIG_num_events(
+								       drv_cfg) +
+							 idx]);
+				}
+			}
+		}
+	}
+
+	read_counter_info = CONTROL_Free_Memory(read_counter_info);
+	prev_counter_data = CONTROL_Free_Memory(prev_counter_data);
+	emon_buffer_driver_helper =
+		CONTROL_Free_Memory(emon_buffer_driver_helper);
+	lwpmudrv_Dump_Tracer("stop", 0);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Normalized_TSC(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Return the current value of the normalized TSC.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Normalized_TSC(IOCTL_ARGS arg)
+{
+	U64 tsc = 0;
+	U64 this_cpu = 0;
+	size_t size_to_copy = sizeof(U64);
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (arg->len_drv_to_usr != size_to_copy ||
+	    arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Invalid arguments!");
+		return OS_INVALID;
+	}
+
+	preempt_disable();
+	UTILITY_Read_TSC(&tsc);
+	this_cpu = CONTROL_THIS_CPU();
+	tsc -= TSC_SKEW(CONTROL_THIS_CPU());
+	preempt_enable();
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+	if (drv_cfg && DRV_CONFIG_use_pcl(drv_cfg) == TRUE) {
+		preempt_disable();
+		tsc = cpu_clock(this_cpu);
+		preempt_enable();
+	} else {
+#endif
+		tsc -= TSC_SKEW(this_cpu);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 32)
+	}
+#endif
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, (VOID *)&tsc, size_to_copy)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+	lwpmudrv_Dump_Tracer("marker", tsc);
+
+	SEP_DRV_LOG_TRACE_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Num_Cores(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Quickly return the (total) number of cpus in the system.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Num_Cores(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+	S32 num = GLOBAL_STATE_num_cpus(driver_state);
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr != sizeof(S32) || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE("Num_Cores is %d, buf_usr_to_drv is 0x%p.", num,
+			  arg->buf_drv_to_usr);
+	status = put_user(num, (S32 __user*)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Set_CPU_Mask(PVOID buf_usr_to_drv, U32 len_usr_to_drv)
+ *
+ * @param buf_usr_to_drv   - pointer to the CPU mask buffer
+ * @param len_usr_to_drv   - size of the CPU mask buffer
+ *
+ * @return OS_STATUS
+ *
+ * @brief  process the CPU mask as requested by the user
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Set_CPU_Mask(PVOID buf_usr_to_drv,
+				       size_t len_usr_to_drv)
+{
+	U32 cpu_count = 0;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (len_usr_to_drv == 0 || buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"len_usr_to_drv == 0 or buf_usr_to_drv is NULL!");
+		return OS_INVALID;
+	}
+
+	cpu_mask_bits = CONTROL_Allocate_Memory((int)len_usr_to_drv);
+	if (!cpu_mask_bits) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for cpu_mask_bits!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(cpu_mask_bits, (void __user *)buf_usr_to_drv,
+			   (int)len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	for (cpu_count = 0;
+	     cpu_count < (U32)GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_count++) {
+		CPU_STATE_accept_interrupt(&pcb[cpu_count]) =
+			cpu_mask_bits[cpu_count] ? 1 : 0;
+		CPU_STATE_initial_mask(&pcb[cpu_count]) =
+			cpu_mask_bits[cpu_count] ? 1 : 0;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_KERNEL_CS(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Return the value of the Kernel symbol KERNEL_CS.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_KERNEL_CS(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+	S32 num = __KERNEL_CS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr != sizeof(S32) || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE("__KERNEL_CS is %d, buf_usr_to_drv is 0x%p.", num,
+			  arg->buf_drv_to_usr);
+	status = put_user(num, (S32 __user *)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+/*
+ * @fn lwpmudrv_Set_UID
+ *
+ * @param     IN   arg      - pointer to the output buffer
+ * @return   OS_STATUS
+ *
+ * @brief  Receive the value of the UID of the collector process.
+ */
+static OS_STATUS lwpmudrv_Set_UID(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_usr_to_drv != sizeof(uid_t) ||
+	    arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	status = get_user(uid, (S32 __user *)arg->buf_usr_to_drv);
+	SEP_DRV_LOG_TRACE("Uid is %d.", uid);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_TSC_Skew_Info(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ * @brief  Return the current value of the TSC skew data
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_TSC_Skew_Info(IOCTL_ARGS arg)
+{
+	S64 *skew_array;
+	size_t skew_array_len;
+	S32 i;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	skew_array_len = GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64);
+	if (arg->len_drv_to_usr < skew_array_len ||
+	    arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Input buffer too small or NULL!");
+		return OS_INVALID;
+	}
+
+	if (!DRV_CONFIG_enable_cp_mode(drv_cfg) &&
+	    GET_DRIVER_STATE() != DRV_STATE_STOPPED) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: cp_mode not enabled and driver is not STOPPED!");
+		return OS_IN_PROGRESS;
+	}
+
+	SEP_DRV_LOG_TRACE("Dispatched with len_drv_to_usr=%lld.",
+			  arg->len_drv_to_usr);
+
+	skew_array = CONTROL_Allocate_Memory(skew_array_len);
+	if (skew_array == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for skew_array!");
+		return OS_NO_MEM;
+	}
+
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		skew_array[i] = TSC_SKEW(i);
+	}
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, skew_array, skew_array_len)) {
+		skew_array = CONTROL_Free_Memory(skew_array);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for skew_array!");
+		return OS_FAULT;
+	}
+
+	skew_array = CONTROL_Free_Memory(skew_array);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Collect_Sys_Config(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that handles the COLLECT_SYS_CONFIG call.
+ * @brief  Builds and collects the SYS_INFO data needed.
+ * @brief  Writes the result into the argument.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Collect_Sys_Config(IOCTL_ARGS arg)
+{
+	OS_STATUS status = OS_SUCCESS;
+	U32 num;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	num = SYS_INFO_Build();
+
+	if (arg->len_drv_to_usr < sizeof(S32) || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE("Size of sys info is %d.", num);
+	status = put_user(num, (S32 __user *)arg->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Sys_Config(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Return the current value of the normalized TSC.
+ *
+ * @brief  Transfers the VTSA_SYS_INFO data back to the abstraction layer.
+ * @brief  The buf_usr_to_drv should have enough space to handle the transfer.
+ */
+static OS_STATUS lwpmudrv_Sys_Config(IOCTL_ARGS arg)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr == 0 || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	SYS_INFO_Transfer(arg->buf_drv_to_usr, arg->len_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Samp_Read_Num_Of_Core_Counters(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read memory mapped i/o physical location
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Samp_Read_Num_Of_Core_Counters(IOCTL_ARGS arg)
+{
+	U64 rax, rbx, rcx, rdx, num_basic_functions;
+	U32 val = 0;
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr == 0 || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	UTILITY_Read_Cpuid(0x0, &num_basic_functions, &rbx, &rcx, &rdx);
+
+	if (num_basic_functions >= 0xA) {
+		UTILITY_Read_Cpuid(0xA, &rax, &rbx, &rcx, &rdx);
+		val = ((U32)(rax >> 8)) & 0xFF;
+	}
+	status = put_user(val, (U32 __user *)arg->buf_drv_to_usr);
+	SEP_DRV_LOG_TRACE("Num of counter is %d.", val);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+#if defined(BUILD_CHIPSET)
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static DRV_BOOL lwpmudrv_Is_Physical_Address_Free(U32 physical_addrss)
+ *
+ * @param physical_address - physical address
+ *
+ * @return DRV_BOOL
+ *
+ * @brief  Check if physical address is available
+ *
+ * <I>Special Notes</I>
+ */
+static DRV_BOOL lwpmudrv_Is_Physical_Address_Free(U32 physical_address)
+{
+	U32 new_value;
+	U32 test_value = 0;
+	U32 value = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"FALSE: driver state is not IDLE!");
+		return FALSE;
+	}
+	if (physical_address == 0) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT("FALSE: is NULL!");
+		return FALSE;
+	}
+
+	// First attempt read
+	//
+	PCI_Read_From_Memory_Address(physical_address, &value);
+
+	// Value must be 0xFFFFFFFFF or there is NO chance
+	// that this memory location is available.
+	//
+	if (value != 0xFFFFFFFF) {
+		SEP_DRV_LOG_TRACE_OUT("FALSE: value is not 0xFFFFFFFF!");
+		return FALSE;
+	}
+
+	//
+	// Try to write a bit to a zero (this probably
+	// isn't too safe, but this is just for testing)
+	//
+	new_value = 0xFFFFFFFE;
+	PCI_Write_To_Memory_Address(physical_address, new_value);
+	PCI_Read_From_Memory_Address(physical_address, &test_value);
+
+	// Write back original
+	PCI_Write_To_Memory_Address(physical_address, value);
+
+	if (new_value == test_value) {
+		// The write appeared to change the
+		// memory, it must be mapped already
+		//
+		SEP_DRV_LOG_TRACE_OUT("FALSE: appears to be mapped already!");
+		return FALSE;
+	}
+
+	if (test_value == 0xFFFFFFFF) {
+		// The write did not change the bit, so
+		// apparently, this memory must not be mapped
+		// to anything.
+		//
+		SEP_DRV_LOG_TRACE_OUT("TRUE: appears not to be mapped!");
+		return TRUE;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("FALSE: Odd case!");
+	return FALSE;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Samp_Find_Physical_Address(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Find a free physical address
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Samp_Find_Physical_Address(IOCTL_ARGS arg)
+{
+	CHIPSET_PCI_SEARCH_ADDR_NODE user_addr;
+	CHIPSET_PCI_SEARCH_ADDR search_addr;
+	U32 addr;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	search_addr = (CHIPSET_PCI_SEARCH_ADDR)arg->buf_usr_to_drv;
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE.");
+		return OS_IN_PROGRESS;
+	}
+
+	if (arg->len_drv_to_usr == 0 || arg->buf_drv_to_usr == NULL ||
+	    arg->len_usr_to_drv == 0 || arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_INVALID;
+	}
+
+	if (!search_addr) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Search_addr is NULL!");
+		return OS_FAULT;
+	}
+
+	if (!access_ok(VERIFY_WRITE, (void __user *)search_addr,
+		       sizeof(CHIPSET_PCI_SEARCH_ADDR_NODE))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Access not OK!");
+		return OS_FAULT;
+	}
+
+	if (copy_from_user(&user_addr, (void __user *)search_addr,
+			   sizeof(CHIPSET_PCI_SEARCH_ADDR_NODE))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Memory copy failure for search_addr!");
+		return OS_FAULT;
+	}
+
+	if (CHIPSET_PCI_SEARCH_ADDR_start(&user_addr) >
+	    CHIPSET_PCI_SEARCH_ADDR_stop(&user_addr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"SEARCH_ADDR_start > SEARCH_ADDR_stop!");
+		return OS_INVALID;
+	}
+
+	CHIPSET_PCI_SEARCH_ADDR_address(&user_addr) = 0;
+
+	for (addr = CHIPSET_PCI_SEARCH_ADDR_start(&user_addr);
+	     addr <= CHIPSET_PCI_SEARCH_ADDR_stop(&user_addr);
+	     addr += CHIPSET_PCI_SEARCH_ADDR_increment(&user_addr)) {
+		SEP_DRV_LOG_TRACE("Addr=%x:", addr);
+		if (lwpmudrv_Is_Physical_Address_Free(addr)) {
+			CHIPSET_PCI_SEARCH_ADDR_address(&user_addr) = addr;
+			break;
+		}
+	}
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, (VOID *)&user_addr,
+			 sizeof(CHIPSET_PCI_SEARCH_ADDR_NODE))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Memory copy failure for user_addr!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Samp_Read_PCI_Config(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Read the PCI Configuration Space
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Samp_Read_PCI_Config(IOCTL_ARGS arg)
+{
+	CHIPSET_PCI_CONFIG rd_pci = NULL;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (arg->len_drv_to_usr == 0 || arg->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_FAULT;
+	}
+
+	rd_pci = CONTROL_Allocate_Memory(arg->len_drv_to_usr);
+	if (rd_pci == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for rd_pci!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(rd_pci, (void __user *)arg->buf_usr_to_drv,
+			   sizeof(CHIPSET_PCI_CONFIG_NODE))) {
+		CONTROL_Free_Memory(rd_pci);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure for rd_pci!");
+		return OS_FAULT;
+	}
+
+	CHIPSET_PCI_CONFIG_value(rd_pci) =
+		PCI_Read_U32(CHIPSET_PCI_CONFIG_bus(rd_pci),
+			     CHIPSET_PCI_CONFIG_device(rd_pci),
+			     CHIPSET_PCI_CONFIG_function(rd_pci),
+			     CHIPSET_PCI_CONFIG_offset(rd_pci));
+
+	if (copy_to_user((void __user *)arg->buf_drv_to_usr, (VOID *)rd_pci,
+			 sizeof(CHIPSET_PCI_CONFIG_NODE))) {
+		CONTROL_Free_Memory(rd_pci);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure for rd_pci!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_TRACE("Value at this PCI address:0x%x.",
+			  CHIPSET_PCI_CONFIG_value(rd_pci));
+
+	CONTROL_Free_Memory(rd_pci);
+
+	SEP_DRV_LOG_FLOW_OUT("Success.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Samp_Write_PCI_Config(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Write to the PCI Configuration Space
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Samp_Write_PCI_Config(IOCTL_ARGS arg)
+{
+	CHIPSET_PCI_CONFIG wr_pci = NULL;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	// the following allows "sep -el -pc" to work, since the command must access the
+	// the driver ioctls before driver is used for a collection
+
+	if (!DRIVER_STATE_IN(GET_DRIVER_STATE(),
+			     STATE_BIT_UNINITIALIZED | STATE_BIT_IDLE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: Driver state is not IDLE or UNINITIALIZED!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (arg->len_usr_to_drv == 0 || arg->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Invalid arguments!");
+		return OS_INVALID;
+	}
+
+	wr_pci = CONTROL_Allocate_Memory(arg->len_usr_to_drv);
+	if (wr_pci == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for wr_pci!");
+		return OS_NO_MEM;
+	}
+	if (copy_from_user(wr_pci, (void __user *)arg->buf_usr_to_drv,
+			   sizeof(CHIPSET_PCI_CONFIG_NODE))) {
+		CONTROL_Free_Memory(wr_pci);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure for wr_pci!");
+		return OS_FAULT;
+	}
+
+	PCI_Write_U32(CHIPSET_PCI_CONFIG_bus(wr_pci),
+		      CHIPSET_PCI_CONFIG_device(wr_pci),
+		      CHIPSET_PCI_CONFIG_function(wr_pci),
+		      CHIPSET_PCI_CONFIG_offset(wr_pci),
+		      CHIPSET_PCI_CONFIG_value(wr_pci));
+
+	CONTROL_Free_Memory(wr_pci);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Samp_Chipset_Init(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Initialize the chipset cnfiguration
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Samp_Chipset_Init(IOCTL_ARGS arg)
+{
+	PVOID buf_usr_to_drv;
+	U32 len_usr_to_drv;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	buf_usr_to_drv = arg->buf_usr_to_drv;
+	len_usr_to_drv = arg->len_usr_to_drv;
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+
+	if (buf_usr_to_drv == NULL || len_usr_to_drv == 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments!");
+		return OS_INVALID;
+	}
+
+	// First things first: Make a copy of the data for global use.
+	pma = CONTROL_Allocate_Memory(len_usr_to_drv);
+
+	if (pma == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for pma!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(pma, (void __user *)buf_usr_to_drv, len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure for pma!");
+		return OS_FAULT;
+	}
+
+#if defined(MY_DEBUG)
+
+	SEP_DRV_LOG_TRACE("Chipset Configuration follows...");
+	SEP_DRV_LOG_TRACE("pma->length=%d.", CHIPSET_CONFIG_length(pma));
+	SEP_DRV_LOG_TRACE("pma->version=%d.",
+			  CHIPSET_CONFIG_major_version(pma));
+	SEP_DRV_LOG_TRACE("pma->processor=%d.", CHIPSET_CONFIG_processor(pma));
+	SEP_DRV_LOG_TRACE("pma->mch_chipset=%d.",
+			  CHIPSET_CONFIG_mch_chipset(pma));
+	SEP_DRV_LOG_TRACE("pma->ich_chipset=%d.",
+			  CHIPSET_CONFIG_ich_chipset(pma));
+	SEP_DRV_LOG_TRACE("pma->gmch_chipset=%d.",
+			  CHIPSET_CONFIG_gmch_chipset(pma));
+	SEP_DRV_LOG_TRACE("pma->mother_board_time=%d.",
+			  CHIPSET_CONFIG_motherboard_time(pma));
+	SEP_DRV_LOG_TRACE("pma->host_proc_run=%d.",
+			  CHIPSET_CONFIG_host_proc_run(pma));
+	SEP_DRV_LOG_TRACE("pma->noa_chipset=%d.",
+			  CHIPSET_CONFIG_noa_chipset(pma));
+	SEP_DRV_LOG_TRACE("pma->bnb_chipset=%d.",
+			  CHIPSET_CONFIG_bnb_chipset(pma));
+
+	if (CHIPSET_CONFIG_mch_chipset(pma)) {
+		SEP_DRV_LOG_TRACE("pma->mch->phys_add=0x%llx.",
+				  CHIPSET_SEGMENT_physical_address(
+					  &CHIPSET_CONFIG_mch(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->mch->size=%d.",
+			CHIPSET_SEGMENT_size(&CHIPSET_CONFIG_mch(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->mch->num_counters=%d.",
+			CHIPSET_SEGMENT_num_counters(&CHIPSET_CONFIG_mch(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->mch->total_events=%d.",
+			CHIPSET_SEGMENT_total_events(&CHIPSET_CONFIG_mch(pma)));
+	}
+
+	if (CHIPSET_CONFIG_ich_chipset(pma)) {
+		SEP_DRV_LOG_TRACE("pma->ich->phys_add=0x%llx.",
+				  CHIPSET_SEGMENT_physical_address(
+					  &CHIPSET_CONFIG_ich(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->ich->size=%d.",
+			CHIPSET_SEGMENT_size(&CHIPSET_CONFIG_ich(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->ich->num_counters=%d.",
+			CHIPSET_SEGMENT_num_counters(&CHIPSET_CONFIG_ich(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->ich->total_events=%d.",
+			CHIPSET_SEGMENT_total_events(&CHIPSET_CONFIG_ich(pma)));
+	}
+
+	if (CHIPSET_CONFIG_gmch_chipset(pma)) {
+		SEP_DRV_LOG_TRACE("pma->gmch->phys_add=0x%llx.",
+				  CHIPSET_SEGMENT_physical_address(
+					  &CHIPSET_CONFIG_gmch(pma)));
+		SEP_DRV_LOG_TRACE(
+			"pma->gmch->size=%d.",
+			CHIPSET_SEGMENT_size(&CHIPSET_CONFIG_gmch(pma)));
+		SEP_DRV_LOG_TRACE("pma->gmch->num_counters=%d.",
+				  CHIPSET_SEGMENT_num_counters(
+					  &CHIPSET_CONFIG_gmch(pma)));
+		SEP_DRV_LOG_TRACE("pma->gmch->total_events=%d.",
+				  CHIPSET_SEGMENT_total_events(
+					  &CHIPSET_CONFIG_gmch(pma)));
+		SEP_DRV_LOG_TRACE("pma->gmch->read_register=0x%x.",
+				  CHIPSET_SEGMENT_read_register(
+					  &CHIPSET_CONFIG_gmch(pma)));
+		SEP_DRV_LOG_TRACE("pma->gmch->write_register=0x%x.",
+				  CHIPSET_SEGMENT_write_register(
+					  &CHIPSET_CONFIG_gmch(pma)));
+	}
+
+#endif
+
+	// Set up the global cs_dispatch table
+	cs_dispatch = UTILITY_Configure_Chipset();
+	if (cs_dispatch == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Unknown chipset family!");
+		return OS_INVALID;
+	}
+
+	// Initialize chipset configuration
+	if (cs_dispatch->init_chipset()) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Failed to initialize the chipset!");
+		return OS_INVALID;
+	}
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Platform_Info(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Platform_Info(IOCTL_ARGS args)
+{
+	U32 size = sizeof(DRV_PLATFORM_INFO_NODE);
+	OS_STATUS status = OS_SUCCESS;
+	DRV_PLATFORM_INFO platform_data = NULL;
+	U32 *dispatch_ids = NULL;
+	DISPATCH dispatch_ptr = NULL;
+	U32 i = 0;
+	U32 num_entries; // # dispatch ids to process
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	num_entries =
+		args->len_usr_to_drv / sizeof(U32); // # dispatch ids to process
+
+	platform_data = CONTROL_Allocate_Memory(sizeof(DRV_PLATFORM_INFO_NODE));
+	if (!platform_data) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for platform_data!");
+		return OS_NO_MEM;
+	}
+
+	memset(platform_data, 0, sizeof(DRV_PLATFORM_INFO_NODE));
+	if (args->len_usr_to_drv > 0 && args->buf_usr_to_drv != NULL) {
+		dispatch_ids = CONTROL_Allocate_Memory(args->len_usr_to_drv);
+		if (!dispatch_ids) {
+			platform_data = CONTROL_Free_Memory(platform_data);
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Memory allocation failure for dispatch_ids!");
+			return OS_NO_MEM;
+		}
+
+		status = copy_from_user(dispatch_ids, (void __user *)args->buf_usr_to_drv,
+					args->len_usr_to_drv);
+		if (status) {
+			platform_data = CONTROL_Free_Memory(platform_data);
+			dispatch_ids = CONTROL_Free_Memory(dispatch_ids);
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Memory copy failure for dispatch_ids!");
+			return status;
+		}
+		for (i = 0; i < num_entries; i++) {
+			if (dispatch_ids[i] > 0) {
+				dispatch_ptr =
+					UTILITY_Configure_CPU(dispatch_ids[i]);
+				if (dispatch_ptr &&
+				    dispatch_ptr->platform_info) {
+					dispatch_ptr->platform_info(
+						(PVOID)platform_data);
+				}
+			}
+		}
+		dispatch_ids = CONTROL_Free_Memory(dispatch_ids);
+	} else if (devices) {
+		dispatch_ptr = LWPMU_DEVICE_dispatch(
+			&devices[0]); //placeholder, needs to be fixed
+		if (dispatch_ptr && dispatch_ptr->platform_info) {
+			dispatch_ptr->platform_info((PVOID)platform_data);
+		}
+	}
+
+	if (args->len_drv_to_usr < size || args->buf_drv_to_usr == NULL) {
+		platform_data = CONTROL_Free_Memory(platform_data);
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments!");
+		return OS_FAULT;
+	}
+
+	status = copy_to_user((void __user *)args->buf_drv_to_usr, platform_data, size);
+	platform_data = CONTROL_Free_Memory(platform_data);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void lwpmudrv_Setup_Cpu_Topology (value)
+ *
+ * @brief       Sets up the per CPU state structures
+ *
+ * @param       IOCTL_ARGS args
+ *
+ * @return      OS_STATUS
+ *
+ * <I>Special Notes:</I>
+ *              This function was added to support abstract dll creation.
+ */
+static OS_STATUS lwpmudrv_Setup_Cpu_Topology(IOCTL_ARGS args)
+{
+	S32 cpu_num;
+	S32 iter;
+	DRV_TOPOLOGY_INFO drv_topology, dt;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Skipped: driver state is not IDLE!");
+		return OS_IN_PROGRESS;
+	}
+	if (args->len_usr_to_drv == 0 || args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Topology information has been misconfigured!");
+		return OS_INVALID;
+	}
+
+	drv_topology = CONTROL_Allocate_Memory(args->len_usr_to_drv);
+	if (drv_topology == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for drv_topology!");
+		return OS_NO_MEM;
+	}
+
+	if (copy_from_user(drv_topology,
+			   (void __user *)(args->buf_usr_to_drv),
+			   args->len_usr_to_drv)) {
+		drv_topology = CONTROL_Free_Memory(drv_topology);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for drv_topology!");
+		return OS_FAULT;
+	}
+	/*
+	 *   Topology Initializations
+	 */
+	num_packages = 0;
+	for (iter = 0; iter < GLOBAL_STATE_num_cpus(driver_state); iter++) {
+		dt = &drv_topology[iter];
+		cpu_num = DRV_TOPOLOGY_INFO_cpu_number(dt);
+		CPU_STATE_socket_master(&pcb[cpu_num]) =
+			DRV_TOPOLOGY_INFO_socket_master(dt);
+		num_packages += CPU_STATE_socket_master(&pcb[cpu_num]);
+		CPU_STATE_core_master(&pcb[cpu_num]) =
+			DRV_TOPOLOGY_INFO_core_master(dt);
+		CPU_STATE_thr_master(&pcb[cpu_num]) =
+			DRV_TOPOLOGY_INFO_thr_master(dt);
+		CPU_STATE_core_type(&pcb[cpu_num]) =
+			DRV_TOPOLOGY_INFO_cpu_core_type(dt);
+		CPU_STATE_cpu_module_num(&pcb[cpu_num]) =
+			(U16)DRV_TOPOLOGY_INFO_cpu_module_num(
+				&drv_topology[iter]);
+		CPU_STATE_cpu_module_master(&pcb[cpu_num]) =
+			(U16)DRV_TOPOLOGY_INFO_cpu_module_master(
+				&drv_topology[iter]);
+		CPU_STATE_system_master(&pcb[cpu_num]) = (iter) ? 0 : 1;
+		SEP_DRV_LOG_TRACE("Cpu %d sm = %d cm = %d tm = %d.", cpu_num,
+				  CPU_STATE_socket_master(&pcb[cpu_num]),
+				  CPU_STATE_core_master(&pcb[cpu_num]),
+				  CPU_STATE_thr_master(&pcb[cpu_num]));
+	}
+	drv_topology = CONTROL_Free_Memory(drv_topology);
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Num_Samples(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief       Returns the number of samples collected during the current
+ * @brief       sampling run
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Num_Samples(IOCTL_ARGS args)
+{
+	S32 cpu_num;
+	U64 samples = 0;
+	OS_STATUS status;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_ERROR("PCB was not initialized.");
+		return OS_FAULT;
+	}
+
+	if (args->len_drv_to_usr == 0 || args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Topology information has been misconfigured!");
+		return OS_INVALID;
+	}
+
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		samples += CPU_STATE_num_samples(&pcb[cpu_num]);
+
+		SEP_DRV_LOG_TRACE("Samples for cpu %d = %lld.", cpu_num,
+				  CPU_STATE_num_samples(&pcb[cpu_num]));
+	}
+	SEP_DRV_LOG_TRACE("Total number of samples %lld.", samples);
+	status = put_user(samples, (U64 __user *)args->buf_drv_to_usr);
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Set_Device_Num_Units(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief       Set the number of devices for the sampling run
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Set_Device_Num_Units(IOCTL_ARGS args)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (GET_DRIVER_STATE() != DRV_STATE_IDLE) {
+		SEP_DRV_LOG_FLOW_OUT(
+			"'Success'/Skipped: driver state is not IDLE!");
+		return OS_SUCCESS;
+	}
+
+	if (args->len_usr_to_drv == 0 || args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	if (copy_from_user(&(LWPMU_DEVICE_num_units(&devices[cur_device])),
+			   (void __user *)args->buf_usr_to_drv, sizeof(U32))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for device num units!");
+		return OS_FAULT;
+	}
+	SEP_DRV_LOG_TRACE("LWP: num_units = %d cur_device = %d.",
+			  LWPMU_DEVICE_num_units(&devices[cur_device]),
+			  cur_device);
+	// on to the next device.
+	cur_device++;
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Interval_Counts(IOCTL_ARGS arg)
+ *
+ * @param arg - Pointer to the IOCTL structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief       Returns the number of samples collected during the current
+ * @brief       sampling run
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Interval_Counts(IOCTL_ARGS args)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Not in CP mode!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr == 0 || args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Interval Counts information has been misconfigured!");
+		return OS_INVALID;
+	}
+	if (!interrupt_counts) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Interrupt_counts is NULL!");
+		return OS_INVALID;
+	}
+
+	if (copy_to_user((void __user *)args->buf_drv_to_usr, interrupt_counts,
+			 args->len_drv_to_usr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Set_Uncore_Topology_Info_And_Scan
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Set_Uncore_Topology_Info_And_Scan(IOCTL_ARGS args)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+	SEP_DRV_LOG_FLOW_OUT("Success [but did not do anything]");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Get_Uncore_Topology
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Get_Uncore_Topology(IOCTL_ARGS args)
+{
+	U32 dev;
+	static UNCORE_TOPOLOGY_INFO_NODE req_uncore_topology;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_usr_to_drv is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_usr_to_drv != sizeof(UNCORE_TOPOLOGY_INFO_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_usr_to_drv value)!");
+		return OS_INVALID;
+	}
+	if (args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_drv_to_usr is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr != sizeof(UNCORE_TOPOLOGY_INFO_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_drv_to_usr value)!");
+		return OS_INVALID;
+	}
+
+	memset((char *)&req_uncore_topology, 0,
+	       sizeof(UNCORE_TOPOLOGY_INFO_NODE));
+	if (copy_from_user(&req_uncore_topology, (void __user *)args->buf_usr_to_drv,
+			   args->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	for (dev = 0; dev < MAX_DEVICES; dev++) {
+		// skip if user does not require to scan this device
+		if (!UNCORE_TOPOLOGY_INFO_device_scan(&req_uncore_topology,
+						      dev)) {
+			continue;
+		}
+		// skip if this device has been discovered
+		if (UNCORE_TOPOLOGY_INFO_device_scan(&uncore_topology, dev)) {
+			continue;
+		}
+		memcpy((U8 *)&(UNCORE_TOPOLOGY_INFO_device(&uncore_topology,
+							   dev)),
+		       (U8 *)&(UNCORE_TOPOLOGY_INFO_device(&req_uncore_topology,
+							   dev)),
+		       sizeof(UNCORE_PCIDEV_NODE));
+		UNC_COMMON_PCI_Scan_For_Uncore((VOID *)&dev, dev, NULL);
+	}
+
+	if (copy_to_user((void __user *)args->buf_drv_to_usr, &uncore_topology,
+			 args->len_drv_to_usr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Get_Platform_Topology
+ *
+ * @brief       Reads the MSR or PCI PLATFORM_INFO register if present
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Get_Platform_Topology(IOCTL_ARGS args)
+{
+	U32 dev;
+	U32 num_topology_devices = 0;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_usr_to_drv is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_usr_to_drv != sizeof(PLATFORM_TOPOLOGY_PROG_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_usr_to_drv value)!");
+		return OS_INVALID;
+	}
+	if (args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_drv_to_usr is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr != sizeof(PLATFORM_TOPOLOGY_PROG_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_drv_to_usr value)!");
+		return OS_INVALID;
+	}
+
+	memset((char *)&req_platform_topology_prog_node, 0,
+	       sizeof(PLATFORM_TOPOLOGY_PROG_NODE));
+	if (copy_from_user(&req_platform_topology_prog_node,
+			   (void __user *)args->buf_usr_to_drv, args->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for req_platform_topology_prog_node!");
+		return OS_FAULT;
+	}
+
+	num_topology_devices = PLATFORM_TOPOLOGY_PROG_num_devices(
+		&req_platform_topology_prog_node);
+	for (dev = 0; dev < num_topology_devices; dev++) {
+		//skip if we have populated the register values already
+		if (PLATFORM_TOPOLOGY_PROG_topology_device_prog_valid(
+			    &platform_topology_prog_node, dev)) {
+			continue;
+		}
+		memcpy((U8 *)&(PLATFORM_TOPOLOGY_PROG_topology_device(
+			       &platform_topology_prog_node, dev)),
+		       (U8 *)&(PLATFORM_TOPOLOGY_PROG_topology_device(
+			       &req_platform_topology_prog_node, dev)),
+		       sizeof(PLATFORM_TOPOLOGY_DISCOVERY_NODE));
+		UNC_COMMON_Get_Platform_Topology(dev);
+	}
+
+	if (copy_to_user((void __user *)args->buf_drv_to_usr, &platform_topology_prog_node,
+			 args->len_drv_to_usr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for platform_topology_prog_node!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          OS_STATUS lwpmudrv_Flush(void)
+ *
+ * @brief       Flushes the current contents of sampling buffers
+ *
+ * @param     - none
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ */
+static OS_STATUS lwpmudrv_Flush(void)
+{
+	OS_STATUS status = OS_FAULT;
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (!DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+		SEP_DRV_LOG_ERROR(
+			"The flush failed. Continuous profiling, -cp, is not enabled!");
+		goto clean_return;
+	}
+
+	if (!DRIVER_STATE_IN(GET_DRIVER_STATE(), STATE_BIT_PAUSED)) {
+		SEP_DRV_LOG_ERROR(
+			"The flush failed. The driver should be paused!");
+		goto clean_return;
+	}
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+#if !defined(DRV_SEP_ACRN_ON)
+		CONTROL_Invoke_Parallel(PEBS_Flush_Buffer, NULL);
+#endif
+	}
+
+	LINUXOS_Uninstall_Hooks();
+	LINUXOS_Enum_Process_Modules(TRUE);
+	status = OUTPUT_Flush();
+	LINUXOS_Install_Hooks();
+
+clean_return:
+	SEP_DRV_LOG_FLOW_OUT("Status: %d.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Get_Driver_log
+ *
+ * @brief       Dumps the driver log
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Get_Driver_Log(IOCTL_ARGS args)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_drv_to_usr is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr < sizeof(*DRV_LOG())) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_drv_to_usr value)!");
+		return OS_INVALID;
+	}
+
+	if (copy_to_user((void __user *)args->buf_drv_to_usr, DRV_LOG(), sizeof(*DRV_LOG()))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_DISAMBIGUATE(); // keeps the driver log's footprint unique (has the highest disambiguator field)
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Control_Driver_log
+ *
+ * @brief       Sets or/and gets the driver log's configuration
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Control_Driver_Log(IOCTL_ARGS args)
+{
+	DRV_LOG_CONTROL_NODE log_control;
+	U32 i;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_usr_to_drv is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_usr_to_drv < sizeof(log_control)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_usr_to_drv value)!");
+		return OS_INVALID;
+	}
+
+	if (copy_from_user(&log_control, (void __user *)args->buf_usr_to_drv,
+			   sizeof(log_control))) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+		return OS_FAULT;
+	}
+
+	if (DRV_LOG_CONTROL_command(&log_control) ==
+	    DRV_LOG_CONTROL_COMMAND_ADJUST_VERBOSITY) {
+		for (i = 0; i < DRV_NB_LOG_CATEGORIES; i++) {
+			if (DRV_LOG_CONTROL_verbosities(&log_control)[i] ==
+			    LOG_VERBOSITY_UNSET) {
+				SEP_DRV_LOG_TRACE(
+					"Current verbosity mask for '%s' is 0x%x",
+					(UTILITY_Log_Category_Strings()[i]),
+					((U32)DRV_LOG_VERBOSITY(i)));
+				DRV_LOG_CONTROL_verbosities(&log_control)[i] =
+					DRV_LOG_VERBOSITY(i);
+			} else if (DRV_LOG_CONTROL_verbosities(
+					   &log_control)[i] ==
+				   LOG_VERBOSITY_DEFAULT) {
+				U32 verbosity;
+				switch (i) {
+				case DRV_LOG_CATEGORY_LOAD:
+					verbosity =
+						DRV_LOG_DEFAULT_LOAD_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_INIT:
+					verbosity =
+						DRV_LOG_DEFAULT_INIT_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_DETECTION:
+					verbosity =
+						DRV_LOG_DEFAULT_DETECTION_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_ERROR:
+					verbosity =
+						DRV_LOG_DEFAULT_ERROR_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_STATE_CHANGE:
+					verbosity =
+						DRV_LOG_DEFAULT_STATE_CHANGE_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_MARK:
+					verbosity =
+						DRV_LOG_DEFAULT_MARK_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_DEBUG:
+					verbosity =
+						DRV_LOG_DEFAULT_DEBUG_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_FLOW:
+					verbosity =
+						DRV_LOG_DEFAULT_FLOW_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_ALLOC:
+					verbosity =
+						DRV_LOG_DEFAULT_ALLOC_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_INTERRUPT:
+					verbosity =
+						DRV_LOG_DEFAULT_INTERRUPT_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_TRACE:
+					verbosity =
+						DRV_LOG_DEFAULT_TRACE_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_REGISTER:
+					verbosity =
+						DRV_LOG_DEFAULT_REGISTER_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_NOTIFICATION:
+					verbosity =
+						DRV_LOG_DEFAULT_NOTIFICATION_VERBOSITY;
+					break;
+				case DRV_LOG_CATEGORY_WARNING:
+					verbosity =
+						DRV_LOG_DEFAULT_WARNING_VERBOSITY;
+					break;
+				// default:
+				// 	SEP_DRV_LOG_ERROR(
+				// 		"Unspecified category '%s' when resetting to default!",
+				// 		UTILITY_Log_Category_Strings()
+				// 			[i]);
+				// 	verbosity = LOG_VERBOSITY_NONE;
+				// 	break;
+				}
+				SEP_DRV_LOG_INIT(
+					"Resetting verbosity mask for '%s' from 0x%x to 0x%x.",
+					UTILITY_Log_Category_Strings()[i],
+					(U32)DRV_LOG_VERBOSITY(i), verbosity);
+				DRV_LOG_VERBOSITY(i) = verbosity;
+				DRV_LOG_CONTROL_verbosities(&log_control)[i] =
+					verbosity;
+			} else {
+				SEP_DRV_LOG_INIT(
+					"Changing verbosity mask for '%s' from 0x%x to 0x%x.",
+					UTILITY_Log_Category_Strings()[i],
+					(U32)DRV_LOG_VERBOSITY(i),
+					(U32)DRV_LOG_CONTROL_verbosities(
+						&log_control)[i]);
+				DRV_LOG_VERBOSITY(i) =
+					DRV_LOG_CONTROL_verbosities(
+						&log_control)[i];
+			}
+		}
+
+		for (; i < DRV_MAX_NB_LOG_CATEGORIES; i++) {
+			DRV_LOG_CONTROL_verbosities(&log_control)[i] =
+				LOG_VERBOSITY_UNSET;
+		}
+
+		if (copy_to_user((void __user *)args->buf_drv_to_usr, &log_control,
+				 sizeof(log_control))) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+			return OS_FAULT;
+		}
+	} else if (DRV_LOG_CONTROL_command(&log_control) ==
+		   DRV_LOG_CONTROL_COMMAND_MARK) {
+		DRV_LOG_CONTROL_message(
+			&log_control)[DRV_LOG_CONTROL_MAX_DATA_SIZE - 1] = 0;
+		SEP_DRV_LOG_MARK("Mark: '%s'.",
+				 DRV_LOG_CONTROL_message(&log_control));
+	} else if (DRV_LOG_CONTROL_command(&log_control) ==
+		   DRV_LOG_CONTROL_COMMAND_QUERY_SIZE) {
+		DRV_LOG_CONTROL_log_size(&log_control) = sizeof(*DRV_LOG());
+		SEP_DRV_LOG_TRACE("Driver log size is %u bytes.",
+				  DRV_LOG_CONTROL_log_size(&log_control));
+		if (copy_to_user((void __user *)args->buf_drv_to_usr, &log_control,
+				 sizeof(log_control))) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT("Memory copy failure!");
+			return OS_FAULT;
+		}
+	} else if (DRV_LOG_CONTROL_command(&log_control) ==
+		   DRV_LOG_CONTROL_COMMAND_BENCHMARK) {
+		U32 nb_iterations =
+			*(U32 *)&DRV_LOG_CONTROL_message(&log_control);
+
+		SEP_DRV_LOG_INIT_IN("Starting benchmark (%u iterations)...",
+				    nb_iterations);
+		for (i = 0; i < nb_iterations; i++) {
+			(void)i;
+		}
+		SEP_DRV_LOG_INIT_OUT("Benchmark complete (%u/%u iterations).",
+				     i, nb_iterations);
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Get_Drv_Setup_Info
+ *
+ * @brief       Get numerous information of driver
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Get_Drv_Setup_Info(IOCTL_ARGS args)
+{
+#define VMM_VENDOR_STR_LEN 12
+	U32 pebs_unavailable = 0;
+	U64 rbx, rcx, rdx, num_basic_functions;
+	S8 vmm_vendor_name[VMM_VENDOR_STR_LEN + 1];
+	S8 *vmm_vmware_str = "VMwareVMware";
+	S8 *vmm_kvm_str = "KVMKVMKVM\0\0\0";
+	S8 *vmm_mshyperv_str = "Microsoft Hv";
+	S8 *vmm_acrn_str = "ACRNACRNACRN";
+#if defined(DRV_USE_KAISER)
+	int *kaiser_enabled_ptr;
+	int *kaiser_pti_option;
+#endif
+	bool is_hypervisor = FALSE;
+
+	SEP_DRV_LOG_FLOW_IN("Args: %p.", args);
+
+	if (args->buf_drv_to_usr == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (buf_drv_to_usr is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr != sizeof(DRV_SETUP_INFO_NODE)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Invalid arguments (unexpected len_drv_to_usr value)!");
+		return OS_INVALID;
+	}
+
+	memset((char *)&req_drv_setup_info, 0, sizeof(DRV_SETUP_INFO_NODE));
+
+	DRV_SETUP_INFO_nmi_mode(&req_drv_setup_info) = 1;
+
+#if defined(DRV_SEP_ACRN_ON)
+	is_hypervisor = TRUE;
+#endif
+	if (boot_cpu_has(X86_FEATURE_HYPERVISOR) || is_hypervisor) {
+		UTILITY_Read_Cpuid(0x40000000, &num_basic_functions, &rbx, &rcx,
+				   &rdx);
+		memcpy(vmm_vendor_name, &rbx, 4);
+		memcpy(vmm_vendor_name + 4, &rcx, 4);
+		memcpy(vmm_vendor_name + 8, &rdx, 4);
+		memcpy(vmm_vendor_name + 12, "\0", 1);
+
+		if (!strncmp(vmm_vendor_name, vmm_vmware_str,
+			     VMM_VENDOR_STR_LEN)) {
+			DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+			DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) =
+				DRV_VMM_VMWARE;
+		} else if (!strncmp(vmm_vendor_name, vmm_kvm_str,
+				    VMM_VENDOR_STR_LEN)) {
+			DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+			DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) =
+				DRV_VMM_KVM;
+		} else if (!strncmp(vmm_vendor_name, vmm_acrn_str,
+				    VMM_VENDOR_STR_LEN)) {
+			DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+			DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) =
+				DRV_VMM_ACRN;
+		} else if (!strncmp(vmm_vendor_name, vmm_mshyperv_str,
+				    VMM_VENDOR_STR_LEN)) {
+			DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+			DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) =
+				DRV_VMM_HYPERV;
+			if (num_basic_functions >= 0x40000003) {
+				UTILITY_Read_Cpuid(0x40000003,
+						   &num_basic_functions, &rbx,
+						   &rcx, &rdx);
+				if (rbx & 0x1) {
+					DRV_SETUP_INFO_vmm_guest_vm(
+						&req_drv_setup_info) = 0;
+				} else {
+					DRV_SETUP_INFO_vmm_guest_vm(
+						&req_drv_setup_info) = 1;
+				}
+			}
+		}
+	}
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 32)
+	else if (xen_domain()) {
+		DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+		DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) = DRV_VMM_XEN;
+
+		if (xen_initial_domain()) {
+			DRV_SETUP_INFO_vmm_guest_vm(&req_drv_setup_info) = 0;
+		} else {
+			DRV_SETUP_INFO_vmm_guest_vm(&req_drv_setup_info) = 1;
+		}
+	}
+#endif
+	else {
+		if (LINUXOS_Check_KVM_Guest_Process()) {
+			DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info) = 1;
+			DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info) =
+				DRV_VMM_KVM;
+		}
+	}
+
+	pebs_unavailable = (SYS_Read_MSR(IA32_MISC_ENABLE) >> 12) & 0x1;
+	if (!pebs_unavailable) {
+		if (!wrmsr_safe(IA32_PEBS_ENABLE, 0, 0)) {
+			DRV_SETUP_INFO_pebs_accessible(&req_drv_setup_info) = 1;
+		}
+	}
+
+#if defined(DRV_USE_KAISER)
+	kaiser_enabled_ptr = (int *)UTILITY_Find_Symbol("kaiser_enabled");
+	if (kaiser_enabled_ptr && *kaiser_enabled_ptr) {
+		SEP_DRV_LOG_INIT(
+			"KAISER is enabled! (&kaiser_enable=%p, val: %d).",
+			kaiser_enabled_ptr, *kaiser_enabled_ptr);
+		DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) =
+			DRV_SETUP_INFO_PTI_KAISER;
+	} else {
+		kaiser_pti_option = (int *)UTILITY_Find_Symbol("pti_option");
+		if (kaiser_pti_option) {
+			SEP_DRV_LOG_INIT(
+				"KAISER pti_option=%p pti_option val=%d",
+				kaiser_pti_option, *kaiser_pti_option);
+#if defined(X86_FEATURE_PTI)
+			if (static_cpu_has(X86_FEATURE_PTI)) {
+				SEP_DRV_LOG_INIT(
+					"KAISER is Enabled or in Auto Enable!\n");
+				DRV_SETUP_INFO_page_table_isolation(
+					&req_drv_setup_info) =
+					DRV_SETUP_INFO_PTI_KAISER;
+			} else {
+				SEP_DRV_LOG_INIT(
+					"KAISER is present but disabled!");
+			}
+#endif
+		}
+	}
+	if (!kaiser_enabled_ptr && !kaiser_pti_option) {
+		SEP_DRV_LOG_ERROR(
+			"Could not find KAISER information. Assuming no KAISER!");
+	}
+#elif defined(DRV_USE_PTI)
+	if (static_cpu_has(X86_FEATURE_PTI)) {
+		SEP_DRV_LOG_INIT("Kernel Page Table Isolation is enabled!");
+		DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) =
+			DRV_SETUP_INFO_PTI_KPTI;
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE("DRV_SETUP_INFO nmi_mode %d.",
+			  DRV_SETUP_INFO_nmi_mode(&req_drv_setup_info));
+	SEP_DRV_LOG_TRACE("DRV_SETUP_INFO vmm_mode %d.",
+			  DRV_SETUP_INFO_vmm_mode(&req_drv_setup_info));
+	SEP_DRV_LOG_TRACE("DRV_SETUP_INFO vmm_vendor %d.",
+			  DRV_SETUP_INFO_vmm_vendor(&req_drv_setup_info));
+	SEP_DRV_LOG_TRACE("DRV_SETUP_INFO vmm_guest_vm %d.",
+			  DRV_SETUP_INFO_vmm_guest_vm(&req_drv_setup_info));
+	SEP_DRV_LOG_TRACE("DRV_SETUP_INFO pebs_accessible %d.",
+			  DRV_SETUP_INFO_pebs_accessible(&req_drv_setup_info));
+	SEP_DRV_LOG_TRACE(
+		"DRV_SETUP_INFO page_table_isolation %d.",
+		DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info));
+
+#if defined(DRV_CPU_HOTPLUG)
+	DRV_SETUP_INFO_cpu_hotplug_mode(&req_drv_setup_info) = 1;
+#endif
+
+	if (copy_to_user((void __user *)args->buf_drv_to_usr, &req_drv_setup_info,
+			 args->len_drv_to_usr)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Memory allocation failure!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Set_Emon_Buffer_Driver_Helper
+ *
+ * @brief       Setup EMON buffer driver helper
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static OS_STATUS lwpmudrv_Set_Emon_Buffer_Driver_Helper(IOCTL_ARGS args)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	if (args->len_usr_to_drv == 0 || args->buf_usr_to_drv == NULL) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error: Invalid arguments.");
+		return OS_INVALID;
+	}
+
+	if (!emon_buffer_driver_helper) {
+		emon_buffer_driver_helper =
+			CONTROL_Allocate_Memory(args->len_usr_to_drv);
+		if (emon_buffer_driver_helper == NULL) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Memory allocation failure for emon_buffer_driver_helper!");
+			return OS_NO_MEM;
+		}
+	}
+
+	if (copy_from_user(emon_buffer_driver_helper, (void __user *)args->buf_usr_to_drv,
+			   args->len_usr_to_drv)) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory copy failure for device num units!");
+		return OS_FAULT;
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("Success");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 lwpmudrv_Set_OSID
+ *
+ * @brief       Set OSID with specified value
+ *
+ * @param arg   Pointer to the IOCTL structure
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+
+static OS_STATUS lwpmudrv_Set_OSID(IOCTL_ARGS args)
+{
+	OS_STATUS status = OS_SUCCESS;
+
+	if (args->buf_usr_to_drv == NULL) {
+		SEP_PRINT_ERROR("Invalid arguments (buf_usr_to_drv is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_usr_to_drv != sizeof(U32)) {
+		SEP_PRINT_ERROR(
+			"Invalid arguments (unexpected len_usr_to_drv value)!");
+		return OS_INVALID;
+	}
+
+	status = get_user(osid, (U32  __user *)args->buf_usr_to_drv);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static OS_STATUS lwpmudrv_Get_Agent_Mode(IOCTL_ARGS arg)
+ *
+ * @param arg - pointer to the IOCTL_ARGS structure
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Local function that copies agent mode from drv to usr code
+ * @brief  Returns status.
+ *
+ * <I>Special Notes</I>
+ */
+static OS_STATUS lwpmudrv_Get_Agent_Mode(IOCTL_ARGS args)
+{
+	OS_STATUS status;
+
+	if (args->buf_drv_to_usr == NULL) {
+		SEP_PRINT_ERROR("Invalid arguments (buf_drv_to_usr is NULL)!");
+		return OS_INVALID;
+	}
+	if (args->len_drv_to_usr != sizeof(U32)) {
+		SEP_PRINT_ERROR(
+			"Invalid arguments (unexpected len_drv_to_usr value)!");
+		return OS_INVALID;
+	}
+
+#if defined(DRV_SEP_ACRN_ON)
+	status = put_user(HOST_VM_AGENT, (U32 __user *)args->buf_drv_to_usr);
+	sched_switch_enabled = TRUE;
+#else
+	status = put_user(-1, (U32 __user *)args->buf_drv_to_usr);
+	SEP_PRINT_ERROR("Invalid agent mode..!");
+	status = OS_INVALID;
+#endif
+
+	return status;
+}
+
+/*******************************************************************************
+ *  External Driver functions - Open
+ *      This function is common to all drivers
+ *******************************************************************************/
+
+static int lwpmu_Open(struct inode *inode, struct file *filp)
+{
+	SEP_DRV_LOG_TRACE_IN("Maj:%d, min:%d", imajor(inode), iminor(inode));
+
+	filp->private_data = container_of(inode->i_cdev, LWPMU_DEV_NODE, cdev);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+	return 0;
+}
+
+/*******************************************************************************
+ *  External Driver functions
+ *      These functions are registered into the file operations table that
+ *      controls this device.
+ *      Open, Close, Read, Write, Release
+ *******************************************************************************/
+
+static ssize_t lwpmu_Read(struct file *filp, char __user *buf, size_t count,
+			  loff_t *f_pos)
+{
+	unsigned long retval;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	/* Transfering data to user space */
+	SEP_DRV_LOG_TRACE("Dispatched with count=%d.", (S32)count);
+	if (copy_to_user((void __user *)buf, &LWPMU_DEV_buffer(lwpmu_control), 1)) {
+		retval = OS_FAULT;
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Memory copy failure!");
+		return retval;
+	}
+	/* Changing reading position as best suits */
+	if (*f_pos == 0) {
+		*f_pos += 1;
+		SEP_DRV_LOG_TRACE_OUT("Return value: 1.");
+		return 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: 0.");
+	return 0;
+}
+
+static ssize_t lwpmu_Write(struct file *filp, const char __user *buf, size_t count,
+			   loff_t *f_pos)
+{
+	unsigned long retval;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	SEP_DRV_LOG_TRACE("Dispatched with count=%d.", (S32)count);
+	if (copy_from_user(&LWPMU_DEV_buffer(lwpmu_control), (void __user *)(buf + count - 1),
+			   1)) {
+		retval = OS_FAULT;
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Memory copy failure!");
+		return retval;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: 1.");
+	return 1;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  extern IOCTL_OP_TYPE lwpmu_Service_IOCTL(IOCTL_USE_NODE, filp, cmd, arg)
+ *
+ * @param   IOCTL_USE_INODE       - Used for pre 2.6.32 kernels
+ * @param   struct   file   *filp - file pointer
+ * @param   unsigned int     cmd  - IOCTL command
+ * @param   unsigned long    arg  - args to the IOCTL command
+ *
+ * @return OS_STATUS
+ *
+ * @brief  SEP Worker function that handles IOCTL requests from the user mode.
+ *
+ * <I>Special Notes</I>
+ */
+static IOCTL_OP_TYPE lwpmu_Service_IOCTL(IOCTL_USE_INODE struct file *filp,
+					 unsigned int cmd,
+					 IOCTL_ARGS_NODE local_args)
+{
+	int status = OS_SUCCESS;
+
+	SEP_DRV_LOG_TRACE_IN("Command: %d.", cmd);
+
+	if (cmd == DRV_OPERATION_GET_DRIVER_STATE) {
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_DRIVER_STATE.");
+		status = lwpmudrv_Get_Driver_State(&local_args);
+		SEP_DRV_LOG_TRACE_OUT("Return value for command %d: %d", cmd,
+				      status);
+		return status;
+	}
+	if (cmd == DRV_OPERATION_GET_DRIVER_LOG) {
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_DRIVER_LOG.");
+		status = lwpmudrv_Get_Driver_Log(&local_args);
+		SEP_DRV_LOG_TRACE_OUT("Return value for command %d: %d", cmd,
+				      status);
+		return status;
+	}
+	if (cmd == DRV_OPERATION_CONTROL_DRIVER_LOG) {
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_CONTROL_DRIVER_LOG.");
+		status = lwpmudrv_Control_Driver_Log(&local_args);
+		SEP_DRV_LOG_TRACE_OUT("Return value for command %d: %d", cmd,
+				      status);
+		return status;
+	}
+	if (GET_DRIVER_STATE() == DRV_STATE_PREPARE_STOP) {
+		SEP_DRV_LOG_TRACE("skipping ioctl -- processing stop.");
+		SEP_DRV_LOG_TRACE_OUT("Return value for command %d: %d", cmd,
+				      status);
+		return status;
+	}
+
+	MUTEX_LOCK(ioctl_lock);
+	UTILITY_Driver_Set_Active_Ioctl(cmd);
+
+	switch (cmd) {
+		/*
+	* Common IOCTL commands
+	*/
+
+	case DRV_OPERATION_VERSION:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_VERSION.");
+		status = lwpmudrv_Version(&local_args);
+		break;
+
+	case DRV_OPERATION_RESERVE:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_RESERVE.");
+		status = lwpmudrv_Reserve(&local_args);
+		break;
+
+	case DRV_OPERATION_INIT_DRIVER:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_INIT_DRIVER.");
+		status = lwpmudrv_Initialize_Driver(local_args.buf_usr_to_drv,
+						    local_args.len_usr_to_drv);
+		break;
+
+	case DRV_OPERATION_INIT:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_INIT.");
+		status = lwpmudrv_Initialize(local_args.buf_usr_to_drv,
+					     local_args.len_usr_to_drv);
+		break;
+
+	case DRV_OPERATION_INIT_PMU:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_INIT_PMU.");
+		status = lwpmudrv_Init_PMU(&local_args);
+		break;
+
+	case DRV_OPERATION_SET_CPU_MASK:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SET_CPU_MASK.");
+		status = lwpmudrv_Set_CPU_Mask(local_args.buf_usr_to_drv,
+					       local_args.len_usr_to_drv);
+		break;
+
+	case DRV_OPERATION_START:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_START.");
+		status = lwpmudrv_Start();
+		break;
+
+	case DRV_OPERATION_STOP:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_STOP.");
+		status = lwpmudrv_Prepare_Stop();
+		UTILITY_Driver_Set_Active_Ioctl(0);
+		MUTEX_UNLOCK(ioctl_lock);
+
+		MUTEX_LOCK(ioctl_lock);
+		UTILITY_Driver_Set_Active_Ioctl(cmd);
+		if (GET_DRIVER_STATE() == DRV_STATE_PREPARE_STOP) {
+			status = lwpmudrv_Finish_Stop();
+			if (status == OS_SUCCESS) {
+				// if stop was successful, relevant memory should have been freed,
+				// so try to compact the memory tracker
+				CONTROL_Memory_Tracker_Compaction();
+			}
+		}
+		break;
+
+	case DRV_OPERATION_PAUSE:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_PAUSE.");
+		status = lwpmudrv_Pause();
+		break;
+
+	case DRV_OPERATION_RESUME:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_RESUME.");
+		status = lwpmudrv_Resume();
+		break;
+
+	case DRV_OPERATION_EM_GROUPS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_EM_GROUPS.");
+		status = lwpmudrv_Set_EM_Config(&local_args);
+		break;
+
+	case DRV_OPERATION_EM_CONFIG_NEXT:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_EM_CONFIG_NEXT.");
+		status = lwpmudrv_Configure_Events(&local_args);
+		break;
+
+	case DRV_OPERATION_NUM_DESCRIPTOR:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_NUM_DESCRIPTOR.");
+		status = lwpmudrv_Set_Sample_Descriptors(&local_args);
+		break;
+
+	case DRV_OPERATION_DESC_NEXT:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_DESC_NEXT.");
+		status = lwpmudrv_Configure_Descriptors(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_NORMALIZED_TSC:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_NORMALIZED_TSC.");
+		status = lwpmudrv_Get_Normalized_TSC(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_NORMALIZED_TSC_STANDALONE:
+		SEP_DRV_LOG_TRACE(
+			"DRV_OPERATION_GET_NORMALIZED_TSC_STANDALONE.");
+		status = lwpmudrv_Get_Normalized_TSC(&local_args);
+		break;
+
+	case DRV_OPERATION_NUM_CORES:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_NUM_CORES.");
+		status = lwpmudrv_Get_Num_Cores(&local_args);
+		break;
+
+	case DRV_OPERATION_KERNEL_CS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_KERNEL_CS.");
+		status = lwpmudrv_Get_KERNEL_CS(&local_args);
+		break;
+
+	case DRV_OPERATION_SET_UID:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SET_UID.");
+		status = lwpmudrv_Set_UID(&local_args);
+		break;
+
+	case DRV_OPERATION_TSC_SKEW_INFO:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_TSC_SKEW_INFO.");
+		status = lwpmudrv_Get_TSC_Skew_Info(&local_args);
+		break;
+
+	case DRV_OPERATION_COLLECT_SYS_CONFIG:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_COLLECT_SYS_CONFIG.");
+		status = lwpmudrv_Collect_Sys_Config(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_SYS_CONFIG:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_SYS_CONFIG.");
+		status = lwpmudrv_Sys_Config(&local_args);
+		break;
+
+	case DRV_OPERATION_TERMINATE:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_TERMINATE.");
+		status = lwpmudrv_Terminate();
+		break;
+
+	case DRV_OPERATION_SET_CPU_TOPOLOGY:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SET_CPU_TOPOLOGY.");
+		status = lwpmudrv_Setup_Cpu_Topology(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_NUM_CORE_CTRS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_NUM_CORE_CTRS.");
+		status = lwpmudrv_Samp_Read_Num_Of_Core_Counters(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_PLATFORM_INFO:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_PLATFORM_INFO.");
+		status = lwpmudrv_Get_Platform_Info(&local_args);
+		break;
+
+	case DRV_OPERATION_READ_MSRS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_READ_MSRs.");
+		status = lwpmudrv_Read_MSRs(&local_args);
+		break;
+
+	case DRV_OPERATION_SWITCH_GROUP:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SWITCH_GROUP.");
+		status = lwpmudrv_Switch_Group();
+		break;
+
+	case DRV_OPERATION_SET_OSID:
+		SEP_DRV_LOG_TRACE("LWPMUDRV_IOCTL_SET_OSID\n");
+		status = lwpmudrv_Set_OSID(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_AGENT_MODE:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_AGENT_MODE\n");
+		status = lwpmudrv_Get_Agent_Mode(&local_args);
+		break;
+
+		/*
+		 * EMON-specific IOCTL commands
+		 */
+	case DRV_OPERATION_READ_MSR:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_READ_MSR.");
+		status = lwpmudrv_Read_MSR_All_Cores(&local_args);
+		break;
+
+	case DRV_OPERATION_WRITE_MSR:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_WRITE_MSR.");
+		status = lwpmudrv_Write_MSR_All_Cores(&local_args);
+		break;
+
+	case DRV_OPERATION_READ_SWITCH_GROUP:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_READ_SWITCH_GROUP.");
+		status = lwpmudrv_Read_Counters_And_Switch_Group(&local_args);
+		break;
+
+	case DRV_OPERATION_READ_AND_RESET:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_READ_AND_RESET.");
+		status = lwpmudrv_Read_And_Reset_Counters(&local_args);
+		break;
+
+		/*
+		 * Platform-specific IOCTL commands (IA32 and Intel64)
+		 */
+
+	case DRV_OPERATION_INIT_UNC:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_INIT_UNC.");
+		status = lwpmudrv_Initialize_UNC(local_args.buf_usr_to_drv,
+						 local_args.len_usr_to_drv);
+		break;
+
+	case DRV_OPERATION_EM_GROUPS_UNC:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_EM_GROUPS_UNC.");
+		status = lwpmudrv_Set_EM_Config_UNC(&local_args);
+		break;
+
+	case DRV_OPERATION_EM_CONFIG_NEXT_UNC:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_EM_CONFIG_NEXT_UNC.");
+		status = lwpmudrv_Configure_Events_UNC(&local_args);
+		break;
+
+	case DRV_OPERATION_LBR_INFO:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_LBR_INFO.");
+		status = lwpmudrv_LBR_Info(&local_args);
+		break;
+
+	case DRV_OPERATION_PWR_INFO:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_PWR_INFO.");
+		status = lwpmudrv_PWR_Info(&local_args);
+		break;
+
+	case DRV_OPERATION_INIT_NUM_DEV:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_INIT_NUM_DEV.");
+		status = lwpmudrv_Initialize_Num_Devices(&local_args);
+		break;
+	case DRV_OPERATION_GET_NUM_SAMPLES:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_NUM_SAMPLES.");
+		status = lwpmudrv_Get_Num_Samples(&local_args);
+		break;
+
+	case DRV_OPERATION_SET_DEVICE_NUM_UNITS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SET_DEVICE_NUM_UNITS.");
+		status = lwpmudrv_Set_Device_Num_Units(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_INTERVAL_COUNTS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_INTERVAL_COUNTS.");
+		lwpmudrv_Get_Interval_Counts(&local_args);
+		break;
+
+	case DRV_OPERATION_SET_SCAN_UNCORE_TOPOLOGY_INFO:
+		SEP_DRV_LOG_TRACE(
+			"DRV_OPERATION_SET_SCAN_UNCORE_TOPOLOGY_INFO.");
+		status =
+			lwpmudrv_Set_Uncore_Topology_Info_And_Scan(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_UNCORE_TOPOLOGY:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_UNCORE_TOPOLOGY.");
+		status = lwpmudrv_Get_Uncore_Topology(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_PLATFORM_TOPOLOGY:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_PLATFORM_TOPOLOGY.");
+		status = lwpmudrv_Get_Platform_Topology(&local_args);
+		break;
+
+	case DRV_OPERATION_FLUSH:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_FLUSH.");
+		status = lwpmudrv_Flush();
+		break;
+
+	case DRV_OPERATION_SET_EMON_BUFFER_DRIVER_HELPER:
+		SEP_DRV_LOG_TRACE(
+			"DRV_OPERATION_SET_EMON_BUFFER_DRIVER_HELPER.");
+		status = lwpmudrv_Set_Emon_Buffer_Driver_Helper(&local_args);
+		break;
+
+		/*
+		 * Graphics IOCTL commands
+		 */
+
+#if defined(BUILD_GFX)
+	case DRV_OPERATION_SET_GFX_EVENT:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_SET_GFX_EVENT.");
+		SEP_DRV_LOG_TRACE("lwpmudrv_Device_Control: enable_gfx=%d.",
+				  (int)DRV_CONFIG_enable_gfx(drv_cfg));
+		status = GFX_Set_Event_Code(&local_args);
+		break;
+#endif
+
+		/*
+		 * Chipset IOCTL commands
+		 */
+
+#if defined(BUILD_CHIPSET)
+	case DRV_OPERATION_PCI_READ: {
+		CHIPSET_PCI_ARG_NODE pci_data;
+
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_PCI_READ.");
+
+		if (local_args.buf_usr_to_drv == NULL ||
+		    local_args.len_usr_to_drv != sizeof(CHIPSET_PCI_ARG_NODE) ||
+		    local_args.buf_drv_to_usr == NULL ||
+		    local_args.len_drv_to_usr != sizeof(CHIPSET_PCI_ARG_NODE)) {
+			status = OS_FAULT;
+			goto cleanup;
+		}
+
+		if (copy_from_user(&pci_data,
+				   (void __user *)local_args.buf_usr_to_drv,
+				   sizeof(CHIPSET_PCI_ARG_NODE))) {
+			status = OS_FAULT;
+			goto cleanup;
+		}
+
+		status = PCI_Read_From_Memory_Address(
+			CHIPSET_PCI_ARG_address(&pci_data),
+			&CHIPSET_PCI_ARG_value(&pci_data));
+
+		if (copy_to_user((void __user *)local_args.buf_drv_to_usr, &pci_data,
+				 sizeof(CHIPSET_PCI_ARG_NODE))) {
+			status = OS_FAULT;
+			goto cleanup;
+		}
+
+		break;
+	}
+
+	case DRV_OPERATION_PCI_WRITE: {
+		CHIPSET_PCI_ARG_NODE pci_data;
+
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_PCI_WRITE.");
+
+		if (local_args.buf_usr_to_drv == NULL ||
+		    local_args.len_usr_to_drv != sizeof(CHIPSET_PCI_ARG_NODE)) {
+			status = OS_FAULT;
+			goto cleanup;
+		}
+
+		if (copy_from_user(&pci_data,
+				   (void __user *)local_args.buf_usr_to_drv,
+				   sizeof(CHIPSET_PCI_ARG_NODE))) {
+			status = OS_FAULT;
+			goto cleanup;
+		}
+
+		status = PCI_Write_To_Memory_Address(
+			CHIPSET_PCI_ARG_address(&pci_data),
+			CHIPSET_PCI_ARG_value(&pci_data));
+		break;
+	}
+
+	case DRV_OPERATION_FD_PHYS:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_FD_PHYS.");
+		status = lwpmudrv_Samp_Find_Physical_Address(&local_args);
+		break;
+
+	case DRV_OPERATION_READ_PCI_CONFIG:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_READ_PCI_CONFIG.");
+		status = lwpmudrv_Samp_Read_PCI_Config(&local_args);
+		break;
+
+	case DRV_OPERATION_WRITE_PCI_CONFIG:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_WRITE_PCI_CONFIG.");
+		status = lwpmudrv_Samp_Write_PCI_Config(&local_args);
+		break;
+
+	case DRV_OPERATION_CHIPSET_INIT:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_CHIPSET_INIT.");
+		SEP_DRV_LOG_TRACE("Enable_chipset=%d.",
+				  (int)DRV_CONFIG_enable_chipset(drv_cfg));
+		status = lwpmudrv_Samp_Chipset_Init(&local_args);
+		break;
+
+	case DRV_OPERATION_GET_CHIPSET_DEVICE_ID:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_CHIPSET_DEVICE_ID.");
+		status = lwpmudrv_Samp_Read_PCI_Config(&local_args);
+		break;
+#endif
+
+	case DRV_OPERATION_GET_DRV_SETUP_INFO:
+		SEP_DRV_LOG_TRACE("DRV_OPERATION_GET_DRV_SETUP_INFO.");
+		status = lwpmudrv_Get_Drv_Setup_Info(&local_args);
+		break;
+
+		/*
+		 * if none of the above, treat as unknown/illegal IOCTL command
+		 */
+
+	default:
+		SEP_DRV_LOG_ERROR("Unknown IOCTL number: %d!", cmd);
+		status = OS_ILLEGAL_IOCTL;
+		break;
+	}
+#if defined(BUILD_CHIPSET)
+cleanup:
+#endif
+	UTILITY_Driver_Set_Active_Ioctl(0);
+	MUTEX_UNLOCK(ioctl_lock);
+
+	SEP_DRV_LOG_TRACE_OUT("Return value for command %d: %d.", cmd, status);
+	return status;
+}
+
+static long lwpmu_Device_Control(IOCTL_USE_INODE struct file *filp,
+				 unsigned int cmd, unsigned long arg)
+{
+	int status = OS_SUCCESS;
+	IOCTL_ARGS_NODE local_args;
+
+	SEP_DRV_LOG_TRACE_IN("Cmd type: %d, subcommand: %d.", _IOC_TYPE(cmd),
+			     _IOC_NR(cmd));
+
+#if !defined(DRV_USE_UNLOCKED_IOCTL)
+	SEP_DRV_LOG_TRACE("Cmd: 0x%x, called on inode maj:%d, min:%d.", cmd,
+			  imajor(inode), iminor(inode));
+#endif
+	SEP_DRV_LOG_TRACE("Type: %d, subcommand: %d.", _IOC_TYPE(cmd),
+			  _IOC_NR(cmd));
+
+	if (_IOC_TYPE(cmd) != LWPMU_IOC_MAGIC) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Unknown IOCTL magic: %d!",
+					    _IOC_TYPE(cmd));
+		return OS_ILLEGAL_IOCTL;
+	}
+
+	memset(&local_args, 0, sizeof(IOCTL_ARGS_NODE));
+
+	if (arg) {
+		status = copy_from_user(&local_args, (void __user *)arg,
+					sizeof(IOCTL_ARGS_NODE));
+	}
+
+	status = lwpmu_Service_IOCTL(IOCTL_USE_INODE filp, _IOC_NR(cmd),
+				     local_args);
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: %d.", status);
+	return status;
+}
+
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+static long lwpmu_Device_Control_Compat(struct file *filp, unsigned int cmd,
+					unsigned long arg)
+{
+	int status = OS_SUCCESS;
+	IOCTL_COMPAT_ARGS_NODE local_args_compat;
+	IOCTL_ARGS_NODE local_args;
+
+	SEP_DRV_LOG_TRACE_IN("Compat: type: %d, subcommand: %d.",
+			     _IOC_TYPE(cmd), _IOC_NR(cmd));
+
+	memset(&local_args_compat, 0, sizeof(IOCTL_COMPAT_ARGS_NODE));
+	SEP_DRV_LOG_TRACE("Compat: type: %d, subcommand: %d.", _IOC_TYPE(cmd),
+			  _IOC_NR(cmd));
+
+	if (_IOC_TYPE(cmd) != LWPMU_IOC_MAGIC) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Unknown IOCTL magic: %d!",
+					    _IOC_TYPE(cmd));
+		return OS_ILLEGAL_IOCTL;
+	}
+
+	if (arg) {
+		status = copy_from_user(&local_args_compat,
+					(void __user *)arg,
+					sizeof(IOCTL_COMPAT_ARGS_NODE));
+	} // NB: status defined above is not being used...
+	local_args.len_drv_to_usr = local_args_compat.len_drv_to_usr;
+	local_args.len_usr_to_drv = local_args_compat.len_usr_to_drv;
+	local_args.buf_drv_to_usr =
+		(char *)compat_ptr(local_args_compat.buf_drv_to_usr);
+	local_args.buf_usr_to_drv =
+		(char *)compat_ptr(local_args_compat.buf_usr_to_drv);
+	local_args.command = _IOC_NR(cmd);
+
+	status = lwpmu_Service_IOCTL(filp, _IOC_NR(cmd), local_args);
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: %d", status);
+	return status;
+}
+#endif
+
+/*
+ * @fn        LWPMUDRV_Abnormal_Terminate(void)
+ *
+ * @brief     This routine is called from linuxos_Exit_Task_Notify if the user process has
+ *            been killed by an uncatchable signal (example kill -9).  The state variable
+ *            abormal_terminate is set to 1 and the clean up routines are called.  In this
+ *            code path the OS notifier hooks should not be unloaded.
+ *
+ * @param     None
+ *
+ * @return    OS_STATUS
+ *
+ * <I>Special Notes:</I>
+ *     <none>
+ */
+static int LWPMUDRV_Abnormal_Terminate(void)
+{
+	int status;
+	SEP_DRV_LOG_FLOW_IN("");
+
+	SEP_DRV_LOG_TRACE("Calling lwpmudrv_Prepare_Stop.");
+	status = lwpmudrv_Prepare_Stop();
+	if (status != OS_SUCCESS)
+		return status;
+
+	SEP_DRV_LOG_TRACE("Calling lwpmudrv_Finish_Stop.");
+	status = lwpmudrv_Finish_Stop();
+	if (status != OS_SUCCESS)
+		return status;
+
+	SEP_DRV_LOG_TRACE("Calling lwpmudrv_Terminate.");
+	status = lwpmudrv_Terminate();
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+static int lwpmudrv_Abnormal_Handler(void *data)
+{
+	SEP_DRV_LOG_FLOW_IN("");
+
+	while (!kthread_should_stop()) {
+		if (wait_event_interruptible_timeout(
+			    wait_exit,
+			    GET_DRIVER_STATE() == DRV_STATE_TERMINATING,
+			    msecs_to_jiffies(350))) {
+			SEP_DRV_LOG_WARNING(
+				"Processing abnormal termination...");
+			MUTEX_LOCK(ioctl_lock);
+			SEP_DRV_LOG_TRACE("Locked ioctl_lock...");
+			LWPMUDRV_Abnormal_Terminate();
+			SEP_DRV_LOG_TRACE("Unlocking ioctl_lock...");
+			MUTEX_UNLOCK(ioctl_lock);
+		}
+	}
+
+	SEP_DRV_LOG_FLOW_OUT("End of thread.");
+	return 0;
+}
+
+/*****************************************************************************************
+ *
+ *   Driver Entry / Exit functions that will be called on when the driver is loaded and
+ *   unloaded
+ *
+ ****************************************************************************************/
+
+/*
+ * Structure that declares the usual file access functions
+ * First one is for lwpmu_c, the control functions
+ */
+static struct file_operations lwpmu_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = lwpmu_Device_Control,
+#if defined(CONFIG_COMPAT) && defined(DRV_EM64T)
+	.compat_ioctl = lwpmu_Device_Control_Compat,
+#endif
+	.read = lwpmu_Read,
+	.write = lwpmu_Write,
+	.open = lwpmu_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/*
+ * Second one is for lwpmu_m, the module notification functions
+ */
+static struct file_operations lwmod_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = NULL, //None needed
+	.read = OUTPUT_Module_Read,
+	.write = NULL, //No writing accepted
+	.open = lwpmu_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/*
+ * Third one is for lwsamp_nn, the sampling functions
+ */
+static struct file_operations lwsamp_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = NULL, //None needed
+	.read = OUTPUT_Sample_Read,
+	.write = NULL, //No writing accepted
+	.open = lwpmu_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/*
+ * Fourth one is for lwsamp_sideband, the pebs process info functions
+ */
+static struct file_operations lwsideband_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = NULL, //None needed
+	.read = OUTPUT_SidebandInfo_Read,
+	.write = NULL, //No writing accepted
+	.open = lwpmu_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/*
+ * Fifth one is for lwsampunc_nn, the uncore sampling functions
+ */
+static struct file_operations lwsampunc_Fops = {
+	.owner = THIS_MODULE,
+	IOCTL_OP = NULL, //None needed
+	.read = OUTPUT_UncSample_Read,
+	.write = NULL, //No writing accepted
+	.open = lwpmu_Open,
+	.release = NULL,
+	.llseek = NULL,
+};
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static int lwpmudrv_setup_cdev(dev, fops, dev_number)
+ *
+ * @param LWPMU_DEV               dev  - pointer to the device object
+ * @param struct file_operations *fops - pointer to the file operations struct
+ * @param dev_t                   dev_number - major/monor device number
+ *
+ * @return OS_STATUS
+ *
+ * @brief  Set up the device object.
+ *
+ * <I>Special Notes</I>
+ */
+static int lwpmu_setup_cdev(LWPMU_DEV dev, struct file_operations *fops,
+			    dev_t dev_number)
+{
+	int res;
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cdev_init(&LWPMU_DEV_cdev(dev), fops);
+	LWPMU_DEV_cdev(dev).owner = THIS_MODULE;
+	LWPMU_DEV_cdev(dev).ops = fops;
+
+	res = cdev_add(&LWPMU_DEV_cdev(dev), dev_number, 1);
+
+	SEP_DRV_LOG_TRACE_OUT("Return value: %d", res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static int lwpmu_Load(void)
+ *
+ * @param none
+ *
+ * @return STATUS
+ *
+ * @brief  Load the driver module into the kernel.  Set up the driver object.
+ * @brief  Set up the initial state of the driver and allocate the memory
+ * @brief  needed to keep basic state information.
+ */
+static int lwpmu_Load(void)
+{
+	int i, j, num_cpus;
+	dev_t lwmod_DevNum;
+	OS_STATUS status = OS_INVALID;
+	char dev_name[MAXNAMELEN];
+	struct device *sep_device;
+#if defined(CONFIG_XEN_HAVE_VPMU)
+	xen_pmu_params_t xenpmu_param;
+	xen_pmu_data_t *xenpmu_data;
+	unsigned long pfn;
+#endif
+
+	SEP_DRV_LOG_LOAD("Driver loading...");
+	if (UTILITY_Driver_Log_Init() !=
+	    OS_SUCCESS) { // Do not use SEP_DRV_LOG_X (where X != LOAD) before this, or if this fails
+		SEP_DRV_LOG_LOAD("Error: could not allocate log buffer.");
+		return OS_NO_MEM;
+	}
+	SEP_DRV_LOG_FLOW_IN("Starting internal log monitoring.");
+
+	CONTROL_Memory_Tracker_Init();
+
+#if defined(DRV_SEP_ACRN_ON)
+	SEP_DRV_LOG_FLOW_IN("Starting internal log monitoring.");
+	vm_info_list =
+		CONTROL_Allocate_Memory(sizeof(struct profiling_vm_info_list));
+	memset(vm_info_list, 0, sizeof(struct profiling_vm_info_list));
+
+	BUG_ON(!virt_addr_valid(vm_info_list));
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_GET_VMINFO,
+			virt_to_phys(vm_info_list));
+#endif
+
+#if !defined(CONFIG_XEN_HAVE_VPMU)
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 32)
+	if (xen_initial_domain()) {
+		SEP_DRV_LOG_LOAD(
+			"PMU virtualization is not enabled on XEN dom0!");
+	}
+#endif
+#endif
+
+	/* Get one major device number and two minor numbers. */
+	/*   The result is formatted as major+minor(0) */
+	/*   One minor number is for control (lwpmu_c), */
+	/*   the other (lwpmu_m) is for modules */
+	SEP_DRV_LOG_INIT("About to register chrdev...");
+
+	lwpmu_DevNum = MKDEV(0, 0);
+	status = alloc_chrdev_region(&lwpmu_DevNum, 0, PMU_DEVICES,
+				     SEP_DRIVER_NAME);
+	SEP_DRV_LOG_INIT("Result of alloc_chrdev_region is %d.", status);
+	if (status < 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Failed to alloc chrdev_region (return = %d).",
+			status);
+		return status;
+	}
+	SEP_DRV_LOG_LOAD("Major number is %d", MAJOR(lwpmu_DevNum));
+	status = lwpmudrv_Initialize_State();
+	if (status < 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Failed to initialize state (return = %d)!", status);
+		return status;
+	}
+	num_cpus = GLOBAL_STATE_num_cpus(driver_state);
+	SEP_DRV_LOG_LOAD("Detected %d total CPUs and %d active CPUs.", num_cpus,
+			 GLOBAL_STATE_active_cpus(driver_state));
+
+#if defined(CONFIG_XEN_HAVE_VPMU)
+	if (xen_initial_domain()) {
+		xenpmu_param.version.maj = XENPMU_VER_MAJ;
+		xenpmu_param.version.min = XENPMU_VER_MIN;
+
+		for (i = 0; i < num_cpus; i++) {
+			xenpmu_data =
+				(xen_pmu_data_t *)get_zeroed_page(GFP_KERNEL);
+			;
+			if (!xenpmu_data) {
+				SEP_DRV_LOG_ERROR_FLOW_OUT(
+					"Memory allocation failure for xenpmu_data!");
+				return OS_NO_MEM;
+			}
+			pfn = vmalloc_to_pfn((char *)xenpmu_data);
+
+			xenpmu_param.val = pfn_to_mfn(pfn);
+			xenpmu_param.vcpu = i;
+			status = HYPERVISOR_xenpmu_op(XENPMU_init,
+						      (PVOID)&xenpmu_param);
+
+			per_cpu(sep_xenpmu_shared, i) = xenpmu_data;
+		}
+		SEP_DRV_LOG_LOAD("VPMU is initialized on XEN Dom0.");
+	}
+#endif
+
+	PCI_Initialize();
+
+	/* Allocate memory for the control structures */
+	lwpmu_control = CONTROL_Allocate_Memory(sizeof(LWPMU_DEV_NODE));
+	lwmod_control = CONTROL_Allocate_Memory(sizeof(LWPMU_DEV_NODE));
+	lwsamp_control =
+		CONTROL_Allocate_Memory(num_cpus * sizeof(LWPMU_DEV_NODE));
+	lwsideband_control =
+		CONTROL_Allocate_Memory(num_cpus * sizeof(LWPMU_DEV_NODE));
+
+	if (!lwsideband_control || !lwsamp_control || !lwpmu_control ||
+	    !lwmod_control) {
+		CONTROL_Free_Memory(lwpmu_control);
+		CONTROL_Free_Memory(lwmod_control);
+		CONTROL_Free_Memory(lwsamp_control);
+		CONTROL_Free_Memory(lwsideband_control);
+
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for control structures!");
+		return OS_NO_MEM;
+	}
+
+	/* Register the file operations with the OS */
+
+	pmu_class = class_create(THIS_MODULE, SEP_DRIVER_NAME);
+	if (IS_ERR(pmu_class)) {
+		SEP_DRV_LOG_ERROR("Error registering SEP control class!");
+	}
+	sep_device = device_create(pmu_class, NULL, lwpmu_DevNum, NULL,
+		      SEP_DRIVER_NAME DRV_DEVICE_DELIMITER "c");
+	if (IS_ERR(sep_device)) {
+		SEP_DRV_LOG_ERROR("Error creating SEP PMU device!");
+	}
+
+	status = lwpmu_setup_cdev(lwpmu_control, &lwpmu_Fops, lwpmu_DevNum);
+	if (status) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error %d when adding lwpmu as char device!", status);
+		unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+		device_destroy(pmu_class, lwpmu_DevNum);
+		unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+		class_destroy(pmu_class);
+		return status;
+	}
+	/* _c init was fine, now try _m */
+	lwmod_DevNum = MKDEV(MAJOR(lwpmu_DevNum), MINOR(lwpmu_DevNum) + 1);
+
+	sep_device = device_create(pmu_class, NULL, lwmod_DevNum, NULL,
+		      SEP_DRIVER_NAME DRV_DEVICE_DELIMITER "m");
+	if (IS_ERR(sep_device)) {
+		SEP_DRV_LOG_ERROR("Error creating SEP PMU device!");
+	}
+
+	status = lwpmu_setup_cdev(lwmod_control, &lwmod_Fops, lwmod_DevNum);
+	if (status) {
+		cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error %d when adding lwpmu as char device!", status);
+		unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+		device_destroy(pmu_class, lwpmu_DevNum);
+		device_destroy(pmu_class, lwpmu_DevNum + 1);
+		cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+		unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+		class_destroy(pmu_class);
+		return status;
+	}
+
+	/* allocate one sampling device per cpu */
+	lwsamp_DevNum = MKDEV(0, 0);
+	status = alloc_chrdev_region(&lwsamp_DevNum, 0, num_cpus,
+				     SEP_SAMPLES_NAME);
+
+	if (status < 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Failed to alloc chrdev_region (return = %d).",
+			status);
+		unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+		device_destroy(pmu_class, lwpmu_DevNum);
+		device_destroy(pmu_class, lwpmu_DevNum + 1);
+		cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+		cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+		unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+		class_destroy(pmu_class);
+		return status;
+	}
+
+	/* Register the file operations with the OS */
+	for (i = 0; i < num_cpus; i++) {
+		snprintf(dev_name, MAXNAMELEN, "%s%ss%d", SEP_DRIVER_NAME,
+			 DRV_DEVICE_DELIMITER, i);
+
+		sep_device = device_create(pmu_class, NULL, lwsamp_DevNum + i, NULL,
+			      dev_name);
+		if (IS_ERR(sep_device)) {
+			SEP_DRV_LOG_ERROR("Error creating SEP PMU device !");
+		}
+		status = lwpmu_setup_cdev(lwsamp_control + i, &lwsamp_Fops,
+					  lwsamp_DevNum + i);
+		if (status) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Error %d when adding lwpmu as char device!",
+				status);
+			unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+			device_destroy(pmu_class, lwpmu_DevNum);
+			device_destroy(pmu_class, lwpmu_DevNum + 1);
+			cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+			cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+			unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+			unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+			for (j = i; j > 0; j--) {
+				device_destroy(pmu_class, lwsamp_DevNum + j);
+				cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[j]));
+			}
+
+			class_destroy(pmu_class);
+			return status;
+		} else {
+			SEP_DRV_LOG_INIT("Added sampling device %d.", i);
+		}
+	}
+
+	lwsideband_DevNum = MKDEV(0, 0);
+	status = alloc_chrdev_region(&lwsideband_DevNum, 0, num_cpus,
+				     SEP_SIDEBAND_NAME);
+
+	if (status < 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for chrdev_region for sideband!");
+		unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+		device_destroy(pmu_class, lwpmu_DevNum);
+		device_destroy(pmu_class, lwpmu_DevNum + 1);
+		cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+		cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+		unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+		unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+		for (j = 0; j < num_cpus; j++) {
+			device_destroy(pmu_class, lwsamp_DevNum + j);
+			cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[j]));
+		}
+
+		class_destroy(pmu_class);
+		return status;
+	}
+
+	for (i = 0; i < num_cpus; i++) {
+		snprintf(dev_name, MAXNAMELEN, "%s%sb%d", SEP_DRIVER_NAME,
+			 DRV_DEVICE_DELIMITER, i);
+		sep_device = device_create(pmu_class, NULL, lwsideband_DevNum + i, NULL,
+			      dev_name);
+		if (IS_ERR(sep_device)) {
+			SEP_DRV_LOG_ERROR("Error creating SEP PMU device!");
+		}
+		status = lwpmu_setup_cdev(lwsideband_control + i,
+					  &lwsideband_Fops,
+					  lwsideband_DevNum + i);
+		if (status) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Error %d when adding lwsideband as char device!",
+				status);
+			unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+			device_destroy(pmu_class, lwpmu_DevNum);
+			device_destroy(pmu_class, lwpmu_DevNum + 1);
+			cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+			cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+			unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+			unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+			unregister_chrdev(MAJOR(lwsideband_DevNum), SEP_SIDEBAND_NAME);
+			for (j = 0; j < num_cpus; j++) {
+				device_destroy(pmu_class, lwsamp_DevNum + j);
+				cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[j]));
+			}
+			for (j = i; j > 0; j--) {
+				device_destroy(pmu_class, lwsideband_DevNum + j);
+				cdev_del(&LWPMU_DEV_cdev(&lwsideband_control[j]));
+			}
+
+			class_destroy(pmu_class);
+			return status;
+		} else {
+			SEP_DRV_LOG_INIT("Added sampling sideband device %d.",
+					 i);
+		}
+	}
+
+	cpu_tsc = (U64 *)CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64));
+	prev_cpu_tsc = (U64 *)CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64));
+	diff_cpu_tsc = (U64 *)CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64));
+
+#if !defined(CONFIG_PREEMPT_COUNT) && !defined(DRV_SEP_ACRN_ON)
+	atomic_set(&read_now, GLOBAL_STATE_num_cpus(driver_state));
+	init_waitqueue_head(&read_tsc_now);
+	CONTROL_Invoke_Parallel(lwpmudrv_Fill_TSC_Info, (PVOID)(size_t)0);
+#endif
+
+	pcb_size = GLOBAL_STATE_num_cpus(driver_state) * sizeof(CPU_STATE_NODE);
+	pcb = CONTROL_Allocate_Memory(pcb_size);
+	if (!pcb) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for PCB!");
+		return OS_NO_MEM;
+	}
+
+	core_to_package_map = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	if (!core_to_package_map) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for core_to_package_map!");
+		return OS_NO_MEM;
+	}
+
+	core_to_phys_core_map = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	if (!core_to_phys_core_map) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for core_to_phys_core_map!");
+		return OS_NO_MEM;
+	}
+
+	core_to_thread_map = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	if (!core_to_thread_map) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for core_to_thread_map!");
+		return OS_NO_MEM;
+	}
+
+	threads_per_core = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	if (!threads_per_core) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for threads_per_core!");
+		return OS_NO_MEM;
+	}
+
+	occupied_core_ids = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32));
+	if (!occupied_core_ids) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Memory allocation failure for occupied_core_ids!");
+		return OS_NO_MEM;
+	}
+	SYS_INFO_Build();
+	memset(pcb, 0, pcb_size);
+
+	if (total_ram <= OUTPUT_MEMORY_THRESHOLD) {
+		output_buffer_size = OUTPUT_SMALL_BUFFER;
+	}
+
+	MUTEX_INIT(ioctl_lock);
+
+	status = UNC_COMMON_Init();
+	if (status) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT("Error %d when init uncore struct!",
+					   status);
+		return status;
+	}
+
+	/* allocate one sampling device per package (for uncore)*/
+	lwsampunc_control =
+		CONTROL_Allocate_Memory(num_packages * sizeof(LWPMU_DEV_NODE));
+	if (!lwsampunc_control) {
+		CONTROL_Free_Memory(lwsampunc_control);
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"lwpmu driver failed to alloc space!\n");
+		return OS_NO_MEM;
+	}
+
+	lwsampunc_DevNum = MKDEV(0, 0);
+	status = alloc_chrdev_region(&lwsampunc_DevNum, 0, num_packages,
+				     SEP_UNCORE_NAME);
+
+	if (status < 0) {
+		SEP_DRV_LOG_ERROR_FLOW_OUT(
+			"Error: Failed to alloc chrdev_region (return = %d).",
+			status);
+		unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+		device_destroy(pmu_class, lwpmu_DevNum);
+		device_destroy(pmu_class, lwpmu_DevNum + 1);
+		cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+		cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+		unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+		unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+		unregister_chrdev(MAJOR(lwsampunc_DevNum), SEP_UNCORE_NAME);
+		unregister_chrdev(MAJOR(lwsideband_DevNum), SEP_SIDEBAND_NAME);
+		for (j = 0; j < num_cpus; j++) {
+			device_destroy(pmu_class, lwsamp_DevNum + j);
+			device_destroy(pmu_class, lwsideband_DevNum + j);
+			cdev_del(&LWPMU_DEV_cdev(&lwsideband_control[j]));
+			cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[j]));
+		}
+
+		class_destroy(pmu_class);
+		unregister_chrdev_region(lwsamp_DevNum, num_cpus);
+		unregister_chrdev_region(lwsideband_DevNum, num_cpus);
+		return status;
+	}
+
+	/* Register the file operations with the OS */
+	for (i = 0; i < num_packages; i++) {
+		snprintf(dev_name, MAXNAMELEN, "%s%su%d", SEP_DRIVER_NAME,
+			 DRV_DEVICE_DELIMITER, i);
+		sep_device = device_create(pmu_class, NULL,
+			lwsampunc_DevNum + i, NULL, dev_name);
+		if (IS_ERR(sep_device)) {
+			SEP_DRV_LOG_ERROR("Error creating SEP PMU device!");
+		}
+		status = lwpmu_setup_cdev(lwsampunc_control + i,
+					&lwsampunc_Fops,
+					lwsampunc_DevNum + i);
+		if (status) {
+			SEP_DRV_LOG_ERROR_FLOW_OUT(
+				"Error %d when adding lwpmu as char device!",
+				status);
+			unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+			device_destroy(pmu_class, lwpmu_DevNum);
+			device_destroy(pmu_class, lwpmu_DevNum + 1);
+			cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+			cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+			unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+			unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+			unregister_chrdev(MAJOR(lwsampunc_DevNum), SEP_UNCORE_NAME);
+			unregister_chrdev(MAJOR(lwsideband_DevNum), SEP_SIDEBAND_NAME);
+			for (j = 0; j < num_cpus; j++) {
+				device_destroy(pmu_class, lwsamp_DevNum + j);
+				device_destroy(pmu_class, lwsideband_DevNum + j);
+				cdev_del(&LWPMU_DEV_cdev(&lwsideband_control[j]));
+				cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[j]));
+			}
+
+			for (j = i; j > 0; j--) {
+				device_destroy(pmu_class, lwsampunc_DevNum + i);
+				cdev_del(&LWPMU_DEV_cdev(&lwsampunc_control[j]));
+			}
+
+			class_destroy(pmu_class);
+			unregister_chrdev_region(lwsamp_DevNum, num_cpus);
+			unregister_chrdev_region(lwsampunc_DevNum, num_packages);
+			unregister_chrdev_region(lwsideband_DevNum, num_cpus);
+
+			return status;
+		} else {
+			SEP_DRV_LOG_INIT("Added sampling device %d.", i);
+		}
+	}
+
+	init_waitqueue_head(&wait_exit);
+	abnormal_handler = kthread_create(lwpmudrv_Abnormal_Handler, NULL,
+					  "SEPDRV_ABNORMAL_HANDLER");
+	if (abnormal_handler) {
+		wake_up_process(abnormal_handler);
+	}
+
+#if defined(DRV_CPU_HOTPLUG)
+	/* Register CPU hotplug notifier */
+	LINUXOS_Register_Hotplug();
+#endif
+	/*
+	 *  Initialize the SEP driver version (done once at driver load time)
+	 */
+	SEP_VERSION_NODE_major(&drv_version) = SEP_MAJOR_VERSION;
+	SEP_VERSION_NODE_minor(&drv_version) = SEP_MINOR_VERSION;
+	SEP_VERSION_NODE_api(&drv_version) = SEP_API_VERSION;
+	SEP_VERSION_NODE_update(&drv_version) = SEP_UPDATE_VERSION;
+
+	//
+	// Display driver version information
+	//
+	SEP_DRV_LOG_LOAD("PMU collection driver v%d.%d.%d %s has been loaded.",
+			 SEP_VERSION_NODE_major(&drv_version),
+			 SEP_VERSION_NODE_minor(&drv_version),
+			 SEP_VERSION_NODE_api(&drv_version),
+			 SEP_RELEASE_STRING);
+
+#if defined(BUILD_CHIPSET)
+	SEP_DRV_LOG_LOAD("Chipset support is enabled.");
+#endif
+
+#if defined(BUILD_GFX)
+	SEP_DRV_LOG_LOAD("Graphics support is enabled.");
+#endif
+
+	SEP_DRV_LOG_LOAD("NMI will be used for handling PMU interrupts.");
+
+	SEP_DRV_LOG_FLOW_OUT("Return value: %d.", status);
+	return status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  static int lwpmu_Unload(void)
+ *
+ * @param none
+ *
+ * @return none
+ *
+ * @brief  Remove the driver module from the kernel.
+ */
+static VOID lwpmu_Unload(void)
+{
+	int i = 0;
+	int num_cpus;
+#if defined(CONFIG_XEN_HAVE_VPMU)
+	xen_pmu_params_t xenpmu_param;
+#endif
+	PVOID tmp_pcb;
+
+	SEP_DRV_LOG_FLOW_IN("");
+
+	SEP_DRV_LOG_LOAD("Driver unloading.");
+
+	num_cpus = GLOBAL_STATE_num_cpus(driver_state);
+
+	if (abnormal_handler) {
+		if (GET_DRIVER_STATE() != DRV_STATE_UNINITIALIZED) {
+			CHANGE_DRIVER_STATE(STATE_BIT_ANY,
+					    DRV_STATE_TERMINATING);
+		}
+		wake_up_interruptible_all(&wait_exit);
+		kthread_stop(abnormal_handler);
+		abnormal_handler = NULL;
+	}
+
+#if defined(CONFIG_XEN_HAVE_VPMU)
+	if (xen_initial_domain()) {
+		xenpmu_param.version.maj = XENPMU_VER_MAJ;
+		xenpmu_param.version.min = XENPMU_VER_MIN;
+
+		for (i = 0; i < num_cpus; i++) {
+			xenpmu_param.vcpu = i;
+			HYPERVISOR_xenpmu_op(XENPMU_finish, &xenpmu_param);
+
+			vfree(per_cpu(sep_xenpmu_shared, i));
+			per_cpu(sep_xenpmu_shared, i) = NULL;
+		}
+		SEP_DRV_LOG_LOAD("VPMU was disabled on XEN Dom0.");
+	}
+#endif
+
+	LINUXOS_Uninstall_Hooks();
+	SYS_INFO_Destroy();
+	OUTPUT_Destroy();
+	cpu_buf = CONTROL_Free_Memory(cpu_buf);
+	unc_buf = CONTROL_Free_Memory(unc_buf);
+	cpu_sideband_buf = CONTROL_Free_Memory(cpu_sideband_buf);
+	module_buf = CONTROL_Free_Memory(module_buf);
+	cpu_tsc = CONTROL_Free_Memory(cpu_tsc);
+	prev_cpu_tsc = CONTROL_Free_Memory(prev_cpu_tsc);
+	diff_cpu_tsc = CONTROL_Free_Memory(diff_cpu_tsc);
+	core_to_package_map = CONTROL_Free_Memory(core_to_package_map);
+	core_to_phys_core_map = CONTROL_Free_Memory(core_to_phys_core_map);
+	core_to_thread_map = CONTROL_Free_Memory(core_to_thread_map);
+	threads_per_core = CONTROL_Free_Memory(threads_per_core);
+	occupied_core_ids = CONTROL_Free_Memory(occupied_core_ids);
+#if defined(DRV_SEP_ACRN_ON)
+	vm_info_list = CONTROL_Free_Memory(vm_info_list);
+#endif
+
+	tmp_pcb = pcb;
+	// Ensures there is no log message written (ERROR, ALLOC, ...)
+	pcb = NULL; // between pcb being freed and pcb being NULL.
+	CONTROL_Free_Memory(tmp_pcb);
+	pcb_size = 0;
+
+	UNC_COMMON_Clean_Up();
+
+	unregister_chrdev(MAJOR(lwpmu_DevNum), SEP_DRIVER_NAME);
+	device_destroy(pmu_class, lwpmu_DevNum);
+	device_destroy(pmu_class, lwpmu_DevNum + 1);
+
+	cdev_del(&LWPMU_DEV_cdev(lwpmu_control));
+	cdev_del(&LWPMU_DEV_cdev(lwmod_control));
+	unregister_chrdev_region(lwpmu_DevNum, PMU_DEVICES);
+
+	unregister_chrdev(MAJOR(lwsamp_DevNum), SEP_SAMPLES_NAME);
+	unregister_chrdev(MAJOR(lwsampunc_DevNum), SEP_UNCORE_NAME);
+	unregister_chrdev(MAJOR(lwsideband_DevNum), SEP_SIDEBAND_NAME);
+
+	for (i = 0; i < num_cpus; i++) {
+		device_destroy(pmu_class, lwsamp_DevNum + i);
+		device_destroy(pmu_class, lwsideband_DevNum + i);
+		cdev_del(&LWPMU_DEV_cdev(&lwsamp_control[i]));
+		cdev_del(&LWPMU_DEV_cdev(&lwsideband_control[i]));
+	}
+
+	for (i = 0; i < num_packages; i++) {
+		device_destroy(pmu_class, lwsampunc_DevNum + i);
+		cdev_del(&LWPMU_DEV_cdev(&lwsampunc_control[i]));
+	}
+
+	class_destroy(pmu_class);
+
+	unregister_chrdev_region(lwsamp_DevNum, num_cpus);
+	unregister_chrdev_region(lwsampunc_DevNum, num_packages);
+	unregister_chrdev_region(lwsideband_DevNum, num_cpus);
+	lwpmu_control = CONTROL_Free_Memory(lwpmu_control);
+	lwmod_control = CONTROL_Free_Memory(lwmod_control);
+	lwsamp_control = CONTROL_Free_Memory(lwsamp_control);
+	lwsampunc_control = CONTROL_Free_Memory(lwsampunc_control);
+	lwsideband_control = CONTROL_Free_Memory(lwsideband_control);
+
+	CONTROL_Memory_Tracker_Free();
+
+#if defined(DRV_CPU_HOTPLUG)
+	/* Unregister CPU hotplug notifier */
+	LINUXOS_Unregister_Hotplug();
+#endif
+
+	SEP_DRV_LOG_FLOW_OUT(
+		"Log deallocation. Cannot track further in internal log.");
+	UTILITY_Driver_Log_Free(); // Do not use SEP_DRV_LOG_X (where X != LOAD) after this
+
+	SEP_DRV_LOG_LOAD(
+		"PMU collection driver v%d.%d.%d %s has been unloaded.",
+		SEP_VERSION_NODE_major(&drv_version),
+		SEP_VERSION_NODE_minor(&drv_version),
+		SEP_VERSION_NODE_api(&drv_version), SEP_RELEASE_STRING);
+}
+
+/* Declaration of the init and exit functions */
+module_init(lwpmu_Load);
+module_exit(lwpmu_Unload);
diff --git a/drivers/platform/x86/sepdk/sep/output.c b/drivers/platform/x86/sepdk/sep/output.c
new file mode 100755
index 000000000000..5fe2d28b4174
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/output.c
@@ -0,0 +1,1177 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/jiffies.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+#include <asm/atomic.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "control.h"
+#include "output.h"
+#include "utility.h"
+#include "inc/linuxos.h"
+#define OTHER_C_DEVICES 1 // one for module
+
+/*
+ *  Global data: Buffer control structure
+ */
+static wait_queue_head_t flush_queue;
+static atomic_t flush_writers;
+static volatile int flush;
+extern DRV_CONFIG drv_cfg;
+extern DRV_BOOL multi_pebs_enabled;
+extern DRV_BOOL sched_switch_enabled;
+extern DRV_BOOL unc_buf_init;
+
+static void output_NMI_Sample_Buffer(unsigned long data);
+
+/*
+ *  @fn output_Free_Buffers(output, size)
+ *
+ *  @param    IN  outbuf      - The output buffer to manipulate
+ *
+ *  @brief   Deallocate the memory associated with the buffer descriptor
+ *
+ */
+static VOID output_Free_Buffers(BUFFER_DESC buffer, size_t size)
+{
+	int j;
+	OUTPUT outbuf;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p, size: %u.", buffer, size);
+
+	if (buffer == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!buffer).");
+		return;
+	}
+	outbuf = &BUFFER_DESC_outbuf(buffer);
+	for (j = 0; j < OUTPUT_NUM_BUFFERS; j++) {
+		CONTROL_Free_Memory(OUTPUT_buffer(outbuf, j));
+		OUTPUT_buffer(outbuf, j) = NULL;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  int OUTPUT_Reserve_Buffer_Space (OUTPUT      outbuf,
+ *                                        U32         size,
+ *                                        U8          in_notification)
+ *
+ *  @param  outbuf          IN output buffer to manipulate
+ *  @param  size            IN The size of data to reserve
+ *  @param  defer           IN wake up directly if FALSE.
+ *                           Otherwise, see below.
+ *  @param  in_notification IN 1 if in notification, 0 if not
+ *
+ *  @result outloc - to the location where data is to be written
+ *
+ *  Reserve space in the output buffers for data. The behavior of this function
+ *  when a buffer is full will vary depending on the 'defer' and 'in_notification'
+ *  parameters, as described in the special notes section.
+ *
+ * <I>Special Notes:</I>
+ *  -----------------------------------------------------------------------------------------------------------------------
+ *  defer | in_notification |                                         description
+ *  -----------------------------------------------------------------------------------------------------------------------
+ *  FALSE |    FALSE/TRUE   | directly signals the buffer's consumer with wake_up_interruptible_sync
+ *  -----------------------------------------------------------------------------------------------------------------------
+ *   TRUE |      FALSE      | defers the call to wake_up_interruptible_sync using tasklet_schedule [needed because calling
+ *        |                 | it directly is not safe from an NMI]
+ *  -----------------------------------------------------------------------------------------------------------------------
+ *        |                 | do not signal -or explicitly schedule the signaling of- the buffer's consumer [needed because
+ *   TRUE |       TRUE      | neither operation is safe from the sched_switch tracepoint callback in kernel version 4.13].
+ *        |                 | Instead relies on the interrupt handler to do it next time there is an interrupt.
+ *  -----------------------------------------------------------------------------------------------------------------------
+ */
+void *OUTPUT_Reserve_Buffer_Space(BUFFER_DESC bd, U32 size,
+					 DRV_BOOL defer, U8 in_notification,
+					 S32 cpu_idx)
+{
+	char *outloc = NULL;
+	OUTPUT outbuf = &BUFFER_DESC_outbuf(bd);
+	S32 this_cpu;
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(
+		in_notification, "Bd: %p, size: %u, defer: %u, notif: %u.", bd,
+		size, defer, in_notification);
+
+	if (DRV_CONFIG_enable_cp_mode(drv_cfg) && flush) {
+		SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(
+			in_notification, "Res: NULL (cp_mode && flush).");
+		return NULL;
+	}
+
+	if (OUTPUT_remaining_buffer_size(outbuf) >= size) {
+		outloc = (char *)
+			(OUTPUT_buffer(outbuf, OUTPUT_current_buffer(outbuf)) +
+			(OUTPUT_total_buffer_size(outbuf) -
+				OUTPUT_remaining_buffer_size(outbuf)));
+	} else {
+		U32 i, j, start;
+		OUTPUT_buffer_full(outbuf, OUTPUT_current_buffer(outbuf)) =
+			OUTPUT_total_buffer_size(outbuf) -
+			OUTPUT_remaining_buffer_size(outbuf);
+
+		//
+		// Massive Naive assumption:  Must find a way to fix it.
+		// In spite of the loop.
+		// The next buffer to fill are monotonically increasing
+		// indicies.
+		//
+		if (!DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+			OUTPUT_signal_full(outbuf) = TRUE;
+		}
+
+		start = OUTPUT_current_buffer(outbuf);
+		for (i = start + 1; i < start + OUTPUT_NUM_BUFFERS; i++) {
+			j = i % OUTPUT_NUM_BUFFERS;
+
+			//don't check if buffer has data when doing CP
+			if (!OUTPUT_buffer_full(outbuf, j) ||
+			    (DRV_CONFIG_enable_cp_mode(drv_cfg))) {
+				OUTPUT_current_buffer(outbuf) = j;
+				OUTPUT_remaining_buffer_size(outbuf) =
+					OUTPUT_total_buffer_size(outbuf);
+				outloc = (char *)OUTPUT_buffer(outbuf, j);
+				if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+					// discarding all the information in the new buffer in CP mode
+					OUTPUT_buffer_full(outbuf, j) = 0;
+					break;
+				}
+			}
+#if !(defined(CONFIG_PREEMPT_RT) || defined(CONFIG_PREEMPT_RT_FULL))
+			else {
+				if (!defer) {
+					OUTPUT_signal_full(outbuf) = FALSE;
+					SEP_DRV_LOG_NOTIFICATION_WARNING(
+						in_notification,
+						"Output buffers are full. Might be dropping some samples!");
+					break;
+				}
+			}
+#endif
+		}
+	}
+
+	if (outloc) {
+		OUTPUT_remaining_buffer_size(outbuf) -= size;
+		memset(outloc, 0, size);
+	}
+
+	if (OUTPUT_signal_full(outbuf)) {
+		if (!defer) {
+#if !(defined(CONFIG_PREEMPT_RT) || defined(CONFIG_PREEMPT_RT_FULL))
+			SEP_DRV_LOG_NOTIFICATION_TRACE(
+				in_notification,
+				"Choosing direct wakeup approach.");
+#if !defined(DRV_SEP_ACRN_ON)
+			wake_up_interruptible_sync(&BUFFER_DESC_queue(bd));
+#endif
+			OUTPUT_signal_full(outbuf) = FALSE;
+#endif
+		} else {
+			if (!OUTPUT_tasklet_queued(outbuf)) {
+				if (cpu_idx == -1) {
+					this_cpu = CONTROL_THIS_CPU();
+				} else {
+					this_cpu = cpu_idx;
+				}
+				if (!in_notification) {
+					SEP_DRV_LOG_NOTIFICATION_TRACE(
+						in_notification,
+						"Scheduling the tasklet on cpu %u.",
+						this_cpu);
+					OUTPUT_tasklet_queued(outbuf) = TRUE;
+#if !defined(DRV_SEP_ACRN_ON)
+					tasklet_schedule(&CPU_STATE_nmi_tasklet(
+						&pcb[this_cpu]));
+#endif
+				} else {
+					static U32 cpt;
+
+					if (!cpt) {
+						SEP_DRV_LOG_WARNING(
+							"Using interrupt-driven sideband buffer flushes for extra safety.");
+						SEP_DRV_LOG_WARNING(
+							"This may result in fewer context switches being recorded.");
+					}
+					SEP_DRV_LOG_TRACE(
+						"Lost context switch information (for the %uth time).",
+						++cpt);
+				}
+			}
+		}
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notification, "Res: %p.", outloc);
+	return outloc;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *
+ * @fn  int  OUTPUT_Buffer_Fill (BUFFER_DESC buf,
+ *                               PVOID  data,
+ *                               U16    size,
+ *                               U8     in_notification)
+ *
+ * @brief     Place a record (can be module, marker, etc) in a buffer
+ *
+ * @param     data            - pointer to a buffer to copy
+ * @param     size            - size of the buffer to cpu
+ * @param     in_notification - 1 if in notification, 0 if not
+ *
+ * @return    number of bytes copied into buffer
+ *
+ * Start by ensuring that output buffer space is available.
+ * If so, then copy the input data to the output buffer and make the necessary
+ * adjustments to manage the output buffers.
+ * If not, signal the read event for this buffer and get another buffer.
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+static int output_Buffer_Fill(BUFFER_DESC bd, PVOID data, U16 size,
+			      U8 in_notification)
+{
+	char *outloc;
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(
+		in_notification, "Bd: %p, data: %p, size: %u.", bd, data, size);
+
+	outloc = (char *)OUTPUT_Reserve_Buffer_Space(bd, size,
+			FALSE, in_notification, -1);
+	if (outloc) {
+		memcpy(outloc, data, size);
+		SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notification,
+						   "Res: %d (outloc).", size);
+		return size;
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notification,
+					   "Res: 0 (!outloc).");
+	return 0;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  int  OUTPUT_Module_Fill (PVOID  data,
+ *                               U16    size,
+ *                               U8     in_notification)
+ *
+ * @brief     Place a module record in a buffer
+ *
+ * @param     data              - pointer to a buffer to copy
+ * @param     size              - size of the buffer to cpu
+ * @param     in_notification   - 1 if in notification, 0 if not
+ *
+ * @return    number of bytes copied into buffer
+ *
+ *
+ */
+int OUTPUT_Module_Fill(PVOID data, U16 size, U8 in_notification)
+{
+	int ret_size;
+	OUTPUT outbuf = &BUFFER_DESC_outbuf(module_buf);
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_IN(in_notification,
+					  "Data: %p, size: %u.", data, size);
+
+	spin_lock(&OUTPUT_buffer_lock(outbuf));
+	ret_size = output_Buffer_Fill(module_buf, data, size, in_notification);
+	spin_unlock(&OUTPUT_buffer_lock(outbuf));
+
+	SEP_DRV_LOG_NOTIFICATION_TRACE_OUT(in_notification, "Res: %d.",
+					   ret_size);
+	return ret_size;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  ssize_t  output_Read(struct file  *filp,
+ *                            char         *buf,
+ *                            size_t        count,
+ *                            loff_t       *f_pos,
+ *                            BUFFER_DESC   kernel_buf)
+ *
+ *  @brief  Return a sample buffer to user-mode. If not full or flush, wait
+ *
+ *  @param *filp          a file pointer
+ *  @param *buf           a sampling buffer
+ *  @param  count         size of the user's buffer
+ *  @param  f_pos         file pointer (current offset in bytes)
+ *  @param  kernel_buf    the kernel output buffer structure
+ *
+ *  @return number of bytes read. zero indicates end of file. Neg means error
+ *
+ *  Place no more than count bytes into the user's buffer.
+ *  Block if unavailable on "BUFFER_DESC_queue(buf)"
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+static ssize_t output_Read(struct file *filp, char __user *buf, size_t count,
+			   loff_t *f_pos, BUFFER_DESC kernel_buf)
+{
+	ssize_t to_copy = 0;
+	ssize_t uncopied;
+	OUTPUT outbuf = &BUFFER_DESC_outbuf(kernel_buf);
+	U32 cur_buf, i;
+	/* Buffer is filled by output_fill_modules. */
+
+	SEP_DRV_LOG_TRACE_IN(
+		"Filp: %p, buf: %p, count: %u, f_pos: %p, kernel_buf: %p.",
+		filp, buf, (U32)count, f_pos, kernel_buf);
+
+	cur_buf = OUTPUT_current_buffer(outbuf);
+	if (!DRV_CONFIG_enable_cp_mode(drv_cfg) || flush) {
+		for (i = 0; i < OUTPUT_NUM_BUFFERS; i++) {
+			//iterate through all buffers
+			cur_buf++;
+			if (cur_buf >= OUTPUT_NUM_BUFFERS) {
+				cur_buf = 0;
+			} //circularly
+			to_copy = OUTPUT_buffer_full(outbuf, cur_buf);
+			if (to_copy != 0) {
+				if (flush &&
+				    DRV_CONFIG_enable_cp_mode(drv_cfg) &&
+				    cur_buf == OUTPUT_current_buffer(outbuf)) {
+					OUTPUT_current_buffer(outbuf)++;
+					if (OUTPUT_current_buffer(outbuf) >=
+					    OUTPUT_NUM_BUFFERS) {
+						OUTPUT_current_buffer(outbuf) =
+							0;
+					}
+					OUTPUT_remaining_buffer_size(outbuf) =
+						OUTPUT_total_buffer_size(
+							outbuf);
+				}
+				break;
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE("buffer %d has %d bytes ready.", (S32)cur_buf,
+			  (S32)to_copy);
+	if (!flush && to_copy == 0) {
+		unsigned long delay = msecs_to_jiffies(1000);
+
+		while (1) {
+			U32 res = wait_event_interruptible_timeout(
+				BUFFER_DESC_queue(kernel_buf),
+				flush || (OUTPUT_buffer_full(outbuf, cur_buf) &&
+					  !DRV_CONFIG_enable_cp_mode(drv_cfg)),
+				delay);
+
+			if (GET_DRIVER_STATE() == DRV_STATE_TERMINATING) {
+				SEP_DRV_LOG_INIT(
+					"Switched to TERMINATING while waiting for BUFFER_DESC_queue!");
+				break;
+			}
+
+			if (res == ERESTARTSYS || res == 0) {
+				SEP_DRV_LOG_TRACE(
+					"Wait_event_interruptible_timeout(BUFFER_DESC_queue): %u.",
+					res);
+				continue;
+			}
+
+			break;
+		}
+
+		if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+			// reset the current buffer index if in CP mode
+			cur_buf = OUTPUT_current_buffer(outbuf);
+			for (i = 0; i < OUTPUT_NUM_BUFFERS;
+			     i++) { //iterate through all buffers
+				cur_buf++;
+				if (cur_buf >= OUTPUT_NUM_BUFFERS) {
+					cur_buf = 0;
+				} //circularly
+				to_copy = OUTPUT_buffer_full(outbuf, cur_buf);
+				if (to_copy != 0) {
+					if (flush &&
+					    DRV_CONFIG_enable_cp_mode(
+						    drv_cfg) &&
+					    cur_buf == OUTPUT_current_buffer(
+							       outbuf)) {
+						OUTPUT_current_buffer(outbuf)++;
+						if (OUTPUT_current_buffer(
+							    outbuf) >=
+						    OUTPUT_NUM_BUFFERS) {
+							OUTPUT_current_buffer(
+								outbuf) = 0;
+						}
+						OUTPUT_remaining_buffer_size(
+							outbuf) =
+							OUTPUT_total_buffer_size(
+								outbuf);
+					}
+					break;
+				}
+			}
+		}
+		SEP_DRV_LOG_TRACE("Get to copy %d.", (S32)cur_buf);
+		to_copy = OUTPUT_buffer_full(outbuf, cur_buf);
+		SEP_DRV_LOG_TRACE(
+			"output_Read awakened, buffer %d has %d bytes.",
+			cur_buf, (int)to_copy);
+	}
+
+	/* Ensure that the user's buffer is large enough */
+	if (to_copy > count) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"OS_NO_MEM (user buffer is too small!).");
+		return OS_NO_MEM;
+	}
+
+	/* Copy data to user space. Note that we use cur_buf as the source */
+	if (GET_DRIVER_STATE() != DRV_STATE_TERMINATING) {
+		uncopied = copy_to_user(buf, OUTPUT_buffer(outbuf, cur_buf),
+					to_copy);
+		/* Mark the buffer empty */
+		OUTPUT_buffer_full(outbuf, cur_buf) = 0;
+		*f_pos += to_copy - uncopied;
+		if (uncopied) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Res: %u (only copied %u of %u bytes!).",
+				(U32)(to_copy - uncopied), (U32)to_copy,
+				(U32)uncopied);
+			return (to_copy - uncopied);
+		}
+	} else {
+		to_copy = 0;
+		SEP_DRV_LOG_TRACE("To copy set to 0.");
+	}
+
+	// At end-of-file, decrement the count of active buffer writers
+
+	if (to_copy == 0) {
+		DRV_BOOL flush_val = atomic_dec_and_test(&flush_writers);
+		SEP_DRV_LOG_TRACE("Decremented flush_writers.");
+		if (flush_val == TRUE) {
+			wake_up_interruptible_sync(&flush_queue);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)to_copy);
+	return to_copy;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  ssize_t  OUTPUT_Module_Read(struct file  *filp,
+ *                                   char         *buf,
+ *                                   size_t        count,
+ *                                   loff_t       *f_pos)
+ *
+ *  @brief  Return a module buffer to user-mode. If not full or flush, wait
+ *
+ *  @param *filp   a file pointer
+ *  @param *buf    a sampling buffer
+ *  @param  count  size of the user's buffer
+ *  @param  f_pos  file pointer (current offset in bytes)
+ *  @param  buf    the kernel output buffer structure
+ *
+ *  @return number of bytes read. zero indicates end of file. Neg means error
+ *
+ *  Place no more than count bytes into the user's buffer.
+ *  Block on "BUFFER_DESC_queue(kernel_buf)" if buffer isn't full.
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+ssize_t OUTPUT_Module_Read(struct file *filp, char __user *buf, size_t count,
+				  loff_t *f_pos)
+{
+	ssize_t res;
+
+	SEP_DRV_LOG_TRACE_IN("");
+	SEP_DRV_LOG_TRACE("Read request for modules on minor.");
+
+	res = output_Read(filp, buf, count, f_pos, module_buf);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  ssize_t  OUTPUT_Sample_Read(struct file  *filp,
+ *                                   char         *buf,
+ *                                   size_t        count,
+ *                                   loff_t       *f_pos)
+ *
+ *  @brief  Return a sample buffer to user-mode. If not full or flush, wait
+ *
+ *  @param *filp   a file pointer
+ *  @param *buf    a sampling buffer
+ *  @param  count  size of the user's buffer
+ *  @param  f_pos  file pointer (current offset in bytes)
+ *  @param  buf    the kernel output buffer structure
+ *
+ *  @return number of bytes read. zero indicates end of file. Neg means error
+ *
+ *  Place no more than count bytes into the user's buffer.
+ *  Block on "BUFFER_DESC_queue(kernel_buf)" if buffer isn't full.
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+ssize_t OUTPUT_Sample_Read(struct file *filp, char __user *buf, size_t count,
+				  loff_t *f_pos)
+{
+	int i;
+	ssize_t res;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	i = iminor(filp->DRV_F_DENTRY
+			   ->d_inode); // kernel pointer - not user pointer
+	SEP_DRV_LOG_TRACE("Read request for samples on minor %d.", i);
+	res = output_Read(filp, buf, count, f_pos, &(cpu_buf[i]));
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  ssize_t  OUTPUT_Sample_Read(struct file  *filp,
+ *                                   char         *buf,
+ *                                   size_t        count,
+ *                                   loff_t       *f_pos)
+ *
+ *  @brief  Return a sample buffer to user-mode. If not full or flush, wait
+ *
+ *  @param *filp   a file pointer
+ *  @param *buf    a sampling buffer
+ *  @param  count  size of the user's buffer
+ *  @param  f_pos  file pointer (current offset in bytes)
+ *  @param  buf    the kernel output buffer structure
+ *
+ *  @return number of bytes read. zero indicates end of file. Neg means error
+ *
+ *  Place no more than count bytes into the user's buffer.
+ *  Block on "BUFFER_DESC_queue(kernel_buf)" if buffer isn't full.
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+ssize_t OUTPUT_UncSample_Read(struct file *filp, char __user *buf,
+			size_t count, loff_t *f_pos)
+{
+	int i;
+	ssize_t res = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	i = iminor(filp->DRV_F_DENTRY
+			   ->d_inode); // kernel pointer - not user pointer
+	SEP_DRV_LOG_TRACE("Read request for samples on minor %d.", i);
+	if (unc_buf_init) {
+		res = output_Read(filp, buf, count, f_pos, &(unc_buf[i]));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ *  @fn  ssize_t  OUTPUT_SidebandInfo_Read(struct file  *filp,
+ *                                         char         *buf,
+ *                                         size_t        count,
+ *                                         loff_t       *f_pos)
+ *
+ *  @brief  Return a sideband info buffer to user-mode. If not full or flush, wait
+ *
+ *  @param *filp   a file pointer
+ *  @param *buf    a sideband info buffer
+ *  @param  count  size of the user's buffer
+ *  @param  f_pos  file pointer (current offset in bytes)
+ *  @param  buf    the kernel output buffer structure
+ *
+ *  @return number of bytes read. zero indicates end of file. Neg means error
+ *
+ *  Place no more than count bytes into the user's buffer.
+ *  Block on "BUFFER_DESC_queue(kernel_buf)" if buffer isn't full.
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+ssize_t OUTPUT_SidebandInfo_Read(struct file *filp, char __user *buf,
+					size_t count, loff_t *f_pos)
+{
+	int i;
+	ssize_t res = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	i = iminor(filp->DRV_F_DENTRY
+			   ->d_inode); // kernel pointer - not user pointer
+	SEP_DRV_LOG_TRACE("Read request for pebs process info on minor %d.", i);
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		res = output_Read(filp, buf, count, f_pos,
+				  &(cpu_sideband_buf[i]));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)res);
+	return res;
+}
+
+/*
+ *  @fn output_Initialized_Buffers()
+ *
+ *  @result OUTPUT
+ *  @param  BUFFER_DESC desc   - descriptor for the buffer being initialized
+ *  @param  U32         factor - multiplier for OUTPUT_BUFFER_SIZE.
+ *                               1 for cpu buffers, 2 for module buffers.
+ *
+ *  @brief  Allocate, initialize, and return an output data structure
+ *
+ * <I>Special Notes:</I>
+ *     Multiple (OUTPUT_NUM_BUFFERS) buffers will be allocated
+ *     Each buffer is of size (OUTPUT_BUFFER_SIZE)
+ *     Each field in the buffer is initialized
+ *     The event queue for the OUTPUT is initialized
+ *
+ */
+static BUFFER_DESC output_Initialized_Buffers(BUFFER_DESC desc, U32 factor)
+{
+	OUTPUT outbuf;
+	int j;
+
+	SEP_DRV_LOG_TRACE_IN("Desc: %p, factor: %u.", desc, factor);
+
+	/*
+ *  Allocate the BUFFER_DESC, then allocate its buffers
+ */
+	if (desc == NULL) {
+		desc = (BUFFER_DESC)CONTROL_Allocate_Memory(
+			sizeof(BUFFER_DESC_NODE));
+		if (desc == NULL) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Res: NULL (failed allocation for desc!).");
+			return NULL;
+		}
+	}
+	outbuf = &(BUFFER_DESC_outbuf(desc));
+	spin_lock_init(&OUTPUT_buffer_lock(outbuf));
+	for (j = 0; j < OUTPUT_NUM_BUFFERS; j++) {
+		if (OUTPUT_buffer(outbuf, j) == NULL) {
+			OUTPUT_buffer(outbuf, j) = CONTROL_Allocate_Memory(
+				(size_t)OUTPUT_BUFFER_SIZE * factor);
+		}
+		OUTPUT_buffer_full(outbuf, j) = 0;
+		if (!OUTPUT_buffer(outbuf, j)) {
+			/*return NULL to tell the caller that allocation failed*/
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Res: NULL (failed alloc for OUTPUT_buffer(output, %d)!).",
+				j);
+			CONTROL_Free_Memory(desc);
+			return NULL;
+		}
+	}
+	/*
+	 *  Initialize the remaining fields in the BUFFER_DESC
+	 */
+	OUTPUT_current_buffer(outbuf) = 0;
+	OUTPUT_signal_full(outbuf) = FALSE;
+	OUTPUT_remaining_buffer_size(outbuf) = OUTPUT_BUFFER_SIZE * factor;
+	OUTPUT_total_buffer_size(outbuf) = OUTPUT_BUFFER_SIZE * factor;
+	OUTPUT_tasklet_queued(outbuf) = FALSE;
+	init_waitqueue_head(&BUFFER_DESC_queue(desc));
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %p.", desc);
+	return desc;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID output_NMI_Sample_Buffer (
+ *                  )
+ *
+ * @brief       Callback from NMI tasklet. The function checks if any buffers
+ *              are full, and if full, signals the reader threads.
+ *
+ * @param       none
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              This callback was added to handle out-of-band event delivery
+ *              when running in NMI mode
+ */
+static void output_NMI_Sample_Buffer(unsigned long data)
+{
+	U32 cpu_id;
+	OUTPUT outbuf;
+
+	SEP_DRV_LOG_NOTIFICATION_IN("Data: %u.", (U32)data);
+
+	if (data == (unsigned long)-1) {
+		cpu_id = CONTROL_THIS_CPU();
+	} else {
+		cpu_id = data;
+	}
+
+	if (cpu_buf) {
+		outbuf = &BUFFER_DESC_outbuf(&cpu_buf[cpu_id]);
+		if (outbuf && OUTPUT_signal_full(outbuf)) {
+			wake_up_interruptible_sync(
+				&BUFFER_DESC_queue(&cpu_buf[cpu_id]));
+			OUTPUT_signal_full(outbuf) = FALSE;
+			OUTPUT_tasklet_queued(outbuf) = FALSE;
+		}
+	}
+
+	if (cpu_sideband_buf) {
+		outbuf = &BUFFER_DESC_outbuf(&cpu_sideband_buf[cpu_id]);
+		if (outbuf && OUTPUT_signal_full(outbuf)) {
+			wake_up_interruptible_sync(
+				&BUFFER_DESC_queue(&cpu_sideband_buf[cpu_id]));
+			OUTPUT_signal_full(outbuf) = FALSE;
+			OUTPUT_tasklet_queued(outbuf) = FALSE;
+		}
+	}
+
+	SEP_DRV_LOG_NOTIFICATION_OUT("");
+}
+
+/*
+ *  @fn extern void OUTPUT_Initialize(void)
+ *
+ *  @returns OS_STATUS
+ *  @brief  Allocate, initialize, and return all output data structure
+ *
+ * <I>Special Notes:</I>
+ *      Initialize the output structures.
+ *      For each CPU in the system, allocate the output buffers.
+ *      Initialize a module buffer and temp file to hold module information
+ *      Initialize the read queues for each sample buffer
+ *
+ */
+OS_STATUS OUTPUT_Initialize(void)
+{
+	BUFFER_DESC unused;
+	S32 i;
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	flush = 0;
+	if (saved_buffer_size != OUTPUT_BUFFER_SIZE) {
+		if (saved_buffer_size > 0) {
+			OUTPUT_Destroy();
+		}
+		saved_buffer_size = OUTPUT_BUFFER_SIZE;
+	}
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		unused = output_Initialized_Buffers(&cpu_buf[i], 1);
+		if (!unused) {
+			OUTPUT_Destroy();
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"OS_NO_MEM (failed to allocate cpu output buffers!).");
+			return OS_NO_MEM;
+		}
+	}
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			unused = output_Initialized_Buffers(
+				&cpu_sideband_buf[i], 1);
+			if (!unused) {
+				OUTPUT_Destroy();
+				SEP_DRV_LOG_ERROR_TRACE_OUT(
+					"OS_NO_MEM (failed to allocate pebs process info output buffers!).");
+				return OS_NO_MEM;
+			}
+		}
+	}
+
+	/*
+	 *  Just need one module buffer
+	 */
+	unused = output_Initialized_Buffers(module_buf, MODULE_BUFF_SIZE);
+	if (!unused) {
+		OUTPUT_Destroy();
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"OS_NO_MEM (failed to create module output buffers!).");
+		return OS_NO_MEM;
+	}
+
+	SEP_DRV_LOG_TRACE("Set up the tasklet for NMI.");
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+#if !defined(DRV_SEP_ACRN_ON)
+		tasklet_init(&CPU_STATE_nmi_tasklet(&pcb[i]),
+			     output_NMI_Sample_Buffer, (unsigned long)-1);
+#else
+		tasklet_init(&CPU_STATE_nmi_tasklet(&pcb[i]),
+			     output_NMI_Sample_Buffer, (unsigned long)i);
+#endif
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)status);
+	return status;
+}
+
+#if defined(DRV_USE_TASKLET_WORKAROUND)
+static struct tasklet_struct dummy_tasklet;
+
+/*
+ *  @fn extern void output_tasklet_waker (PVOID ptr)
+ *
+ *  @returns None
+ *  @brief   Schedules a dummy tasklet to wake up the tasklet handler on the current core
+ *
+ * <I>Special Notes:</I>
+ *      Workaround for a rare situation where some tasklets are scheduled, but the core's TASKLET softirq bit was reset.
+ *      [NB: this may be caused by a kernel bug; so far, this issue was only observed on kernel version 3.10.0-123.el7]
+ *      Scheduling a (new) tasklet raises a new softirq, and gives 'forgotten' tasklets another chance to be processed.
+ *      This workaround is not fool-proof: if this new tasklet gets 'forgotten' too, the driver will get stuck in the
+ *      Clean Up routine until it gets processed (thanks to an external event raising the TASKLET softirq on this core),
+ *      which might never happen.
+ *
+ */
+static void output_tasklet_waker(PVOID ptr)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+	tasklet_schedule(&dummy_tasklet);
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ *  @fn extern void output_dummy_tasklet_handler (unsigned long dummy)
+ *
+ *  @returns None
+ *  @brief   Dummy tasklet handler.
+ *
+ * <I>Special Notes:</I>
+ *      If this gets executed, the aforementioned workaround was successful.
+ *
+ */
+static void output_dummy_tasklet_handler(unsigned long dummy)
+{
+	SEP_DRV_LOG_NOTIFICATION_IN("Workaround was successful!");
+	SEP_DRV_LOG_NOTIFICATION_OUT("");
+}
+#endif
+
+/*
+ *  @fn extern void OUTPUT_Cleanup (void)
+ *
+ *  @returns None
+ *  @brief   Cleans up NMI tasklets if needed
+ *
+ * <I>Special Notes:</I>
+ *      Waits until all NMI tasklets are complete.
+ *
+ */
+void OUTPUT_Cleanup(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!pcb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pcb).");
+		return;
+	} else {
+		int i;
+		SEP_DRV_LOG_TRACE("Killing all NMI tasklets...");
+
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			SEP_DRV_LOG_TRACE("Killing NMI tasklet %d...", i);
+
+			if (CPU_STATE_nmi_tasklet(&pcb[i]).state) {
+#if defined(DRV_USE_TASKLET_WORKAROUND)
+				SEP_DRV_LOG_ERROR(
+					"Tasklet %d is probably stuck! Trying workaround...",
+					i);
+				tasklet_init(&dummy_tasklet,
+					     output_dummy_tasklet_handler, 0);
+				CONTROL_Invoke_Cpu(i, output_tasklet_waker,
+						   NULL);
+				tasklet_kill(&dummy_tasklet);
+				SEP_DRV_LOG_ERROR(
+					"Workaround was successful for tasklet %d.",
+					i);
+#else
+				SEP_DRV_LOG_ERROR(
+					"Tasklet %d may be stuck. Try to set USE_TASKLET_WORKAROUND=YES in the Makefile if you observe unexpected behavior (e.g. cannot terminate a collection or initiate a new one).",
+					i);
+#endif
+			}
+
+			tasklet_kill(&CPU_STATE_nmi_tasklet(&pcb[i]));
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ *  @fn extern void OUTPUT_Initialize_UNC()
+ *
+ *  @returns OS_STATUS
+ *  @brief  Allocate, initialize, and return all output data structure
+ *
+ * <I>Special Notes:</I>
+ *      Initialize the output structures.
+ *      For each CPU in the system, allocate the output buffers.
+ *      Initialize a module buffer and temp file to hold module information
+ *      Initialize the read queues for each sample buffer
+ *
+ */
+OS_STATUS OUTPUT_Initialize_UNC(void)
+{
+	BUFFER_DESC unused;
+	int i;
+	OS_STATUS status = OS_SUCCESS;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	for (i = 0; i < num_packages; i++) {
+		unused = output_Initialized_Buffers(&unc_buf[i], 1);
+		if (!unused) {
+			OUTPUT_Destroy();
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Failed to allocate package output buffers!");
+			return OS_NO_MEM;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)status);
+	return status;
+}
+
+/*
+ *  @fn OS_STATUS  OUTPUT_Flush()
+ *
+ *  @brief  Flush the module buffers and sample buffers
+ *
+ *  @return OS_STATUS
+ *
+ *  For each CPU in the system, set buffer full to the byte count to flush.
+ *  Flush the modules buffer, as well.
+ *
+ */
+int OUTPUT_Flush(void)
+{
+	int i;
+	int writers = 0;
+	OUTPUT outbuf;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	/*
+	 *  Flush all remaining data to files
+	 *  set up a flush event
+	 */
+	init_waitqueue_head(&flush_queue);
+	SEP_DRV_LOG_TRACE(
+		"Waiting for %d writers.",
+		(GLOBAL_STATE_num_cpus(driver_state) + OTHER_C_DEVICES));
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		if (CPU_STATE_initial_mask(&pcb[i]) == 0) {
+			continue;
+		}
+		outbuf = &(cpu_buf[i].outbuf);
+		writers += 1;
+
+		OUTPUT_buffer_full(outbuf, OUTPUT_current_buffer(outbuf)) =
+			OUTPUT_total_buffer_size(outbuf) -
+			OUTPUT_remaining_buffer_size(outbuf);
+	}
+
+	if (unc_buf_init) {
+		for (i = 0; i < num_packages; i++) {
+			outbuf = &(unc_buf[i].outbuf);
+			writers += 1;
+
+			OUTPUT_buffer_full(outbuf,
+					   OUTPUT_current_buffer(outbuf)) =
+				OUTPUT_total_buffer_size(outbuf) -
+				OUTPUT_remaining_buffer_size(outbuf);
+		}
+	}
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			if (CPU_STATE_initial_mask(&pcb[i]) == 0) {
+				continue;
+			}
+			outbuf = &(cpu_sideband_buf[i].outbuf);
+			writers += 1;
+
+			OUTPUT_buffer_full(outbuf,
+					   OUTPUT_current_buffer(outbuf)) =
+				OUTPUT_total_buffer_size(outbuf) -
+				OUTPUT_remaining_buffer_size(outbuf);
+		}
+	}
+
+	atomic_set(&flush_writers, writers + OTHER_C_DEVICES);
+	// Flip the switch to terminate the output threads
+	// Do not do this earlier, as threads may terminate before all the data is flushed
+	flush = 1;
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		if (CPU_STATE_initial_mask(&pcb[i]) == 0) {
+			continue;
+		}
+		outbuf = &BUFFER_DESC_outbuf(&cpu_buf[i]);
+		OUTPUT_buffer_full(outbuf, OUTPUT_current_buffer(outbuf)) =
+			OUTPUT_total_buffer_size(outbuf) -
+			OUTPUT_remaining_buffer_size(outbuf);
+		wake_up_interruptible_sync(&BUFFER_DESC_queue(&cpu_buf[i]));
+	}
+
+	if (unc_buf_init) {
+		for (i = 0; i < num_packages; i++) {
+			outbuf = &BUFFER_DESC_outbuf(&unc_buf[i]);
+			OUTPUT_buffer_full(outbuf,
+					   OUTPUT_current_buffer(outbuf)) =
+				OUTPUT_total_buffer_size(outbuf) -
+				OUTPUT_remaining_buffer_size(outbuf);
+			wake_up_interruptible_sync(
+				&BUFFER_DESC_queue(&unc_buf[i]));
+		}
+	}
+
+	if (multi_pebs_enabled || sched_switch_enabled) {
+		for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+			if (CPU_STATE_initial_mask(&pcb[i]) == 0) {
+				continue;
+			}
+			outbuf = &BUFFER_DESC_outbuf(&cpu_sideband_buf[i]);
+			OUTPUT_buffer_full(outbuf,
+					   OUTPUT_current_buffer(outbuf)) =
+				OUTPUT_total_buffer_size(outbuf) -
+				OUTPUT_remaining_buffer_size(outbuf);
+			wake_up_interruptible_sync(
+				&BUFFER_DESC_queue(&cpu_sideband_buf[i]));
+		}
+	}
+	// Flush all data from the module buffers
+
+	outbuf = &BUFFER_DESC_outbuf(module_buf);
+
+	OUTPUT_buffer_full(outbuf, OUTPUT_current_buffer(outbuf)) =
+		OUTPUT_total_buffer_size(outbuf) -
+		OUTPUT_remaining_buffer_size(outbuf);
+
+	SEP_DRV_LOG_TRACE("Waking up module_queue.");
+	wake_up_interruptible_sync(&BUFFER_DESC_queue(module_buf));
+
+	//Wait for buffers to empty
+	while (atomic_read(&flush_writers) != 0) {
+		unsigned long delay;
+		U32 res;
+		delay = msecs_to_jiffies(1000);
+		res = wait_event_interruptible_timeout(
+			flush_queue, atomic_read(&flush_writers) == 0, delay);
+
+		if (res == ERESTARTSYS || res == 0) {
+			SEP_DRV_LOG_TRACE(
+				"Wait_event_interruptible_timeout(flush_queue): %u, %u writers.",
+				res, atomic_read(&flush_writers));
+			continue;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE("Awakened from flush_queue.");
+	flush = 0;
+
+	SEP_DRV_LOG_TRACE_OUT("Res: 0.");
+	return 0;
+}
+
+/*
+ *  @fn extern void OUTPUT_Destroy()
+ *
+ *  @param   buffer  -  seed name of the output file
+ *  @param   len     -  length of the seed name
+ *  @returns OS_STATUS
+ *  @brief   Deallocate output structures
+ *
+ * <I>Special Notes:</I>
+ *      Free the module buffers
+ *      For each CPU in the system, free the sampling buffers
+ */
+int OUTPUT_Destroy(void)
+{
+	int i, n;
+	OUTPUT outbuf;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (module_buf) {
+		outbuf = &BUFFER_DESC_outbuf(module_buf);
+		output_Free_Buffers(module_buf,
+				    OUTPUT_total_buffer_size(outbuf));
+	}
+
+	if (cpu_buf != NULL) {
+		n = GLOBAL_STATE_num_cpus(driver_state);
+		for (i = 0; i < n; i++) {
+			outbuf = &BUFFER_DESC_outbuf(&cpu_buf[i]);
+			output_Free_Buffers(&cpu_buf[i],
+					    OUTPUT_total_buffer_size(outbuf));
+		}
+	}
+
+	if (unc_buf != NULL) {
+		n = num_packages;
+		for (i = 0; i < n; i++) {
+			outbuf = &BUFFER_DESC_outbuf(&unc_buf[i]);
+			output_Free_Buffers(&unc_buf[i],
+					    OUTPUT_total_buffer_size(outbuf));
+		}
+	}
+
+	if (cpu_sideband_buf != NULL) {
+		n = GLOBAL_STATE_num_cpus(driver_state);
+		for (i = 0; i < n; i++) {
+			outbuf = &BUFFER_DESC_outbuf(&cpu_sideband_buf[i]);
+			output_Free_Buffers(&cpu_sideband_buf[i],
+					    OUTPUT_total_buffer_size(outbuf));
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: 0.");
+	return 0;
+}
diff --git a/drivers/platform/x86/sepdk/sep/pci.c b/drivers/platform/x86/sepdk/sep/pci.c
new file mode 100755
index 000000000000..12a93804975c
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/pci.c
@@ -0,0 +1,661 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/errno.h>
+#include <linux/pci.h>
+#include <linux/types.h>
+#include <asm/page.h>
+#include <asm/io.h>
+
+#include "lwpmudrv_types.h"
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#endif
+
+#include "inc/lwpmudrv.h"
+#include "inc/pci.h"
+#include "inc/utility.h"
+
+static struct pci_bus *pci_buses[MAX_BUSNO];
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern VOID PCI_Initialize(void)
+ *
+ * @param   none
+ *
+ * @return  none
+ *
+ * @brief   Initializes the pci_buses array.
+ *
+ */
+VOID PCI_Initialize(void)
+{
+	U32 i;
+	U32 num_found_buses = 0;
+
+	SEP_DRV_LOG_INIT_IN("Initializing pci_buses...");
+
+	for (i = 0; i < MAX_BUSNO; i++) {
+		pci_buses[i] = pci_find_bus(0, i);
+		if (pci_buses[i]) {
+			SEP_DRV_LOG_DETECTION("Found PCI bus 0x%x at %p.", i,
+					      pci_buses[i]);
+			num_found_buses++;
+		}
+		SEP_DRV_LOG_TRACE("pci_buses[%u]: %p.", i, pci_buses[i]);
+	}
+
+	SEP_DRV_LOG_INIT_OUT("Found %u buses.", num_found_buses);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Read_U32(bus, device, function, offset)
+ *
+ * @param    bus        - target bus
+ * @param    device     - target device
+ * @param    function   - target function
+ * @param    offset     - target register offset
+ *
+ * @return  Value at this location
+ *
+ * @brief   Reads a U32 from PCI configuration space
+ *
+ */
+U32 PCI_Read_U32(U32 bus, U32 device, U32 function, U32 offset)
+{
+	U32 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read BDF(%x:%x:%x)[0x%x](4B)...", bus,
+				device, function, offset);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset, 4,
+					  &res);
+	} else {
+		SEP_DRV_LOG_ERROR(
+			"Could not read BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has read BDF(%x:%x:%x)[0x%x](4B): 0x%x.", bus,
+				 device, function, offset, res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Read_U32_Valid(bus,device,function,offset,invalid_value)
+ *
+ * @param    bus            - target bus
+ * @param    device         - target device
+ * @param    function       - target function
+ * @param    offset         - target register offset
+ * @param    invalid_value  - value against which to compare PCI-obtained value
+ *
+ * @return  Value at this location (if value != invalid_value), 0 otherwise
+ *
+ * @brief   Reads a U32 from PCI configuration space
+ *
+ */
+U32 PCI_Read_U32_Valid(U32 bus, U32 device, U32 function, U32 offset,
+			      U32 invalid_value)
+{
+	U32 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read BDF(%x:%x:%x)[0x%x](4B)...", bus,
+				device, function, offset);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset, 4,
+					  &res);
+		if (res == invalid_value) {
+			res = 0;
+			SEP_DRV_LOG_REGISTER_OUT(
+				"Has read BDF(%x:%x:%x)[0x%x](4B): 0x%x(invalid value)",
+				bus, device, function, offset, res);
+		} else {
+			SEP_DRV_LOG_REGISTER_OUT(
+				"Has read BDF(%x:%x:%x)[0x%x](4B): 0x%x.", bus,
+				device, function, offset, res);
+		}
+	} else {
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Could not read BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+	}
+
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Read_U64(bus, device, function, offset)
+ *
+ * @param   bus        - target bus
+ * @param   device     - target device
+ * @param   function   - target function
+ * @param   offset     - target register offset
+ *
+ * @return  Value at this location
+ *
+ * @brief   Reads a U64 from PCI configuration space
+ *
+ */
+U64 PCI_Read_U64(U32 bus, U32 device, U32 function, U32 offset)
+{
+	U64 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read BDF(%x:%x:%x)[0x%x](8B)...", bus,
+				device, function, offset);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset, 4,
+					  (U32 *)&res);
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset + 4, 4,
+					  ((U32 *)&res) + 1);
+	} else {
+		SEP_DRV_LOG_ERROR(
+			"Could not read BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has read BDF(%x:%x:%x)[0x%x](8B): 0x%llx.",
+				 bus, device, function, offset, res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Read_U64_Valid(bus,device,function,offset,invalid_value)
+ *
+ * @param   bus             - target bus
+ * @param   device          - target device
+ * @param   function        - target function
+ * @param   offset          - target register offset
+ * @param   invalid_value   - value against which to compare PCI-obtained value
+ *
+ * @return  Value at this location (if value != invalid_value), 0 otherwise
+ *
+ * @brief   Reads a U64 from PCI configuration space
+ *
+ */
+U64 PCI_Read_U64_Valid(U32 bus, U32 device, U32 function, U32 offset,
+			      U64 invalid_value)
+{
+	U64 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read BDF(%x:%x:%x)[0x%x](8B)...", bus,
+				device, function, offset);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset, 4,
+					  (U32 *)&res);
+		pci_buses[bus]->ops->read(pci_buses[bus], devfn, offset + 4, 4,
+					  ((U32 *)&res) + 1);
+
+		if (res == invalid_value) {
+			res = 0;
+			SEP_DRV_LOG_REGISTER_OUT(
+				"Has read BDF(%x:%x:%x)[0x%x](8B): 0x%llx(invalid val)",
+				bus, device, function, offset, res);
+		} else {
+			SEP_DRV_LOG_REGISTER_OUT(
+				"Has read BDF(%x:%x:%x)[0x%x](8B): 0x%llx.",
+				bus, device, function, offset, res);
+		}
+	} else {
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Could not read BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+	}
+
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Write_U32(bus, device, function, offset, value)
+ *
+ * @param    bus            - target bus
+ * @param    device         - target device
+ * @param    function       - target function
+ * @param    offset         - target register offset
+ * @param    value          - value to write
+ *
+ * @return  0 in case of success, 1 otherwise
+ *
+ * @brief    Writes a U32 to PCI configuration space
+ *
+ */
+U32 PCI_Write_U32(U32 bus, U32 device, U32 function, U32 offset,
+			 U32 value)
+{
+	U32 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will write BDF(%x:%x:%x)[0x%x](4B): 0x%x...",
+				bus, device, function, offset, value);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->write(pci_buses[bus], devfn, offset, 4,
+					   value);
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Has written BDF(%x:%x:%x)[0x%x](4B): 0x%x.", bus,
+			device, function, offset, value);
+	} else {
+		SEP_DRV_LOG_ERROR(
+			"Could not write BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+		res = 1;
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Failed to write BDF(%x:%x:%x)[0x%x](4B): 0x%x.", bus,
+			device, function, offset, value);
+	}
+
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Write_U64(bus, device, function, offset, value)
+ *
+ * @param    bus            - target bus
+ * @param    device         - target device
+ * @param    function       - target function
+ * @param    offset         - target register offset
+ * @param    value          - value to write
+ *
+ * @return  0 in case of success, 1 otherwise
+ *
+ * @brief    Writes a U64 to PCI configuration space
+ *
+ */
+U32 PCI_Write_U64(U32 bus, U32 device, U32 function, U32 offset,
+			 U64 value)
+{
+	U32 res = 0;
+	U32 devfn = (device << 3) | (function & 0x7);
+
+	SEP_DRV_LOG_REGISTER_IN("Will write BDF(%x:%x:%x)[0x%x](8B): 0x%llx...",
+				bus, device, function, offset, value);
+
+	if (bus < MAX_BUSNO && pci_buses[bus]) {
+		pci_buses[bus]->ops->write(pci_buses[bus], devfn, offset, 4,
+					   (U32)value);
+		pci_buses[bus]->ops->write(pci_buses[bus], devfn, offset + 4, 4,
+					   (U32)(value >> 32));
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Has written BDF(%x:%x:%x)[0x%x](8B): 0x%llx.", bus,
+			device, function, offset, value);
+	} else {
+		SEP_DRV_LOG_ERROR(
+			"Could not write BDF(%x:%x:%x)[0x%x]: bus not found!",
+			bus, device, function, offset);
+		res = 1;
+		SEP_DRV_LOG_REGISTER_OUT(
+			"Failed to write BDF(%x:%x:%x)[0x%x](8B): 0x%llx.", bus,
+			device, function, offset, value);
+	}
+
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern int PCI_Read_From_Memory_Address(addr, val)
+ *
+ * @param    addr    - physical address in mmio
+ * @param   *value  - value at this address
+ *
+ * @return  status
+ *
+ * @brief   Read memory mapped i/o physical location
+ *
+ */
+int PCI_Read_From_Memory_Address(U32 addr, U32 *val)
+{
+	U32 aligned_addr, offset, value;
+	PVOID base;
+
+	SEP_DRV_LOG_TRACE_IN("Addr: %x, val_pointer: %p.", addr, val);
+
+	if (addr <= 0) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (addr <= 0!).");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE("Preparing to reading physical address: %x.", addr);
+	offset = addr & ~PAGE_MASK;
+	aligned_addr = addr & PAGE_MASK;
+	SEP_DRV_LOG_TRACE("Aligned physical address: %x, offset: %x.",
+			  aligned_addr, offset);
+
+	base = (PVOID)ioremap_nocache(aligned_addr, PAGE_SIZE);
+	if (base == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (mapping failed!).");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_REGISTER_IN("Will read PCI address %u (mapped at %p).",
+				addr, base + offset);
+	value = readl((void __iomem *)(base + offset));
+	SEP_DRV_LOG_REGISTER_OUT("Read PCI address %u (mapped at %p): %x.",
+				 addr, base + offset, value);
+
+	*val = value;
+
+	iounmap((void __iomem *)base);
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern int PCI_Write_To_Memory_Address(addr, val)
+ *
+ * @param   addr   - physical address in mmio
+ * @param   value  - value to be written
+ *
+ * @return  status
+ *
+ * @brief   Write to memory mapped i/o physical location
+ *
+ */
+int PCI_Write_To_Memory_Address(U32 addr, U32 val)
+{
+	U32 aligned_addr, offset;
+	PVOID base;
+
+	SEP_DRV_LOG_TRACE_IN("Addr: %x, val: %x.", addr, val);
+
+	if (addr <= 0) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (addr <= 0!).");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_TRACE(
+		"Preparing to writing physical address: %x (val: %x).", addr,
+		val);
+	offset = addr & ~PAGE_MASK;
+	aligned_addr = addr & PAGE_MASK;
+	SEP_DRV_LOG_TRACE("Aligned physical address: %x, offset: %x (val: %x).",
+			  aligned_addr, offset, val);
+
+	base = (PVOID)ioremap_nocache(aligned_addr, PAGE_SIZE);
+	if (base == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (mapping failed!).");
+		return OS_INVALID;
+	}
+
+	SEP_DRV_LOG_REGISTER_IN("Will write PCI address %u (mapped at %p): %x.",
+				addr, base + offset, val);
+	writel(val, (void __iomem *)(base + offset));
+	SEP_DRV_LOG_REGISTER_OUT("Wrote PCI address %u (mapped at %p): %x.",
+				 addr, base + offset, val);
+
+	iounmap((void __iomem *)base);
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern U32 PCI_Map_Memory(SEP_MMIO_NODE *node, U64 phy_address,
+ *				U64 map_size)
+ *
+ * @param    node        - MAP NODE to use
+ * @param    phy_address - Address to be mapped
+ * @param    map_size    - Amount of memory to map (has to be a multiple of 4k)
+ *
+ * @return   OS_SUCCESS or OS_INVALID
+ *
+ * @brief    Maps a physical address to a virtual address
+ *
+ */
+OS_STATUS PCI_Map_Memory(SEP_MMIO_NODE *node, U64 phy_address,
+				U32 map_size)
+{
+	U8 *res;
+
+	SEP_DRV_LOG_INIT_IN("Node: %p, phy_address: %llx, map_size: %u.", node,
+			    phy_address, map_size);
+
+	if (!node || !phy_address || !map_size || (phy_address & 4095)) {
+		SEP_DRV_LOG_ERROR_INIT_OUT("Invalid parameters, aborting!");
+		return OS_INVALID;
+	}
+
+	res = (U8 *)ioremap_nocache(phy_address, map_size);
+	if (!res) {
+		SEP_DRV_LOG_ERROR_INIT_OUT("Map operation failed!");
+		return OS_INVALID;
+	}
+
+	SEP_MMIO_NODE_physical_address(node) = (UIOP)phy_address;
+	SEP_MMIO_NODE_virtual_address(node) = (UIOP)res;
+	SEP_MMIO_NODE_map_token(node) = (UIOP)res;
+	SEP_MMIO_NODE_size(node) = map_size;
+
+	SEP_DRV_LOG_INIT_OUT("Addr:0x%llx->0x%llx, tok:0x%llx, sz:%u.",
+			     SEP_MMIO_NODE_physical_address(node),
+			     SEP_MMIO_NODE_virtual_address(node),
+			     SEP_MMIO_NODE_map_token(node),
+			     SEP_MMIO_NODE_size(node));
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void PCI_Unmap_Memory(SEP_MMIO_NODE *node)
+ *
+ * @param   node - memory map node to clean up
+ *
+ * @return  none
+ *
+ * @brief   Unmaps previously mapped memory
+ *
+ */
+void PCI_Unmap_Memory(SEP_MMIO_NODE *node)
+{
+	SEP_DRV_LOG_INIT_IN("Unmapping node %p.", node);
+
+	if (node) {
+		if (SEP_MMIO_NODE_size(node)) {
+			SEP_DRV_LOG_TRACE(
+				"Unmapping token 0x%llx (0x%llx->0x%llx)[%uB].",
+				SEP_MMIO_NODE_map_token(node),
+				SEP_MMIO_NODE_physical_address(node),
+				SEP_MMIO_NODE_virtual_address(node),
+				SEP_MMIO_NODE_size(node));
+			iounmap((void __iomem *)(UIOP)SEP_MMIO_NODE_map_token(node));
+			SEP_MMIO_NODE_size(node) = 0;
+			SEP_MMIO_NODE_map_token(node) = 0;
+			SEP_MMIO_NODE_virtual_address(node) = 0;
+			SEP_MMIO_NODE_physical_address(node) = 0;
+		} else {
+			SEP_DRV_LOG_TRACE("Already unmapped.");
+		}
+	}
+
+	SEP_DRV_LOG_INIT_OUT("Unmapped node %p.", node);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn U32 PCI_MMIO_Read_U32(virtual_address_base, offset)
+ *
+ * @param   virtual_address_base   - Virtual address base
+ * @param   offset                 - Register offset
+ *
+ * @return  U32 read from an MMIO register
+ *
+ * @brief   Reads U32 value from MMIO
+ *
+ */
+U32 PCI_MMIO_Read_U32(U64 virtual_address_base, U32 offset)
+{
+	U32 temp_u32 = 0LL;
+	U32 *computed_address;
+
+	computed_address =
+		(U32 *)(((char *)(UIOP)virtual_address_base) + offset);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read U32(0x%llx + 0x%x = 0x%p).",
+				virtual_address_base, offset, computed_address);
+
+	if (!virtual_address_base) {
+		SEP_DRV_LOG_ERROR("Invalid base for U32(0x%llx + 0x%x = 0x%p)!",
+				  virtual_address_base, offset,
+				  computed_address);
+		temp_u32 = 0;
+	} else {
+		temp_u32 = *computed_address;
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has read U32(0x%llx + 0x%x): 0x%x.",
+				 virtual_address_base, offset, temp_u32);
+	return temp_u32;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn U64 PCI_MMIO_Read_U64(virtual_address_base, offset)
+ *
+ * @param   virtual_address_base   - Virtual address base
+ * @param   offset                 - Register offset
+ *
+ * @return  U64 read from an MMIO register
+ *
+ * @brief   Reads U64 value from MMIO
+ *
+ */
+U64 PCI_MMIO_Read_U64(U64 virtual_address_base, U32 offset)
+{
+	U64 temp_u64 = 0LL;
+	U64 *computed_address;
+
+	computed_address =
+		(U64 *)(((char *)(UIOP)virtual_address_base) + offset);
+
+	SEP_DRV_LOG_REGISTER_IN("Will read U64(0x%llx + 0x%x = 0x%p).",
+				virtual_address_base, offset, computed_address);
+
+	if (!virtual_address_base) {
+		SEP_DRV_LOG_ERROR("Invalid base for U32(0x%llx + 0x%x = 0x%p)!",
+				  virtual_address_base, offset,
+				  computed_address);
+		temp_u64 = 0;
+	} else {
+		temp_u64 = *computed_address;
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has read U64(0x%llx + 0x%x): 0x%llx.",
+				 virtual_address_base, offset, temp_u64);
+	return temp_u64;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void PCI_MMIO_Write_U32(virtual_address_base, offset, value)
+ *
+ * @param   virtual_address_base   - Virtual address base
+ * @param   offset                 - Register offset
+ * @param   value                  - Value to write
+ *
+ * @return  U32 write into an MMIO register
+ *
+ * @brief   Writes U32 value to MMIO
+ *
+ */
+void PCI_MMIO_Write_U32(U64 virtual_address_base, U32 offset, U32 value)
+{
+	U32 *computed_address;
+
+	computed_address =
+		(U32 *)(((char *)(UIOP)virtual_address_base) + offset);
+
+	SEP_DRV_LOG_REGISTER_IN("Writing 0x%x to U32(0x%llx + 0x%x = 0x%p).",
+				value, virtual_address_base, offset,
+				computed_address);
+
+	if (!virtual_address_base) {
+		SEP_DRV_LOG_ERROR("Invalid base for U32(0x%llx + 0x%x = 0x%p)!",
+				  virtual_address_base, offset,
+				  computed_address);
+	} else {
+		*computed_address = value;
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has written 0x%x to U32(0x%llx + 0x%x).",
+				 value, virtual_address_base, offset);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void PCI_MMIO_Write_U64(virtual_address_base, offset, value)
+ *
+ * @param   virtual_address_base   - Virtual address base
+ * @param   offset                 - Register offset
+ * @param   value                  - Value to write
+ *
+ * @return  U64 write into an MMIO register
+ *
+ * @brief   Writes U64 value to MMIO
+ *
+ */
+void PCI_MMIO_Write_U64(U64 virtual_address_base, U32 offset, U64 value)
+{
+	U64 *computed_address;
+
+	computed_address =
+		(U64 *)(((char *)(UIOP)virtual_address_base) + offset);
+
+	SEP_DRV_LOG_REGISTER_IN("Writing 0x%llx to U64(0x%llx + 0x%x = 0x%p).",
+				value, virtual_address_base, offset,
+				computed_address);
+
+	if (!virtual_address_base) {
+		SEP_DRV_LOG_ERROR("Invalid base for U32(0x%llx + 0x%x = 0x%p)!",
+				  virtual_address_base, offset,
+				  computed_address);
+	} else {
+		*computed_address = value;
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has written 0x%llx to U64(0x%llx + 0x%x).",
+				 value, virtual_address_base, offset);
+}
diff --git a/drivers/platform/x86/sepdk/sep/pebs.c b/drivers/platform/x86/sepdk/sep/pebs.c
new file mode 100755
index 000000000000..7537c1136ec4
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/pebs.c
@@ -0,0 +1,1957 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/percpu.h>
+#include <linux/mm.h>
+#include <linux/uaccess.h>
+#include <asm/segment.h>
+#include <asm/page.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "core2.h"
+#include "utility.h"
+#include "output.h"
+#include "ecb_iterators.h"
+#include "pebs.h"
+
+#if defined(DRV_USE_KAISER)
+#include <asm/kaiser.h>
+#include <linux/kallsyms.h>
+int (*local_kaiser_add_mapping)(unsigned long, unsigned long,
+				unsigned long) = NULL;
+void (*local_kaiser_remove_mapping)(unsigned long, unsigned long) = NULL;
+#elif defined(DRV_USE_PTI)
+#include <asm/cpu_entry_area.h>
+#include <linux/kallsyms.h>
+#include <asm/pgtable_types.h>
+#include <asm/intel_ds.h>
+#include <asm/tlbflush.h>
+static void (*local_cea_set_pte)(void *cea_vaddr, phys_addr_t pa,
+			  pgprot_t flags) = NULL;
+static void (*local_do_kernel_range_flush)(void *info) = NULL;
+static DEFINE_PER_CPU(PVOID, dts_buffer_cea);
+#endif
+
+static PVOID pebs_global_memory;
+static size_t pebs_global_memory_size;
+
+extern DRV_CONFIG drv_cfg;
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+
+#if defined(DRV_USE_PTI)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Update_CEA (S32)
+ *
+ * @brief       Flush the TLB entries related to PEBS buffer in cpu entry area
+ *
+ * @param       this_cpu current cpu
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID pebs_Update_CEA(S32 this_cpu)
+{
+	unsigned long cea_start_addr;
+	unsigned long cea_end_addr;
+
+	SEP_DRV_LOG_TRACE_IN("This_cpu: %d.", this_cpu);
+
+	if (per_cpu(dts_buffer_cea, this_cpu) != 0) {
+		cea_start_addr =
+			(unsigned long)per_cpu(dts_buffer_cea, this_cpu);
+		cea_end_addr = cea_start_addr +
+			       (unsigned long)CPU_STATE_dts_buffer_size(
+				       &pcb[this_cpu]);
+		if (local_do_kernel_range_flush) {
+			struct flush_tlb_info info;
+			info.start = cea_start_addr;
+			info.end = cea_end_addr;
+			local_do_kernel_range_flush(&info);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Corei7_Initialize_Threshold
+ *		(dts, LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]))
+ *
+ * @brief       The nehalem specific initialization
+ *
+ * @param       dts  - dts description
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID pebs_Corei7_Initialize_Threshold(DTS_BUFFER_EXT dts)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Dts: %p.", dts);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	DTS_BUFFER_EXT_pebs_threshold(dts) =
+		DTS_BUFFER_EXT_pebs_base(dts) +
+		(LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) *
+		(U64)DEV_CONFIG_pebs_record_num(pcfg));
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Corei7_Overflow ()
+ *
+ * @brief       The Nehalem specific overflow check
+ *
+ * @param       this_cpu        - cpu id
+ *              overflow_status - overflow status
+ *              rec_index       - record index
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *    Check the global overflow field of the buffer descriptor.
+ *    Precise events can be allocated on any of the 4 general purpose
+ *    registers.
+ */
+static U64 pebs_Corei7_Overflow(S32 this_cpu, U64 overflow_status,
+				U32 rec_index)
+{
+	DTS_BUFFER_EXT dtes;
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	PEBS_REC_EXT pb;
+	U8 pebs_ptr_check = FALSE;
+	U32 dev_idx = core_to_dev_map[this_cpu];
+
+	SEP_DRV_LOG_TRACE_IN(
+		"This_cpu: %d, overflow_status: %llx, rec_index: %u.", this_cpu,
+		overflow_status, rec_index);
+
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	SEP_DRV_LOG_TRACE("This_cpu: %d, dtes %p.", this_cpu, dtes);
+
+	if (!dtes) {
+		return overflow_status;
+	}
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	SEP_DRV_LOG_TRACE("This_cpu: %d, pebs_base %p.", this_cpu, pebs_base);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+	if (pebs_ptr_check) {
+		pb = (PEBS_REC_EXT)pebs_ptr;
+		overflow_status |= PEBS_REC_EXT_glob_perf_overflow(pb);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llx.", overflow_status);
+	return overflow_status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Corei7_Overflow_APEBS ()
+ *
+ * @brief       Overflow check
+ *
+ * @param       this_cpu        - cpu id
+ *              overflow_status - overflow status
+ *              rec_index       - record index
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *    Check the global overflow field of the buffer descriptor.
+ *    Precise events can be allocated on any of the 8 general purpose
+ *    registers or 4 fixed registers.
+ */
+static U64 pebs_Corei7_Overflow_APEBS(S32 this_cpu, U64 overflow_status,
+				      U32 rec_index)
+{
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	ADAPTIVE_PEBS_BASIC_INFO pb;
+	DTS_BUFFER_EXT1 dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+	U8 pebs_ptr_check = FALSE;
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	DEV_CONFIG pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!dtes) {
+		return overflow_status;
+	}
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT1_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT1_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT1_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+
+	if (pebs_ptr_check && DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+		pb = (ADAPTIVE_PEBS_BASIC_INFO)pebs_ptr;
+		overflow_status |=
+			ADAPTIVE_PEBS_BASIC_INFO_applicable_counters(pb);
+	}
+
+	return overflow_status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Core2_Initialize_Threshold
+ *		(dts, LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]))
+ *
+ * @brief       The Core2 specific initialization
+ *
+ * @param       dts - dts description
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID pebs_Core2_Initialize_Threshold(DTS_BUFFER_EXT dts)
+{
+	SEP_DRV_LOG_TRACE_IN("Dts: %p.", dts);
+
+	DTS_BUFFER_EXT_pebs_threshold(dts) = DTS_BUFFER_EXT_pebs_base(dts);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Core2_Overflow
+ *		(dts, LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]))
+ *
+ * @brief       The Core2 specific overflow check
+ *
+ * @param       this_cpu        - cpu id
+ *              overflow_status - overflow status
+ *              rec_index       - record index
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *    Check the base and the index fields of the circular buffer, if they are
+ *    not the same, then a precise event has overflowed.  Precise events are
+ *    allocated only on register#0.
+ */
+static U64 pebs_Core2_Overflow(S32 this_cpu, U64 overflow_status, U32 rec_index)
+{
+	DTS_BUFFER_EXT dtes;
+	U8 status = FALSE;
+
+	SEP_DRV_LOG_TRACE_IN(
+		"This_cpu: %d, overflow_status: %llx, rec_index: %u.", this_cpu,
+		overflow_status, rec_index);
+
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Res: %llx (dtes is NULL!).",
+					    overflow_status);
+		return overflow_status;
+	}
+	status = (U8)((dtes) && (DTS_BUFFER_EXT_pebs_index(dtes) !=
+				 DTS_BUFFER_EXT_pebs_base(dtes)));
+	if (status) {
+		// Merom allows only for GP register 0 to be precise capable
+		overflow_status |= 0x1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llx.", overflow_status);
+	return overflow_status;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Modify_IP (sample, is_64bit_addr)
+ *
+ * @brief       Change the IP field in the sample to that in the PEBS record
+ *
+ * @param       sample        - sample buffer
+ * @param       is_64bit_addr - are we in a 64 bit module
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID pebs_Modify_IP(void *sample, DRV_BOOL is_64bit_addr, U32 rec_index)
+{
+	SampleRecordPC *psamp = sample;
+	DTS_BUFFER_EXT dtes;
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	PEBS_REC_EXT pb;
+	U8 pebs_ptr_check = FALSE;
+	U32 this_cpu;
+	U32 dev_idx;
+
+	SEP_DRV_LOG_TRACE_IN("Sample: %p, is_64bit_addr: %u, rec_index: %u.",
+			     sample, is_64bit_addr, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes || !psamp) {
+		return;
+	}
+	SEP_DRV_LOG_TRACE("In PEBS Fill Buffer: cpu %d.", CONTROL_THIS_CPU());
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+	if (pebs_ptr_check) {
+		pb = (PEBS_REC_EXT)pebs_ptr;
+		if (is_64bit_addr) {
+			SAMPLE_RECORD_iip(psamp) = PEBS_REC_EXT_linear_ip(pb);
+			SAMPLE_RECORD_ipsr(psamp) = PEBS_REC_EXT_r_flags(pb);
+		} else {
+			SAMPLE_RECORD_eip(psamp) =
+				PEBS_REC_EXT_linear_ip(pb) & 0xFFFFFFFF;
+			SAMPLE_RECORD_eflags(psamp) =
+				PEBS_REC_EXT_r_flags(pb) & 0xFFFFFFFF;
+		}
+	}
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Modify_IP_With_Eventing_IP (sample, is_64bit_addr)
+ *
+ * @brief       Change the IP field in the sample to that in the PEBS record
+ *
+ * @param       sample        - sample buffer
+ * @param       is_64bit_addr - are we in a 64 bit module
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID pebs_Modify_IP_With_Eventing_IP(void *sample,
+					    DRV_BOOL is_64bit_addr,
+					    U32 rec_index)
+{
+	SampleRecordPC *psamp = sample;
+	DTS_BUFFER_EXT dtes;
+	S8 *pebs_ptr, *pebs_base, *pebs_index;
+	U64 ip = 0, flags = 0;
+	U8 pebs_ptr_check = FALSE;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Sample: %p, is_64bit_addr: %u, rec_index: %u.",
+			     sample, is_64bit_addr, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes || !psamp) {
+		return;
+	}
+
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+
+	if (!pebs_ptr_check) {
+		return;
+	}
+	if (DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+		ip = ADAPTIVE_PEBS_BASIC_INFO_eventing_ip(
+			(ADAPTIVE_PEBS_BASIC_INFO)pebs_ptr);
+		if (DEV_CONFIG_apebs_collect_gpr(pcfg)) {
+			flags = ADAPTIVE_PEBS_GPR_INFO_rflags((
+				ADAPTIVE_PEBS_GPR_INFO)(
+				pebs_ptr + LWPMU_DEVICE_apebs_gpr_offset(
+						   &devices[dev_idx])));
+		}
+	} else {
+		ip = PEBS_REC_EXT1_eventing_ip((PEBS_REC_EXT1)pebs_ptr);
+		flags = PEBS_REC_EXT1_r_flags((PEBS_REC_EXT1)pebs_ptr);
+	}
+	if (is_64bit_addr) {
+		SAMPLE_RECORD_iip(psamp) = ip;
+		SAMPLE_RECORD_ipsr(psamp) = flags;
+	} else {
+		SAMPLE_RECORD_eip(psamp) = ip & 0xFFFFFFFF;
+		SAMPLE_RECORD_eflags(psamp) = flags & 0xFFFFFFFF;
+	}
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Modify_TSC (sample)
+ *
+ * @brief       Change the TSC field in the sample to that in the PEBS record
+ *
+ * @param       sample        - sample buffer
+ *              rec_index     - record index
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID pebs_Modify_TSC(void *sample, U32 rec_index)
+{
+	SampleRecordPC *psamp = sample;
+	DTS_BUFFER_EXT dtes;
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	U64 tsc;
+	U8 pebs_ptr_check = FALSE;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Sample: %p, rec_index: %u.", sample, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes || !psamp) {
+		return;
+	}
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+	if (!pebs_ptr_check) {
+		return;
+	}
+
+	if (DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+		tsc = ADAPTIVE_PEBS_BASIC_INFO_tsc(
+			(ADAPTIVE_PEBS_BASIC_INFO)pebs_ptr);
+	} else {
+		tsc = PEBS_REC_EXT2_tsc((PEBS_REC_EXT2)pebs_ptr);
+	}
+	SAMPLE_RECORD_tsc(psamp) = tsc;
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U32 pebs_Get_Num_Records_Filled ()
+ *
+ * @brief       get number of PEBS records filled in PEBS buffer
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static U32 pebs_Get_Num_Records_Filled(VOID)
+{
+	U32 num = 0;
+	DTS_BUFFER_EXT dtes;
+	S8 *pebs_base, *pebs_index;
+	U32 this_cpu;
+	U32 dev_idx;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes) {
+		return num;
+	}
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	if (pebs_base != pebs_index) {
+		num = (U32)(pebs_index - pebs_base) /
+		      LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", num);
+	return num;
+}
+
+/*
+ * Initialize the pebs micro dispatch tables
+ */
+PEBS_DISPATCH_NODE core2_pebs = {
+	.initialize_threshold = pebs_Core2_Initialize_Threshold,
+	.overflow = pebs_Core2_Overflow,
+	.modify_ip = pebs_Modify_IP,
+	.modify_tsc = NULL,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+PEBS_DISPATCH_NODE core2p_pebs = {
+	.initialize_threshold = pebs_Corei7_Initialize_Threshold,
+	.overflow = pebs_Core2_Overflow,
+	.modify_ip = pebs_Modify_IP,
+	.modify_tsc = NULL,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+PEBS_DISPATCH_NODE corei7_pebs = {
+	.initialize_threshold = pebs_Corei7_Initialize_Threshold,
+	.overflow = pebs_Corei7_Overflow,
+	.modify_ip = pebs_Modify_IP,
+	.modify_tsc = NULL,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+PEBS_DISPATCH_NODE haswell_pebs = {
+	.initialize_threshold = pebs_Corei7_Initialize_Threshold,
+	.overflow = pebs_Corei7_Overflow,
+	.modify_ip = pebs_Modify_IP_With_Eventing_IP,
+	.modify_tsc = NULL,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+PEBS_DISPATCH_NODE perfver4_pebs = {
+	.initialize_threshold = pebs_Corei7_Initialize_Threshold,
+	.overflow = pebs_Corei7_Overflow,
+	.modify_ip = pebs_Modify_IP_With_Eventing_IP,
+	.modify_tsc = pebs_Modify_TSC,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+// adaptive PEBS
+PEBS_DISPATCH_NODE perfver4_apebs = {
+	.initialize_threshold = pebs_Corei7_Initialize_Threshold,
+	.overflow = pebs_Corei7_Overflow_APEBS,
+	.modify_ip = pebs_Modify_IP_With_Eventing_IP,
+	.modify_tsc = pebs_Modify_TSC,
+	.get_num_records_filled = pebs_Get_Num_Records_Filled
+};
+
+#define PER_CORE_BUFFER_SIZE(dts_size, record_size, record_num)                \
+	(dts_size + (record_num + 1) * (record_size) + 64)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID* pebs_Alloc_DTS_Buffer (VOID)
+ *
+ * @brief       Allocate buffers used for latency and pebs sampling
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ * Allocate the memory needed to hold the DTS and PEBS records buffer.
+ * This routine is called by a thread that corresponds to a single core
+ */
+static VOID *pebs_Alloc_DTS_Buffer(VOID)
+{
+	UIOP pebs_base;
+	U32 dts_size;
+	PVOID dts_buffer = NULL;
+	DTS_BUFFER_EXT dts;
+	int this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	PEBS_DISPATCH pebs_dispatch;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	/*
+	 * one PEBS record... need 2 records so that
+	 * threshold can be less than absolute max
+	 */
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	dts_size = sizeof(DTS_BUFFER_EXT_NODE);
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	pebs_dispatch = LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]);
+
+	if (DEV_CONFIG_enable_adaptive_pebs(pcfg) ||
+	    DEV_CONFIG_collect_fixed_counter_pebs(pcfg)) {
+		dts_size = sizeof(DTS_BUFFER_EXT1_NODE);
+	}
+
+	/*
+	 * account for extra bytes to align PEBS base to cache line boundary
+	 */
+	if (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+	    DRV_SETUP_INFO_PTI_KPTI) {
+#if defined(DRV_USE_PTI) &&  defined(CONFIG_CPU_SUP_INTEL)
+		struct page *page;
+		U32 buffer_size;
+
+		SEP_DRV_LOG_INIT("Allocating PEBS buffer using KPTI approach.");
+		buffer_size = (PER_CORE_BUFFER_SIZE(
+				       dts_size,
+				       LWPMU_DEVICE_pebs_record_size(
+					       &devices[dev_idx]),
+				       DEV_CONFIG_pebs_record_num(pcfg)) /
+				       PAGE_SIZE +
+			       1) *
+			      PAGE_SIZE;
+		if (buffer_size > PEBS_BUFFER_SIZE) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Can't allocate more buffer than CEA allows!");
+			return NULL;
+		}
+
+		page = __alloc_pages_node(cpu_to_node(this_cpu),
+					  GFP_ATOMIC | __GFP_ZERO,
+					  get_order(buffer_size));
+		if (!page) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"NULL (failed to allocate space for DTS buffer!).");
+			return NULL;
+		}
+		dts_buffer = page_address(page);
+		per_cpu(dts_buffer_cea, this_cpu) =
+			&get_cpu_entry_area(this_cpu)
+				->cpu_debug_buffers.pebs_buffer;
+		if (!per_cpu(dts_buffer_cea, this_cpu)) {
+			if (dts_buffer) {
+				free_pages((unsigned long)dts_buffer,
+					   get_order(buffer_size));
+			}
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"CEA pebs_buffer ptr is NULL!");
+			return NULL;
+		}
+
+		CPU_STATE_dts_buffer(pcpu) = dts_buffer;
+		CPU_STATE_dts_buffer_size(pcpu) = buffer_size;
+
+		if (local_cea_set_pte) {
+			size_t idx;
+			phys_addr_t phys_addr;
+			PVOID cea_ptr = per_cpu(dts_buffer_cea, this_cpu);
+
+			phys_addr = virt_to_phys(dts_buffer);
+
+			preempt_disable();
+			for (idx = 0; idx < buffer_size; idx += PAGE_SIZE,
+			    phys_addr += PAGE_SIZE, cea_ptr += PAGE_SIZE) {
+				local_cea_set_pte(cea_ptr, phys_addr,
+						  PAGE_KERNEL);
+			}
+			pebs_Update_CEA(this_cpu);
+			preempt_enable();
+		}
+		pebs_base =
+			(UIOP)(per_cpu(dts_buffer_cea, this_cpu)) + dts_size;
+		SEP_DRV_LOG_TRACE("This_cpu: %d, pebs_base %p.", this_cpu,
+				  pebs_base);
+
+		dts = (DTS_BUFFER_EXT)(per_cpu(dts_buffer_cea, this_cpu));
+#else
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"KPTI is enabled without PAGE_TABLE_ISOLATION kernel configuration!");
+		return NULL;
+#endif
+	} else {
+		dts_buffer = (char *)pebs_global_memory +
+			     CPU_STATE_dts_buffer_offset(pcpu);
+		if (!dts_buffer) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"NULL (failed to allocate space for DTS buffer!).");
+			return NULL;
+		}
+		pebs_base = (UIOP)(dts_buffer) + dts_size;
+
+		CPU_STATE_dts_buffer(pcpu) = dts_buffer;
+		CPU_STATE_dts_buffer_size(pcpu) = PER_CORE_BUFFER_SIZE(
+			dts_size,
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]),
+			DEV_CONFIG_pebs_record_num(pcfg));
+
+		//  Make 32 byte aligned
+		if ((pebs_base & 0x000001F) != 0x0) {
+			pebs_base = ALIGN_32(pebs_base);
+		}
+
+		dts = (DTS_BUFFER_EXT)dts_buffer;
+	}
+
+	/*
+	 * Program the DTES Buffer for Precise EBS.
+	 * Set PEBS buffer for one PEBS record
+	 */
+	DTS_BUFFER_EXT_base(dts) = 0;
+	DTS_BUFFER_EXT_index(dts) = 0;
+	DTS_BUFFER_EXT_max(dts) = 0;
+	DTS_BUFFER_EXT_threshold(dts) = 0;
+	DTS_BUFFER_EXT_pebs_base(dts) = pebs_base;
+	DTS_BUFFER_EXT_pebs_index(dts) = pebs_base;
+	DTS_BUFFER_EXT_pebs_max(dts) = pebs_base +
+		((UIOP)DEV_CONFIG_pebs_record_num(pcfg) + 1) *
+		LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]);
+
+	pebs_dispatch->initialize_threshold(dts);
+
+	SEP_DRV_LOG_TRACE("base --- %llx.", DTS_BUFFER_EXT_pebs_base(dts));
+	SEP_DRV_LOG_TRACE("index --- %llu.", DTS_BUFFER_EXT_pebs_index(dts));
+	SEP_DRV_LOG_TRACE("max --- %llu.", DTS_BUFFER_EXT_pebs_max(dts));
+	SEP_DRV_LOG_TRACE("threahold --- %llu.",
+			  DTS_BUFFER_EXT_pebs_threshold(dts));
+	SEP_DRV_LOG_TRACE("DTES buffer allocated for PEBS: %p.", dts_buffer);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %p.", dts_buffer);
+	return dts;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID* pebs_Allocate_Buffers (VOID *params)
+ *
+ * @brief       Allocate memory and set up MSRs in preparation for PEBS
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              Set up the DS area and program the DS_AREA msrs in preparation
+ *              for a PEBS run.  Save away the old value in the DS_AREA.
+ *              This routine is called via the parallel thread call.
+ */
+static VOID pebs_Allocate_Buffers(VOID *params)
+{
+	U64 value;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	PVOID dts_ptr = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("Params: %p.", params);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!DEV_CONFIG_pebs_mode(pcfg)) {
+		return;
+	}
+
+	SYS_Write_MSR(IA32_PEBS_ENABLE, 0LL);
+	value = SYS_Read_MSR(IA32_MISC_ENABLE);
+	if ((value & 0x80) && !(value & 0x1000)) {
+		CPU_STATE_old_dts_buffer(pcpu) =
+			(PVOID)(UIOP)SYS_Read_MSR(IA32_DS_AREA);
+		dts_ptr = pebs_Alloc_DTS_Buffer();
+		if (!dts_ptr) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT("dts_ptr is NULL!");
+			return;
+		}
+		SEP_DRV_LOG_TRACE("Old dts buffer - %p.",
+				  CPU_STATE_old_dts_buffer(pcpu));
+		SEP_DRV_LOG_TRACE("New dts buffer - %p.", dts_ptr);
+		SYS_Write_MSR(IA32_DS_AREA, (U64)(UIOP)dts_ptr);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID pebs_Dellocate_Buffers (VOID *params)
+ *
+ * @brief       Clean up PEBS buffers and restore older values into the DS_AREA
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ * Clean up the DS area and all restore state prior to the sampling run
+ * This routine is called via the parallel thread call.
+ */
+static VOID pebs_Deallocate_Buffers(VOID *params)
+{
+	CPU_STATE pcpu;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Params: %p.", params);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!DEV_CONFIG_pebs_mode(pcfg)) {
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("Entered deallocate buffers.");
+	SYS_Write_MSR(IA32_DS_AREA, (U64)(UIOP)CPU_STATE_old_dts_buffer(pcpu));
+
+	if (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+	    DRV_SETUP_INFO_PTI_KPTI) {
+#if defined(DRV_USE_PTI)
+		SEP_DRV_LOG_INIT("Freeing PEBS buffer using KPTI approach.");
+
+		if (local_cea_set_pte) {
+			size_t idx;
+			PVOID cea_ptr = per_cpu(dts_buffer_cea, this_cpu);
+			preempt_disable();
+			for (idx = 0; idx < CPU_STATE_dts_buffer_size(pcpu);
+			     idx += PAGE_SIZE, cea_ptr += PAGE_SIZE) {
+				local_cea_set_pte(cea_ptr, 0, PAGE_KERNEL);
+			}
+			pebs_Update_CEA(this_cpu);
+			preempt_enable();
+		}
+
+		if (CPU_STATE_dts_buffer(pcpu)) {
+			free_pages((unsigned long)CPU_STATE_dts_buffer(pcpu),
+				   get_order(CPU_STATE_dts_buffer_size(pcpu)));
+			CPU_STATE_dts_buffer(pcpu) = NULL;
+		}
+#endif
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 PEBS_Overflowed (this_cpu, overflow_status)
+ *
+ * @brief       Figure out if the PEBS event caused an overflow
+ *
+ * @param       this_cpu        -- the current cpu
+ *              overflow_status -- current value of the global overflow status
+ *
+ * @return      updated overflow_status
+ *
+ * <I>Special Notes:</I>
+ *              Figure out if the PEBS area has data that need to be transferred
+ *              to the output sample.
+ *              Update the overflow_status that is passed and return this value.
+ *              The overflow_status defines the events/status to be read
+ */
+U64 PEBS_Overflowed(S32 this_cpu, U64 overflow_status, U32 rec_index)
+{
+	U64 res;
+	U32 dev_idx;
+	PEBS_DISPATCH pebs_dispatch;
+
+	SEP_DRV_LOG_TRACE_IN(
+		"This_cpu: %d, overflow_status: %llx, rec_index: %u.", this_cpu,
+		overflow_status, rec_index);
+
+	dev_idx = core_to_dev_map[this_cpu];
+	pebs_dispatch = LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]);
+
+	res = pebs_dispatch->overflow(this_cpu, overflow_status, rec_index);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llx.", overflow_status);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Reset_Index (this_cpu)
+ *
+ * @brief       Reset the PEBS index pointer
+ *
+ * @param       this_cpu        -- the current cpu
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              reset index to next PEBS record to base of buffer
+ */
+VOID PEBS_Reset_Index(S32 this_cpu)
+{
+	DTS_BUFFER_EXT dtes;
+
+	SEP_DRV_LOG_TRACE_IN("This_cpu: %d.", this_cpu);
+
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dtes) {
+		return;
+	}
+	SEP_DRV_LOG_TRACE("PEBS Reset Index: %d.", this_cpu);
+	DTS_BUFFER_EXT_pebs_index(dtes) = DTS_BUFFER_EXT_pebs_base(dtes);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+extern U32 pmi_Get_CSD(U32, U32 *, U32 *);
+#define EFLAGS_V86_MASK 0x00020000L
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Flush_Buffer (VOID * param)
+ *
+ * @brief       generate sampling records from PEBS records in PEBS buffer
+ *
+ * @param       param        -- not used
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ */
+VOID PEBS_Flush_Buffer(VOID *param)
+{
+	U32 i, this_cpu, index, desc_id;
+	U64 pebs_overflow_status = 0;
+	U64 lbr_tos_from_ip = 0ULL;
+	DRV_BOOL counter_overflowed = FALSE;
+	// ECB pecb;
+	CPU_STATE pcpu;
+	EVENT_DESC evt_desc;
+	BUFFER_DESC bd;
+	SampleRecordPC *psamp_pebs;
+	U32 is_64bit_addr = FALSE;
+	U32 u32PebsRecordNumFilled;
+#if defined(DRV_IA32)
+	U32 seg_cs;
+	U32 csdlo;
+	U32 csdhi;
+#endif
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 cur_grp;
+	DRV_BOOL multi_pebs_enabled;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	bd = &cpu_buf[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	cur_grp = CPU_STATE_current_group(pcpu);
+	multi_pebs_enabled =
+		(DEV_CONFIG_pebs_mode(pcfg) &&
+		 (DEV_CONFIG_pebs_record_num(pcfg) > 1) &&
+		 (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+		  DRV_SETUP_INFO_PTI_DISABLED));
+
+	if (!multi_pebs_enabled) {
+		SEP_DRV_LOG_TRACE_OUT("PEBS_Flush_Buffer is not supported.");
+		return;
+	}
+
+	u32PebsRecordNumFilled = PEBS_Get_Num_Records_Filled();
+	for (i = 0; i < u32PebsRecordNumFilled; i++) {
+		pebs_overflow_status = PEBS_Overflowed(this_cpu, 0, i);
+		SEP_DRV_LOG_TRACE("Pebs_overflow_status = 0x%llx, i=%d.",
+				  pebs_overflow_status, i);
+
+		// pecb = LWPMU_DEVICE_PMU_register_data(
+		// 	&devices[dev_idx])[cur_grp];
+		FOR_EACH_DATA_REG(pecb, j)
+		{
+			if ((!DEV_CONFIG_enable_adaptive_pebs(pcfg) &&
+			     !ECB_entries_is_gp_reg_get(pecb, j)) ||
+			    !ECB_entries_precise_get(pecb, j)) {
+				continue;
+			}
+			if (ECB_entries_fixed_reg_get(pecb, j)) {
+				index = ECB_entries_reg_id(pecb, j) -
+					IA32_FIXED_CTR0;
+				if (pebs_overflow_status &
+				    ((U64)1 << (32 + index))) {
+					counter_overflowed = TRUE;
+				}
+			} else {
+				index = ECB_entries_reg_id(pecb, j) - IA32_PMC0;
+				if (pebs_overflow_status & (U64)1 << index) {
+					counter_overflowed = TRUE;
+				}
+			}
+			if (counter_overflowed) {
+				desc_id = ECB_entries_event_id_index(pecb, j);
+				evt_desc = desc_data[desc_id];
+				SEP_DRV_LOG_TRACE(
+					"Event_id_index=%u, desc_id=%u.",
+					ECB_entries_event_id_index(pecb, j),
+					desc_id);
+				psamp_pebs = (SampleRecordPC *)
+					OUTPUT_Reserve_Buffer_Space(
+						bd,
+						EVENT_DESC_sample_size(
+							evt_desc),
+						(NMI_mode) ? TRUE : FALSE,
+						!SEP_IN_NOTIFICATION,
+						(S32)this_cpu);
+				if (!psamp_pebs) {
+					SEP_DRV_LOG_ERROR(
+						"Could not generate samples from PEBS records.");
+					continue;
+				}
+
+				lbr_tos_from_ip = 0ULL;
+				CPU_STATE_num_samples(&pcb[this_cpu]) += 1;
+				SAMPLE_RECORD_descriptor_id(psamp_pebs) =
+					desc_id;
+				SAMPLE_RECORD_event_index(psamp_pebs) =
+					ECB_entries_event_id_index(pecb, j);
+				SAMPLE_RECORD_pid_rec_index(psamp_pebs) =
+					(U32)-1;
+				SAMPLE_RECORD_pid_rec_index_raw(psamp_pebs) = 1;
+				SAMPLE_RECORD_tid(psamp_pebs) = (U32)-1;
+				SAMPLE_RECORD_cpu_num(psamp_pebs) =
+					(U16)this_cpu;
+				SAMPLE_RECORD_osid(psamp_pebs) = 0;
+
+#if defined(DRV_IA32)
+				PEBS_Modify_IP((S8 *)psamp_pebs, is_64bit_addr,
+					       i);
+				SAMPLE_RECORD_cs(psamp_pebs) = __KERNEL_CS;
+				if (SAMPLE_RECORD_eflags(psamp_pebs) &
+				    EFLAGS_V86_MASK) {
+					csdlo = 0;
+					csdhi = 0;
+				} else {
+					seg_cs = SAMPLE_RECORD_cs(psamp_pebs);
+					SYS_Get_CSD(seg_cs, &csdlo, &csdhi);
+				}
+				SAMPLE_RECORD_csd(psamp_pebs).u1.lowWord =
+					csdlo;
+				SAMPLE_RECORD_csd(psamp_pebs).u2.highWord =
+					csdhi;
+#elif defined(DRV_EM64T)
+				SAMPLE_RECORD_cs(psamp_pebs) = __KERNEL_CS;
+				pmi_Get_CSD(SAMPLE_RECORD_cs(psamp_pebs),
+					    &SAMPLE_RECORD_csd(psamp_pebs)
+						     .u1.lowWord,
+					    &SAMPLE_RECORD_csd(psamp_pebs)
+						     .u2.highWord);
+				is_64bit_addr =
+					(SAMPLE_RECORD_csd(psamp_pebs)
+						 .u2.s2.reserved_0 == 1);
+				if (is_64bit_addr) {
+					SAMPLE_RECORD_ia64_pc(psamp_pebs) =
+						TRUE;
+				} else {
+					SAMPLE_RECORD_ia64_pc(psamp_pebs) =
+						FALSE;
+
+					SEP_DRV_LOG_TRACE(
+						"SAMPLE_RECORD_eip(psamp_pebs) 0x%x.",
+						SAMPLE_RECORD_eip(psamp_pebs));
+					SEP_DRV_LOG_TRACE(
+						"SAMPLE_RECORD_eflags(psamp_pebs) %x.",
+						SAMPLE_RECORD_eflags(
+							psamp_pebs));
+				}
+#endif
+				if (EVENT_DESC_pebs_offset(evt_desc) ||
+				    EVENT_DESC_latency_offset_in_sample(
+					    evt_desc)) {
+					lbr_tos_from_ip = PEBS_Fill_Buffer(
+						(S8 *)psamp_pebs, evt_desc, i);
+				}
+				PEBS_Modify_IP((S8 *)psamp_pebs, is_64bit_addr,
+					       i);
+				PEBS_Modify_TSC((S8 *)psamp_pebs, i);
+				if (ECB_entries_branch_evt_get(pecb, j) &&
+				    DEV_CONFIG_precise_ip_lbrs(pcfg) &&
+				    lbr_tos_from_ip) {
+					if (is_64bit_addr) {
+						SAMPLE_RECORD_iip(psamp_pebs) =
+							lbr_tos_from_ip;
+						SEP_DRV_LOG_TRACE(
+							"UPDATED SAMPLE_RECORD_iip(psamp) 0x%llx.",
+							SAMPLE_RECORD_iip(
+								psamp_pebs));
+					} else {
+						SAMPLE_RECORD_eip(psamp_pebs) =
+							(U32)lbr_tos_from_ip;
+						SEP_DRV_LOG_TRACE(
+							"UPDATED SAMPLE_RECORD_eip(psamp) 0x%x.",
+							SAMPLE_RECORD_eip(
+								psamp_pebs));
+					}
+				}
+			}
+		}
+		END_FOR_EACH_DATA_REG;
+	}
+	PEBS_Reset_Index(this_cpu);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Reset_Counter (this_cpu, index, value)
+ *
+ * @brief       set reset value for PMC after overflow
+ *
+ * @param       this_cpu        -- the current cpu
+ *              index           -- PMC register index
+ *              value           -- reset value for PMC after overflow
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ */
+VOID PEBS_Reset_Counter(S32 this_cpu, U32 index, U64 value)
+{
+	DTS_BUFFER_EXT dts;
+	DTS_BUFFER_EXT1 dts_ext = NULL;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("This_cpu: %d, index: %u, value: %llx.", this_cpu,
+			     index, value);
+
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dts = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (!dts) {
+		return;
+	}
+	SEP_DRV_LOG_TRACE(
+		"PEBS Reset GP Counters[0:4]: cpu %d, index=%u, value=%llx.",
+		this_cpu, index, value);
+	switch (index) {
+	case 0:
+		DTS_BUFFER_EXT_counter_reset0(dts) = value;
+		break;
+	case 1:
+		DTS_BUFFER_EXT_counter_reset1(dts) = value;
+		break;
+	case 2:
+		DTS_BUFFER_EXT_counter_reset2(dts) = value;
+		break;
+	case 3:
+		DTS_BUFFER_EXT_counter_reset3(dts) = value;
+		break;
+	}
+
+	if (DEV_CONFIG_enable_adaptive_pebs(pcfg) ||
+	    DEV_CONFIG_collect_fixed_counter_pebs(pcfg)) {
+		dts_ext = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+	}
+	if (!dts_ext) {
+		return;
+	}
+	SEP_DRV_LOG_TRACE("PEBS Reset Fixed Counters and GP Counters[4:7]: \
+		 cpu %d, index=%u, value=%llx.",
+			  this_cpu, index, value);
+	switch (index) {
+	case 4:
+		DTS_BUFFER_EXT1_counter_reset4(dts_ext) = value;
+		break;
+	case 5:
+		DTS_BUFFER_EXT1_counter_reset5(dts_ext) = value;
+		break;
+	case 6:
+		DTS_BUFFER_EXT1_counter_reset6(dts_ext) = value;
+		break;
+	case 7:
+		DTS_BUFFER_EXT1_counter_reset7(dts_ext) = value;
+		break;
+	case 8:
+		DTS_BUFFER_EXT1_fixed_counter_reset0(dts_ext) = value;
+		break;
+	case 9:
+		DTS_BUFFER_EXT1_fixed_counter_reset1(dts_ext) = value;
+		break;
+	case 10:
+		DTS_BUFFER_EXT1_fixed_counter_reset2(dts_ext) = value;
+		break;
+	case 11:
+		DTS_BUFFER_EXT1_fixed_counter_reset3(dts_ext) = value;
+		break;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Modify_IP (sample, is_64bit_addr)
+ *
+ * @brief       Change the IP field in the sample to that in the PEBS record
+ *
+ * @param       sample        - sample buffer
+ * @param       is_64bit_addr - are we in a 64 bit module
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+VOID PEBS_Modify_IP(void *sample, DRV_BOOL is_64bit_addr, U32 rec_index)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	PEBS_DISPATCH pebs_dispatch;
+
+	SEP_DRV_LOG_TRACE_IN("Sample: %p, is_64bit_addr: %u, rec_index: %u.",
+			     sample, is_64bit_addr, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pebs_dispatch = LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]);
+
+	pebs_dispatch->modify_ip(sample, is_64bit_addr, rec_index);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Modify_TSC (sample)
+ *
+ * @brief       Change the TSC field in the sample to that in the PEBS record
+ *
+ * @param       sample        - sample buffer
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+VOID PEBS_Modify_TSC(void *sample, U32 rec_index)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	PEBS_DISPATCH pebs_dispatch;
+
+	SEP_DRV_LOG_TRACE_IN("Sample: %p, rec_index: %u.", sample, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pebs_dispatch = LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]);
+
+	if (pebs_dispatch->modify_tsc != NULL) {
+		pebs_dispatch->modify_tsc(sample, rec_index);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+U32 PEBS_Get_Num_Records_Filled(VOID)
+{
+	U32 this_cpu;
+	U32 dev_idx;
+	PEBS_DISPATCH pebs_dispatch;
+	U32 num = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pebs_dispatch = LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]);
+
+	if (pebs_dispatch->get_num_records_filled != NULL) {
+		num = pebs_dispatch->get_num_records_filled();
+		SEP_DRV_LOG_TRACE("Num=%u.", num);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", num);
+	return num;
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Fill_Phy_Addr (LATENCY_INFO latency_info)
+ *
+ * @brief       Fill latency node with phy addr when applicable
+ *
+ * @param       latency_info             - pointer to LATENCY_INFO struct
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+
+static VOID PEBS_Fill_Phy_Addr(LATENCY_INFO latency_info)
+{
+#if defined(DRV_EM64T) && LINUX_VERSION_CODE >= KERNEL_VERSION(3, 6, 0)
+	U64 lin_addr;
+	U64 offset;
+	struct page *page = NULL;
+
+	if (!DRV_CONFIG_virt_phys_translation(drv_cfg)) {
+		return;
+	}
+	lin_addr = (U64)LATENCY_INFO_linear_address(latency_info);
+	if (lin_addr != 0) {
+		offset = (U64)(lin_addr & 0x0FFF);
+		if (__virt_addr_valid(lin_addr)) {
+			LATENCY_INFO_phys_addr(latency_info) =
+				(U64)__pa(lin_addr);
+		} else if (lin_addr < __PAGE_OFFSET) {
+			pagefault_disable();
+			if (__get_user_pages_fast(lin_addr, 1, 1, &page)) {
+				LATENCY_INFO_phys_addr(latency_info) =
+					(U64)page_to_phys(page) + offset;
+				put_page(page);
+			}
+			pagefault_enable();
+		}
+	}
+#endif
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn    U64 PEBS_Fill_Buffer (S8 *buffer, EVENT_DESC evt_desc, U32 rec_index)
+ *
+ * @brief       Fill the buffer with the pebs data
+ *
+ * @param       buffer                   -  area to write the data into
+ *              event_desc               -  event descriptor of the pebs event
+		rec_index                - current pebs record index
+ *
+ * @return      if APEBS return LBR_TOS_FROM_IP else return 0
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+U64 PEBS_Fill_Buffer(S8 *buffer, EVENT_DESC evt_desc, U32 rec_index)
+{
+	DTS_BUFFER_EXT dtes;
+	LATENCY_INFO_NODE latency_info = { 0 };
+	PEBS_REC_EXT1 pebs_base_ext1;
+	PEBS_REC_EXT2 pebs_base_ext2;
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	U8 pebs_ptr_check = FALSE;
+	U64 lbr_tos_from_ip = 0ULL;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p, evt_desc: %p, rec_index: %u.", buffer,
+			     evt_desc, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	if (DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+		lbr_tos_from_ip =
+			APEBS_Fill_Buffer(buffer, evt_desc, rec_index);
+		return lbr_tos_from_ip;
+	}
+
+	SEP_DRV_LOG_TRACE("In PEBS Fill Buffer: cpu %d.", CONTROL_THIS_CPU());
+
+	if (!dtes) {
+		return lbr_tos_from_ip;
+	}
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+	if (!pebs_ptr_check) {
+		return lbr_tos_from_ip;
+	}
+	pebs_base = pebs_ptr;
+	if (EVENT_DESC_pebs_offset(evt_desc)) {
+		SEP_DRV_LOG_TRACE("PEBS buffer has data available.");
+		memcpy(buffer + EVENT_DESC_pebs_offset(evt_desc), pebs_base,
+		       EVENT_DESC_pebs_size(evt_desc));
+	}
+	if (EVENT_DESC_eventing_ip_offset(evt_desc)) {
+		pebs_base_ext1 = (PEBS_REC_EXT1)pebs_base;
+		*(U64 *)(buffer + EVENT_DESC_eventing_ip_offset(evt_desc)) =
+			PEBS_REC_EXT1_eventing_ip(pebs_base_ext1);
+	}
+	if (EVENT_DESC_hle_offset(evt_desc)) {
+		pebs_base_ext1 = (PEBS_REC_EXT1)pebs_base;
+		*(U64 *)(buffer + EVENT_DESC_hle_offset(evt_desc)) =
+			PEBS_REC_EXT1_hle_info(pebs_base_ext1);
+	}
+	if (EVENT_DESC_latency_offset_in_sample(evt_desc)) {
+		pebs_base_ext1 = (PEBS_REC_EXT1)pebs_base;
+		memcpy(&latency_info,
+		       pebs_base + EVENT_DESC_latency_offset_in_pebs_record(
+					   evt_desc),
+		       EVENT_DESC_latency_size_from_pebs_record(evt_desc));
+		memcpy(&LATENCY_INFO_stack_pointer(&latency_info),
+		       &PEBS_REC_EXT1_rsp(pebs_base_ext1), sizeof(U64));
+
+		LATENCY_INFO_phys_addr(&latency_info) = 0;
+		PEBS_Fill_Phy_Addr(&latency_info);
+
+		memcpy(buffer + EVENT_DESC_latency_offset_in_sample(evt_desc),
+		       &latency_info, sizeof(LATENCY_INFO_NODE));
+	}
+	if (EVENT_DESC_pebs_tsc_offset(evt_desc)) {
+		pebs_base_ext2 = (PEBS_REC_EXT2)pebs_base;
+		*(U64 *)(buffer + EVENT_DESC_pebs_tsc_offset(evt_desc)) =
+			PEBS_REC_EXT2_tsc(pebs_base_ext2);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+	return lbr_tos_from_ip;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  U64 APEBS_Fill_Buffer (S8 *buffer, EVENT_DESC evt_desc, U32 rec_index)
+ *
+ * @brief       Fill the buffer with the pebs data
+ *
+ * @param       buffer                   -  area to write the data into
+ *              event_desc               -  event descriptor of the pebs event
+ *              rec_index                - current pebs record index
+ *
+ * @return      LBR_TOS_FROM_IP
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+U64 APEBS_Fill_Buffer(S8 *buffer, EVENT_DESC evt_desc, U32 rec_index)
+{
+	DTS_BUFFER_EXT1 dtes;
+	LATENCY_INFO_NODE latency_info = { 0 };
+	U64 dtes_record_size = 0;
+	U64 dtes_record_format = 0;
+	ADAPTIVE_PEBS_MEM_INFO apebs_mem = NULL;
+	ADAPTIVE_PEBS_GPR_INFO apebs_gpr = NULL;
+	ADAPTIVE_PEBS_BASIC_INFO apebs_basic = NULL;
+	S8 *pebs_base, *pebs_index, *pebs_ptr;
+	U8 pebs_ptr_check = FALSE;
+	U64 lbr_tos_from_ip = 0ULL;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p, evt_desc: %p, rec_index: %u.", buffer,
+			     evt_desc, rec_index);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dtes = CPU_STATE_dts_buffer(&pcb[this_cpu]);
+
+	SEP_DRV_LOG_TRACE("In APEBS Fill Buffer: cpu %d.", this_cpu);
+
+	if (!dtes || !DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+		return lbr_tos_from_ip;
+	}
+
+	pebs_base = (S8 *)(UIOP)DTS_BUFFER_EXT1_pebs_base(dtes);
+	pebs_index = (S8 *)(UIOP)DTS_BUFFER_EXT1_pebs_index(dtes);
+	pebs_ptr = (S8 *)((UIOP)DTS_BUFFER_EXT1_pebs_base(dtes) +
+			  ((UIOP)rec_index *
+			   LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])));
+	pebs_ptr_check =
+		(pebs_ptr && pebs_base != pebs_index && pebs_ptr < pebs_index);
+	if (!pebs_ptr_check) {
+		return lbr_tos_from_ip;
+	}
+
+	pebs_base = pebs_ptr;
+	apebs_basic = (ADAPTIVE_PEBS_BASIC_INFO)(
+		pebs_base + LWPMU_DEVICE_apebs_basic_offset(&devices[dev_idx]));
+	dtes_record_size = (ADAPTIVE_PEBS_BASIC_INFO_record_info(apebs_basic) &
+			    APEBS_RECORD_SIZE_MASK) >> 48; // [63:48]
+	dtes_record_format =
+		(ADAPTIVE_PEBS_BASIC_INFO_record_info(apebs_basic) &
+		(U64)APEBS_RECORD_FORMAT_MASK); // [47:0]
+
+	if (dtes_record_size !=
+	    LWPMU_DEVICE_pebs_record_size(&devices[dev_idx])) {
+		SEP_DRV_LOG_TRACE(
+			"PEBS record size does not match with ucode\n");
+	}
+	if (EVENT_DESC_pebs_offset(evt_desc)) {
+		*(U64 *)(buffer + EVENT_DESC_pebs_offset(evt_desc)) =
+			ADAPTIVE_PEBS_BASIC_INFO_record_info(apebs_basic);
+	}
+	if (EVENT_DESC_eventing_ip_offset(evt_desc)) {
+		*(U64 *)(buffer + EVENT_DESC_eventing_ip_offset(evt_desc)) =
+			ADAPTIVE_PEBS_BASIC_INFO_eventing_ip(apebs_basic);
+	}
+	if (EVENT_DESC_pebs_tsc_offset(evt_desc)) {
+		*(U64 *)(buffer + EVENT_DESC_pebs_tsc_offset(evt_desc)) =
+			ADAPTIVE_PEBS_BASIC_INFO_tsc(apebs_basic);
+	}
+	if (EVENT_DESC_applicable_counters_offset(evt_desc)) {
+		*(U64 *)(buffer +
+			 EVENT_DESC_applicable_counters_offset(evt_desc)) =
+			ADAPTIVE_PEBS_BASIC_INFO_applicable_counters(
+				apebs_basic);
+	}
+	if (DEV_CONFIG_apebs_collect_gpr(pcfg) &&
+	    EVENT_DESC_gpr_info_offset(evt_desc)) {
+		if (!(dtes_record_format & APEBS_GPR_RECORD_FORMAT_MASK)) {
+			SEP_DRV_LOG_WARNING(
+				"GPR info not found in DS PEBS record.");
+		}
+		memcpy(buffer + EVENT_DESC_gpr_info_offset(evt_desc),
+		       pebs_base +
+			       LWPMU_DEVICE_apebs_gpr_offset(&devices[dev_idx]),
+		       EVENT_DESC_gpr_info_size(evt_desc));
+	}
+	if (DEV_CONFIG_apebs_collect_mem_info(pcfg) &&
+	    EVENT_DESC_latency_offset_in_sample(evt_desc)) {
+		if (!(dtes_record_format & APEBS_MEM_RECORD_FORMAT_MASK)) {
+			SEP_DRV_LOG_WARNING(
+				"MEM info not found in DS PEBS record.");
+		}
+		apebs_mem = (ADAPTIVE_PEBS_MEM_INFO)(
+			pebs_base +
+			LWPMU_DEVICE_apebs_mem_offset(&devices[dev_idx]));
+		memcpy(&LATENCY_INFO_linear_address(&latency_info),
+		       &ADAPTIVE_PEBS_MEM_INFO_data_linear_address(apebs_mem),
+		       sizeof(U64));
+		memcpy(&LATENCY_INFO_data_source(&latency_info),
+		       &ADAPTIVE_PEBS_MEM_INFO_data_source(apebs_mem),
+		       sizeof(U64));
+		memcpy(&LATENCY_INFO_latency(&latency_info),
+		       &ADAPTIVE_PEBS_MEM_INFO_latency(apebs_mem), sizeof(U64));
+		LATENCY_INFO_stack_pointer(&latency_info) = 0;
+		if (DEV_CONFIG_apebs_collect_gpr(pcfg)) {
+			apebs_gpr = (ADAPTIVE_PEBS_GPR_INFO)(
+				pebs_base + LWPMU_DEVICE_apebs_gpr_offset(
+						    &devices[dev_idx]));
+			memcpy(&LATENCY_INFO_stack_pointer(&latency_info),
+			       &ADAPTIVE_PEBS_GPR_INFO_rsp(apebs_gpr),
+			       sizeof(U64));
+		}
+
+		LATENCY_INFO_phys_addr(&latency_info) = 0;
+		PEBS_Fill_Phy_Addr(&latency_info);
+		memcpy(buffer + EVENT_DESC_latency_offset_in_sample(evt_desc),
+		       &latency_info, sizeof(LATENCY_INFO_NODE));
+	}
+	if (DEV_CONFIG_apebs_collect_mem_info(pcfg) &&
+	    EVENT_DESC_hle_offset(evt_desc)) {
+		*(U64 *)(buffer + EVENT_DESC_hle_offset(evt_desc)) =
+			ADAPTIVE_PEBS_MEM_INFO_hle_info((
+				ADAPTIVE_PEBS_MEM_INFO)(
+				pebs_base + LWPMU_DEVICE_apebs_mem_offset(
+						    &devices[dev_idx])));
+	}
+	if (DEV_CONFIG_apebs_collect_xmm(pcfg) &&
+	    EVENT_DESC_xmm_info_offset(evt_desc)) {
+		if (!(dtes_record_format & APEBS_XMM_RECORD_FORMAT_MASK)) {
+			SEP_DRV_LOG_WARNING(
+				"XMM info not found in DS PEBS record.");
+		}
+		memcpy(buffer + EVENT_DESC_xmm_info_offset(evt_desc),
+		       pebs_base +
+			       LWPMU_DEVICE_apebs_xmm_offset(&devices[dev_idx]),
+		       EVENT_DESC_xmm_info_size(evt_desc));
+	}
+	if (DEV_CONFIG_apebs_collect_lbrs(pcfg) &&
+	    EVENT_DESC_lbr_offset(evt_desc)) {
+		if (!(dtes_record_format & APEBS_LBR_RECORD_FORMAT_MASK)) {
+			SEP_DRV_LOG_WARNING(
+				"LBR info not found in DS PEBS record\n");
+		}
+		if ((dtes_record_format >> 24) !=
+		    (DEV_CONFIG_apebs_num_lbr_entries(pcfg) - 1)) {
+			SEP_DRV_LOG_WARNING(
+				"DRV_CONFIG_apebs_num_lbr_entries does not match with PEBS record\n");
+		}
+		*(U64 *)(buffer + EVENT_DESC_lbr_offset(evt_desc)) =
+			DEV_CONFIG_apebs_num_lbr_entries(pcfg) - 1;
+		//Top-of-Stack(TOS) pointing to last entry
+		//Populating lbr callstack as SST_ENTRY_N to SST_ENTRY_0 in
+		// tb util, hence setting TOS to SST_ENTRY_N
+		memcpy(buffer + EVENT_DESC_lbr_offset(evt_desc) + sizeof(U64),
+		       pebs_base +
+			       LWPMU_DEVICE_apebs_lbr_offset(&devices[dev_idx]),
+		       EVENT_DESC_lbr_info_size(evt_desc) - sizeof(U64));
+		lbr_tos_from_ip = ADAPTIVE_PEBS_LBR_INFO_lbr_from(
+			(ADAPTIVE_PEBS_LBR_INFO)(pebs_base +
+						 LWPMU_DEVICE_apebs_lbr_offset(
+							 &devices[dev_idx])));
+	}
+	return lbr_tos_from_ip;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          OS_STATUS PEBS_Initialize (DEV_CONFIG pcfg)
+ *
+ * @brief       Initialize the pebs buffers
+ *
+ * @param       dev_idx -  Device index
+ *
+ * @return      status
+ *
+ * <I>Special Notes:</I>
+ *              If the user is asking for PEBS information.  Allocate the DS area
+ */
+OS_STATUS PEBS_Initialize(U32 dev_idx)
+{
+	DEV_CONFIG pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	SEP_DRV_LOG_TRACE_IN("Pcfg: %p.", pcfg);
+
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		switch (DEV_CONFIG_pebs_mode(pcfg)) {
+		case 1:
+			SEP_DRV_LOG_INIT("Set up the Core2 dispatch table.");
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&core2_pebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(PEBS_REC_NODE);
+			break;
+		case 2:
+			SEP_DRV_LOG_INIT("Set up the Nehalem dispatch.");
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&corei7_pebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(PEBS_REC_EXT_NODE);
+			break;
+		case 3:
+			SEP_DRV_LOG_INIT(
+				"Set up the Core2 (PNR) dispatch table.");
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&core2p_pebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(PEBS_REC_NODE);
+			break;
+		case 4:
+			SEP_DRV_LOG_INIT("Set up the Haswell dispatch table.");
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&haswell_pebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(PEBS_REC_EXT1_NODE);
+			break;
+		case 5:
+			SEP_DRV_LOG_INIT(
+				"Set up the Perf version4 dispatch table.");
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&perfver4_pebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(PEBS_REC_EXT2_NODE);
+			break;
+		case 6:
+			if (!DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+				SEP_DRV_LOG_TRACE(
+					"APEBS need to be enabled in perf version4 SNC dispatch mode.");
+			}
+			LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) =
+				&perfver4_apebs;
+			LWPMU_DEVICE_pebs_record_size(&devices[dev_idx]) =
+				sizeof(ADAPTIVE_PEBS_BASIC_INFO_NODE);
+			if (DEV_CONFIG_apebs_collect_mem_info(pcfg)) {
+				LWPMU_DEVICE_apebs_mem_offset(
+					&devices[dev_idx]) =
+					LWPMU_DEVICE_pebs_record_size(
+						&devices[dev_idx]);
+				LWPMU_DEVICE_pebs_record_size(
+					&devices[dev_idx]) +=
+					sizeof(ADAPTIVE_PEBS_MEM_INFO_NODE);
+			}
+			if (DEV_CONFIG_apebs_collect_gpr(pcfg)) {
+				LWPMU_DEVICE_apebs_gpr_offset(
+					&devices[dev_idx]) =
+					LWPMU_DEVICE_pebs_record_size(
+						&devices[dev_idx]);
+				LWPMU_DEVICE_pebs_record_size(
+					&devices[dev_idx]) +=
+					sizeof(ADAPTIVE_PEBS_GPR_INFO_NODE);
+			}
+			if (DEV_CONFIG_apebs_collect_xmm(pcfg)) {
+				LWPMU_DEVICE_apebs_xmm_offset(
+					&devices[dev_idx]) =
+					LWPMU_DEVICE_pebs_record_size(
+						&devices[dev_idx]);
+				LWPMU_DEVICE_pebs_record_size(
+					&devices[dev_idx]) +=
+					sizeof(ADAPTIVE_PEBS_XMM_INFO_NODE);
+			}
+			if (DEV_CONFIG_apebs_collect_lbrs(pcfg)) {
+				LWPMU_DEVICE_apebs_lbr_offset(
+					&devices[dev_idx]) =
+					LWPMU_DEVICE_pebs_record_size(
+						&devices[dev_idx]);
+				LWPMU_DEVICE_pebs_record_size(
+					&devices[dev_idx]) +=
+					(sizeof(ADAPTIVE_PEBS_LBR_INFO_NODE) *
+					 DEV_CONFIG_apebs_num_lbr_entries(
+						 pcfg));
+			}
+			SEP_DRV_LOG_TRACE("Size of adaptive pebs record - %d.",
+					  LWPMU_DEVICE_pebs_record_size(
+						  &devices[dev_idx]));
+			break;
+		default:
+			SEP_DRV_LOG_INIT(
+				"Unknown PEBS type. Will not collect PEBS information.");
+			break;
+		}
+	}
+	if (LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx]) &&
+	    !DEV_CONFIG_pebs_record_num(pcfg)) {
+		DEV_CONFIG_pebs_record_num(pcfg) = 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          OS_STATUS PEBS_Allocate (void)
+ *
+ * @brief       Allocate the pebs related buffers
+ *
+ * @param       NONE
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *             Allocated the DS area used for PEBS capture
+ */
+OS_STATUS PEBS_Allocate(VOID)
+{
+	S32 cpu_num;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	U32 dts_size;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_INIT_IN("");
+
+	for (cpu_num = 0; cpu_num < GLOBAL_STATE_num_cpus(driver_state);
+	     cpu_num++) {
+		pcpu = &pcb[cpu_num];
+		dev_idx = core_to_dev_map[cpu_num];
+		pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+		if (LWPMU_DEVICE_pebs_dispatch(&devices[dev_idx])) {
+			dts_size = sizeof(DTS_BUFFER_EXT_NODE);
+			if (DEV_CONFIG_enable_adaptive_pebs(pcfg)) {
+				dts_size = sizeof(DTS_BUFFER_EXT1_NODE);
+			}
+			CPU_STATE_dts_buffer_offset(pcpu) =
+				pebs_global_memory_size;
+			pebs_global_memory_size += PER_CORE_BUFFER_SIZE(
+				dts_size,
+				LWPMU_DEVICE_pebs_record_size(
+					&devices[dev_idx]),
+				DEV_CONFIG_pebs_record_num(pcfg));
+		}
+	}
+	if (pebs_global_memory_size) {
+		if (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+		    DRV_SETUP_INFO_PTI_DISABLED) {
+			SEP_DRV_LOG_INIT(
+				"Allocating global PEBS buffer using regular control routine.");
+			pebs_global_memory = (PVOID)CONTROL_Allocate_KMemory(
+				pebs_global_memory_size);
+			if (!pebs_global_memory) {
+				SEP_DRV_LOG_ERROR_TRACE_OUT(
+					"Failed to allocate PEBS buffer!");
+				return OS_NO_MEM;
+			}
+			memset(pebs_global_memory, 0, pebs_global_memory_size);
+		} else {
+#if defined(DRV_USE_KAISER)
+			SEP_DRV_LOG_INIT(
+				"Allocating PEBS buffer using KAISER-compatible approach.");
+
+			if (!local_kaiser_add_mapping) {
+				local_kaiser_add_mapping =
+					(PVOID)UTILITY_Find_Symbol(
+						"kaiser_add_mapping");
+				if (!local_kaiser_add_mapping) {
+					SEP_DRV_LOG_ERROR(
+						"Could not find 'kaiser_add_mapping'!");
+					goto kaiser_error_handling;
+				}
+			}
+
+			if (!local_kaiser_remove_mapping) {
+				local_kaiser_remove_mapping =
+					(PVOID)UTILITY_Find_Symbol(
+						"kaiser_remove_mapping");
+				if (!local_kaiser_remove_mapping) {
+					SEP_DRV_LOG_ERROR(
+						"Could not find 'kaiser_remove_mapping'!");
+					goto kaiser_error_handling;
+				}
+			}
+
+			pebs_global_memory = (PVOID)__get_free_pages(
+				GFP_KERNEL | __GFP_ZERO,
+				get_order(pebs_global_memory_size));
+
+			if (pebs_global_memory) {
+				SEP_DRV_LOG_TRACE(
+					"Successful memory allocation for pebs_global_memory.");
+
+				if (local_kaiser_add_mapping(
+					    (unsigned long)pebs_global_memory,
+					    pebs_global_memory_size,
+					    __PAGE_KERNEL) >= 0) {
+					SEP_DRV_LOG_TRACE(
+						"Successful kaiser_add_mapping.");
+				} else {
+					SEP_DRV_LOG_ERROR(
+						"KAISER mapping failed!");
+					free_pages(
+						(unsigned long)
+							pebs_global_memory,
+						get_order(
+							pebs_global_memory_size));
+					pebs_global_memory = NULL;
+					goto kaiser_error_handling;
+				}
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"Failed memory allocation for pebs_global_memory!");
+			}
+
+		kaiser_error_handling:
+			if (!pebs_global_memory) {
+				SEP_DRV_LOG_ERROR_TRACE_OUT(
+					"Failed to setup PEBS buffer!");
+				return OS_NO_MEM;
+			}
+#elif defined(DRV_USE_PTI)
+			if (!local_cea_set_pte) {
+				local_cea_set_pte = (PVOID)UTILITY_Find_Symbol(
+					"cea_set_pte");
+				if (!local_cea_set_pte) {
+					SEP_DRV_LOG_ERROR_TRACE_OUT(
+						"Could not find 'cea_set_pte'!");
+					return OS_FAULT;
+				}
+			}
+			if (!local_do_kernel_range_flush) {
+				local_do_kernel_range_flush =
+					(PVOID)UTILITY_Find_Symbol(
+						"do_kernel_range_flush");
+				if (!local_do_kernel_range_flush) {
+					SEP_DRV_LOG_ERROR_TRACE_OUT(
+						"Could not find 'do_kernel_range_flush'!");
+					return OS_FAULT;
+				}
+			}
+#endif // DRV_USE_PTI
+		}
+	}
+
+	CONTROL_Invoke_Parallel(pebs_Allocate_Buffers, (VOID *)NULL);
+
+	SEP_DRV_LOG_INIT_OUT("");
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID PEBS_Destroy (void)
+ *
+ * @brief       Clean up the pebs related buffers
+ *
+ * @param       pcfg  -  Driver Configuration
+ *
+ * @return      NONE
+ *
+ * <I>Special Notes:</I>
+ *             Deallocated the DS area used for PEBS capture
+ */
+VOID PEBS_Destroy(VOID)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	CONTROL_Invoke_Parallel(pebs_Deallocate_Buffers, (VOID *)(size_t)0);
+	if (pebs_global_memory) {
+		if (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+		    DRV_SETUP_INFO_PTI_DISABLED) {
+			SEP_DRV_LOG_INIT(
+				"Freeing PEBS buffer using regular control routine.");
+			pebs_global_memory =
+				CONTROL_Free_Memory(pebs_global_memory);
+		}
+#if defined(DRV_USE_KAISER)
+		else if (DRV_SETUP_INFO_page_table_isolation(
+				 &req_drv_setup_info) ==
+			 DRV_SETUP_INFO_PTI_KAISER) {
+			SEP_DRV_LOG_INIT(
+				"Freeing PEBS buffer using KAISER-compatible approach.");
+			if (local_kaiser_remove_mapping) {
+				local_kaiser_remove_mapping(
+					(unsigned long)pebs_global_memory,
+					pebs_global_memory_size);
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"Could not call 'kaiser_remove_mapping'!");
+			}
+			free_pages((unsigned long)pebs_global_memory,
+				   get_order(pebs_global_memory_size));
+			pebs_global_memory = NULL;
+		}
+#endif // DRV_USE_KAISER
+
+		pebs_global_memory_size = 0;
+		SEP_DRV_LOG_INIT("PEBS buffer successfully freed.");
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/perfver4.c b/drivers/platform/x86/sepdk/sep/perfver4.c
new file mode 100755
index 000000000000..ae8fa717f4bf
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/perfver4.c
@@ -0,0 +1,1972 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "lwpmudrv.h"
+#include "utility.h"
+#include "control.h"
+#include "output.h"
+#include "perfver4.h"
+#include "ecb_iterators.h"
+#include "pebs.h"
+#include "apic.h"
+
+extern U64 *read_counter_info;
+extern DRV_CONFIG drv_cfg;
+extern U64 *interrupt_counts;
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+static U64 perf_metrics_counter_reload_value;
+
+typedef struct SADDR_S {
+	S64 addr : PERFVER4_LBR_DATA_BITS;
+} SADDR;
+
+static U32 restore_reg_addr[3];
+
+#define SADDR_addr(x) ((x).addr)
+#define MSR_ENERGY_MULTIPLIER 0x606 // Energy Multiplier MSR
+
+#define IS_FIXED_CTR_ENABLED(ia32_perf_global_ctrl_reg_val)                    \
+	((ia32_perf_global_ctrl_reg_val)&0x700000000ULL)
+#define IS_FOUR_FIXED_CTR_ENABLED(ia32_perf_global_ctrl_reg_val)               \
+	((ia32_perf_global_ctrl_reg_val)&0xF00000000ULL)
+#define IS_PMC_PEBS_ENABLED_GP(ia32_perf_global_ctrl_reg_val,                  \
+			       ia32_pebs_enable_reg_val)                       \
+	(((ia32_perf_global_ctrl_reg_val)&0xfULL) ==                           \
+	 ((ia32_pebs_enable_reg_val)&0xfULL))
+#define IS_PMC_PEBS_ENABLED_FP_AND_GP(ia32_perf_global_ctrl_reg_val,           \
+				      ia32_pebs_enable_reg_val)                \
+	(((ia32_perf_global_ctrl_reg_val)&0xf000000ffULL) ==                   \
+	 ((ia32_pebs_enable_reg_val)&0xf000000ffULL))
+
+#define DISABLE_FRZ_ON_PMI(ia32_debug_ctrl_reg_val)                            \
+	(0xefff & (ia32_debug_ctrl_reg_val))
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Write_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initial set up of the PMU registers
+ *
+ * <I>Special Notes</I>
+ *         Initial write of PMU registers.
+ *         Walk through the enties and write the value of the register accordingly.
+ *         Assumption:  For CCCR registers the enable bit is set to value 0.
+ *         When current_group = 0, then this is the first time this routine is called,
+ *         initialize the locks and set up EM tables.
+ */
+static VOID perfver4_Write_PMU(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	EVENT_CONFIG ec;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+#if defined(DRV_SEP_ACRN_ON)
+	struct profiling_pmi_config *pmi_config;
+	U32 index;
+	S32 msr_idx;
+#else
+	U32 counter_index;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	if (param == NULL) {
+		preempt_disable();
+		this_cpu = CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		this_cpu = *(S32 *)param;
+	}
+
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	counter_index = 0;
+	if (CPU_STATE_current_group(pcpu) == 0) {
+		if (EVENT_CONFIG_mode(ec) != EM_DISABLED) {
+			U32 index;
+			U32 st_index;
+			U32 j;
+
+			/* Save all the initialization values away into an array for Event Multiplexing. */
+			for (j = 0; j < EVENT_CONFIG_num_groups(ec); j++) {
+				CPU_STATE_current_group(pcpu) = j;
+				st_index = CPU_STATE_current_group(pcpu) *
+					   EVENT_CONFIG_max_gp_events(ec);
+				FOR_EACH_REG_CORE_OPERATION(
+					pecb, i, PMU_OPERATION_DATA_GP)
+				{
+					index = st_index + i -
+						ECB_operations_register_start(
+							pecb,
+							PMU_OPERATION_DATA_GP);
+					CPU_STATE_em_tables(pcpu)[index] =
+						ECB_entries_reg_value(pecb, i);
+				}
+				END_FOR_EACH_REG_CORE_OPERATION;
+			}
+			/* Reset the current group to the very first one. */
+			CPU_STATE_current_group(pcpu) =
+				this_cpu % EVENT_CONFIG_num_groups(ec);
+		}
+	}
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	/* Clear outstanding frozen bits */
+	SYS_Write_MSR(IA32_PERF_GLOBAL_OVF_CTRL, PERFVER4_FROZEN_BIT_MASK);
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		/*
+		 * Writing the GLOBAL Control register enables the PMU to start counting.
+		 * So write 0 into the register to prevent any counting from starting.
+		 */
+		if (i == ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+		/*
+		 *  PEBS is enabled for this collection session
+		 */
+		if (DRV_SETUP_INFO_pebs_accessible(&req_drv_setup_info) &&
+		    i == ECB_SECTION_REG_INDEX(pecb, PEBS_ENABLE_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS) &&
+		    ECB_entries_reg_value(pecb, i)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+
+		if (DEV_CONFIG_pebs_mode(pcfg) &&
+		    (ECB_entries_precise_get(pecb, i) == 1)) {
+			if (ECB_entries_fixed_reg_get(pecb, i)) {
+				counter_index = (ECB_entries_reg_id(pecb, i) -
+						 IA32_FIXED_CTR0 + 8);
+			} else {
+				counter_index = (ECB_entries_reg_id(pecb, i) -
+						 IA32_PMC0);
+			}
+			PEBS_Reset_Counter(this_cpu, counter_index,
+					   ECB_entries_reg_value(pecb, i));
+		}
+
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+#if defined(MYDEBUG)
+		{
+			U64 val = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE(
+				"Write reg 0x%x --- value 0x%llx -- read 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				ECB_entries_reg_value(pecb, i), val);
+		}
+#endif
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+#else
+	pmi_config = (struct profiling_pmi_config *)CONTROL_Allocate_Memory(
+		sizeof(struct profiling_pmi_config));
+	if (pmi_config == NULL) {
+		SEP_PRINT_ERROR("pmi_config memory allocation failed\n");
+		return;
+	}
+	memset(pmi_config, 0, sizeof(struct profiling_pmi_config));
+
+	msr_idx = 0;
+	pmi_config->num_groups = 1;
+
+	pmi_config->initial_list[0][msr_idx].msr_id = IA32_PERF_GLOBAL_CTRL;
+	pmi_config->initial_list[0][msr_idx].op_type = MSR_OP_WRITE;
+	pmi_config->initial_list[0][msr_idx].reg_type = PMU_MSR_CCCR;
+	pmi_config->initial_list[0][msr_idx].value = 0x0;
+	pmi_config->initial_list[0][msr_idx].param = 0x0;
+	msr_idx++;
+
+	FOR_EACH_CCCR_REG_CPU(pecb, i, this_cpu)
+	{
+		if ((ECB_entries_reg_id(pecb, i) == IA32_PERF_GLOBAL_CTRL) ||
+		    (ECB_entries_reg_id(pecb, i) == IA32_PEBS_ENABLE)) {
+			continue;
+		}
+
+		pmi_config->initial_list[0][msr_idx].msr_id =
+			ECB_entries_reg_id(pecb, i);
+		pmi_config->initial_list[0][msr_idx].op_type = MSR_OP_WRITE;
+		pmi_config->initial_list[0][msr_idx].reg_type = PMU_MSR_CCCR;
+		pmi_config->initial_list[0][msr_idx].value =
+			ECB_entries_reg_value(pecb, i);
+		pmi_config->initial_list[0][msr_idx].param = 0x0;
+		msr_idx++;
+		BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+	}
+	END_FOR_EACH_CCCR_REG_CPU;
+
+	FOR_EACH_ESCR_REG_CPU(pecb, i, this_cpu)
+	{
+		pmi_config->initial_list[0][msr_idx].msr_id =
+			ECB_entries_reg_id(pecb, i);
+		pmi_config->initial_list[0][msr_idx].op_type = MSR_OP_WRITE;
+		pmi_config->initial_list[0][msr_idx].reg_type = PMU_MSR_ESCR;
+		pmi_config->initial_list[0][msr_idx].value =
+			ECB_entries_reg_value(pecb, i);
+		pmi_config->initial_list[0][msr_idx].param = 0x0;
+		msr_idx++;
+		BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+	}
+	END_FOR_EACH_ESCR_REG_CPU;
+
+	FOR_EACH_DATA_REG_CPU(pecb, i, this_cpu)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = ECB_entries_reg_id(pecb, i) - IA32_FIXED_CTR0 +
+				0x20;
+		} else if (ECB_entries_is_gp_reg_get(pecb, i)) {
+			index = ECB_entries_reg_id(pecb, i) - IA32_PMC0;
+		} else {
+			continue;
+		}
+		pmi_config->initial_list[0][msr_idx].msr_id =
+			ECB_entries_reg_id(pecb, i);
+		pmi_config->initial_list[0][msr_idx].op_type = MSR_OP_WRITE;
+		pmi_config->initial_list[0][msr_idx].reg_type = PMU_MSR_DATA;
+		pmi_config->initial_list[0][msr_idx].value =
+			ECB_entries_reg_value(pecb, i);
+		pmi_config->initial_list[0][msr_idx].param = index;
+		msr_idx++;
+		BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+	}
+	END_FOR_EACH_DATA_REG_CPU;
+	pmi_config->initial_list[0][msr_idx].msr_id = -1;
+
+	FOR_EACH_CCCR_REG_CPU(pecb, i, this_cpu)
+	{
+		if (ECB_entries_reg_id(pecb, i) == IA32_PERF_GLOBAL_CTRL) {
+			pmi_config->start_list[0][0].msr_id =
+				IA32_PERF_GLOBAL_CTRL;
+			pmi_config->start_list[0][0].op_type = MSR_OP_WRITE;
+			pmi_config->start_list[0][0].reg_type = PMU_MSR_CCCR;
+			pmi_config->start_list[0][0].value =
+				ECB_entries_reg_value(pecb, i);
+			pmi_config->start_list[0][0].param = 0x0;
+			pmi_config->start_list[0][1].msr_id = -1;
+			break;
+		}
+	}
+	END_FOR_EACH_CCCR_REG_CPU;
+
+	pmi_config->stop_list[0][0].msr_id = IA32_PERF_GLOBAL_CTRL;
+	pmi_config->stop_list[0][0].op_type = MSR_OP_WRITE;
+	pmi_config->stop_list[0][0].reg_type = PMU_MSR_CCCR;
+	pmi_config->stop_list[0][0].value = 0x0;
+	pmi_config->stop_list[0][0].param = 0x0;
+	pmi_config->stop_list[0][1].msr_id = -1;
+
+	if (DRV_CONFIG_counting_mode(drv_cfg) == FALSE) {
+		pmi_config->entry_list[0][0].msr_id = IA32_PERF_GLOBAL_CTRL;
+		pmi_config->entry_list[0][0].op_type = MSR_OP_WRITE;
+		pmi_config->entry_list[0][0].reg_type = PMU_MSR_CCCR;
+		pmi_config->entry_list[0][0].value = 0x0;
+		pmi_config->entry_list[0][0].param = 0x0;
+		pmi_config->entry_list[0][1].msr_id = -1;
+
+		msr_idx = 0;
+		FOR_EACH_CCCR_REG_CPU(pecb, i, this_cpu)
+		{
+			if ((ECB_entries_reg_id(pecb, i) ==
+			     IA32_PERF_GLOBAL_CTRL) ||
+			    (ECB_entries_reg_id(pecb, i) == IA32_PEBS_ENABLE)) {
+				continue;
+			}
+
+			pmi_config->exit_list[0][msr_idx].msr_id =
+				ECB_entries_reg_id(pecb, i);
+			pmi_config->exit_list[0][msr_idx].op_type =
+				MSR_OP_WRITE;
+			pmi_config->exit_list[0][msr_idx].reg_type =
+				PMU_MSR_CCCR;
+			pmi_config->exit_list[0][msr_idx].value =
+				ECB_entries_reg_value(pecb, i);
+			pmi_config->exit_list[0][msr_idx].param = 0x0;
+			msr_idx++;
+			BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+		}
+		END_FOR_EACH_CCCR_REG_CPU;
+
+		FOR_EACH_ESCR_REG_CPU(pecb, i, this_cpu)
+		{
+			pmi_config->exit_list[0][msr_idx].msr_id =
+				ECB_entries_reg_id(pecb, i);
+			pmi_config->exit_list[0][msr_idx].op_type =
+				MSR_OP_WRITE;
+			pmi_config->exit_list[0][msr_idx].reg_type =
+				PMU_MSR_ESCR;
+			pmi_config->exit_list[0][msr_idx].value =
+				ECB_entries_reg_value(pecb, i);
+			pmi_config->exit_list[0][msr_idx].param = 0x0;
+			msr_idx++;
+			BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+		}
+		END_FOR_EACH_ESCR_REG_CPU;
+
+		FOR_EACH_DATA_REG_CPU(pecb, i, this_cpu)
+		{
+			if (ECB_entries_fixed_reg_get(pecb, i)) {
+				index = ECB_entries_reg_id(pecb, i) -
+					IA32_FIXED_CTR0 + 0x20;
+			} else if (ECB_entries_is_gp_reg_get(pecb, i)) {
+				index = ECB_entries_reg_id(pecb, i) - IA32_PMC0;
+			} else {
+				continue;
+			}
+			pmi_config->exit_list[0][msr_idx].msr_id =
+				ECB_entries_reg_id(pecb, i);
+			pmi_config->exit_list[0][msr_idx].op_type =
+				MSR_OP_WRITE;
+			pmi_config->exit_list[0][msr_idx].reg_type =
+				PMU_MSR_DATA;
+			pmi_config->exit_list[0][msr_idx].value =
+				ECB_entries_reg_value(pecb, i);
+			pmi_config->exit_list[0][msr_idx].param = index;
+			msr_idx++;
+			BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+		}
+		END_FOR_EACH_DATA_REG_CPU;
+
+		FOR_EACH_CCCR_REG_CPU(pecb, i, this_cpu)
+		{
+			if (ECB_entries_reg_id(pecb, i) ==
+			    IA32_PERF_GLOBAL_CTRL) {
+				pmi_config->exit_list[0][msr_idx].msr_id =
+					IA32_PERF_GLOBAL_CTRL;
+				pmi_config->exit_list[0][msr_idx].op_type =
+					MSR_OP_WRITE;
+				pmi_config->exit_list[0][msr_idx].reg_type =
+					PMU_MSR_CCCR;
+				pmi_config->exit_list[0][msr_idx].value =
+					ECB_entries_reg_value(pecb, i);
+				pmi_config->exit_list[0][msr_idx].param = 0x0;
+				msr_idx++;
+				BUG_ON(msr_idx >= MAX_MSR_LIST_NUM);
+				break;
+			}
+		}
+		END_FOR_EACH_CCCR_REG_CPU;
+		pmi_config->exit_list[0][msr_idx].msr_id = -1;
+	}
+
+	BUG_ON(!virt_addr_valid(pmi_config));
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_CONFIG_PMI,
+			virt_to_phys(pmi_config));
+
+	pmi_config = CONTROL_Free_Memory(pmi_config);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Disable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Zero out the global control register.  This automatically disables the PMU counters.
+ *
+ */
+static VOID perfver4_Disable_PMU(PVOID param)
+{
+#if !defined(DRV_SEP_ACRN_ON)
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		// no programming for this device for this group
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	if (GET_DRIVER_STATE() != DRV_STATE_RUNNING) {
+		SEP_DRV_LOG_TRACE("Driver state = %d.", GET_DRIVER_STATE());
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+		if (DEV_CONFIG_pebs_mode(pcfg)) {
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, PEBS_ENABLE_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				0LL);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Enable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Set the enable bit for all the Control registers
+ *
+ */
+static VOID perfver4_Enable_PMU(PVOID param)
+{
+#if !defined(DRV_SEP_ACRN_ON)
+	/*
+	 * Get the value from the event block
+	 *   0 == location of the global control reg for this block.
+	 *   Generalize this location awareness when possible
+	 */
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+	U64 global_control_val;
+	U64 pebs_enable_val;
+	DRV_BOOL multi_pebs_enabled;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		// no programming for this device for this group
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	if (KVM_guest_mode) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+	if (GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		APIC_Enable_Pmi();
+
+		/* Clear outstanding frozen bits */
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb,
+				      ECB_SECTION_REG_INDEX(
+					      pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					      PMU_OPERATION_GLOBAL_REGS)),
+			      PERFVER4_FROZEN_BIT_MASK);
+
+		if (CPU_STATE_reset_mask(pcpu)) {
+			SEP_DRV_LOG_TRACE("Overflow reset mask %llx.",
+					  CPU_STATE_reset_mask(pcpu));
+			// Reinitialize the global overflow control register
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			CPU_STATE_reset_mask(pcpu) = 0LL;
+		}
+		if (CPU_STATE_group_swap(pcpu)) {
+			CPU_STATE_group_swap(pcpu) = 0;
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			if (DEV_CONFIG_pebs_mode(pcfg) ||
+			    DEV_CONFIG_latency_capture(pcfg)) {
+				SYS_Write_MSR(
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					ECB_entries_reg_value(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)));
+			}
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+#if defined(MYDEBUG)
+			{
+				U64 val;
+				val = SYS_Read_MSR(ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+				SEP_DRV_LOG_TRACE(
+					"Write reg 0x%x--- read 0x%llx.",
+					ECB_entries_reg_id(pecb, 0), val);
+			}
+#endif
+		}
+
+		multi_pebs_enabled = (DEV_CONFIG_pebs_mode(pcfg) &&
+				      (DEV_CONFIG_pebs_record_num(pcfg) > 1) &&
+				      (DRV_SETUP_INFO_page_table_isolation(
+					       &req_drv_setup_info) ==
+				       DRV_SETUP_INFO_PTI_DISABLED));
+
+		// FIXME: workaround for sampling both pebs event and non-pebs event
+		//        with pebs buffer size > 1
+		if (multi_pebs_enabled) {
+			global_control_val = SYS_Read_MSR(ECB_entries_reg_id(
+				pecb, ECB_SECTION_REG_INDEX(
+					      pecb, GLOBAL_CTRL_REG_INDEX,
+					      PMU_OPERATION_GLOBAL_REGS)));
+			pebs_enable_val = SYS_Read_MSR(ECB_entries_reg_id(
+				pecb, ECB_SECTION_REG_INDEX(
+					      pecb, PEBS_ENABLE_REG_INDEX,
+					      PMU_OPERATION_GLOBAL_REGS)));
+			if (IS_FIXED_CTR_ENABLED(global_control_val) ||
+			    !IS_PMC_PEBS_ENABLED_GP(global_control_val,
+						    pebs_enable_val)) {
+				SEP_DRV_LOG_TRACE(
+					"Global_control_val = 0x%llx pebs_enable_val = 0x%llx.",
+					global_control_val, pebs_enable_val);
+				SYS_Write_MSR(
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							DEBUG_CTRL_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					DISABLE_FRZ_ON_PMI(ECB_entries_reg_value(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							DEBUG_CTRL_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS))));
+			}
+		}
+	}
+	SEP_DRV_LOG_TRACE("Reenabled PMU with value 0x%llx.",
+			  ECB_entries_reg_value(pecb, 0));
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Read_PMU_Data(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read all the data MSR's into a buffer.  Called by the interrupt handler.
+ *
+ */
+static void perfver4_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+#if defined(DRV_SEP_ACRN_ON)
+	S32 start_index, cpu_idx, msr_idx;
+	struct profiling_msr_ops_list *msr_list;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	if (param == NULL) {
+		preempt_disable();
+		this_cpu = CONTROL_THIS_CPU();
+		preempt_enable();
+	} else {
+		this_cpu = *(S32 *)param;
+	}
+
+#if !defined(DRV_SEP_ACRN_ON)
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("PMU control_data 0x%p, buffer 0x%p.",
+			  LWPMU_DEVICE_PMU_register_data(&devices[dev_idx]),
+			  buffer);
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		j = EMON_BUFFER_CORE_EVENT_OFFSET(
+			EMON_BUFFER_DRIVER_HELPER_core_index_to_thread_offset_map(
+				emon_buffer_driver_helper)[this_cpu],
+			ECB_entries_core_event_id(pecb, i));
+
+		buffer[j] = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u, event_id=%u", j,
+				  buffer[j], this_cpu,
+				  ECB_entries_core_event_id(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+#else
+	if (DRV_CONFIG_counting_mode(drv_cfg) == TRUE) {
+		msr_list = (struct profiling_msr_ops_list *)
+			CONTROL_Allocate_Memory(
+				GLOBAL_STATE_num_cpus(driver_state) *
+				sizeof(struct profiling_msr_ops_list));
+		memset(msr_list, 0,
+		       GLOBAL_STATE_num_cpus(driver_state) *
+			       sizeof(struct profiling_msr_ops_list));
+		for (cpu_idx = 0; cpu_idx < GLOBAL_STATE_num_cpus(driver_state);
+		     cpu_idx++) {
+			pcpu = &pcb[cpu_idx];
+			dev_idx = core_to_dev_map[cpu_idx];
+			cur_grp = CPU_STATE_current_group(pcpu);
+			pecb = LWPMU_DEVICE_PMU_register_data(
+				&devices[dev_idx])[cur_grp];
+
+			if (!pecb) {
+				continue;
+			}
+
+			msr_idx = 0;
+			FOR_EACH_DATA_REG_CPU(pecb, i, cpu_idx)
+			{
+				msr_list[cpu_idx].entries[msr_idx].msr_id =
+					ECB_entries_reg_id(pecb, i);
+				msr_list[cpu_idx].entries[msr_idx].op_type =
+					MSR_OP_READ_CLEAR;
+				msr_list[cpu_idx].entries[msr_idx].value = 0LL;
+				msr_idx++;
+			}
+			END_FOR_EACH_DATA_REG_CPU;
+			msr_list[cpu_idx].num_entries = msr_idx;
+			msr_list[cpu_idx].msr_op_state = MSR_OP_REQUESTED;
+		}
+
+		BUG_ON(!virt_addr_valid(msr_list));
+
+		acrn_hypercall2(HC_PROFILING_OPS, PROFILING_MSR_OPS,
+				virt_to_phys(msr_list));
+
+		for (cpu_idx = 0; cpu_idx < GLOBAL_STATE_num_cpus(driver_state);
+		     cpu_idx++) {
+			pcpu = &pcb[cpu_idx];
+			dev_idx = core_to_dev_map[cpu_idx];
+			cur_grp = CPU_STATE_current_group(pcpu);
+			pecb = LWPMU_DEVICE_PMU_register_data(
+				&devices[dev_idx])[cur_grp];
+
+			if (!pecb) {
+				continue;
+			}
+
+			start_index = ECB_num_events(pecb) * cpu_idx;
+			msr_idx = 0;
+			FOR_EACH_DATA_REG_CPU(pecb, i, cpu_idx)
+			{
+				j = start_index +
+				    ECB_entries_event_id_index(pecb, i);
+				buffer[j] =
+					msr_list[cpu_idx].entries[msr_idx].value;
+				msr_idx++;
+			}
+			END_FOR_EACH_DATA_REG_CPU;
+		}
+
+		msr_list = CONTROL_Free_Memory(msr_list);
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Check_Overflow(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static void perfver4_Check_Overflow(DRV_MASKS masks)
+{
+	U32 index;
+	U64 overflow_status = 0;
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+	DISPATCH dispatch;
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("Masks: %p.", masks);
+
+	this_cpu = CONTROL_THIS_CPU();
+	bd = &cpu_buf[this_cpu];
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg) &&
+	    (DEV_CONFIG_pebs_record_num(pcfg) == 1)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+
+	if (dispatch->check_overflow_gp_errata) {
+		overflow_status = dispatch->check_overflow_gp_errata(
+			pecb, &overflow_status_clr);
+	}
+	SEP_DRV_LOG_TRACE("Overflow: cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+			if (dispatch->check_overflow_errata) {
+				overflow_status =
+					dispatch->check_overflow_errata(
+						pecb, i, overflow_status);
+			}
+		} else if (ECB_entries_is_gp_reg_get(pecb, i)) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow: cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"Register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      ECB_entries_reg_value(pecb, i));
+
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_uncore_get(pecb, i)) {
+				DRV_EVENT_MASK_uncore_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_branch_evt_get(pecb, i)) {
+				DRV_EVENT_MASK_branch(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("Overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("Slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	/* Clear outstanding overflow bits */
+	SYS_Write_MSR(ECB_entries_reg_id(
+			      pecb, ECB_SECTION_REG_INDEX(
+					    pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS)),
+		      overflow_status_clr & PERFVER4_OVERFLOW_BIT_MASK_HT_ON);
+
+	SEP_DRV_LOG_TRACE("Check overflow completed %d.", this_cpu);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Swap_Group(restart)
+ *
+ * @param    restart    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Perform the mechanics of swapping the event groups for event mux operations
+ *
+ * <I>Special Notes</I>
+ *         Swap function for event multiplexing.
+ *         Freeze the counting.
+ *         Swap the groups.
+ *         Enable the counting.
+ *         Reset the event trigger count
+ *
+ */
+static VOID perfver4_Swap_Group(DRV_BOOL restart)
+{
+	U32 index;
+	U32 next_group;
+	U32 st_index;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+	EVENT_CONFIG ec;
+	U32 counter_index;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy restart: %u.", restart);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	counter_index = 0;
+
+	st_index =
+		CPU_STATE_current_group(pcpu) * EVENT_CONFIG_max_gp_events(ec);
+	next_group = (CPU_STATE_current_group(pcpu) + 1);
+	if (next_group >= EVENT_CONFIG_num_groups(ec)) {
+		next_group = 0;
+	}
+
+	SEP_DRV_LOG_TRACE("Current group : 0x%x.",
+			  CPU_STATE_current_group(pcpu));
+	SEP_DRV_LOG_TRACE("Next group : 0x%x.", next_group);
+
+	// Save the counters for the current group
+	if (!DRV_CONFIG_event_based_counts(drv_cfg)) {
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			CPU_STATE_em_tables(pcpu)[index] =
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE("Saved value for reg 0x%x : 0x%llx.",
+					  ECB_entries_reg_id(pecb, i),
+					  CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	CPU_STATE_current_group(pcpu) = next_group;
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	// First write the GP control registers (eventsel)
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_CTRL_GP)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DRV_CONFIG_event_based_counts(drv_cfg)) {
+		// In EBC mode, reset the counts for all events except for trigger event
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+		{
+			if (ECB_entries_event_id_index(pecb, i) !=
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			}
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	} else {
+		// Then write the gp count registers
+		st_index = CPU_STATE_current_group(pcpu) *
+			   EVENT_CONFIG_max_gp_events(ec);
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      CPU_STATE_em_tables(pcpu)[index]);
+			SEP_DRV_LOG_TRACE(
+				"Restore value for reg 0x%x : 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_OCR)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DEV_CONFIG_pebs_record_num(pcfg)) {
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+		{
+			if (ECB_entries_precise_get(pecb, i) == 1) {
+				if (ECB_entries_fixed_reg_get(pecb, i)) {
+					counter_index =
+						i -
+						ECB_operations_register_start(
+							pecb,
+							PMU_OPERATION_DATA_FIXED) +
+						8;
+				} else {
+					counter_index =
+						i -
+						ECB_operations_register_start(
+							pecb,
+							PMU_OPERATION_DATA_GP);
+				}
+				PEBS_Reset_Counter(this_cpu, counter_index,
+						   ECB_entries_reg_value(pecb,
+									 i));
+			}
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	/*
+	 *  reset the em factor when a group is swapped
+	 */
+	CPU_STATE_trigger_count(pcpu) = EVENT_CONFIG_em_factor(ec);
+
+	/*
+	 * The enable routine needs to rewrite the control registers
+	 */
+	CPU_STATE_reset_mask(pcpu) = 0LL;
+	CPU_STATE_group_swap(pcpu) = 1;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Initialize(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initialize the PMU setting up for collection
+ *
+ * <I>Special Notes</I>
+ *         Saves the relevant PMU state (minimal set of MSRs required
+ *         to avoid conflicts with other Linux tools, such as Oprofile).
+ *         This function should be called in parallel across all CPUs
+ *         prior to the start of sampling, before PMU state is changed.
+ *
+ */
+static VOID perfver4_Initialize(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 cur_grp;
+	ECB pecb = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pcb).");
+		return;
+	}
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	CPU_STATE_pmu_state(pcpu) = pmu_state + (this_cpu * 3);
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to save PMU state on CPU %d.", this_cpu);
+		return;
+	}
+
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	restore_reg_addr[0] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, DEBUG_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[1] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[2] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, FIXED_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+
+	// save the original PMU state on this CPU (NOTE: must only be called ONCE per collection)
+	CPU_STATE_pmu_state(pcpu)[0] = SYS_Read_MSR(restore_reg_addr[0]);
+	CPU_STATE_pmu_state(pcpu)[1] = SYS_Read_MSR(restore_reg_addr[1]);
+	CPU_STATE_pmu_state(pcpu)[2] = SYS_Read_MSR(restore_reg_addr[2]);
+
+	if (DRV_CONFIG_ds_area_available(drv_cfg) &&
+	    DEV_CONFIG_pebs_mode(pcfg)) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, PEBS_ENABLE_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+
+	SEP_DRV_LOG_TRACE("Saving PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[0]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[1]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[2]);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Destroy(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Reset the PMU setting up after collection
+ *
+ * <I>Special Notes</I>
+ *         Restores the previously saved PMU state done in pmv_v4_Initialize.
+ *         This function should be called in parallel across all CPUs
+ *         after sampling collection ends/terminates.
+ *
+ */
+static VOID perfver4_Destroy(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pcb).");
+		return;
+	}
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	pcpu = &pcb[this_cpu];
+
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to restore PMU state on CPU %d.", this_cpu);
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("Clearing PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0x0.");
+
+	SYS_Write_MSR(restore_reg_addr[0], 0);
+	SYS_Write_MSR(restore_reg_addr[1], 0);
+	SYS_Write_MSR(restore_reg_addr[2], 0);
+
+	CPU_STATE_pmu_state(pcpu) = NULL;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn perfver4_Read_LBRs(buffer)
+ *
+ * @param   IN buffer - pointer to the buffer to write the data into
+ * @return  Last branch source IP address
+ *
+ * @brief   Read all the LBR registers into the buffer provided and return
+ *
+ */
+static U64 perfver4_Read_LBRs(VOID *buffer, PVOID data)
+{
+	U32 i, count = 0;
+	U64 *lbr_buf = NULL;
+	U64 value = 0;
+	U64 tos_ip_addr = 0;
+	U64 tos_ptr = 0;
+	SADDR saddr;
+	U32 pairs = 0;
+	U32 this_cpu;
+	U32 dev_idx;
+	LBR lbr;
+	DEV_CONFIG pcfg;
+#if defined(DRV_SEP_ACRN_ON)
+	struct lbr_pmu_sample *lbr_data = NULL;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	lbr = LWPMU_DEVICE_lbr(&devices[dev_idx]);
+
+	if (lbr == NULL) {
+		return 0;
+	}
+
+#if defined(DRV_SEP_ACRN_ON)
+	if (data == NULL) {
+		return 0;
+	}
+	lbr_data = (struct lbr_pmu_sample *)data;
+#endif
+
+	if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+		lbr_buf = (U64 *)buffer;
+	}
+
+	if (LBR_num_entries(lbr) > 0) {
+		pairs = (LBR_num_entries(lbr) - 1) / 3;
+	}
+	for (i = 0; i < LBR_num_entries(lbr); i++) {
+#if !defined(DRV_SEP_ACRN_ON)
+		value = SYS_Read_MSR(LBR_entries_reg_id(lbr, i));
+#else
+		if (i == 0) {
+			value = lbr_data->lbr_tos;
+		} else {
+			if (LBR_entries_etype(lbr, i) == LBR_ENTRY_FROM_IP) {
+				value = lbr_data->lbr_from_ip[i - 1];
+			} else if (LBR_entries_etype(lbr, i) ==
+				   LBR_ENTRY_TO_IP) {
+				value = lbr_data->lbr_to_ip[i - pairs - 1];
+			} else {
+				value = lbr_data->lbr_info[i - 2 * pairs - 1];
+			}
+		}
+#endif
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			*lbr_buf = value;
+		}
+		if (DEV_CONFIG_collect_callstacks(pcfg)) {
+			if ((LBR_entries_etype(lbr, i) == LBR_ENTRY_FROM_IP &&
+			     i > tos_ptr + 1) ||
+			    (LBR_entries_etype(lbr, i) == LBR_ENTRY_TO_IP &&
+			     i > tos_ptr + pairs + 1) ||
+			    (LBR_entries_etype(lbr, i) == LBR_ENTRY_INFO &&
+			     i > tos_ptr + 2 * pairs + 1)) {
+				if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+					*lbr_buf = 0x0ULL;
+					lbr_buf++;
+				}
+				continue;
+			}
+		}
+		SEP_DRV_LOG_TRACE("LBR %u, 0x%llx.", i, value);
+		if (i == 0) {
+			tos_ptr = value;
+		} else {
+			if (LBR_entries_etype(lbr, i) ==
+			    LBR_ENTRY_FROM_IP) { // LBR from register
+				if (tos_ptr == count) {
+					SADDR_addr(saddr) =
+						value & PERFVER4_LBR_BITMASK;
+					tos_ip_addr = (U64)SADDR_addr(
+						saddr); // Add signed extension
+					SEP_DRV_LOG_TRACE(
+						"Tos_ip_addr %llu, 0x%llx.",
+						tos_ptr, value);
+				}
+				count++;
+			}
+		}
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			lbr_buf++;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llu.", tos_ip_addr);
+	return tos_ip_addr;
+}
+
+/*
+ * @fn perfver4_Clean_Up(param)
+ *
+ * @param   IN param - currently not used
+ *
+ * @brief   Clean up registers in ECB
+ *
+ */
+static VOID perfver4_Clean_Up(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb = NULL;
+	U32 dev_idx;
+	U32 cur_grp;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		if (ECB_entries_clean_up_get(pecb, i)) {
+			SEP_DRV_LOG_TRACE("Clean up set --- RegId --- %x.",
+					  ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	/* Clear outstanding frozen bits */
+	if (pecb) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb,
+				      ECB_SECTION_REG_INDEX(
+					      pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					      PMU_OPERATION_GLOBAL_REGS)),
+			      PERFVER4_FROZEN_BIT_MASK);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+	return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Check_Overflow_Htoff_Mode(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static void perfver4_Check_Overflow_Htoff_Mode(DRV_MASKS masks)
+{
+	U32 index;
+	U64 value = 0;
+	U64 overflow_status = 0;
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("Masks: %p.", masks);
+
+	this_cpu = CONTROL_THIS_CPU();
+	bd = &cpu_buf[this_cpu];
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg) &&
+	    (DEV_CONFIG_pebs_record_num(pcfg) == 1)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+	SEP_DRV_LOG_TRACE("Overflow: cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+
+	if (dispatch->check_overflow_gp_errata) {
+		overflow_status = dispatch->check_overflow_gp_errata(
+			pecb, &overflow_status_clr);
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+		} else if (ECB_entries_is_gp_reg_get(pecb, i) &&
+			   ECB_entries_reg_value(pecb, i) != 0) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+			if (index >= 4 && index <= 7) {
+				value = SYS_Read_MSR(
+					ECB_entries_reg_id(pecb, i));
+				if (value > 0 && value <= 0x100000000LL) {
+					overflow_status |= ((U64)1 << index);
+				}
+			}
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"Register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      ECB_entries_reg_value(pecb, i));
+
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("Overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("Slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	/* Clear outstanding overflow bits */
+	SYS_Write_MSR(ECB_entries_reg_id(
+			      pecb, ECB_SECTION_REG_INDEX(
+					    pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS)),
+		      overflow_status_clr & PERFVER4_OVERFLOW_BIT_MASK_HT_OFF);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#define MAX_COUNTER 0xFFFFFFFFFFFFLLU
+#define FIXED_CTR3_BIT_INDEX 35
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Check_Overflow_Nonht_Mode(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static VOID perfver4_Check_Overflow_Nonht_Mode(DRV_MASKS masks)
+{
+	U32 index;
+	U64 overflow_status = 0;
+	U32 this_cpu = CONTROL_THIS_CPU();
+	BUFFER_DESC bd = &cpu_buf[this_cpu];
+	CPU_STATE pcpu = &pcb[this_cpu];
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	U32 cur_grp = CPU_STATE_current_group(pcpu);
+	DEV_CONFIG pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	ECB pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg) &&
+	    (DEV_CONFIG_pebs_record_num(pcfg) == 1)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+	SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+		} else if (ECB_entries_is_gp_reg_get(pecb, i) &&
+			   ECB_entries_reg_value(pecb, i) != 0) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow:  cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (DEV_CONFIG_enable_perf_metrics(pcfg) &&
+			    index == FIXED_CTR3_BIT_INDEX) {
+				perf_metrics_counter_reload_value =
+					ECB_entries_reg_value(
+						pecb, i); // saving reload value
+				// Writing positive SAV into data register before reading metrics
+				SYS_Write_MSR(
+					ECB_entries_reg_id(pecb, i),
+					((~(ECB_entries_reg_value(pecb, i)) +
+					  1) &
+					 MAX_COUNTER));
+				DRV_EVENT_MASK_perf_metrics_capture(
+					&event_flag) = 1;
+			} else {
+				SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+					      ECB_entries_reg_value(pecb, i));
+			}
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_uncore_get(pecb, i)) {
+				DRV_EVENT_MASK_uncore_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_branch_evt_get(pecb, i)) {
+				DRV_EVENT_MASK_branch(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("Overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("Slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	/* Clear outstanding overflow bits */
+	SYS_Write_MSR(ECB_entries_reg_id(
+			      pecb, ECB_SECTION_REG_INDEX(
+					    pecb, GLOBAL_OVF_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS)),
+		      overflow_status_clr & PERFVER4_OVERFLOW_BIT_MASK_NON_HT);
+
+	SEP_DRV_LOG_TRACE("Check Overflow completed %d.", this_cpu);
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void perfver4_Read_Power(buffer)
+ *
+ * @param    buffer   - pointer to the buffer to write the data into
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Read all the power MSRs into the buffer provided and return.
+ *
+ */
+static VOID perfver4_Read_Power(VOID *buffer)
+{
+	U32 i;
+	U64 *pwr_buf = (U64 *)buffer;
+	U32 this_cpu;
+	U32 dev_idx;
+	PWR pwr;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pwr = LWPMU_DEVICE_pwr(&devices[dev_idx]);
+
+	for (i = 0; i < PWR_num_entries(pwr); i++) {
+		*pwr_buf = SYS_Read_MSR(PWR_entries_reg_id(pwr, i));
+		pwr_buf++;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Read_Counts(param, id)
+ *
+ * @param    param    The read thread node to process
+ * @param    id       The event id for the which the sample is generated
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read CPU event based counts data and store into the buffer param;
+ *           For the case of the trigger event, store the SAV value.
+ */
+static VOID perfver4_Read_Counts(PVOID param, U32 id)
+{
+	U64 *data;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 event_id = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (DEV_CONFIG_ebc_group_id_offset(pcfg)) {
+		// Write GroupID
+		data = (U64 *)((S8 *)param +
+			       DEV_CONFIG_ebc_group_id_offset(pcfg));
+		*data = CPU_STATE_current_group(pcpu) + 1;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_counter_event_offset(pecb, i) == 0) {
+			continue;
+		}
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, i));
+		event_id = ECB_entries_event_id_index(pecb, i);
+		if (event_id == id) {
+			*data = ~(ECB_entries_reg_value(pecb, i) - 1) &
+				ECB_entries_max_bits(pecb, i);
+			;
+		} else {
+			*data = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn perfver4_Read_Metrics(buffer, id)
+ *
+ * @param    param        buffer to write metrics into
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read hardware metrics from IA32_PERF_METRICS MSR
+ */
+static VOID perfver4_Read_Metrics(PVOID buffer)
+{
+	U64 *data, metrics = 0;
+	U32 j;
+	U32 this_cpu = CONTROL_THIS_CPU();
+	U32 dev_idx = core_to_dev_map[this_cpu];
+	DEV_CONFIG pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	data = (U64 *)buffer;
+	FOR_EACH_NONEVENT_REG(pecb, i)
+	{
+		metrics = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+		for (j = 0; j < DEV_CONFIG_num_perf_metrics(pcfg); j++) {
+			*data = (metrics & (0xFFULL << 8 * j)) >> 8 * j;
+			data++;
+		}
+	}
+	END_FOR_EACH_NONEVENT_REG;
+
+	if (DRV_CONFIG_emon_mode(drv_cfg)) {
+		SYS_Write_MSR(IA32_FIXED_CTR3, 0LL);
+	} else {
+		SYS_Write_MSR(IA32_FIXED_CTR3,
+			      perf_metrics_counter_reload_value);
+		perf_metrics_counter_reload_value = 0;
+	}
+
+	SYS_Write_MSR(IA32_PERF_METRICS, 0LL);
+}
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 perfver4_Platform_Info
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param       void
+ *
+ * @return      value read from the register
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID perfver4_Platform_Info(PVOID data)
+{
+	DRV_PLATFORM_INFO platform_data = (DRV_PLATFORM_INFO)data;
+	U64 value = 0;
+	U64 energy_multiplier;
+
+	SEP_DRV_LOG_TRACE_IN("Data: %p.", data);
+
+	if (!platform_data) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!platform_data).");
+		return;
+	}
+
+#define IA32_MSR_PLATFORM_INFO 0xCE
+	value = SYS_Read_MSR(IA32_MSR_PLATFORM_INFO);
+
+	DRV_PLATFORM_INFO_info(platform_data) = value;
+	DRV_PLATFORM_INFO_ddr_freq_index(platform_data) = 0;
+
+#define IA32_MSR_MISC_ENABLE 0x1A4
+	DRV_PLATFORM_INFO_misc_valid(platform_data) = 1;
+	value = SYS_Read_MSR(IA32_MSR_MISC_ENABLE);
+	DRV_PLATFORM_INFO_misc_info(platform_data) = value;
+#undef IA32_MSR_MISC_ENABLE
+
+	energy_multiplier = SYS_Read_MSR(MSR_ENERGY_MULTIPLIER);
+	SEP_DRV_LOG_TRACE("MSR_ENERGY_MULTIPLIER: %llx.", energy_multiplier);
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) =
+		(U32)(energy_multiplier & 0x00001F00) >> 8;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+DISPATCH_NODE perfver4_dispatch = { .init = perfver4_Initialize,
+				    .fini = perfver4_Destroy,
+				    .write = perfver4_Write_PMU,
+				    .freeze = perfver4_Disable_PMU,
+				    .restart = perfver4_Enable_PMU,
+				    .read_data = perfver4_Read_PMU_Data,
+				    .check_overflow = perfver4_Check_Overflow,
+				    .swap_group = perfver4_Swap_Group,
+				    .read_lbrs = perfver4_Read_LBRs,
+				    .cleanup = perfver4_Clean_Up,
+				    .hw_errata = NULL,
+				    .read_power = perfver4_Read_Power,
+				    .check_overflow_errata = NULL,
+				    .read_counts = perfver4_Read_Counts,
+				    .check_overflow_gp_errata = NULL,
+				    .read_ro = NULL,
+				    .platform_info = perfver4_Platform_Info,
+				    .trigger_read = NULL,
+				    .scan_for_uncore = NULL,
+				    .read_metrics = NULL };
+
+DISPATCH_NODE perfver4_dispatch_htoff_mode = {
+	.init = perfver4_Initialize,
+	.fini = perfver4_Destroy,
+	.write = perfver4_Write_PMU,
+	.freeze = perfver4_Disable_PMU,
+	.restart = perfver4_Enable_PMU,
+	.read_data = perfver4_Read_PMU_Data,
+	.check_overflow = perfver4_Check_Overflow_Htoff_Mode,
+	.swap_group = perfver4_Swap_Group,
+	.read_lbrs = perfver4_Read_LBRs,
+	.cleanup = perfver4_Clean_Up,
+	.hw_errata = NULL,
+	.read_power = perfver4_Read_Power,
+	.check_overflow_errata = NULL,
+	.read_counts = perfver4_Read_Counts,
+	.check_overflow_gp_errata = NULL,
+	.read_ro = NULL,
+	.platform_info = perfver4_Platform_Info,
+	.trigger_read = NULL,
+	.scan_for_uncore = NULL,
+	.read_metrics = NULL
+};
+
+DISPATCH_NODE perfver4_dispatch_nonht_mode = {
+	.init = perfver4_Initialize,
+	.fini = perfver4_Destroy,
+	.write = perfver4_Write_PMU,
+	.freeze = perfver4_Disable_PMU,
+	.restart = perfver4_Enable_PMU,
+	.read_data = perfver4_Read_PMU_Data,
+	.check_overflow = perfver4_Check_Overflow_Nonht_Mode,
+	.swap_group = perfver4_Swap_Group,
+	.read_lbrs = perfver4_Read_LBRs,
+	.cleanup = perfver4_Clean_Up,
+	.hw_errata = NULL,
+	.read_power = perfver4_Read_Power,
+	.check_overflow_errata = NULL,
+	.read_counts = perfver4_Read_Counts,
+	.check_overflow_gp_errata = NULL,
+	.read_ro = NULL,
+	.platform_info = perfver4_Platform_Info,
+	.trigger_read = NULL,
+	.scan_for_uncore = NULL,
+	.read_metrics = perfver4_Read_Metrics
+};
diff --git a/drivers/platform/x86/sepdk/sep/pmi.c b/drivers/platform/x86/sepdk/sep/pmi.c
new file mode 100755
index 000000000000..44f335dbc885
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/pmi.c
@@ -0,0 +1,640 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/ptrace.h>
+#if defined(DRV_EM64T)
+#include <asm/desc.h>
+#endif
+#include <asm/apic.h>
+#include <asm/nmi.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "apic.h"
+#include "lwpmudrv.h"
+#include "output.h"
+#include "control.h"
+#include "pmi.h"
+#include "utility.h"
+#include "pebs.h"
+#include "ecb_iterators.h"
+#include "msrdefs.h"
+
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#endif
+#include "sepdrv_p_state.h"
+
+// Desc id #0 is used for module records
+#define COMPUTE_DESC_ID(index) ((index))
+
+extern DRV_CONFIG drv_cfg;
+extern uid_t uid;
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+#define EFLAGS_V86_MASK 0x00020000L
+
+/*********************************************************************
+ * Global Variables / State
+ *********************************************************************/
+
+/*********************************************************************
+ * Interrupt Handler
+ *********************************************************************/
+
+/*
+ *  PMI_Interrupt_Handler
+ *      Arguments
+ *          IntFrame - Pointer to the Interrupt Frame
+ *
+ *      Returns
+ *          None
+ *
+ *      Description
+ *  Grab the data that is needed to populate the sample records
+ */
+#if defined(DRV_EM64T)
+#define IS_LDT_BIT 0x4
+#define SEGMENT_SHIFT 3
+IDTGDT_DESC gdt_desc;
+
+U32 pmi_Get_CSD(U32 seg, U32 *low, U32 *high)
+{
+	PVOID gdt_max_addr;
+	struct desc_struct *gdt;
+	CodeDescriptor *csd;
+
+	SEP_DRV_LOG_TRACE_IN("Seg: %u, low: %p, high: %p.", seg, low, high);
+
+	gdt_max_addr =
+		(PVOID)(((U64)gdt_desc.idtgdt_base) + gdt_desc.idtgdt_limit);
+	gdt = gdt_desc.idtgdt_base;
+
+	if (seg & IS_LDT_BIT) {
+		*low = 0;
+		*high = 0;
+		SEP_DRV_LOG_TRACE_OUT("FALSE [%u, %u] (IS_LDT_BIT).", *low,
+				      *high);
+		return FALSE;
+	}
+
+	// segment offset is based on dropping the bottom 3 bits...
+	csd = (CodeDescriptor *)&(gdt[seg >> SEGMENT_SHIFT]);
+
+	if (((PVOID)csd) >= gdt_max_addr) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"FALSE (segment too big in get_CSD(0x%x)!).", seg);
+		return FALSE;
+	}
+
+	*low = csd->u1.lowWord;
+	*high = csd->u2.highWord;
+
+	SEP_DRV_LOG_TRACE("Seg 0x%x, low %08x, high %08x, reserved_0: %d.", seg,
+			  *low, *high, csd->u2.s2.reserved_0);
+	SEP_DRV_LOG_TRACE_OUT("TRUE [%u, %u].", *low, *high);
+
+	return TRUE;
+}
+#endif
+
+asmlinkage VOID PMI_Interrupt_Handler(struct pt_regs *regs)
+{
+	SampleRecordPC *psamp;
+	CPU_STATE pcpu;
+	BUFFER_DESC bd;
+#if defined(DRV_IA32)
+	U32 csdlo; // low  half code seg descriptor
+	U32 csdhi; // high half code seg descriptor
+	U32 seg_cs; // code seg selector
+#endif
+	DRV_MASKS_NODE event_mask;
+	U32 this_cpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+	U32 i;
+	U32 is_64bit_addr = FALSE;
+	U32 pid;
+	U32 tid;
+	U64 tsc;
+	U32 desc_id;
+	EVENT_DESC evt_desc;
+	U32 accept_interrupt = 1;
+#if defined(SECURE_SEP)
+	uid_t l_uid;
+#endif
+	U64 lbr_tos_from_ip = 0;
+	DRV_BOOL multi_pebs_enabled;
+
+	SEP_DRV_LOG_INTERRUPT_IN(
+		"PID: %d, TID: %d.", current->pid,
+		GET_CURRENT_TGID()); // needs to be before function calls for the tracing to make sense
+	// may later want to separate the INTERRUPT_IN from the PID/TID logging
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	bd = &cpu_buf[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	multi_pebs_enabled =
+		(DEV_CONFIG_pebs_mode(pcfg) &&
+		 (DEV_CONFIG_pebs_record_num(pcfg) > 1) &&
+		 (DRV_SETUP_INFO_page_table_isolation(&req_drv_setup_info) ==
+		  DRV_SETUP_INFO_PTI_DISABLED));
+	SYS_Locked_Inc(&CPU_STATE_in_interrupt(
+		pcpu)); // needs to be before dispatch->freeze to ensure printk is never called from an interrupt
+
+	// Disable the counter control
+	dispatch->freeze(NULL);
+
+	CPU_STATE_nmi_handled(&pcb[this_cpu])++;
+
+#if defined(SECURE_SEP)
+	l_uid = DRV_GET_UID(current);
+	accept_interrupt = (l_uid == uid);
+#endif
+	dispatch->check_overflow(&event_mask);
+	if (GET_DRIVER_STATE() != DRV_STATE_RUNNING ||
+	    CPU_STATE_accept_interrupt(&pcb[this_cpu]) != 1) {
+		goto pmi_cleanup;
+	}
+
+	pid = GET_CURRENT_TGID();
+	tid = current->pid;
+
+	if (DRV_CONFIG_target_pid(drv_cfg) > 0 &&
+	    pid != DRV_CONFIG_target_pid(drv_cfg)) {
+		accept_interrupt = 0;
+	}
+
+	if (accept_interrupt == 0) {
+		goto pmi_cleanup;
+	}
+	UTILITY_Read_TSC(&tsc);
+	if (multi_pebs_enabled && PEBS_Get_Num_Records_Filled() > 0) {
+		PEBS_Flush_Buffer(NULL);
+	}
+
+	SEP_DRV_LOG_TRACE("Nb overflowed events: %d.", event_mask.masks_num);
+	for (i = 0; i < event_mask.masks_num; i++) {
+		if (multi_pebs_enabled &&
+		    (DRV_EVENT_MASK_precise(&event_mask.eventmasks[i]))) {
+			continue;
+		}
+		if (DRV_CONFIG_event_based_counts(drv_cfg) == 0) {
+			desc_id = COMPUTE_DESC_ID(DRV_EVENT_MASK_event_idx(
+				&event_mask.eventmasks[i]));
+		} else {
+			desc_id = CPU_STATE_current_group(pcpu);
+		}
+		evt_desc = desc_data[desc_id];
+		psamp = (SampleRecordPC *)OUTPUT_Reserve_Buffer_Space(
+			bd, EVENT_DESC_sample_size(evt_desc),
+			(NMI_mode) ? TRUE : FALSE, !SEP_IN_NOTIFICATION,
+			(S32)this_cpu);
+
+		if (!psamp) {
+			continue;
+		}
+		lbr_tos_from_ip = 0;
+		CPU_STATE_num_samples(pcpu) += 1;
+		SAMPLE_RECORD_descriptor_id(psamp) = desc_id;
+		SAMPLE_RECORD_tsc(psamp) = tsc;
+		SAMPLE_RECORD_pid_rec_index_raw(psamp) = 1;
+		SAMPLE_RECORD_pid_rec_index(psamp) = pid;
+		SAMPLE_RECORD_tid(psamp) = tid;
+		SAMPLE_RECORD_cpu_num(psamp) = (U16)this_cpu;
+#if defined(DRV_IA32)
+		SAMPLE_RECORD_eip(psamp) = REGS_eip(regs);
+		SAMPLE_RECORD_eflags(psamp) = REGS_eflags(regs);
+		SAMPLE_RECORD_cs(psamp) = (U16)REGS_xcs(regs);
+
+		if (SAMPLE_RECORD_eflags(psamp) & EFLAGS_V86_MASK) {
+			csdlo = 0;
+			csdhi = 0;
+		} else {
+			seg_cs = SAMPLE_RECORD_cs(psamp);
+			SYS_Get_CSD(seg_cs, &csdlo, &csdhi);
+		}
+		SAMPLE_RECORD_csd(psamp).u1.lowWord = csdlo;
+		SAMPLE_RECORD_csd(psamp).u2.highWord = csdhi;
+#elif defined(DRV_EM64T)
+		SAMPLE_RECORD_cs(psamp) = (U16)REGS_cs(regs);
+
+		pmi_Get_CSD(SAMPLE_RECORD_cs(psamp),
+			    &SAMPLE_RECORD_csd(psamp).u1.lowWord,
+			    &SAMPLE_RECORD_csd(psamp).u2.highWord);
+#endif
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_pid_rec_index(psamp) %x.",
+				  SAMPLE_RECORD_pid_rec_index(psamp));
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_tid(psamp) %x.",
+				  SAMPLE_RECORD_tid(psamp));
+#if defined(DRV_IA32)
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_eip(psamp) %x.",
+				  SAMPLE_RECORD_eip(psamp));
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_eflags(psamp) %x.",
+				  SAMPLE_RECORD_eflags(psamp));
+#endif
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_cpu_num(psamp) %x.",
+				  SAMPLE_RECORD_cpu_num(psamp));
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_cs(psamp) %x.",
+				  SAMPLE_RECORD_cs(psamp));
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_csd(psamp).lowWord %x.",
+				  SAMPLE_RECORD_csd(psamp).u1.lowWord);
+		SEP_DRV_LOG_TRACE("SAMPLE_RECORD_csd(psamp).highWord %x.",
+				  SAMPLE_RECORD_csd(psamp).u2.highWord);
+
+#if defined(DRV_EM64T)
+		is_64bit_addr =
+			(SAMPLE_RECORD_csd(psamp).u2.s2.reserved_0 == 1);
+		if (is_64bit_addr) {
+			SAMPLE_RECORD_iip(psamp) = REGS_rip(regs);
+			SAMPLE_RECORD_ipsr(psamp) =
+				(REGS_eflags(regs) & 0xffffffff) |
+				(((U64)SAMPLE_RECORD_csd(psamp).u2.s2.dpl)
+				 << 32);
+			SAMPLE_RECORD_ia64_pc(psamp) = TRUE;
+		} else {
+			SAMPLE_RECORD_eip(psamp) = REGS_rip(regs);
+			SAMPLE_RECORD_eflags(psamp) = REGS_eflags(regs);
+			SAMPLE_RECORD_ia64_pc(psamp) = FALSE;
+
+			SEP_DRV_LOG_TRACE("SAMPLE_RECORD_eip(psamp) 0x%x.",
+					  SAMPLE_RECORD_eip(psamp));
+			SEP_DRV_LOG_TRACE("SAMPLE_RECORD_eflags(psamp) %x.",
+					  SAMPLE_RECORD_eflags(psamp));
+		}
+#endif
+
+		SAMPLE_RECORD_event_index(psamp) =
+			DRV_EVENT_MASK_event_idx(&event_mask.eventmasks[i]);
+		if (DEV_CONFIG_pebs_mode(pcfg) &&
+		    DRV_EVENT_MASK_precise(&event_mask.eventmasks[i])) {
+			if (EVENT_DESC_pebs_offset(evt_desc) ||
+			    EVENT_DESC_latency_offset_in_sample(evt_desc)) {
+				lbr_tos_from_ip = PEBS_Fill_Buffer((S8 *)psamp,
+								   evt_desc, 0);
+			}
+			PEBS_Modify_IP((S8 *)psamp, is_64bit_addr, 0);
+			PEBS_Modify_TSC((S8 *)psamp, 0);
+		}
+		if (DEV_CONFIG_collect_lbrs(pcfg) &&
+		    DRV_EVENT_MASK_lbr_capture(&event_mask.eventmasks[i]) &&
+		    !DEV_CONFIG_apebs_collect_lbrs(pcfg)) {
+			lbr_tos_from_ip = dispatch->read_lbrs(
+				!DEV_CONFIG_store_lbrs(pcfg) ?
+					NULL :
+					((S8 *)(psamp) +
+					 EVENT_DESC_lbr_offset(evt_desc)),
+				NULL);
+		}
+		if (DRV_EVENT_MASK_branch(&event_mask.eventmasks[i]) &&
+		    DEV_CONFIG_precise_ip_lbrs(pcfg) && lbr_tos_from_ip) {
+			if (is_64bit_addr) {
+				SAMPLE_RECORD_iip(psamp) = lbr_tos_from_ip;
+				SEP_DRV_LOG_TRACE(
+					"UPDATED SAMPLE_RECORD_iip(psamp) 0x%llx.",
+					SAMPLE_RECORD_iip(psamp));
+			} else {
+				SAMPLE_RECORD_eip(psamp) = (U32)lbr_tos_from_ip;
+				SEP_DRV_LOG_TRACE(
+					"UPDATED SAMPLE_RECORD_eip(psamp) 0x%x.",
+					SAMPLE_RECORD_eip(psamp));
+			}
+		}
+		if (DEV_CONFIG_power_capture(pcfg)) {
+			dispatch->read_power(
+				((S8 *)(psamp) +
+				 EVENT_DESC_power_offset_in_sample(evt_desc)));
+		}
+
+#if defined(BUILD_CHIPSET)
+		if (DRV_CONFIG_enable_chipset(drv_cfg)) {
+			cs_dispatch->read_counters(
+				((S8 *)(psamp) +
+				 DRV_CONFIG_chipset_offset(drv_cfg)));
+		}
+#endif
+		if (DRV_CONFIG_event_based_counts(drv_cfg)) {
+			dispatch->read_counts(
+				(S8 *)psamp,
+				DRV_EVENT_MASK_event_idx(
+					&event_mask.eventmasks[i]));
+		}
+		if (DEV_CONFIG_enable_perf_metrics(pcfg) &&
+		    DRV_EVENT_MASK_perf_metrics_capture(
+			    &event_mask.eventmasks[i])) {
+			dispatch->read_metrics(
+				(S8 *)(psamp) +
+				EVENT_DESC_perfmetrics_offset(evt_desc));
+		}
+		if (DRV_CONFIG_enable_p_state(drv_cfg)) {
+			if (DRV_CONFIG_read_pstate_msrs(drv_cfg) &&
+			    (DRV_CONFIG_p_state_trigger_index(drv_cfg) == -1 ||
+			     SAMPLE_RECORD_event_index(psamp) ==
+				     DRV_CONFIG_p_state_trigger_index(
+					     drv_cfg))) {
+				SEPDRV_P_STATE_Read(
+					(S8 *)(psamp) +
+						EVENT_DESC_p_state_offset(
+							evt_desc),
+					pcpu);
+			}
+			if (!DRV_CONFIG_event_based_counts(drv_cfg) &&
+			    CPU_STATE_p_state_counting(pcpu)) {
+				dispatch->read_counts(
+					(S8 *)psamp,
+					DRV_EVENT_MASK_event_idx(
+						&event_mask.eventmasks[i]));
+			}
+		}
+	}
+
+pmi_cleanup:
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		if (!multi_pebs_enabled) {
+			PEBS_Reset_Index(this_cpu);
+		} else {
+			if (cpu_sideband_buf) {
+				OUTPUT outbuf = &BUFFER_DESC_outbuf(
+					&cpu_sideband_buf[this_cpu]);
+				if (OUTPUT_signal_full(outbuf) &&
+				    !OUTPUT_tasklet_queued(outbuf)) {
+					SEP_DRV_LOG_TRACE(
+						"Interrupt-driven sideband buffer flush tasklet scheduling.");
+					OUTPUT_tasklet_queued(outbuf) = TRUE;
+					tasklet_schedule(&CPU_STATE_nmi_tasklet(
+						&pcb[this_cpu]));
+				}
+			}
+		}
+	}
+
+	// Reset the data counters
+	if (CPU_STATE_trigger_count(&pcb[this_cpu]) == 0) {
+		dispatch->swap_group(FALSE);
+	}
+	// Re-enable the counter control
+	dispatch->restart(NULL);
+	SYS_Locked_Dec(&CPU_STATE_in_interrupt(
+		&pcb[this_cpu])); // do not use SEP_DRV_LOG_X (where X != INTERRUPT) below this
+
+	SEP_DRV_LOG_INTERRUPT_OUT("");
+}
+
+#if defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn  S32 PMI_Buffer_Handler(PVOID data)
+ *
+ * @param data - Pointer to data
+ *
+ * @return S32
+ *
+ * @brief Handle the PMI sample data in buffer
+ *
+ * <I>Special Notes</I>
+ */
+S32 PMI_Buffer_Handler(PVOID data)
+{
+	SampleRecordPC *psamp;
+	CPU_STATE pcpu;
+	BUFFER_DESC bd;
+	S32 cpu_id, j;
+	U32 desc_id;
+	EVENT_DESC evt_desc;
+	U64 lbr_tos_from_ip = 0;
+	ECB pecb;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	DEV_CONFIG pcfg;
+
+	struct data_header header;
+	struct pmu_sample psample;
+	S32 data_size, payload_size, expected_payload_size, index;
+	U64 overflow_status = 0;
+
+	if (!pcb || !cpu_buf || !devices) {
+		return 0;
+	}
+	cpu_id = (S32)(size_t)data;
+
+	pcpu = &pcb[cpu_id];
+	bd = &cpu_buf[cpu_id];
+	dev_idx = core_to_dev_map[cpu_id];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	pecb = LWPMU_DEVICE_PMU_register_data(
+		&devices[dev_idx])[CPU_STATE_current_group(pcpu)];
+
+	while (1) {
+		if ((GLOBAL_STATE_current_phase(driver_state) ==
+		     DRV_STATE_PREPARE_STOP) ||
+		    (GLOBAL_STATE_current_phase(driver_state) ==
+		     DRV_STATE_TERMINATING) ||
+		    (GLOBAL_STATE_current_phase(driver_state) ==
+		     DRV_STATE_STOPPED)) {
+			goto handler_cleanup;
+		}
+
+		data_size =
+			sbuf_get(samp_buf_per_cpu[cpu_id], (uint8_t *)&header);
+		if (data_size <= 0) {
+			continue;
+		}
+		payload_size = 0;
+		if ((header.data_type == (1 << CORE_PMU_SAMPLING)) ||
+		    (header.data_type == (1 << LBR_PMU_SAMPLING))) {
+			if (header.data_type == (1 << CORE_PMU_SAMPLING)) {
+				expected_payload_size = CORE_PMU_SAMPLE_SIZE;
+			} else if (header.data_type ==
+				   (1 << LBR_PMU_SAMPLING)) {
+				expected_payload_size = CORE_PMU_SAMPLE_SIZE +
+							LBR_PMU_SAMPLE_SIZE;
+			} else {
+				expected_payload_size = 0;
+			}
+			for (j = 0; j < (expected_payload_size - 1) /
+							TRACE_ELEMENT_SIZE +
+						1;
+			     j++) {
+				while (1) {
+					data_size = sbuf_get(
+						samp_buf_per_cpu[cpu_id],
+						(uint8_t *)&psample +
+							j * TRACE_ELEMENT_SIZE);
+					if (data_size <= 0) {
+						if ((GLOBAL_STATE_current_phase(
+							     driver_state) ==
+						     DRV_STATE_PREPARE_STOP) ||
+						    (GLOBAL_STATE_current_phase(
+							     driver_state) ==
+						     DRV_STATE_TERMINATING) ||
+						    (GLOBAL_STATE_current_phase(
+							     driver_state) ==
+						     DRV_STATE_STOPPED)) {
+							goto handler_cleanup;
+						}
+					} else {
+						break;
+					}
+				}
+
+				payload_size += data_size;
+			}
+			if (header.payload_size > payload_size) {
+				// Mismatch in payload size in header info
+				SEP_PRINT_ERROR(
+					"Mismatch in data size: header=%llu, payload_size=%d\n",
+					header.payload_size, payload_size);
+				break;
+			}
+			if (header.cpu_id != cpu_id) {
+				// Mismatch in cpu index in header info
+				SEP_PRINT_ERROR(
+					"Mismatch in cpu idx: header=%u, buffer=%d\n",
+					header.cpu_id, cpu_id);
+				break;
+			}
+
+			// Now, handle the sample data in buffer
+			overflow_status = psample.csample.overflow_status;
+			SEP_PRINT_DEBUG("overflow_status cpu%d, value=0x%llx\n",
+					cpu_id, overflow_status);
+
+			FOR_EACH_DATA_REG_CPU(pecb, i, cpu_id)
+			{
+				if (ECB_entries_is_gp_reg_get(pecb, i)) {
+					index = ECB_entries_reg_id(pecb, i) -
+						IA32_PMC0;
+				} else if (ECB_entries_fixed_reg_get(pecb, i)) {
+					index = ECB_entries_reg_id(pecb, i) -
+						IA32_FIXED_CTR0 + 0x20;
+				} else {
+					continue;
+				}
+
+				if (overflow_status & ((U64)1 << index)) {
+					desc_id = COMPUTE_DESC_ID(
+						ECB_entries_event_id_index(pecb,
+									   i));
+					evt_desc = desc_data[desc_id];
+					SEP_PRINT_DEBUG(
+						"In Interrupt handler: event_id_index=%u, desc_id=%u\n",
+						ECB_entries_event_id_index(pecb,
+									   i),
+						desc_id);
+
+					psamp = (SampleRecordPC *)
+						OUTPUT_Reserve_Buffer_Space(
+							bd,
+							EVENT_DESC_sample_size(
+								evt_desc),
+							TRUE,
+							!SEP_IN_NOTIFICATION,
+							cpu_id);
+					if (!psamp) {
+						SEP_PRINT_DEBUG(
+							"In Interrupt handler: psamp is NULL. No output buffer allocated\n");
+						continue;
+					}
+
+					CPU_STATE_num_samples(pcpu) += 1;
+					SAMPLE_RECORD_descriptor_id(psamp) =
+						desc_id;
+					SAMPLE_RECORD_event_index(psamp) =
+						ECB_entries_event_id_index(pecb,
+									   i);
+					SAMPLE_RECORD_osid(psamp) =
+						psample.csample.os_id;
+					SAMPLE_RECORD_tsc(psamp) = header.tsc;
+					SAMPLE_RECORD_pid_rec_index_raw(psamp) =
+						1;
+					SAMPLE_RECORD_pid_rec_index(psamp) = 0;
+					SAMPLE_RECORD_pid_rec_index(psamp) = 0;
+					SAMPLE_RECORD_tid(psamp) = 0;
+					SAMPLE_RECORD_cpu_num(psamp) =
+						(U16)header.cpu_id;
+					SAMPLE_RECORD_cs(psamp) =
+						(U16)psample.csample.cs;
+
+					SAMPLE_RECORD_iip(psamp) =
+						psample.csample.rip;
+					SAMPLE_RECORD_ipsr(psamp) =
+						(psample.csample.rflags &
+						 0xffffffff) |
+						(((U64)SAMPLE_RECORD_csd(psamp)
+							  .u2.s2.dpl)
+						 << 32);
+					SAMPLE_RECORD_ia64_pc(psamp) = TRUE;
+
+					if (DEV_CONFIG_collect_lbrs(pcfg) &&
+
+					    !DEV_CONFIG_apebs_collect_lbrs(
+						    pcfg) &&
+					    header.data_type ==
+						    (1 << LBR_PMU_SAMPLING)) {
+						lbr_tos_from_ip = dispatch->read_lbrs(
+							!DEV_CONFIG_store_lbrs(
+								pcfg) ?
+								NULL :
+								((S8 *)(psamp) +
+								 EVENT_DESC_lbr_offset(
+									 evt_desc)),
+							&psample.lsample);
+					}
+
+					SEP_PRINT_DEBUG(
+						"SAMPLE_RECORD_cpu_num(psamp) %x\n",
+						SAMPLE_RECORD_cpu_num(psamp));
+					SEP_PRINT_DEBUG(
+						"SAMPLE_RECORD_iip(psamp) %x\n",
+						SAMPLE_RECORD_iip(psamp));
+					SEP_PRINT_DEBUG(
+						"SAMPLE_RECORD_cs(psamp) %x\n",
+						SAMPLE_RECORD_cs(psamp));
+					SEP_PRINT_DEBUG(
+						"SAMPLE_RECORD_csd(psamp).lowWord %x\n",
+						SAMPLE_RECORD_csd(psamp)
+							.u1.lowWord);
+					SEP_PRINT_DEBUG(
+						"SAMPLE_RECORD_csd(psamp).highWord %x\n",
+						SAMPLE_RECORD_csd(psamp)
+							.u2.highWord);
+				}
+			}
+			END_FOR_EACH_DATA_REG_CPU;
+		}
+	}
+
+handler_cleanup:
+	return 0;
+}
+#endif
diff --git a/drivers/platform/x86/sepdk/sep/sepdrv_p_state.c b/drivers/platform/x86/sepdk/sep/sepdrv_p_state.c
new file mode 100755
index 000000000000..e91b9be4d582
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/sepdrv_p_state.c
@@ -0,0 +1,88 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "inc/control.h"
+#include "inc/utility.h"
+#include "inc/sepdrv_p_state.h"
+
+/*!
+ * @fn     OS_STATUS SEPDRV_P_STATE_Read
+ *
+ * @brief  Reads the APERF and MPERF counters into the buffer provided for the purpose
+ *
+ * @param  buffer  - buffer to read the counts into
+ *
+ * @param  pcpu - pcpu struct that contains the previous APERF/MPERF values
+ *
+ * @return OS_SUCCESS if read succeeded, otherwise error
+ *
+ * @note
+ */
+OS_STATUS SEPDRV_P_STATE_Read(S8 *buffer, CPU_STATE pcpu)
+{
+	U64 *samp = (U64 *)buffer;
+	U64 new_APERF = 0;
+	U64 new_MPERF = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p, pcpu: %p.", buffer, pcpu);
+
+	if ((samp == NULL) || (pcpu == NULL)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("OS_INVALID (!samp || !pcpu).");
+		return OS_INVALID;
+	}
+
+	new_APERF = SYS_Read_MSR(DRV_APERF_MSR);
+	new_MPERF = SYS_Read_MSR(DRV_MPERF_MSR);
+
+	if (CPU_STATE_last_p_state_valid(pcpu)) {
+		// there is a previous APERF/MPERF value
+		if ((CPU_STATE_last_aperf(pcpu)) > new_APERF) {
+			// a wrap-around has occurred.
+			samp[1] = CPU_STATE_last_aperf(pcpu) - new_APERF;
+		} else {
+			samp[1] = new_APERF - CPU_STATE_last_aperf(pcpu);
+		}
+
+		if ((CPU_STATE_last_mperf(pcpu)) > new_MPERF) {
+			// a wrap-around has occurred.
+			samp[0] = CPU_STATE_last_mperf(pcpu) - new_MPERF;
+		} else {
+			samp[0] = new_MPERF - CPU_STATE_last_mperf(pcpu);
+		}
+	} else {
+		// there is no previous valid APERF/MPERF values, thus no delta calculations
+		(CPU_STATE_last_p_state_valid(pcpu)) = TRUE;
+		samp[0] = 0;
+		samp[1] = 0;
+	}
+
+	CPU_STATE_last_aperf(pcpu) = new_APERF;
+	CPU_STATE_last_mperf(pcpu) = new_MPERF;
+
+	SEP_DRV_LOG_TRACE_OUT("OS_SUCCESS.");
+	return OS_SUCCESS;
+}
diff --git a/drivers/platform/x86/sepdk/sep/silvermont.c b/drivers/platform/x86/sepdk/sep/silvermont.c
new file mode 100755
index 000000000000..d69930395923
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/silvermont.c
@@ -0,0 +1,1113 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "lwpmudrv.h"
+#include "utility.h"
+#include "control.h"
+#include "output.h"
+#include "silvermont.h"
+#include "ecb_iterators.h"
+#include "pebs.h"
+#include "apic.h"
+
+extern U64 *read_counter_info;
+extern DRV_CONFIG drv_cfg;
+extern U64 *interrupt_counts;
+extern DRV_SETUP_INFO_NODE req_drv_setup_info;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+static U32 restore_reg_addr[3];
+
+typedef struct SADDR_S {
+	S64 addr : SILVERMONT_LBR_DATA_BITS;
+} SADDR;
+
+#define SADDR_addr(x) ((x).addr)
+#define ADD_ERRATA_FIX_FOR_FIXED_CTR0
+#define MSR_ENERGY_MULTIPLIER 0x606 // Energy Multiplier MSR
+
+#if defined(DRV_IA32)
+#define ENABLE_IA32_PERFEVTSEL0_CTR 0x00400000
+#define ENABLE_FIXED_CTR0 0x00000003
+#elif defined(DRV_EM64T)
+#define ENABLE_IA32_PERFEVTSEL0_CTR 0x0000000000400000
+#define ENABLE_FIXED_CTR0 0x0000000000000003
+#else
+#error "Unexpected Architecture seen"
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void silvermont_Write_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initial set up of the PMU registers
+ *
+ * <I>Special Notes</I>
+ *         Initial write of PMU registers.
+ *         Walk through the enties and write the value of the register accordingly.
+ *         Assumption:  For CCCR registers the enable bit is set to value 0.
+ *         When current_group = 0, then this is the first time this routine is called,
+ *         initialize the locks and set up EM tables.
+ */
+static VOID silvermont_Write_PMU(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DISPATCH dispatch;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	if (CPU_STATE_current_group(pcpu) == 0) {
+		if (EVENT_CONFIG_mode(ec) != EM_DISABLED) {
+			U32 index;
+			U32 st_index;
+			U32 j;
+
+			/* Save all the initialization values away into an array for Event Multiplexing. */
+			for (j = 0; j < EVENT_CONFIG_num_groups(ec); j++) {
+				CPU_STATE_current_group(pcpu) = j;
+				st_index = CPU_STATE_current_group(pcpu) *
+					   EVENT_CONFIG_max_gp_events(ec);
+				FOR_EACH_REG_CORE_OPERATION(
+					pecb, i, PMU_OPERATION_DATA_GP)
+				{
+					index = st_index + i -
+						ECB_operations_register_start(
+							pecb,
+							PMU_OPERATION_DATA_GP);
+					CPU_STATE_em_tables(pcpu)[index] =
+						ECB_entries_reg_value(pecb, i);
+				}
+				END_FOR_EACH_REG_CORE_OPERATION;
+			}
+			/* Reset the current group to the very first one. */
+			CPU_STATE_current_group(pcpu) =
+				this_cpu % EVENT_CONFIG_num_groups(ec);
+		}
+	}
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		/*
+		 * Writing the GLOBAL Control register enables the PMU to start counting.
+		 * So write 0 into the register to prevent any counting from starting.
+		 */
+		if (i == ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+		/*
+		 *  PEBS is enabled for this collection session
+		 */
+		if (DRV_SETUP_INFO_pebs_accessible(&req_drv_setup_info) &&
+		    i == ECB_SECTION_REG_INDEX(pecb, PEBS_ENABLE_REG_INDEX,
+					       PMU_OPERATION_GLOBAL_REGS) &&
+		    ECB_entries_reg_value(pecb, i)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			continue;
+		}
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+#if defined(MYDEBUG)
+		{
+			U64 val = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE(
+				"Write reg 0x%x --- value 0x%llx -- read 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				ECB_entries_reg_value(pecb, i), val);
+		}
+#endif
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+#if defined(ADD_ERRATA_FIX_FOR_FIXED_CTR0)
+	{
+		U64 fixed_ctr0 = SYS_Read_MSR(IA32_FIXED_CTRL);
+		fixed_ctr0 = (fixed_ctr0 & (ENABLE_FIXED_CTR0));
+		if (fixed_ctr0 != 0x0) {
+			U64 val = SYS_Read_MSR(IA32_PERFEVTSEL0);
+			val |= ENABLE_IA32_PERFEVTSEL0_CTR;
+			SYS_Write_MSR(IA32_PERFEVTSEL0, val);
+		}
+	}
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void silvermont_Disable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Zero out the global control register.  This automatically disables the PMU counters.
+ *
+ */
+static VOID silvermont_Disable_PMU(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"No programming for this device in this group.");
+		return;
+	}
+
+	if (GET_DRIVER_STATE() != DRV_STATE_RUNNING) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+		if (DEV_CONFIG_pebs_mode(pcfg)) {
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, PEBS_ENABLE_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				0LL);
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void silvermont_Enable_PMU(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Set the enable bit for all the Control registers
+ *
+ */
+static VOID silvermont_Enable_PMU(PVOID param)
+{
+	/*
+	 * Get the value from the event block
+	 *   0 == location of the global control reg for this block.
+	 *   Generalize this location awareness when possible
+	 */
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	if (KVM_guest_mode) {
+		SYS_Write_MSR(ECB_entries_reg_id(
+				      pecb, ECB_SECTION_REG_INDEX(
+						    pecb, GLOBAL_CTRL_REG_INDEX,
+						    PMU_OPERATION_GLOBAL_REGS)),
+			      0LL);
+	}
+	if (GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		APIC_Enable_Pmi();
+		if (CPU_STATE_reset_mask(pcpu)) {
+			SEP_DRV_LOG_TRACE("Overflow reset mask %llx.",
+					  CPU_STATE_reset_mask(pcpu));
+			// Reinitialize the global overflow control register
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			CPU_STATE_reset_mask(pcpu) = 0LL;
+		}
+		if (CPU_STATE_group_swap(pcpu)) {
+			CPU_STATE_group_swap(pcpu) = 0;
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+			if (DEV_CONFIG_pebs_mode(pcfg)) {
+				SYS_Write_MSR(
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					ECB_entries_reg_value(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							PEBS_ENABLE_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)));
+			}
+			SYS_Write_MSR(
+				ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)),
+				ECB_entries_reg_value(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, DEBUG_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+#if defined(MYDEBUG)
+			{
+				U64 val;
+				val = SYS_Read_MSR(ECB_entries_reg_id(
+					pecb,
+					ECB_SECTION_REG_INDEX(
+						pecb, GLOBAL_CTRL_REG_INDEX,
+						PMU_OPERATION_GLOBAL_REGS)));
+				SEP_DRV_LOG_TRACE(
+					"Write reg 0x%x--- read 0x%llx.",
+					ECB_entries_reg_id(
+						pecb,
+						ECB_SECTION_REG_INDEX(
+							pecb,
+							GLOBAL_CTRL_REG_INDEX,
+							PMU_OPERATION_GLOBAL_REGS)),
+					val);
+			}
+#endif
+		}
+	}
+	SEP_DRV_LOG_TRACE("Reenabled PMU with value 0x%llx.",
+			  ECB_entries_reg_value(pecb, 0));
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn silvermont_Read_PMU_Data(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read all the data MSR's into a buffer.  Called by the interrupt handler.
+ *
+ */
+static void silvermont_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("PMU control_data 0x%p, buffer 0x%p.",
+			  LWPMU_DEVICE_PMU_register_data(&devices[dev_idx]),
+			  buffer);
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		j = EMON_BUFFER_CORE_EVENT_OFFSET(
+			EMON_BUFFER_DRIVER_HELPER_core_index_to_thread_offset_map(
+				emon_buffer_driver_helper)[this_cpu],
+			ECB_entries_core_event_id(pecb, i));
+
+		buffer[j] = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u, event_id=%u", j,
+				  buffer[j], this_cpu,
+				  ECB_entries_core_event_id(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void silvermont_Check_Overflow(masks)
+ *
+ * @param    masks    the mask structure to populate
+ *
+ * @return   None     No return needed
+ *
+ * @brief  Called by the data processing method to figure out which registers have overflowed.
+ *
+ */
+static void silvermont_Check_Overflow(DRV_MASKS masks)
+{
+	U32 index;
+	U64 overflow_status = 0;
+	U32 this_cpu;
+	BUFFER_DESC bd;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+	DEV_CONFIG pcfg;
+	DISPATCH dispatch;
+	U64 overflow_status_clr = 0;
+	DRV_EVENT_MASK_NODE event_flag;
+
+	SEP_DRV_LOG_TRACE_IN("Masks: %p.", masks);
+
+	this_cpu = CONTROL_THIS_CPU();
+	bd = &cpu_buf[this_cpu];
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	// initialize masks
+	DRV_MASKS_masks_num(masks) = 0;
+
+	overflow_status = SYS_Read_MSR(ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_STATUS_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_STATUS)));
+
+	if (DEV_CONFIG_pebs_mode(pcfg)) {
+		overflow_status = PEBS_Overflowed(this_cpu, overflow_status, 0);
+	}
+	overflow_status_clr = overflow_status;
+
+	if (dispatch->check_overflow_gp_errata) {
+		overflow_status = dispatch->check_overflow_gp_errata(
+			pecb, &overflow_status_clr);
+	}
+	SEP_DRV_LOG_TRACE("Overflow: cpu: %d, status 0x%llx.", this_cpu,
+			  overflow_status);
+	index = 0;
+	BUFFER_DESC_sample_count(bd) = 0;
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_fixed_reg_get(pecb, i)) {
+			index = i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_FIXED) +
+				0x20;
+			if (dispatch->check_overflow_errata) {
+				overflow_status =
+					dispatch->check_overflow_errata(
+						pecb, i, overflow_status);
+			}
+		} else if (ECB_entries_is_gp_reg_get(pecb, i)) {
+			index = i - ECB_operations_register_start(
+					    pecb, PMU_OPERATION_DATA_GP);
+		} else {
+			continue;
+		}
+		if (overflow_status & ((U64)1 << index)) {
+			SEP_DRV_LOG_TRACE("Overflow: cpu: %d, index %d.",
+					  this_cpu, index);
+			SEP_DRV_LOG_TRACE(
+				"Register 0x%x --- val 0%llx.",
+				ECB_entries_reg_id(pecb, i),
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i)));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      ECB_entries_reg_value(pecb, i));
+
+			if (DRV_CONFIG_enable_cp_mode(drv_cfg)) {
+				/* Increment the interrupt count. */
+				if (interrupt_counts) {
+					interrupt_counts
+						[this_cpu *
+							 DRV_CONFIG_num_events(
+								 drv_cfg) +
+						 ECB_entries_event_id_index(
+							 pecb, i)] += 1;
+				}
+			}
+
+			DRV_EVENT_MASK_bitFields1(&event_flag) = (U8)0;
+			if (ECB_entries_fixed_reg_get(pecb, i)) {
+				CPU_STATE_p_state_counting(pcpu) = 1;
+			}
+			if (ECB_entries_precise_get(pecb, i)) {
+				DRV_EVENT_MASK_precise(&event_flag) = 1;
+			}
+			if (ECB_entries_lbr_value_get(pecb, i)) {
+				DRV_EVENT_MASK_lbr_capture(&event_flag) = 1;
+			}
+			if (ECB_entries_uncore_get(pecb, i)) {
+				DRV_EVENT_MASK_uncore_capture(&event_flag) = 1;
+			}
+
+			if (DRV_MASKS_masks_num(masks) < MAX_OVERFLOW_EVENTS) {
+				DRV_EVENT_MASK_bitFields1(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					DRV_EVENT_MASK_bitFields1(&event_flag);
+				DRV_EVENT_MASK_event_idx(
+					DRV_MASKS_eventmasks(masks) +
+					DRV_MASKS_masks_num(masks)) =
+					ECB_entries_event_id_index(pecb, i);
+				DRV_MASKS_masks_num(masks)++;
+			} else {
+				SEP_DRV_LOG_ERROR(
+					"The array for event masks is full.");
+			}
+
+			SEP_DRV_LOG_TRACE("Overflow -- 0x%llx, index 0x%llx.",
+					  overflow_status, (U64)1 << index);
+			SEP_DRV_LOG_TRACE("Slot# %d, reg_id 0x%x, index %d.", i,
+					  ECB_entries_reg_id(pecb, i), index);
+			if (ECB_entries_event_id_index(pecb, i) ==
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				CPU_STATE_trigger_count(pcpu)--;
+			}
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	CPU_STATE_reset_mask(pcpu) = overflow_status_clr;
+	// Reinitialize the global overflow control register
+	SYS_Write_MSR(IA32_PERF_GLOBAL_OVF_CTRL, overflow_status_clr);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn silvermont_Swap_Group(restart)
+ *
+ * @param    restart    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Perform the mechanics of swapping the event groups for event mux operations
+ *
+ * <I>Special Notes</I>
+ *         Swap function for event multiplexing.
+ *         Freeze the counting.
+ *         Swap the groups.
+ *         Enable the counting.
+ *         Reset the event trigger count
+ *
+ */
+static VOID silvermont_Swap_Group(DRV_BOOL restart)
+{
+	U32 index;
+	U32 next_group;
+	U32 st_index;
+	U32 this_cpu = CONTROL_THIS_CPU();
+	CPU_STATE pcpu = &pcb[this_cpu];
+	U32 dev_idx;
+	DISPATCH dispatch;
+	EVENT_CONFIG ec;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy restart: %u.", restart);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	dispatch = LWPMU_DEVICE_dispatch(&devices[dev_idx]);
+	ec = LWPMU_DEVICE_ec(&devices[dev_idx]);
+	st_index =
+		CPU_STATE_current_group(pcpu) * EVENT_CONFIG_max_gp_events(ec);
+	next_group = (CPU_STATE_current_group(pcpu) + 1);
+
+	if (next_group >= EVENT_CONFIG_num_groups(ec)) {
+		next_group = 0;
+	}
+
+	SEP_DRV_LOG_TRACE("Current group : 0x%x.",
+			  CPU_STATE_current_group(pcpu));
+	SEP_DRV_LOG_TRACE("Next group : 0x%x.", next_group);
+
+	// Save the counters for the current group
+	if (!DRV_CONFIG_event_based_counts(drv_cfg)) {
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			CPU_STATE_em_tables(pcpu)[index] =
+				SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SEP_DRV_LOG_TRACE("Saved value for reg 0x%x : 0x%llx.",
+					  ECB_entries_reg_id(pecb, i),
+					  CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	CPU_STATE_current_group(pcpu) = next_group;
+
+	if (dispatch->hw_errata) {
+		dispatch->hw_errata();
+	}
+
+	// First write the GP control registers (eventsel)
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_CTRL_GP)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DRV_CONFIG_event_based_counts(drv_cfg)) {
+		// In EBC mode, reset the counts for all events except for trigger event
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+		{
+			if (ECB_entries_event_id_index(pecb, i) !=
+			    CPU_STATE_trigger_event_num(pcpu)) {
+				SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+			}
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	} else {
+		// Then write the gp count registers
+		st_index = CPU_STATE_current_group(pcpu) *
+			   EVENT_CONFIG_max_gp_events(ec);
+		FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_GP)
+		{
+			index = st_index + i -
+				ECB_operations_register_start(
+					pecb, PMU_OPERATION_DATA_GP);
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+				      CPU_STATE_em_tables(pcpu)[index]);
+			SEP_DRV_LOG_TRACE(
+				"Restore value for reg 0x%x : 0x%llx.",
+				ECB_entries_reg_id(pecb, i),
+				CPU_STATE_em_tables(pcpu)[index]);
+		}
+		END_FOR_EACH_REG_CORE_OPERATION;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_OCR)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, i),
+			      ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	/*
+	 *  reset the em factor when a group is swapped
+	 */
+	CPU_STATE_trigger_count(pcpu) = EVENT_CONFIG_em_factor(ec);
+
+	/*
+	 * The enable routine needs to rewrite the control registers
+	 */
+	CPU_STATE_reset_mask(pcpu) = 0LL;
+	CPU_STATE_group_swap(pcpu) = 1;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn silvermont_Initialize(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Initialize the PMU setting up for collection
+ *
+ * <I>Special Notes</I>
+ *         Saves the relevant PMU state (minimal set of MSRs required
+ *         to avoid conflicts with other Linux tools, such as Oprofile).
+ *         This function should be called in parallel across all CPUs
+ *         prior to the start of sampling, before PMU state is changed.
+ *
+ */
+static VOID silvermont_Initialize(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	ECB pecb;
+	U32 dev_idx;
+	U32 cur_grp;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pcb).");
+		return;
+	}
+
+	pcpu = &pcb[this_cpu];
+	cur_grp = CPU_STATE_current_group(pcpu);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	CPU_STATE_pmu_state(pcpu) = pmu_state + (this_cpu * 3);
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to save PMU state on CPU %d!", this_cpu);
+		return;
+	}
+
+	restore_reg_addr[0] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, DEBUG_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[1] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, GLOBAL_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+	restore_reg_addr[2] = ECB_entries_reg_id(
+		pecb, ECB_SECTION_REG_INDEX(pecb, FIXED_CTRL_REG_INDEX,
+					    PMU_OPERATION_GLOBAL_REGS));
+
+	// save the original PMU state on this CPU (NOTE: must only be called ONCE per collection)
+	CPU_STATE_pmu_state(pcpu)[0] = SYS_Read_MSR(restore_reg_addr[0]);
+	CPU_STATE_pmu_state(pcpu)[1] = SYS_Read_MSR(restore_reg_addr[1]);
+	CPU_STATE_pmu_state(pcpu)[2] = SYS_Read_MSR(restore_reg_addr[2]);
+
+	SEP_DRV_LOG_TRACE("Saving PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[0]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[1]);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0x%llx.",
+			  CPU_STATE_pmu_state(pcpu)[2]);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn silvermont_Destroy(params)
+ *
+ * @param    params    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Reset the PMU setting up after collection
+ *
+ * <I>Special Notes</I>
+ *         Restores the previously saved PMU state done in core2_Initialize.
+ *         This function should be called in parallel across all CPUs
+ *         after sampling collection ends/terminates.
+ *
+ */
+static VOID silvermont_Destroy(VOID *param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	if (pcb == NULL) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pcb).");
+		return;
+	}
+
+	preempt_disable();
+	this_cpu = CONTROL_THIS_CPU();
+	preempt_enable();
+	pcpu = &pcb[this_cpu];
+
+	if (CPU_STATE_pmu_state(pcpu) == NULL) {
+		SEP_DRV_LOG_WARNING_TRACE_OUT(
+			"Unable to restore PMU state on CPU %d!", this_cpu);
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("Clearing PMU state on CPU %d:", this_cpu);
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_DEBUG_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_PERF_GLOBAL_CTRL)=0x0.");
+	SEP_DRV_LOG_TRACE("    msr_val(IA32_FIXED_CTRL)=0.");
+
+	SYS_Write_MSR(restore_reg_addr[0], 0);
+	SYS_Write_MSR(restore_reg_addr[1], 0);
+	SYS_Write_MSR(restore_reg_addr[2], 0);
+
+	CPU_STATE_pmu_state(pcpu) = NULL;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * @fn silvermont_Read_LBRs(buffer)
+ *
+ * @param   IN buffer - pointer to the buffer to write the data into
+ * @return  None
+ *
+ * @brief   Read all the LBR registers into the buffer provided and return
+ *
+ */
+static U64 silvermont_Read_LBRs(VOID *buffer, PVOID data)
+{
+	U32 i, count = 0;
+	U64 *lbr_buf = NULL;
+	U64 value;
+	U64 tos_ip_addr = 0;
+	U64 tos_ptr = 0;
+	SADDR saddr;
+	U32 this_cpu;
+	U32 dev_idx;
+	LBR lbr;
+	DEV_CONFIG pcfg;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	this_cpu = CONTROL_THIS_CPU();
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	lbr = LWPMU_DEVICE_lbr(&devices[dev_idx]);
+
+	if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+		lbr_buf = (U64 *)buffer;
+	}
+
+	for (i = 0; i < LBR_num_entries(lbr); i++) {
+		value = SYS_Read_MSR(LBR_entries_reg_id(lbr, i));
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			*lbr_buf = value;
+		}
+		SEP_DRV_LOG_TRACE("LBR %u, 0x%llx.", i, value);
+		if (i == 0) {
+			tos_ptr = value;
+		} else {
+			if (LBR_entries_etype(lbr, i) == LBR_ENTRY_FROM_IP) {
+				if (tos_ptr == count) {
+					SADDR_addr(saddr) =
+						value & SILVERMONT_LBR_BITMASK;
+					tos_ip_addr = (U64)SADDR_addr(
+						saddr); // Add signed extension
+					SEP_DRV_LOG_TRACE(
+						"Tos_ip_addr %llu, 0x%llx.",
+						tos_ptr, value);
+				}
+				count++;
+			}
+		}
+		if (buffer && DEV_CONFIG_store_lbrs(pcfg)) {
+			lbr_buf++;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llu.", tos_ip_addr);
+	return tos_ip_addr;
+}
+
+static VOID silvermont_Clean_Up(VOID *param)
+{
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_ALL_REG)
+	{
+		if (ECB_entries_clean_up_get(pecb, i)) {
+			SEP_DRV_LOG_TRACE("Clean up set --- RegId --- %x.",
+					  ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn silvermont_Read_Counts(param, id)
+ *
+ * @param    param    The read thread node to process
+ * @param    id       The event id for the which the sample is generated
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read CPU event based counts data and store into the buffer param;
+ *           For the case of the trigger event, store the SAV value.
+ */
+static VOID silvermont_Read_Counts(PVOID param, U32 id)
+{
+	U64 *data;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 dev_idx;
+	DEV_CONFIG pcfg;
+	U32 event_id = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_idx = core_to_dev_map[this_cpu];
+	pcfg = LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+
+	if (DEV_CONFIG_ebc_group_id_offset(pcfg)) {
+		// Write GroupID
+		data = (U64 *)((S8 *)param +
+			       DEV_CONFIG_ebc_group_id_offset(pcfg));
+		*data = CPU_STATE_current_group(pcpu) + 1;
+	}
+
+	FOR_EACH_REG_CORE_OPERATION(pecb, i, PMU_OPERATION_DATA_ALL)
+	{
+		if (ECB_entries_counter_event_offset(pecb, i) == 0) {
+			continue;
+		}
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, i));
+		event_id = ECB_entries_event_id_index(pecb, i);
+		if (event_id == id) {
+			*data = ~(ECB_entries_reg_value(pecb, i) - 1) &
+				ECB_entries_max_bits(pecb, i);
+		} else {
+			*data = SYS_Read_MSR(ECB_entries_reg_id(pecb, i));
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_CORE_OPERATION;
+
+	if (DRV_CONFIG_enable_p_state(drv_cfg)) {
+		CPU_STATE_p_state_counting(pcpu) = 0;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          U64 silvermont_Platform_Info
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param       void
+ *
+ * @return      value read from the register
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static void silvermont_Platform_Info(PVOID data)
+{
+	U64 index = 0;
+	DRV_PLATFORM_INFO platform_data = (DRV_PLATFORM_INFO)data;
+	U64 value = 0;
+	U64 clock_value = 0;
+	U64 energy_multiplier;
+
+	SEP_DRV_LOG_TRACE_IN("Data: %p.", data);
+
+	if (!platform_data) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!platform_data).");
+		return;
+	}
+
+#define IA32_MSR_PLATFORM_INFO 0xCE
+	value = SYS_Read_MSR(IA32_MSR_PLATFORM_INFO);
+
+#define IA32_MSR_PSB_CLOCK_STS 0xCD
+#define FREQ_MASK_BITS 0x03
+
+	clock_value = SYS_Read_MSR(IA32_MSR_PSB_CLOCK_STS);
+	index = clock_value & FREQ_MASK_BITS;
+	DRV_PLATFORM_INFO_info(platform_data) = value;
+	DRV_PLATFORM_INFO_ddr_freq_index(platform_data) = index;
+
+#undef IA32_MSR_PLATFORM_INFO
+#undef IA32_MSR_PSB_CLOCK_STS
+#undef FREQ_MASK_BITS
+	energy_multiplier = SYS_Read_MSR(MSR_ENERGY_MULTIPLIER);
+	SEP_DRV_LOG_TRACE("MSR_ENERGY_MULTIPLIER: %llx.", energy_multiplier);
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) =
+		(U32)(energy_multiplier & 0x00001F00) >> 8;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          VOID knights_Platform_Info
+ *
+ * @brief       Reads the MSR_PLATFORM_INFO register if present
+ *
+ * @param       void
+ *
+ * @return      value read from the register
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+static VOID knights_Platform_Info(PVOID data)
+{
+	DRV_PLATFORM_INFO platform_data = (DRV_PLATFORM_INFO)data;
+	U64 value = 0;
+	U64 energy_multiplier;
+
+	SEP_DRV_LOG_TRACE_IN("Data: %p.", data);
+
+	if (!platform_data) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!platform_data).");
+		return;
+	}
+
+#define IA32_MSR_PLATFORM_INFO 0xCE
+	value = SYS_Read_MSR(IA32_MSR_PLATFORM_INFO);
+
+	DRV_PLATFORM_INFO_info(platform_data) = value;
+	DRV_PLATFORM_INFO_ddr_freq_index(platform_data) = 0;
+	energy_multiplier = SYS_Read_MSR(MSR_ENERGY_MULTIPLIER);
+	SEP_DRV_LOG_TRACE("MSR_ENERGY_MULTIPLIER: %llx.", energy_multiplier);
+	DRV_PLATFORM_INFO_energy_multiplier(platform_data) =
+		(U32)(energy_multiplier & 0x00001F00) >> 8;
+}
+
+/*
+ * Initialize the dispatch table
+ */
+DISPATCH_NODE silvermont_dispatch = { .init = silvermont_Initialize,
+				      .fini = silvermont_Destroy,
+				      .write = silvermont_Write_PMU,
+				      .freeze = silvermont_Disable_PMU,
+				      .restart = silvermont_Enable_PMU,
+				      .read_data = silvermont_Read_PMU_Data,
+				      .check_overflow =
+					      silvermont_Check_Overflow,
+				      .swap_group = silvermont_Swap_Group,
+				      .read_lbrs = silvermont_Read_LBRs,
+				      .cleanup = silvermont_Clean_Up,
+				      .hw_errata = NULL,
+				      .read_power = NULL,
+				      .check_overflow_errata = NULL,
+				      .read_counts = silvermont_Read_Counts,
+				      .check_overflow_gp_errata = NULL,
+				      .read_ro = NULL,
+				      .platform_info = silvermont_Platform_Info,
+				      .trigger_read = NULL,
+				      .scan_for_uncore = NULL,
+				      .read_metrics = NULL };
+
+DISPATCH_NODE knights_dispatch = { .init = silvermont_Initialize,
+				   .fini = silvermont_Destroy,
+				   .write = silvermont_Write_PMU,
+				   .freeze = silvermont_Disable_PMU,
+				   .restart = silvermont_Enable_PMU,
+				   .read_data = silvermont_Read_PMU_Data,
+				   .check_overflow = silvermont_Check_Overflow,
+				   .swap_group = silvermont_Swap_Group,
+				   .read_lbrs = silvermont_Read_LBRs,
+				   .cleanup = silvermont_Clean_Up,
+				   .hw_errata = NULL,
+				   .read_power = NULL,
+				   .check_overflow_errata = NULL,
+				   .read_counts = silvermont_Read_Counts,
+				   .check_overflow_gp_errata = NULL,
+				   .read_ro = NULL,
+				   .platform_info = knights_Platform_Info,
+				   .trigger_read = NULL,
+				   .scan_for_uncore = NULL,
+				   .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/sys32.S b/drivers/platform/x86/sepdk/sep/sys32.S
new file mode 100755
index 000000000000..eb4c12304cdc
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/sys32.S
@@ -0,0 +1,200 @@
+#     Copyright(C) 2002-2018 Intel Corporation.  All Rights Reserved.
+#
+#     This file is part of SEP Development Kit
+#
+#     SEP Development Kit is free software; you can redistribute it
+#     and/or modify it under the terms of the GNU General Public License
+#     version 2 as published by the Free Software Foundation.
+#
+#     SEP Development Kit is distributed in the hope that it will be useful,
+#     but WITHOUT ANY WARRANTY; without even the implied warranty of
+#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#     GNU General Public License for more details.
+#
+#     As a special exception, you may use this file as part of a free software
+#     library without restriction.  Specifically, if other files instantiate
+#     templates or use macros or inline functions from this file, or you compile
+#     this file and link it with other files to produce an executable, this
+#     file does not by itself cause the resulting executable to be covered by
+#     the GNU General Public License.  This exception does not however
+#     invalidate any other reasons why the executable file might be covered by
+#     the GNU General Public License.
+
+
+#include <linux/version.h>
+#include <asm/segment.h>
+
+#if LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 20)
+#define USE_KERNEL_PERCPU_SEGMENT_GS
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 21) && LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 29)
+#define USE_KERNEL_PERCPU_SEGMENT_FS
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 30)
+#define USE_KERNEL_PERCPU_SEGMENT_FS
+#define USE_KERNEL_PERCPU_SEGMENT_GS
+#endif
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 20)
+#if !defined(__KERNEL_PERCPU)
+#define __KERNEL_PERCPU __KERNEL_PDA
+#endif
+#endif
+
+#if defined(USE_KERNEL_PERCPU_SEGMENT_GS)
+#if defined(__KERNEL_STACK_CANARY)
+#define SEP_GS_SEG_VALUE __KERNEL_STACK_CANARY
+#else
+#define SEP_GS_SEG_VALUE __KERNEL_PERCPU
+#endif
+#endif
+
+#***********************************************************************
+#
+#    SYS_Get_IDT_Base_HWR
+#            Get the IDT Desc address
+#
+#    Entry: none
+#
+#    Exit:  base address in eax
+#
+# void SYS_Get_IDT_Base_HWR(U64 *pIdtDesc);
+#
+#***********************************************************************
+		.text
+		.align  4
+	.global SYS_IO_Delay
+SYS_IO_Delay:
+	ret
+
+		.global SYS_Get_IDT_Base_HWR
+SYS_Get_IDT_Base_HWR:
+		subl    $8, %esp
+		sidt    2(%esp)
+	movl    4(%esp), %eax
+	addl    $8, %esp
+		ret
+	.global SYS_Get_cs
+SYS_Get_cs:
+	mov	%cs, %ax
+	andl	$0x0000ffff, %eax
+	ret
+
+	.global SYS_Get_TSC
+SYS_Get_TSC:
+	rdtsc
+	ret
+		.text
+		.align  4
+		.global SYS_Perfvec_Handler
+SYS_Perfvec_Handler:
+										# This is the same as KERNEL's
+		pushl   %eax                    # Filler for Error Code
+
+		cld
+		pushl   %es                     # SAVE_ALL macro to access pt_regs
+		pushl   %ds                     # inside our ISR.
+#if defined(USE_KERNEL_PERCPU_SEGMENT_GS)
+		pushl   %gs
+#endif
+#if defined(USE_KERNEL_PERCPU_SEGMENT_FS)
+		pushl   %fs
+#endif
+		pushl   %eax
+		pushl   %ebp
+		pushl   %edi
+		pushl   %esi
+		pushl   %edx
+		pushl   %ecx
+		pushl   %ebx
+
+		movl    $(__KERNEL_DS), %edx    # Use KERNEL DS selector
+		movl    %edx, %ds                # Make sure we set Kernel
+		movl    %edx, %es                # DS into local DS and ES
+
+#if defined(USE_KERNEL_PERCPU_SEGMENT_GS)
+		movl    $(SEP_GS_SEG_VALUE), %edx    # Use kernel percpu segment
+		movl    %edx, %gs                    # ... and load it into %gs
+#endif
+#if defined(USE_KERNEL_PERCPU_SEGMENT_FS)
+		movl    $(__KERNEL_PERCPU), %edx    # Use kernel percpu segment
+		movl    %edx, %fs                    # ... and load it into %fs
+#endif
+
+	movl	%esp, %ebx		# get ready to put *pt_regs on stack
+
+		pushl   %ebx			# put *pt_regs on the stack
+		call PMI_Interrupt_Handler
+		addl $0x4, %esp			# pop to nowhere...
+
+	   pop     %ebx                    # restore register set
+		pop     %ecx
+		pop     %edx
+		pop     %esi
+		pop     %edi
+		pop     %ebp
+		pop     %eax
+#if defined(USE_KERNEL_PERCPU_SEGMENT_FS)
+		pop     %fs
+#endif
+#if defined(USE_KERNEL_PERCPU_SEGMENT_GS)
+		pop     %gs
+#endif
+		pop     %ds
+		pop     %es
+		pop     %eax
+
+		iret
+# ----------------------------------------------------------------------------
+# name:         get_CSD
+#
+# description:  get the CS descriptor
+#
+# input:        code segment selector
+#
+# output:       code segment descriptor
+# ----------------------------------------------------------------------------
+		.text
+		.align  4
+		.globl  SYS_Get_CSD
+
+SYS_Get_CSD:
+		pushl   %ebp
+		movl    %esp, %ebp
+		pushal                                  # save regs
+
+		subl    $8, %esp
+		xorl    %eax, %eax
+		movw    8(%ebp), %ax                    # eax.lo = cs
+		sgdt    (%esp)                          # store gdt reg
+		leal    (%esp), %ebx                    # ebx = gdt reg ptr
+		movl    2(%ebx), %ecx                   # ecx = gdt base
+		xorl    %edx, %edx
+		movw    %ax, %dx
+		andl    $4, %edx
+		cmpl    $0, %edx                        # test ti. GDT?
+		jz      .bsr_10                         # ..yes
+		xorl    %edx, %edx
+		sldt    %dx                             # ..no dx=ldtsel
+		andb    $0xf8, %dl                      # clear ti, rpl
+		addl    2(%ebx), %edx                   # add gdt base
+		movb    7(%edx), %cl                    # ecx = ldt base
+		shll    $8, %ecx                        # ..
+		movb    4(%edx), %cl                    # ..
+		shll    $16, %ecx                       # ..
+		movw    2(%edx), %cx                    # ..
+.bsr_10:
+		andb    $0xf8, %al                      # clear ti & rpl
+		addl    %eax, %ecx                      # add to gdt/ldt
+		movl    (%ecx), %eax                    # copy code seg
+		movl    12(%ebp), %edx                  # ..descriptor (csdlo)
+		movl    %eax, (%edx)                    # ..descriptor (csdlo)
+		movl    4(%ecx), %eax                   # ..from gdt or
+		movl    16(%ebp), %edx                  # ..ldt to sample (csdhi)
+		movl    %eax, (%edx)                    # ..ldt to sample (csdhi)
+		addl    $8, %esp
+		popal                                   # restore regs
+		leave
+		ret
diff --git a/drivers/platform/x86/sepdk/sep/sys64.S b/drivers/platform/x86/sepdk/sep/sys64.S
new file mode 100755
index 000000000000..1deb8db3cdb7
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/sys64.S
@@ -0,0 +1,140 @@
+#     Copyright(C) 2002-2018 Intel Corporation.  All Rights Reserved.
+#
+#     This file is part of SEP Development Kit
+#
+#     SEP Development Kit is free software; you can redistribute it
+#     and/or modify it under the terms of the GNU General Public License
+#     version 2 as published by the Free Software Foundation.
+#
+#     SEP Development Kit is distributed in the hope that it will be useful,
+#     but WITHOUT ANY WARRANTY; without even the implied warranty of
+#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#     GNU General Public License for more details.
+#
+#     As a special exception, you may use this file as part of a free software
+#     library without restriction.  Specifically, if other files instantiate
+#     templates or use macros or inline functions from this file, or you compile
+#     this file and link it with other files to produce an executable, this
+#     file does not by itself cause the resulting executable to be covered by
+#     the GNU General Public License.  This exception does not however
+#     invalidate any other reasons why the executable file might be covered by
+#     the GNU General Public License.
+
+#include "inc/asm_helper.h"
+#include <asm/msr.h>
+
+.text
+
+#***********************************************************************
+#
+#    SYS_Get_IDT_Base
+#            Get the IDT Desc address
+#
+#    Entry:  pointer to location to store idt Desc
+#
+#    Exit:  none
+#
+# void SYS_Get_IDT_Base(U64 *pIdtDesc);
+#
+#***********************************************************************
+	.global SYS_Get_IDT_Base
+SYS_Get_IDT_Base:
+	SIDT (%rdi)
+	ret
+
+#***********************************************************************
+#
+#    SYS_Get_GDT_Base
+#            Get the GDT Desc address
+#
+#    Entry:  pointer to location to store gdt Desc
+#
+#    Exit:  none
+#
+# void SYS_Get_GDT_Base(U64 *pGdtDesc);
+#
+#***********************************************************************
+	.global SYS_Get_GDT_Base
+SYS_Get_GDT_Base:
+	SGDT (%rdi)
+	ret
+
+#***********************************************************************
+#
+#    SYS_Get_TSC
+#            Get the current TSC
+#
+#    Entry:  pointer to location to store gdt Desc
+#
+#    Exit:  none
+#
+# void SYS_Get_TSC(U64 *tsc);
+#
+#***********************************************************************
+#        .global SYS_Get_TSC
+#SYS_Get_TSC:
+#        rdtsc
+#        ret
+
+#***********************************************************************
+#
+#    SYS_IO_Delay
+#            Add a short delay to the instruction stream
+#
+#    Entry:  none
+#
+#    Exit:  none
+#
+# void SYS_IO_Delay(void);
+#
+#***********************************************************************
+	.global SYS_IO_Delay
+SYS_IO_Delay:
+	ret
+
+# ----------------------------------------------------------------------------
+# name:         SYS_PerfVec_Handler
+#
+# description:  ISR entry for local APIC PERF interrupt vector
+#
+# Input:        n/a
+#
+# Output:       n/a
+# ----------------------------------------------------------------------------
+
+	.global SYS_Perfvec_Handler
+SYS_Perfvec_Handler:
+	CFI_STARTPROC
+		pushq %rax          # fake an error code...
+		cld                 # cause the kernel likes it this way...
+
+		SAVE_ALL            # Save the world!
+
+		movl  $MSR_GS_BASE, %ecx     # for the moment, do the safe swapgs check
+		rdmsr
+		xorl  %ebx, %ebx             # assume no swapgs (ebx == 0)
+		testl %edx, %edx
+		js    1f
+		swapgs
+		movl  $1, %ebx               # ebx == 1 means we did a swapgs
+1:		movq %rsp, %rdi              # pt_regs is the first argument
+
+		#
+		# ebx is zero if no swap, one if swap
+		# ebx is preserved in C calling convention...
+		#
+		# NOTE: the C code is responsible for ACK'ing the APIC!!!
+		#
+		call PMI_Interrupt_Handler
+
+		#
+		# Don't want an interrupt while we are doing the swapgs stuff
+		#
+		cli
+		testl %ebx, %ebx
+		jz 2f
+		swapgs
+2:		RESTORE_ALL
+		popq    %rax
+		iretq
+	CFI_ENDPROC
diff --git a/drivers/platform/x86/sepdk/sep/sys_info.c b/drivers/platform/x86/sepdk/sep/sys_info.c
new file mode 100755
index 000000000000..b72ce2894c82
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/sys_info.c
@@ -0,0 +1,1111 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/mm.h>
+#include <asm/apic.h>
+
+#include "lwpmudrv_types.h"
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "utility.h"
+#include "apic.h"
+#include "sys_info.h"
+
+#define VTSA_CPUID VTSA_CPUID_X86
+
+extern U64 total_ram;
+static IOCTL_SYS_INFO *ioctl_sys_info;
+static size_t ioctl_sys_info_size;
+static U32 *cpuid_entry_count;
+static U32 *cpuid_total_count;
+U32 *cpu_built_sysinfo;
+
+static U32 cpu_threads_per_core;
+static VOID *gen_per_cpu_ptr;
+
+#define VTSA_NA64 ((U64)-1)
+#define VTSA_NA32 ((U32)-1)
+#define VTSA_NA ((U32)-1)
+
+#define SYS_INFO_NUM_SETS(rcx) ((rcx) + 1)
+#define SYS_INFO_LINE_SIZE(rbx) (((rbx)&0xfff) + 1)
+#define SYS_INFO_LINE_PARTITIONS(rbx) ((((rbx) >> 12) & 0x3ff) + 1)
+#define SYS_INFO_NUM_WAYS(rbx) ((((rbx) >> 22) & 0x3ff) + 1)
+
+#define SYS_INFO_CACHE_SIZE(rcx, rbx)                                          \
+	(SYS_INFO_NUM_SETS((rcx)) * SYS_INFO_LINE_SIZE((rbx)) *                \
+	 SYS_INFO_LINE_PARTITIONS((rbx)) * SYS_INFO_NUM_WAYS((rbx)))
+
+#define MSR_FB_PCARD_ID_FUSE 0x17 // platform id fuses MSR
+
+#define LOW_PART(x) (x & 0xFFFFFFFF)
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static U64 sys_info_nbits(number)
+ *
+ * @param number  - the number to check
+ * @return the number of bit.
+ *
+ * @brief  This routine gets the number of useful bits with the given number.
+ *         It will round the number up to power of 2, and adjust to 0 based number.
+ *         sys_info_nbits(0x3) = 2
+ *         sys_info_nbits(0x4) = 2
+ *
+ */
+static U64 sys_info_nbits(U64 number)
+{
+	U64 i;
+
+	SEP_DRV_LOG_TRACE_IN("Number: %llx.",
+			     number); // is %llu portable in the kernel?
+
+	if (number < 2) {
+		SEP_DRV_LOG_TRACE_OUT("Res: %u. (early exit)", (U32)number);
+		return number;
+	}
+
+	// adjust to 0 based number, and round up to power of 2
+	number--;
+	for (i = 0; number > 0; i++) {
+		number >>= 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", (U32)i);
+	return i;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static U64 sys_info_bitmask(nbits)
+ *
+ * @param number  - the number of bits
+ * @return  the bit mask for the nbits number
+ *
+ * @brief  This routine gets the bitmask for the nbits number.
+ */
+static U64 sys_info_bitmask(U64 nbits)
+{
+	U64 mask = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Nbits: %u.", (U32)nbits);
+
+	mask = (U64)1 << nbits;
+	mask--;
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %llx.", mask);
+
+	return mask;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void sys_info_Get_Num_Cpuid_Funcs(basic_funcs, basic_4_funcs, extended_funcs)
+ *
+ * @param basic_functions    - pointer to the number of basic functions
+ * @param basic_4_funcs      - pointer to the basic 4 functions
+ * @param extended_funcs     - pointer to the number of extended functions
+ * @return total number of cpuid functions
+ *
+ * @brief  This routine gets the number of basic and extended cpuid functions.
+ *
+ */
+static U32 sys_info_Get_Num_Cpuid_Funcs(OUT U32 *basic_funcs,
+					OUT U32 *basic_4_funcs,
+					OUT U32 *extended_funcs)
+{
+	U64 num_basic_funcs = 0x0LL;
+	U64 num_basic_4_funcs = 0x0LL;
+	U64 num_extended_funcs = 0x0LL;
+	U64 rax;
+	U64 rbx;
+	U64 rcx;
+	U64 rdx;
+	U64 i;
+	U32 res;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	UTILITY_Read_Cpuid(0, &num_basic_funcs, &rbx, &rcx, &rdx);
+	UTILITY_Read_Cpuid(0x80000000, &num_extended_funcs, &rbx, &rcx, &rdx);
+
+	if (num_extended_funcs & 0x80000000) {
+		num_extended_funcs -= 0x80000000;
+	}
+
+	//
+	// make sure num_extended_funcs is not bogus
+	//
+	if (num_extended_funcs > 0x1000) {
+		num_extended_funcs = 0;
+	}
+
+	//
+	// if number of basic funcs is greater than 4, figure out how many
+	// time we should call CPUID with eax = 0x4.
+	//
+	num_basic_4_funcs = 0;
+	if (num_basic_funcs >= 4) {
+		for (i = 0, rax = (U64)-1; (rax & 0x1f) != 0; i++) {
+			rcx = i;
+			UTILITY_Read_Cpuid(4, &rax, &rbx, &rcx, &rdx);
+		}
+		num_basic_4_funcs = i - 1;
+	}
+	if (num_basic_funcs >= 0xb) {
+		i = 0;
+		do {
+			rcx = i;
+			UTILITY_Read_Cpuid(0xb, &rax, &rbx, &rcx, &rdx);
+			i++;
+		} while (!(LOW_PART(rax) == 0 && LOW_PART(rbx) == 0));
+		num_basic_4_funcs += i;
+	}
+	SEP_DRV_LOG_TRACE("Num_basic_4_funcs = %llx.", num_basic_4_funcs);
+
+	//
+	// adjust number to include 0 and 0x80000000 functions.
+	//
+	num_basic_funcs++;
+	num_extended_funcs++;
+
+	SEP_DRV_LOG_TRACE("num_basic_funcs: %llx, num_extended_funcs: %llx.",
+			  num_basic_funcs, num_extended_funcs);
+
+	//
+	// fill-in the parameter for the caller
+	//
+	if (basic_funcs != NULL) {
+		*basic_funcs = (U32)num_basic_funcs;
+	}
+	if (basic_4_funcs != NULL) {
+		*basic_4_funcs = (U32)num_basic_4_funcs;
+	}
+	if (extended_funcs != NULL) {
+		*extended_funcs = (U32)num_extended_funcs;
+	}
+
+	res = (U32)(num_basic_funcs + num_basic_4_funcs + num_extended_funcs);
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void sys_info_Get_Cpuid_Entry_Cpunt(buffer)
+ *
+ * @param  buffer    - pointer to the buffer to hold the info
+ * @return None
+ *
+ * @brief  Service Routine to query the CPU for the number of entries needed
+ *
+ */
+static VOID sys_info_Get_Cpuid_Entry_Count(PVOID param)
+{
+	S32 current_processor;
+	U32 *current_cpu_buffer;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	if (param == NULL) {
+		current_processor = CONTROL_THIS_CPU();
+	} else {
+		current_processor = *(S32 *)param;
+	}
+	SEP_DRV_LOG_TRACE("Beginning on CPU %u.", current_processor);
+
+	current_cpu_buffer = (U32 *)((U8 *)cpuid_entry_count +
+				     current_processor * sizeof(U32));
+
+#if defined(ALLOW_ASSERT)
+	ASSERT(((U8 *)current_cpu_buffer + sizeof(U32)) <=
+	       ((U8 *)current_cpu_buffer +
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32)));
+#endif
+	*current_cpu_buffer = sys_info_Get_Num_Cpuid_Funcs(NULL, NULL, NULL);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static U32 sys_info_Get_Cpuid_Buffer_Size(cpuid_entries)
+ *
+ * @param    cpuid_entries   - number of cpuid entries
+ * @return   size of buffer needed in bytes
+ *
+ * @brief  This routine returns number of bytes needed to hold the CPU_CS_INFO
+ * @brief  structure.
+ *
+ */
+static U32 sys_info_Get_Cpuid_Buffer_Size(U32 cpuid_entries)
+{
+	U32 cpuid_size;
+	U32 buffer_size;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cpuid_size = sizeof(VTSA_CPUID);
+
+	buffer_size =
+		sizeof(IOCTL_SYS_INFO) + sizeof(VTSA_GEN_ARRAY_HDR) +
+		sizeof(VTSA_NODE_INFO) + sizeof(VTSA_GEN_ARRAY_HDR) +
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(VTSA_GEN_PER_CPU) +
+		GLOBAL_STATE_num_cpus(driver_state) *
+			sizeof(VTSA_GEN_ARRAY_HDR) +
+		cpuid_entries * cpuid_size;
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", buffer_size);
+
+	return buffer_size;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void sys_info_Fill_CPUID(...)
+ *
+ * @param        num_cpuids,
+ * @param        basic_funcs,
+ * @param        extended_funcs,
+ * @param        cpu,
+ * @param       *current_cpuid
+ * @param       *gen_per_cpu,
+ * @param       *local_gpc
+ *
+ * @return   None
+ *
+ * @brief  This routine is called to build per cpu information.
+ * @brief  Fills in the cpuid for the processor in the right location in the buffer
+ *
+ */
+static void sys_info_Fill_CPUID(U32 num_cpuids, U32 basic_funcs,
+				U32 extended_funcs, U32 cpu,
+				VTSA_CPUID *current_cpuid,
+				VTSA_GEN_PER_CPU *gen_per_cpu,
+				VTSA_GEN_PER_CPU *local_gpc)
+{
+	U32 i, index, j;
+	U64 cpuid_function;
+	U64 rax, rbx, rcx, rdx;
+	VTSA_CPUID *cpuid_el;
+	U32 shift_nbits_core = 0;
+	U32 shift_nbits_pkg = 0;
+	// U32 family = 0;
+	U32 model = 0;
+	DRV_BOOL ht_supported = FALSE;
+	U32 apic_id = 0;
+	U32 num_logical_per_physical = 0;
+	U32 cores_per_die = 1;
+	U32 thread_id = 0;
+	U32 core_id = 0;
+	U32 package_id = 0;
+	U32 module_id = 0;
+	U32 cores_sharing_cache = 0;
+	U32 cache_mask_width = 0;
+	U32 num_cores = 0;
+
+	SEP_DRV_LOG_TRACE_IN("CPU: %x.", cpu);
+
+	apic_id = CPU_STATE_apic_id(&pcb[cpu]);
+	SEP_DRV_LOG_TRACE("Cpu %x: apic_id = %d.", cpu, apic_id);
+
+	for (i = 0, index = 0; index < num_cpuids; i++) {
+		cpuid_function =
+			(i < basic_funcs) ? i : (0x80000000 + i - basic_funcs);
+
+		if (cpuid_function == 0x4) {
+			for (j = 0, rax = (U64)-1; (rax & 0x1f) != 0; j++) {
+				rcx = j;
+				UTILITY_Read_Cpuid(cpuid_function, &rax, &rbx,
+						   &rcx, &rdx);
+				cpuid_el = &current_cpuid[index];
+				index++;
+
+#if defined(ALLOW_ASSERT)
+				ASSERT(((U8 *)cpuid_el + sizeof(VTSA_CPUID)) <=
+				       cpuid_buffer_limit);
+#endif
+
+				VTSA_CPUID_X86_cpuid_eax_input(cpuid_el) =
+					(U32)cpuid_function;
+				VTSA_CPUID_X86_cpuid_eax(cpuid_el) = (U32)rax;
+				VTSA_CPUID_X86_cpuid_ebx(cpuid_el) = (U32)rbx;
+				VTSA_CPUID_X86_cpuid_ecx(cpuid_el) = (U32)rcx;
+				VTSA_CPUID_X86_cpuid_edx(cpuid_el) = (U32)rdx;
+				SEP_DRV_LOG_TRACE("Function: %x.",
+						  (U32)cpuid_function);
+				SEP_DRV_LOG_TRACE(
+					"rax: %x, rbx: %x, rcx: %x, rdx: %x.",
+					(U32)rax, (U32)rbx, (U32)rcx, (U32)rdx);
+
+				if ((rax & 0x1f) != 0) {
+					local_gpc = &gen_per_cpu[cpu];
+					if (((rax >> 5) & 0x3) == 2) {
+						VTSA_GEN_PER_CPU_cpu_cache_L2(
+							local_gpc) =
+							(U32)(SYS_INFO_CACHE_SIZE(
+								      rcx,
+								      rbx) >>
+							      10);
+						SEP_DRV_LOG_TRACE(
+							"L2 Cache: %x.",
+							VTSA_GEN_PER_CPU_cpu_cache_L2(
+								local_gpc));
+						cores_sharing_cache =
+							((U16)(rax >> 14) &
+							 0xfff) +
+							1;
+						SEP_DRV_LOG_TRACE(
+							"CORES_SHARING_CACHE=%d j=%d cpu=%d.",
+							cores_sharing_cache, j,
+							cpu);
+					}
+
+					if (((rax >> 5) & 0x3) == 3) {
+						VTSA_GEN_PER_CPU_cpu_cache_L3(
+							local_gpc) =
+							(U32)(SYS_INFO_CACHE_SIZE(
+								      rcx,
+								      rbx) >>
+							      10);
+						SEP_DRV_LOG_TRACE(
+							"L3 Cache: %x.",
+							VTSA_GEN_PER_CPU_cpu_cache_L3(
+								local_gpc));
+					}
+				}
+				if (j == 0) {
+					cores_per_die =
+						((U16)(rax >> 26) & 0x3f) + 1;
+				}
+			}
+			if (cores_sharing_cache != 0) {
+				cache_mask_width = (U32)sys_info_nbits(
+					cores_sharing_cache);
+				SEP_DRV_LOG_TRACE("CACHE MASK WIDTH=%x.",
+						  cache_mask_width);
+			}
+		} else if (cpuid_function == 0xb) {
+			j = 0;
+			do {
+				rcx = j;
+				UTILITY_Read_Cpuid(cpuid_function, &rax, &rbx,
+						   &rcx, &rdx);
+				cpuid_el = &current_cpuid[index];
+				index++;
+
+#if defined(ALLOW_ASSERT)
+				ASSERT(((U8 *)cpuid_el +
+					sizeof(VTSA_CPUID_X86)) <=
+				       cpuid_buffer_limit);
+#endif
+
+				VTSA_CPUID_X86_cpuid_eax_input(cpuid_el) =
+					(U32)cpuid_function;
+				VTSA_CPUID_X86_cpuid_eax(cpuid_el) = (U32)rax;
+				VTSA_CPUID_X86_cpuid_ebx(cpuid_el) = (U32)rbx;
+				VTSA_CPUID_X86_cpuid_ecx(cpuid_el) = (U32)rcx;
+				VTSA_CPUID_X86_cpuid_edx(cpuid_el) = (U32)rdx;
+				SEP_DRV_LOG_TRACE("Function: %x.",
+						  (U32)cpuid_function);
+				SEP_DRV_LOG_TRACE(
+					"rax: %x, rbx: %x, rcx: %x, rdx: %x.",
+					(U32)rax, (U32)rbx, (U32)rcx, (U32)rdx);
+				if (j == 0) {
+					shift_nbits_core =
+						rax &
+						0x1f; //No. of bits to shift APIC ID to get Core ID
+				}
+				if (j == 1) {
+					shift_nbits_pkg =
+						rax &
+						0x1f; //No. of bits to shift APIC ID to get Pkg ID
+				}
+				j++;
+			} while (!(LOW_PART(rax) == 0 && LOW_PART(rbx) == 0));
+		} else {
+			UTILITY_Read_Cpuid(cpuid_function, &rax, &rbx, &rcx,
+					   &rdx);
+			cpuid_el = &current_cpuid[index];
+			index++;
+
+			SEP_DRV_LOG_TRACE(
+				"Cpu %x: num_cpuids = %x i = %x index = %x.",
+				cpu, num_cpuids, i, index);
+
+#if defined(ALLOW_ASSERT)
+			ASSERT(((U8 *)cpuid_el + sizeof(VTSA_CPUID_X86)) <=
+			       cpuid_buffer_limit);
+
+			ASSERT(((U8 *)cpuid_el + sizeof(VTSA_CPUID_X86)) <=
+			       ((U8 *)current_cpuid +
+				(num_cpuids * sizeof(VTSA_CPUID_X86))));
+#endif
+
+			VTSA_CPUID_X86_cpuid_eax_input(cpuid_el) =
+				(U32)cpuid_function;
+			VTSA_CPUID_X86_cpuid_eax(cpuid_el) = (U32)rax;
+			VTSA_CPUID_X86_cpuid_ebx(cpuid_el) = (U32)rbx;
+			VTSA_CPUID_X86_cpuid_ecx(cpuid_el) = (U32)rcx;
+			VTSA_CPUID_X86_cpuid_edx(cpuid_el) = (U32)rdx;
+			SEP_DRV_LOG_TRACE("Function: %x.", (U32)cpuid_function);
+			SEP_DRV_LOG_TRACE("rax: %x, rbx: %x, rcx: %x, rdx: %x.",
+					  (U32)rax, (U32)rbx, (U32)rcx,
+					  (U32)rdx);
+
+			if (cpuid_function == 0) {
+				if ((U32)rbx == 0x756e6547 &&
+				    (U32)rcx == 0x6c65746e &&
+				    (U32)rdx == 0x49656e69) {
+					VTSA_GEN_PER_CPU_platform_id(
+						local_gpc) =
+						SYS_Read_MSR(
+							MSR_FB_PCARD_ID_FUSE);
+				}
+			} else if (cpuid_function == 1) {
+				// family = (U32)(rax >> 8 & 0x0f);
+				/* extended model bits */
+				model = (U32)(rax >> 12 & 0xf0) |
+					(U32)(rax >> 4 & 0x0f);
+				// model |= (U32)(rax >> 4 & 0x0f);
+				ht_supported = (rdx >> 28) & 1 ? TRUE : FALSE;
+				num_logical_per_physical =
+					(U32)((rbx & 0xff0000) >> 16);
+				if (num_logical_per_physical == 0) {
+					num_logical_per_physical = 1;
+				}
+			} else if (cpuid_function == 0xa) {
+				VTSA_GEN_PER_CPU_arch_perfmon_ver(local_gpc) =
+					(U32)(rax & 0xFF);
+				VTSA_GEN_PER_CPU_num_gp_counters(local_gpc) =
+					(U32)((rax >> 8) & 0xFF);
+				VTSA_GEN_PER_CPU_num_fixed_counters(local_gpc) =
+					(U32)(rdx & 0x1F);
+			}
+		}
+	}
+
+	// set cpu_cache_L2 if not already set using 0x80000006 function
+	if (gen_per_cpu[cpu].cpu_cache_L2 == VTSA_NA && extended_funcs >= 6) {
+		UTILITY_Read_Cpuid(0x80000006, &rax, &rbx, &rcx, &rdx);
+		VTSA_GEN_PER_CPU_cpu_cache_L2(local_gpc) = (U32)(rcx >> 16);
+	}
+
+	if (!ht_supported || num_logical_per_physical == cores_per_die) {
+		threads_per_core[cpu] = 1;
+		thread_id = 0;
+	} else {
+		// each core has 4 threads for MIC system, otherwise, it has 2 threads when ht is enabled
+		threads_per_core[cpu] = cpu_threads_per_core;
+		thread_id = (U16)(apic_id & (cpu_threads_per_core - 1));
+	}
+
+	core_id = (apic_id >> shift_nbits_core) &
+		  sys_info_bitmask(shift_nbits_pkg - shift_nbits_core);
+	package_id = apic_id >> shift_nbits_pkg;
+
+	if (cache_mask_width) {
+		module_id = (U32)(core_id / 2);
+	}
+	SEP_DRV_LOG_TRACE("MODULE ID=%d CORE ID=%d for cpu=%d PACKAGE ID=%d.",
+			  module_id, core_id, cpu, package_id);
+	SEP_DRV_LOG_TRACE("Num_logical_per_physical=%d cores_per_die=%d.",
+			  num_logical_per_physical, cores_per_die);
+	SEP_DRV_LOG_TRACE("Package_id %d, apic_id %x.", package_id, apic_id);
+	SEP_DRV_LOG_TRACE(
+		"Sys_info_nbits[cores_per_die, threads_per_core[%u]]: [%lld, %lld].",
+		cpu, sys_info_nbits(cores_per_die),
+		sys_info_nbits(threads_per_core[cpu]));
+
+	VTSA_GEN_PER_CPU_cpu_intel_processor_number(local_gpc) = VTSA_NA32;
+	VTSA_GEN_PER_CPU_cpu_package_num(local_gpc) = (U16)package_id;
+	VTSA_GEN_PER_CPU_cpu_core_num(local_gpc) = (U16)core_id;
+	VTSA_GEN_PER_CPU_cpu_hw_thread_num(local_gpc) = (U16)thread_id;
+	VTSA_GEN_PER_CPU_cpu_threads_per_core(local_gpc) =
+		(U16)threads_per_core[cpu];
+	VTSA_GEN_PER_CPU_cpu_module_num(local_gpc) = (U16)module_id;
+	num_cores = GLOBAL_STATE_num_cpus(driver_state) / threads_per_core[cpu];
+	VTSA_GEN_PER_CPU_cpu_num_modules(local_gpc) =
+		(U16)(num_cores / 2); // Relavent to Atom processors, Always 2
+	VTSA_GEN_PER_CPU_cpu_core_type(local_gpc) = 0;
+	GLOBAL_STATE_num_modules(driver_state) =
+		VTSA_GEN_PER_CPU_cpu_num_modules(local_gpc);
+	SEP_DRV_LOG_TRACE("MODULE COUNT=%d.",
+			  GLOBAL_STATE_num_modules(driver_state));
+
+	core_to_package_map[cpu] = package_id;
+	core_to_phys_core_map[cpu] = core_id;
+	core_to_thread_map[cpu] = thread_id;
+	occupied_core_ids[core_id] = 1;
+
+	if (num_packages < package_id + 1) {
+		num_packages = package_id + 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+#if !defined(DRV_SEP_ACRN_ON)
+/* ------------------------------------------------------------------------- */
+/*!
+* @fn static void sys_info_Update_Hyperthreading_Info(buffer)
+*
+* @param    buffer  -  points to the base of GEN_PER_CPU structure
+* @return   None
+*
+* @brief  This routine is called to update per cpu information based on HT ON/OFF.
+*
+*/
+static VOID sys_info_Update_Hyperthreading_Info(VOID *buffer)
+{
+	U32 cpu;
+	VTSA_GEN_PER_CPU *gen_per_cpu, *local_gpc;
+	U32 i = 0;
+	U32 num_cores = 0;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cpu = CONTROL_THIS_CPU();
+
+	// get the GEN_PER_CPU entry for the current processor.
+	gen_per_cpu = (VTSA_GEN_PER_CPU *)buffer;
+
+	// Update GEN_PER_CPU
+	local_gpc = &(gen_per_cpu[cpu]);
+
+	while (i < (U32)GLOBAL_STATE_num_cpus(driver_state)) {
+		if (cpu_built_sysinfo[i] == 1) {
+			i++;
+		}
+	}
+
+	for (i = 0; i < (U32)GLOBAL_STATE_num_cpus(driver_state); i++) {
+		if (occupied_core_ids[i] == 1) {
+			num_cores++;
+		}
+	}
+	threads_per_core[cpu] = (U32)(GLOBAL_STATE_num_cpus(driver_state) /
+				      (num_cores * num_packages));
+	if (VTSA_GEN_PER_CPU_cpu_threads_per_core(local_gpc) !=
+	    (U16)threads_per_core[cpu]) {
+		VTSA_GEN_PER_CPU_cpu_threads_per_core(local_gpc) =
+			(U16)threads_per_core[cpu];
+		VTSA_GEN_PER_CPU_cpu_num_modules(local_gpc) =
+			(U16)(num_cores / 2);
+		GLOBAL_STATE_num_modules(driver_state) =
+			VTSA_GEN_PER_CPU_cpu_num_modules(local_gpc);
+	}
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void sys_info_Build_Percpu(buffer)
+ *
+ * @param    buffer  -  points to the base of GEN_PER_CPU structure
+ * @return   None
+ *
+ * @brief  This routine is called to build per cpu information.
+ *
+ */
+static VOID sys_info_Build_Percpu(PVOID param)
+{
+	U32 basic_funcs, basic_4_funcs, extended_funcs;
+	U32 num_cpuids;
+	S32 cpu;
+	VTSA_CPUID *current_cpuid;
+	VTSA_GEN_ARRAY_HDR *cpuid_gen_array_hdr;
+	VTSA_GEN_PER_CPU *gen_per_cpu, *local_gpc;
+	VTSA_FIXED_SIZE_PTR *fsp;
+	U8 *cpuid_gen_array_hdr_base;
+#if defined(ALLOW_ASSERT)
+	U8 *cpuid_buffer_limit;
+#endif
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p.", buffer);
+
+	if (param == NULL) {
+		cpu = CONTROL_THIS_CPU();
+	} else {
+		cpu = *(S32 *)param;
+	}
+	num_cpuids = (U32)sys_info_Get_Num_Cpuid_Funcs(
+		&basic_funcs, &basic_4_funcs, &extended_funcs);
+
+	// get the GEN_PER_CPU entry for the current processor.
+	gen_per_cpu = (VTSA_GEN_PER_CPU *)gen_per_cpu_ptr;
+	SEP_DRV_LOG_TRACE("cpu %x: gen_per_cpu = %p.", cpu, gen_per_cpu);
+
+	// get GEN_ARRAY_HDR and cpuid array base
+	cpuid_gen_array_hdr_base =
+		(U8 *)gen_per_cpu +
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(VTSA_GEN_PER_CPU);
+
+	SEP_DRV_LOG_TRACE("cpuid_gen_array_hdr_base = %p.",
+			  cpuid_gen_array_hdr_base);
+	SEP_DRV_LOG_TRACE("cpu = %x.", cpu);
+	SEP_DRV_LOG_TRACE("cpuid_total_count[cpu] = %x.",
+			  cpuid_total_count[cpu]);
+	SEP_DRV_LOG_TRACE("sizeof(VTSA_CPUID) = %lx.", sizeof(VTSA_CPUID));
+
+	cpuid_gen_array_hdr =(VTSA_GEN_ARRAY_HDR *)
+		((U8 *)cpuid_gen_array_hdr_base +
+		sizeof(VTSA_GEN_ARRAY_HDR) * cpu +
+		cpuid_total_count[cpu] * sizeof(VTSA_CPUID));
+
+	// get current cpuid array base.
+	current_cpuid = (VTSA_CPUID *)((U8 *)cpuid_gen_array_hdr +
+				       sizeof(VTSA_GEN_ARRAY_HDR));
+#if defined(ALLOW_ASSERT)
+	// get the absolute buffer limit
+	cpuid_buffer_limit =
+		(U8 *)ioctl_sys_info +
+		GENERIC_IOCTL_size(&IOCTL_SYS_INFO_gen(ioctl_sys_info));
+#endif
+
+	//
+	// Fill in GEN_PER_CPU
+	//
+	local_gpc = &(gen_per_cpu[cpu]);
+
+	if (VTSA_GEN_PER_CPU_cpu_intel_processor_number(local_gpc)) {
+		SEP_DRV_LOG_TRACE_OUT(
+			"Early exit (VTSA_GEN_PER_CPU_cpu_intel_processor_number).");
+		return;
+	}
+	VTSA_GEN_PER_CPU_cpu_number(local_gpc) = cpu;
+	VTSA_GEN_PER_CPU_cpu_speed_mhz(local_gpc) = VTSA_NA32;
+	VTSA_GEN_PER_CPU_cpu_fsb_mhz(local_gpc) = VTSA_NA32;
+
+	fsp = &VTSA_GEN_PER_CPU_cpu_cpuid_array(local_gpc);
+	VTSA_FIXED_SIZE_PTR_is_ptr(fsp) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(fsp) =
+		(U64)((U8 *)cpuid_gen_array_hdr -
+		      (U8 *)&IOCTL_SYS_INFO_sys_info(ioctl_sys_info));
+
+	/*
+	 * Get the time stamp difference between this cpu and cpu 0.
+	 * This value will be used by user mode code to generate standardize
+	 * time needed for sampling over time (SOT) functionality.
+	 */
+	VTSA_GEN_PER_CPU_cpu_tsc_offset(local_gpc) = TSC_SKEW(cpu);
+
+	//
+	// fill GEN_ARRAY_HDR
+	//
+	fsp = &VTSA_GEN_ARRAY_HDR_hdr_next_gen_hdr(cpuid_gen_array_hdr);
+	VTSA_GEN_ARRAY_HDR_hdr_size(cpuid_gen_array_hdr) =
+		sizeof(VTSA_GEN_ARRAY_HDR);
+	VTSA_FIXED_SIZE_PTR_is_ptr(fsp) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(fsp) = 0;
+	VTSA_GEN_ARRAY_HDR_array_num_entries(cpuid_gen_array_hdr) = num_cpuids;
+	VTSA_GEN_ARRAY_HDR_array_entry_size(cpuid_gen_array_hdr) =
+		sizeof(VTSA_CPUID);
+	VTSA_GEN_ARRAY_HDR_array_type(cpuid_gen_array_hdr) = GT_CPUID;
+#if defined(DRV_IA32)
+	VTSA_GEN_ARRAY_HDR_array_subtype(cpuid_gen_array_hdr) = GST_X86;
+#elif defined(DRV_EM64T)
+	VTSA_GEN_ARRAY_HDR_array_subtype(cpuid_gen_array_hdr) = GST_EM64T;
+#endif
+
+	//
+	// fill out cpu id information
+	//
+	sys_info_Fill_CPUID(num_cpuids, basic_funcs, extended_funcs, cpu,
+			    current_cpuid, gen_per_cpu, local_gpc);
+	/*
+	 *  Mark cpu info on this cpu as successfully built
+	 */
+	cpu_built_sysinfo[cpu] = 1;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static void sys_info_Get_Processor_Info(NULL)
+ *
+ * @param    None
+ * @return   None
+ *
+ * @brief  This routine is called to get global informaton on the processor in general,
+ *         it include:
+ *             cpu_thread_per_core
+ *
+ */
+static VOID sys_info_Get_Processor_Info(VOID *param)
+{
+	U64 rax;
+	U64 rbx;
+	U64 rcx;
+	U64 rdx;
+	U32 family;
+	U32 model;
+	DRV_BOOL ht_supported = FALSE;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// read cpuid with function 1 to find family/model
+	UTILITY_Read_Cpuid(1, &rax, &rbx, &rcx, &rdx);
+	family = (U32)(rax >> 8 & 0x0f);
+	model = (U32)(rax >> 12 & 0xf0); /* extended model bits */
+	model |= (U32)(rax >> 4 & 0x0f);
+	if (is_Knights_family(family, model)) {
+		cpu_threads_per_core = 4;
+	} else {
+		ht_supported = (rdx >> 28) & 1 ? TRUE : FALSE;
+		if (ht_supported) {
+			cpu_threads_per_core = 2;
+		} else {
+			cpu_threads_per_core = 1;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void SYS_Info_Build(void)
+ *
+ * @param    None
+ * @return   None
+ *
+ * @brief  This is the driver routine that constructs the VTSA_SYS_INFO
+ * @brief  structure used to report system information into the tb5 file
+ *
+ */
+U32 SYS_INFO_Build(void)
+{
+	VTSA_GEN_ARRAY_HDR *gen_array_hdr;
+	VTSA_NODE_INFO *node_info;
+	VTSA_SYS_INFO *sys_info;
+	VTSA_FIXED_SIZE_PTR *fsp;
+	U32 buffer_size;
+	U32 total_cpuid_entries;
+	S32 i;
+	struct sysinfo k_sysinfo;
+	U32 res;
+
+	SEP_DRV_LOG_TRACE_IN("");
+	SEP_DRV_LOG_TRACE("Entered.");
+
+	if (ioctl_sys_info) {
+		/* The sys info has already been computed.  Do not redo */
+		buffer_size =
+			GENERIC_IOCTL_size(&IOCTL_SYS_INFO_gen(ioctl_sys_info));
+		return buffer_size - sizeof(GENERIC_IOCTL);
+	}
+
+	si_meminfo(&k_sysinfo);
+
+	buffer_size = GLOBAL_STATE_num_cpus(driver_state) * sizeof(U32);
+	cpu_built_sysinfo = CONTROL_Allocate_Memory(buffer_size);
+	if (cpu_built_sysinfo == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Cpu_built_sysinfo memory alloc failed!");
+		return 0;
+	}
+
+	cpuid_entry_count = CONTROL_Allocate_Memory(buffer_size);
+	if (cpuid_entry_count == NULL) {
+		cpu_built_sysinfo = CONTROL_Free_Memory(cpu_built_sysinfo);
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Memory alloc failed for cpuid_entry_count!");
+		return 0;
+	}
+
+	cpuid_total_count = CONTROL_Allocate_Memory(buffer_size);
+	if (cpuid_total_count == NULL) {
+		cpu_built_sysinfo = CONTROL_Free_Memory(cpu_built_sysinfo);
+		cpuid_entry_count = CONTROL_Free_Memory(cpuid_entry_count);
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Memory alloc failed for cpuid_total_count!");
+		return 0;
+	}
+
+	// checking on family-model to set threads_per_core as 4: MIC,  2: ht-on; 1: rest
+	sys_info_Get_Processor_Info(NULL);
+
+#if defined(DRV_SEP_ACRN_ON)
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		sys_info_Get_Cpuid_Entry_Count(&i);
+	}
+#else
+	CONTROL_Invoke_Parallel(sys_info_Get_Cpuid_Entry_Count, NULL);
+#endif
+
+	total_cpuid_entries = 0;
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		//if cpu is offline, set its cpuid count same as cpu0
+		if (cpuid_entry_count[i] == 0) {
+			cpuid_entry_count[i] = cpuid_entry_count[0];
+			cpu_built_sysinfo[i] = 0;
+		}
+		cpuid_total_count[i] = total_cpuid_entries;
+		total_cpuid_entries += cpuid_entry_count[i];
+	}
+
+	ioctl_sys_info_size =
+		sys_info_Get_Cpuid_Buffer_Size(total_cpuid_entries);
+	ioctl_sys_info = CONTROL_Allocate_Memory(ioctl_sys_info_size);
+	if (ioctl_sys_info == NULL) {
+		cpuid_entry_count = CONTROL_Free_Memory(cpuid_entry_count);
+		cpuid_total_count = CONTROL_Free_Memory(cpuid_total_count);
+
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Memory alloc failed for ioctl_sys_info!");
+		//        return STATUS_INSUFFICIENT_RESOURCES;
+		return 0;
+	}
+
+	//
+	// fill in ioctl and cpu_cs_info fields.
+	//
+	GENERIC_IOCTL_size(&IOCTL_SYS_INFO_gen(ioctl_sys_info)) =
+		ioctl_sys_info_size;
+	GENERIC_IOCTL_ret(&IOCTL_SYS_INFO_gen(ioctl_sys_info)) = VT_SUCCESS;
+
+	sys_info = &IOCTL_SYS_INFO_sys_info(ioctl_sys_info);
+	VTSA_SYS_INFO_min_app_address(sys_info) = VTSA_NA64;
+	VTSA_SYS_INFO_max_app_address(sys_info) = VTSA_NA64;
+	VTSA_SYS_INFO_page_size(sys_info) = k_sysinfo.mem_unit;
+	VTSA_SYS_INFO_allocation_granularity(sys_info) = k_sysinfo.mem_unit;
+
+	//
+	// offset from ioctl_sys_info
+	//
+	VTSA_FIXED_SIZE_PTR_is_ptr(&VTSA_SYS_INFO_node_array(sys_info)) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(&VTSA_SYS_INFO_node_array(sys_info)) =
+		sizeof(VTSA_SYS_INFO);
+
+	//
+	// fill in node_info array header
+	//
+	gen_array_hdr =	(VTSA_GEN_ARRAY_HDR *)((U8 *)sys_info +
+			VTSA_FIXED_SIZE_PTR_fs_offset(
+			&VTSA_SYS_INFO_node_array(sys_info)));
+
+	SEP_DRV_LOG_TRACE("Gen_array_hdr = %p.", gen_array_hdr);
+	fsp = &VTSA_GEN_ARRAY_HDR_hdr_next_gen_hdr(gen_array_hdr);
+	VTSA_FIXED_SIZE_PTR_is_ptr(fsp) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(fsp) = 0;
+
+	VTSA_GEN_ARRAY_HDR_hdr_size(gen_array_hdr) = sizeof(VTSA_GEN_ARRAY_HDR);
+	VTSA_GEN_ARRAY_HDR_array_num_entries(gen_array_hdr) = 1;
+	VTSA_GEN_ARRAY_HDR_array_entry_size(gen_array_hdr) =
+		sizeof(VTSA_NODE_INFO);
+	VTSA_GEN_ARRAY_HDR_array_type(gen_array_hdr) = GT_NODE;
+	VTSA_GEN_ARRAY_HDR_array_subtype(gen_array_hdr) = GST_UNK;
+
+	//
+	// fill in node_info
+	//
+	node_info = (VTSA_NODE_INFO *)((U8 *)gen_array_hdr +
+				       sizeof(VTSA_GEN_ARRAY_HDR));
+	SEP_DRV_LOG_TRACE("Node_info = %p.", node_info);
+
+	VTSA_NODE_INFO_node_type_from_shell(node_info) = VTSA_NA32;
+
+	VTSA_NODE_INFO_node_id(node_info) = VTSA_NA32;
+	VTSA_NODE_INFO_node_num_available(node_info) =
+		GLOBAL_STATE_num_cpus(driver_state);
+	VTSA_NODE_INFO_node_num_used(node_info) = VTSA_NA32;
+	total_ram = k_sysinfo.totalram << PAGE_SHIFT;
+	VTSA_NODE_INFO_node_physical_memory(node_info) = total_ram;
+
+	fsp = &VTSA_NODE_INFO_node_percpu_array(node_info);
+	VTSA_FIXED_SIZE_PTR_is_ptr(fsp) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(fsp) = sizeof(VTSA_SYS_INFO) +
+					     sizeof(VTSA_GEN_ARRAY_HDR) +
+					     sizeof(VTSA_NODE_INFO);
+	//
+	// fill in gen_per_cpu array header
+	//
+	gen_array_hdr =
+		(VTSA_GEN_ARRAY_HDR *)((U8 *)sys_info +
+				       VTSA_FIXED_SIZE_PTR_fs_offset(fsp));
+	SEP_DRV_LOG_TRACE("Gen_array_hdr = %p.", gen_array_hdr);
+
+	fsp = &VTSA_GEN_ARRAY_HDR_hdr_next_gen_hdr(gen_array_hdr);
+	VTSA_FIXED_SIZE_PTR_is_ptr(fsp) = 0;
+	VTSA_FIXED_SIZE_PTR_fs_offset(fsp) = 0;
+
+	VTSA_GEN_ARRAY_HDR_hdr_size(gen_array_hdr) = sizeof(VTSA_GEN_ARRAY_HDR);
+	VTSA_GEN_ARRAY_HDR_array_num_entries(gen_array_hdr) =
+		GLOBAL_STATE_num_cpus(driver_state);
+	VTSA_GEN_ARRAY_HDR_array_entry_size(gen_array_hdr) =
+		sizeof(VTSA_GEN_PER_CPU);
+	VTSA_GEN_ARRAY_HDR_array_type(gen_array_hdr) = GT_PER_CPU;
+
+#if defined(DRV_IA32)
+	VTSA_GEN_ARRAY_HDR_array_subtype(gen_array_hdr) = GST_X86;
+#elif defined(DRV_EM64T)
+	VTSA_GEN_ARRAY_HDR_array_subtype(gen_array_hdr) = GST_EM64T;
+#endif
+
+	gen_per_cpu_ptr = (U8 *)gen_array_hdr + sizeof(VTSA_GEN_ARRAY_HDR);
+
+#if defined(DRV_SEP_ACRN_ON)
+	for (i = 0; i < GLOBAL_STATE_num_cpus(driver_state); i++) {
+		APIC_Init(&i);
+		sys_info_Build_Percpu(&i);
+	}
+#else
+	CONTROL_Invoke_Parallel(APIC_Init, NULL);
+	CONTROL_Invoke_Parallel(sys_info_Build_Percpu, NULL);
+	CONTROL_Invoke_Parallel(sys_info_Update_Hyperthreading_Info,
+				(VOID *)gen_per_cpu_ptr);
+#endif
+
+	/*
+	 * Cleanup - deallocate memory that is no longer needed
+	 */
+	cpuid_entry_count = CONTROL_Free_Memory(cpuid_entry_count);
+
+	res = ioctl_sys_info_size - sizeof(GENERIC_IOCTL);
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", res);
+	return res;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void SYS_Info_Transfer(buf_usr_to_drv, len_usr_to_drv)
+ *
+ * @param  buf_usr_to_drv      - pointer to the buffer to write the data into
+ * @param  len_usr_to_drv  - length of the buffer passed in
+ *
+ * @brief  Transfer the data collected via the SYS_INFO_Build routine
+ * @brief  back to the caller.
+ *
+ */
+VOID SYS_INFO_Transfer(PVOID buf_usr_to_drv, unsigned long len_usr_to_drv)
+{
+	unsigned long exp_size;
+	ssize_t unused;
+
+	SEP_DRV_LOG_TRACE_IN("Buffer: %p, buffer_len: %u.", buf_usr_to_drv,
+			     (U32)len_usr_to_drv);
+
+	if (ioctl_sys_info == NULL || len_usr_to_drv == 0) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Ioctl_sys_info is NULL or len_usr_to_drv is 0!");
+		return;
+	}
+	exp_size = GENERIC_IOCTL_size(&IOCTL_SYS_INFO_gen(ioctl_sys_info)) -
+		   sizeof(GENERIC_IOCTL);
+	if (len_usr_to_drv < exp_size) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Insufficient Space!");
+		return;
+	}
+	unused = copy_to_user((void __user *)buf_usr_to_drv,
+			      &(IOCTL_SYS_INFO_sys_info(ioctl_sys_info)),
+			      len_usr_to_drv);
+	if (unused) {
+		// no-op ... eliminates "variable not used" compiler warning
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void SYS_Info_Destroy(void)
+ *
+ * @param    None
+ * @return   None
+ *
+ * @brief  Free any memory associated with the sys info before unloading the driver
+ *
+ */
+VOID SYS_INFO_Destroy(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	cpuid_total_count = CONTROL_Free_Memory(cpuid_total_count);
+	cpu_built_sysinfo = CONTROL_Free_Memory(cpu_built_sysinfo);
+	ioctl_sys_info = CONTROL_Free_Memory(ioctl_sys_info);
+	ioctl_sys_info_size = 0;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void SYS_INFO_Build_Cpu(PVOID param)
+ *
+ * @param    PVOID param
+ * @return   None
+ *
+ * @brief  call routine to populate cpu info
+ *
+ */
+VOID SYS_INFO_Build_Cpu(PVOID param)
+{
+	VTSA_GEN_ARRAY_HDR *gen_array_hdr;
+	VTSA_NODE_INFO *node_info;
+	VTSA_SYS_INFO *sys_info;
+	VTSA_FIXED_SIZE_PTR *fsp;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (!ioctl_sys_info) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Ioctl_sys_info is null!");
+		return;
+	}
+	sys_info = &IOCTL_SYS_INFO_sys_info(ioctl_sys_info);
+	gen_array_hdr =
+		(VTSA_GEN_ARRAY_HDR *)((U8 *)sys_info +
+				       VTSA_FIXED_SIZE_PTR_fs_offset(
+					       &VTSA_SYS_INFO_node_array(
+						       sys_info)));
+	SEP_DRV_LOG_TRACE("Gen_array_hdr = %p.", gen_array_hdr);
+
+	node_info = (VTSA_NODE_INFO *)((U8 *)gen_array_hdr +
+				       sizeof(VTSA_GEN_ARRAY_HDR));
+	SEP_DRV_LOG_TRACE("Node_info = %p.", node_info);
+	fsp = &VTSA_NODE_INFO_node_percpu_array(node_info);
+
+	gen_array_hdr =
+		(VTSA_GEN_ARRAY_HDR *)((U8 *)sys_info +
+				       VTSA_FIXED_SIZE_PTR_fs_offset(fsp));
+	SEP_DRV_LOG_TRACE("Gen_array_hdr = %p.", gen_array_hdr);
+	gen_per_cpu_ptr = (U8 *)gen_array_hdr + sizeof(VTSA_GEN_ARRAY_HDR);
+
+	sys_info_Build_Percpu(NULL);
+
+#if !defined(DRV_SEP_ACRN_ON)
+	sys_info_Update_Hyperthreading_Info((VOID *)gen_per_cpu_ptr);
+#endif
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/unc_common.c b/drivers/platform/x86/sepdk/sep/unc_common.c
new file mode 100755
index 000000000000..5442734a91f7
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_common.c
@@ -0,0 +1,388 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/pci.h"
+#include "inc/unc_common.h"
+#include "inc/utility.h"
+
+extern UNCORE_TOPOLOGY_INFO_NODE uncore_topology;
+extern PLATFORM_TOPOLOGY_PROG_NODE platform_topology_prog_node;
+extern U64 *read_counter_info;
+
+/* this is the table to keep pci_bus structure for PCI devices
+ * for both pci config access and mmio access
+ */
+UNC_PCIDEV_NODE unc_pcidev_map[MAX_DEVICES];
+
+#define GET_PACKAGE_NUM(device_type, cpu)                                      \
+	(((device_type) == DRV_SINGLE_INSTANCE) ? 0 : core_to_package_map[cpu])
+
+/************************************************************/
+/*
+ * unc common Dispatch functions
+ *
+ ************************************************************/
+void UNC_COMMON_Dummy_Func(PVOID param)
+{
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+	SEP_DRV_LOG_TRACE_OUT("Empty function.");
+}
+
+/************************************************************/
+/*
+ * UNC common PCI  based API
+ *
+ ************************************************************/
+
+/*!
+ * @fn          OS_STATUS UNC_COMMON_Add_Bus_Map
+ *
+ * @brief       This code discovers which package's data is read off of which bus.
+ *
+ * @param       None
+ *
+ * @return      OS_STATUS
+ *
+ * <I>Special Notes:</I>
+ *     This probably will move to the UBOX once that is programmed.
+ */
+OS_STATUS
+UNC_COMMON_Add_Bus_Map(U32 uncore_did, U32 dev_node, U32 bus_no)
+{
+	U32 i = 0;
+	U32 entries = 0;
+
+	if (!UNC_PCIDEV_busno_list(&(unc_pcidev_map[dev_node]))) {
+		// allocate array for holding bus mapping
+		// package based device: an entry per package, all units in the same package are in the same bus.
+		// system based device:  an entry per unit if in different bus
+		entries = GET_MAX_PCIDEV_ENTRIES(num_packages);
+		UNC_PCIDEV_busno_list(&(unc_pcidev_map[dev_node])) =
+			CONTROL_Allocate_Memory(entries * sizeof(S32));
+		if (UNC_PCIDEV_busno_list(&(unc_pcidev_map[dev_node])) ==
+		    NULL) {
+			SEP_DRV_LOG_ERROR("Memory allocation failure!");
+			return OS_NO_MEM;
+		}
+		UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node])) = 0;
+		UNC_PCIDEV_max_entries(&(unc_pcidev_map[dev_node])) = entries;
+		for (i = 0; i < entries; i++) {
+			UNC_PCIDEV_busno_entry(&(unc_pcidev_map[dev_node]), i) =
+				INVALID_BUS_NUMBER;
+		}
+	} else {
+		entries = UNC_PCIDEV_max_entries(&(unc_pcidev_map[dev_node]));
+	}
+
+	for (i = 0; i < UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node]));
+	     i++) {
+		if (UNC_PCIDEV_busno_entry(&(unc_pcidev_map[dev_node]), i) ==
+		    (S32)bus_no) {
+			SEP_DRV_LOG_TRACE(
+				"Already in the map,  another unit, no add.");
+			return OS_SUCCESS;
+		}
+	}
+	if (i < entries) {
+		UNC_PCIDEV_busno_entry(&(unc_pcidev_map[dev_node]), i) =
+			(S32)bus_no;
+		UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node]))++;
+		SEP_DRV_LOG_TRACE("Add numpackages=%d busno=%x  devnode=%d.",
+				  num_packages, bus_no, dev_node);
+		return OS_SUCCESS;
+	}
+	SEP_DRV_LOG_ERROR_TRACE_OUT(
+		"Exceed max map entries, drop this bus map!");
+	return OS_NO_MEM;
+}
+
+OS_STATUS UNC_COMMON_Init(void)
+{
+	U32 i = 0;
+
+	for (i = 0; i < MAX_DEVICES; i++) {
+		memset(&(unc_pcidev_map[i]), 0, sizeof(UNC_PCIDEV_NODE));
+	}
+
+	memset((char *)&uncore_topology, 0, sizeof(UNCORE_TOPOLOGY_INFO_NODE));
+	memset((char *)&platform_topology_prog_node, 0,
+	       sizeof(PLATFORM_TOPOLOGY_PROG_NODE));
+
+	return OS_SUCCESS;
+}
+
+/*!
+ * @fn         extern VOID UNC_COMMON_Clean_Up(PVOID)
+ *
+ * @brief      clear out out programming
+ *
+ * @param      None
+ *
+ * @return     None
+ */
+void UNC_COMMON_Clean_Up(void)
+{
+	U32 i = 0;
+	for (i = 0; i < MAX_DEVICES; i++) {
+		if (UNC_PCIDEV_busno_list(&(unc_pcidev_map[i]))) {
+			UNC_PCIDEV_busno_list(&(unc_pcidev_map[i])) =
+				CONTROL_Free_Memory(UNC_PCIDEV_busno_list(
+					&(unc_pcidev_map[i])));
+		}
+		if (UNC_PCIDEV_mmio_map(&(unc_pcidev_map[i]))) {
+			UNC_PCIDEV_mmio_map(&(unc_pcidev_map[i])) =
+				CONTROL_Free_Memory(UNC_PCIDEV_mmio_map(
+					&(unc_pcidev_map[i])));
+		}
+		memset(&(unc_pcidev_map[i]), 0, sizeof(UNC_PCIDEV_NODE));
+	}
+}
+
+/*!
+ * @fn          static VOID UNC_COMMON_PCI_Scan_For_Uncore(VOID*)
+ *
+ * @brief       Initial write of PMU registers
+ *              Walk through the enties and write the value of the register accordingly.
+ *              When current_group = 0, then this is the first time this routine is called,
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+
+VOID UNC_COMMON_PCI_Scan_For_Uncore(PVOID param, U32 dev_node,
+					   DEVICE_CALLBACK callback)
+{
+	U32 device_id;
+	U32 value;
+	U32 vendor_id;
+	U32 busno;
+	U32 j, k, l;
+	U32 device_found = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p, dev_node: %u, callback: %p.",
+			     param, dev_node, callback);
+
+	for (busno = 0; busno < 256; busno++) {
+		for (j = 0; j < MAX_PCI_DEVNO; j++) {
+			if (!(UNCORE_TOPOLOGY_INFO_pcidev_valid(
+				    &uncore_topology, dev_node, j))) {
+				continue;
+			}
+			for (k = 0; k < MAX_PCI_FUNCNO; k++) {
+				if (!(UNCORE_TOPOLOGY_INFO_pcidev_is_devno_funcno_valid(
+					    &uncore_topology, dev_node, j,
+					    k))) {
+					continue;
+				}
+				device_found = 0;
+				value = PCI_Read_U32_Valid(busno, j, k, 0, 0);
+				CONTINUE_IF_NOT_GENUINE_INTEL_DEVICE(
+					value, vendor_id, device_id);
+				SEP_DRV_LOG_TRACE("Uncore device ID = 0x%x.",
+						  device_id);
+
+				for (l = 0;
+				     l <
+				     UNCORE_TOPOLOGY_INFO_num_deviceid_entries(
+					     &uncore_topology, dev_node);
+				     l++) {
+					if (UNCORE_TOPOLOGY_INFO_deviceid(
+						    &uncore_topology, dev_node,
+						    l) == device_id) {
+						device_found = 1;
+						break;
+					}
+				}
+				if (device_found) {
+					if (UNC_COMMON_Add_Bus_Map(
+						    device_id, dev_node,
+						    busno) == OS_SUCCESS) {
+						UNCORE_TOPOLOGY_INFO_pcidev_num_entries_found(
+							&uncore_topology,
+							dev_node, j, k)++;
+						SEP_DRV_LOG_DETECTION(
+							"Found device 0x%x at BDF(%x:%x:%x) [%u unit(s) so far].",
+							device_id, busno, j, k,
+							UNCORE_TOPOLOGY_INFO_pcidev_num_entries_found(
+								&uncore_topology,
+								dev_node, j,
+								k));
+					}
+				}
+			}
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          extern VOID UNC_COMMON_Get_Platform_Topology()
+ *
+ * @brief       This function will walk through the platform registers to retrieve information and calculate the bus no.
+ *              Reads appropriate pci_config regs and populates the PLATFORM_TOPOLOGY_PROG_NODE structure with the reg value.
+ *
+ * @param       U32             dev_node - Device no.
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ *                   device_num corresponds to Memory controller
+ *                   func_num  corresponds to Channel number
+ *                   reg_offset corresponds to dimm slot
+ */
+VOID UNC_COMMON_Get_Platform_Topology(U32 dev_node)
+{
+	U32 num_registers = 0;
+	// U32 device_index = 0;
+	U32 bus_num = 0;
+	U32 i = 0;
+	U32 func_num = 0;
+	U32 num_pkgs = num_packages;
+	U32 device_num = 0;
+	U32 reg_offset = 0;
+	U32 len = 0;
+	U64 reg_value = 0;
+	U32 device_value = 0;
+	U64 reg_mask = 0;
+	U32 vendor_id;
+	U32 device_id;
+	U32 valid;
+
+	PLATFORM_TOPOLOGY_REG topology_regs = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("Dev_node: %u.", dev_node);
+	PLATFORM_TOPOLOGY_PROG_topology_device_prog_valid(
+		&platform_topology_prog_node, dev_node) = 1;
+
+	if (num_packages > MAX_PACKAGES) {
+		SEP_DRV_LOG_ERROR(
+			"Num_packages %d > MAX_PACKAGE, getting for only %d packages.",
+			num_packages, MAX_PACKAGES);
+		num_pkgs = MAX_PACKAGES;
+	}
+
+	num_registers = PLATFORM_TOPOLOGY_PROG_topology_device_num_registers(
+		&platform_topology_prog_node, dev_node);
+	topology_regs = PLATFORM_TOPOLOGY_PROG_topology_topology_regs(
+		&platform_topology_prog_node, dev_node);
+	// device_index = PLATFORM_TOPOLOGY_PROG_topology_device_device_index(
+		// &platform_topology_prog_node, dev_node);
+
+	for (i = 0; i < num_pkgs; i++) {
+		for (len = 0; len < num_registers; len++) {
+			if (PLATFORM_TOPOLOGY_REG_reg_type(
+				    topology_regs, len) == PMU_REG_PROG_MSR) {
+				reg_value = SYS_Read_MSR(
+					PLATFORM_TOPOLOGY_REG_reg_id(
+						topology_regs, len));
+				reg_mask = PLATFORM_TOPOLOGY_REG_reg_mask(
+					topology_regs, len);
+				PLATFORM_TOPOLOGY_REG_reg_value(topology_regs,
+								len, i) =
+					reg_value & reg_mask;
+				SEP_DRV_LOG_TRACE(
+					"Read UNCORE_MSR_FREQUENCY 0x%x\n",
+					PLATFORM_TOPOLOGY_REG_reg_id(
+						topology_regs, len));
+			} else {
+				if (!IS_BUS_MAP_VALID(dev_node, i)) {
+					continue;
+				}
+				bus_num = GET_BUS_MAP(dev_node, i);
+				device_num = PLATFORM_TOPOLOGY_REG_device(
+					topology_regs, len);
+				func_num = PLATFORM_TOPOLOGY_REG_function(
+					topology_regs, len);
+				reg_offset = PLATFORM_TOPOLOGY_REG_reg_id(
+					topology_regs, len);
+				device_value = PCI_Read_U32_Valid(
+					bus_num, device_num, func_num, 0, 0);
+				CHECK_IF_GENUINE_INTEL_DEVICE(device_value,
+							      vendor_id,
+							      device_id, valid);
+				SEP_DRV_LOG_TRACE("Uncore device ID = 0x%x.",
+						  device_id);
+				if (!valid) {
+					PLATFORM_TOPOLOGY_REG_device_valid(
+						topology_regs, len) = 0;
+				}
+				PLATFORM_TOPOLOGY_REG_reg_value(topology_regs,
+								len, i) =
+					PCI_Read_U32_Valid(bus_num, device_num,
+							   func_num, reg_offset,
+							   PCI_INVALID_VALUE);
+			}
+		}
+		if (PLATFORM_TOPOLOGY_PROG_topology_device_scope(
+			    &platform_topology_prog_node, dev_node) ==
+		    SYSTEM_EVENT) {
+			break;
+		}
+	}
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/************************************************************/
+/*
+ * UNC common MSR  based API
+ *
+ ************************************************************/
+
+/*!
+ * @fn         VOID UNC_COMMON_MSR_Clean_Up(PVOID)
+ *
+ * @brief      clear out out programming
+ *
+ * @param      None
+ *
+ * @return     None
+ */
+VOID UNC_COMMON_MSR_Clean_Up(VOID *param)
+{
+	U32 dev_idx;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+	dev_idx = *((U32 *)param);
+	FOR_EACH_REG_ENTRY_UNC(pecb, dev_idx, i)
+	{
+		if (ECB_entries_clean_up_get(pecb, i)) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, i), 0LL);
+		}
+	}
+	END_FOR_EACH_REG_ENTRY_UNC;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
diff --git a/drivers/platform/x86/sepdk/sep/unc_gt.c b/drivers/platform/x86/sepdk/sep/unc_gt.c
new file mode 100755
index 000000000000..34e7650da94b
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_gt.c
@@ -0,0 +1,470 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/unc_common.h"
+#include "inc/utility.h"
+#include "inc/pci.h"
+#include "inc/unc_gt.h"
+
+extern U64 *read_counter_info;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+
+static U64 unc_gt_virtual_address;
+static SEP_MMIO_NODE unc_gt_map;
+static U32 unc_gt_rc6_reg1;
+static U32 unc_gt_rc6_reg2;
+static U32 unc_gt_clk_gt_reg1;
+static U32 unc_gt_clk_gt_reg2;
+static U32 unc_gt_clk_gt_reg3;
+static U32 unc_gt_clk_gt_reg4;
+
+/*!
+ * @fn          static VOID unc_gt_Write_PMU(VOID*)
+ *
+ * @brief       Initial write of PMU registers
+ *              Walk through the enties and write the value of the register accordingly.
+ *
+ * @param       device id
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_gt_Write_PMU(VOID *param)
+{
+	U32 dev_idx;
+	ECB pecb;
+	DRV_PCI_DEVICE_ENTRY_NODE dpden;
+	U64 device_id;
+	U32 vendor_id;
+	U64 bar_lo;
+	U32 offset_delta;
+	U32 tmp_value;
+	U32 this_cpu;
+	U32 value;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[0];
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+
+	dpden = ECB_pcidev_entry_node(pecb);
+	value = PCI_Read_U32(DRV_PCI_DEVICE_ENTRY_bus_no(&dpden),
+			     DRV_PCI_DEVICE_ENTRY_dev_no(&dpden),
+			     DRV_PCI_DEVICE_ENTRY_func_no(&dpden), 0);
+	vendor_id = DRV_GET_PCI_VENDOR_ID(value);
+	device_id = DRV_GET_PCI_DEVICE_ID(value);
+
+	if (DRV_IS_INTEL_VENDOR_ID(vendor_id) &&
+	    DRV_IS_GT_DEVICE_ID(device_id)) {
+		SEP_DRV_LOG_TRACE("Found Desktop GT.");
+	}
+
+	bar_lo = PCI_Read_U32(DRV_PCI_DEVICE_ENTRY_bus_no(&dpden),
+			      DRV_PCI_DEVICE_ENTRY_dev_no(&dpden),
+			      DRV_PCI_DEVICE_ENTRY_func_no(&dpden),
+			      DRV_PCI_DEVICE_ENTRY_bar_offset(&dpden));
+	bar_lo &= UNC_GT_BAR_MASK;
+
+	PCI_Map_Memory(&unc_gt_map, bar_lo, GT_MMIO_SIZE);
+	unc_gt_virtual_address = SEP_MMIO_NODE_virtual_address(&unc_gt_map);
+
+	FOR_EACH_PCI_DATA_REG_RAW(pecb, i, dev_idx)
+	{
+		offset_delta = ECB_entries_reg_offset(pecb, i);
+		// this is needed for overflow detection of the accumulators.
+		if (LWPMU_DEVICE_counter_mask(&devices[dev_idx]) == 0) {
+			LWPMU_DEVICE_counter_mask(&devices[dev_idx]) =
+				(U64)ECB_entries_max_bits(pecb, i);
+		}
+	}
+	END_FOR_EACH_PCI_CCCR_REG_RAW;
+
+	//enable the global control to clear the counter first
+	SYS_Write_MSR(PERF_GLOBAL_CTRL, ECB_entries_reg_value(pecb, 0));
+	FOR_EACH_PCI_CCCR_REG_RAW(pecb, i, dev_idx)
+	{
+		offset_delta = ECB_entries_reg_offset(pecb, i);
+		if (offset_delta == PERF_GLOBAL_CTRL) {
+			continue;
+		}
+		PCI_MMIO_Write_U32(unc_gt_virtual_address, offset_delta,
+				   GT_CLEAR_COUNTERS);
+
+		SEP_DRV_LOG_TRACE("CCCR offset delta is 0x%x W is clear ctrs.",
+				  offset_delta);
+	}
+	END_FOR_EACH_PCI_CCCR_REG_RAW;
+
+	//disable the counters
+	SYS_Write_MSR(PERF_GLOBAL_CTRL, 0LL);
+
+	FOR_EACH_PCI_CCCR_REG_RAW(pecb, i, dev_idx)
+	{
+		offset_delta = ECB_entries_reg_offset(pecb, i);
+		if (offset_delta == PERF_GLOBAL_CTRL) {
+			continue;
+		}
+		PCI_MMIO_Write_U32(unc_gt_virtual_address, offset_delta,
+				   ((U32)ECB_entries_reg_value(pecb, i)));
+		tmp_value =
+			PCI_MMIO_Read_U32(unc_gt_virtual_address, offset_delta);
+
+		// remove compiler warning on unused variables
+		if (tmp_value) {
+		}
+
+		SEP_DRV_LOG_TRACE(
+			"CCCR offset delta is 0x%x R is 0x%x W is 0x%llx.",
+			offset_delta, tmp_value,
+			ECB_entries_reg_value(pecb, i));
+	}
+	END_FOR_EACH_PCI_CCCR_REG_RAW;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          static VOID unc_gt_Disable_RC6_Clock_Gating(void)
+ *
+ * @brief       This snippet of code allows GT events to count by
+ *              disabling settings related to clock gating/power
+ * @param       none
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_gt_Disable_RC6_Clock_Gating(void)
+{
+	U32 tmp;
+
+	SEP_DRV_LOG_TRACE_IN("");
+
+	// Disable RC6
+	unc_gt_rc6_reg1 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_RC6_REG1);
+	tmp = unc_gt_rc6_reg1 | UNC_GT_RC6_REG1_OR_VALUE;
+	unc_gt_rc6_reg2 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_RC6_REG2);
+
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_RC6_REG2,
+			   UNC_GT_RC6_REG2_VALUE);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_RC6_REG1, tmp);
+
+	SEP_DRV_LOG_TRACE("Original value of RC6 rc6_1 = 0x%x, rc6_2 = 0x%x.",
+			  unc_gt_rc6_reg1, unc_gt_rc6_reg2);
+
+	// Disable clock gating
+	// Save
+	unc_gt_clk_gt_reg1 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG1);
+	unc_gt_clk_gt_reg2 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG2);
+	unc_gt_clk_gt_reg3 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG3);
+	unc_gt_clk_gt_reg4 =
+		PCI_MMIO_Read_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG4);
+
+	SEP_DRV_LOG_TRACE("Original value of RC6 ck_1 = 0x%x, ck_2 = 0x%x.",
+			  unc_gt_clk_gt_reg1, unc_gt_clk_gt_reg2);
+	SEP_DRV_LOG_TRACE("Original value of RC6 ck_3 = 0x%x, ck_4 = 0x%x.",
+			  unc_gt_clk_gt_reg3, unc_gt_clk_gt_reg4);
+
+	// Disable
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG1,
+			   UNC_GT_GCPUNIT_REG1_VALUE);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG2,
+			   UNC_GT_GCPUNIT_REG2_VALUE);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG3,
+			   UNC_GT_GCPUNIT_REG3_VALUE);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG4,
+			   UNC_GT_GCPUNIT_REG4_VALUE);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn          static VOID unc_gt_Restore_RC6_Clock_Gating(void)
+ *
+ * @brief       This snippet of code restores the system settings
+ *              for clock gating/power
+ * @param       none
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_gt_Restore_RC6_Clock_Gating(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_RC6_REG2,
+			   unc_gt_rc6_reg2);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_RC6_REG1,
+			   unc_gt_rc6_reg1);
+
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG1,
+			   unc_gt_clk_gt_reg1);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG2,
+			   unc_gt_clk_gt_reg2);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG3,
+			   unc_gt_clk_gt_reg3);
+	PCI_MMIO_Write_U32(unc_gt_virtual_address, UNC_GT_GCPUNIT_REG4,
+			   unc_gt_clk_gt_reg4);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID unc_gt_Enable_PMU(PVOID)
+ *
+ * @brief      Disable the clock gating and Set the global enable
+ *
+ * @param      device_id
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_gt_Enable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	ECB pecb;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[0];
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+
+	unc_gt_Disable_RC6_Clock_Gating();
+
+	if (pecb && GET_DRIVER_STATE() == DRV_STATE_RUNNING) {
+		SYS_Write_MSR(PERF_GLOBAL_CTRL, ECB_entries_reg_value(pecb, 0));
+		SEP_DRV_LOG_TRACE("Enabling GT Global control = 0x%llx.",
+				  ECB_entries_reg_value(pecb, 0));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+/*!
+ * @fn         static VOID unc_gt_Disable_PMU(PVOID)
+ *
+ * @brief      Unmap the virtual address when sampling/driver stops
+ *             and restore system values for clock gating settings
+ *
+ * @param      None
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_gt_Disable_PMU(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_TRACE_IN("Dummy param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+	unc_gt_Restore_RC6_Clock_Gating();
+
+	if (unc_gt_virtual_address &&
+	    (cur_driver_state == DRV_STATE_STOPPED ||
+	     cur_driver_state == DRV_STATE_PREPARE_STOP ||
+	     cur_driver_state == DRV_STATE_TERMINATING)) {
+		SYS_Write_MSR(PERF_GLOBAL_CTRL, 0LL);
+		PCI_Unmap_Memory(&unc_gt_map);
+		unc_gt_virtual_address = 0;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn unc_gt_Read_Counts(param, id)
+ *
+ * @param    param    The read thread node to process
+ * @param    id       The id refers to the device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param;
+ *
+ */
+static VOID unc_gt_Read_Counts(PVOID param, U32 id)
+{
+	U64 *data = (U64 *)param;
+	U32 cur_grp;
+	ECB pecb;
+	U32 offset_delta;
+	U32 tmp_value_lo = 0;
+	U32 tmp_value_hi = 0;
+	GT_CTR_NODE gt_ctr_value;
+	U32 this_cpu;
+	U32 package_num;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	// Write GroupID
+	data = (U64 *)((S8 *)data + ECB_group_offset(pecb));
+	*data = cur_grp + 1;
+	GT_CTR_NODE_value_reset(gt_ctr_value);
+
+	//Read in the counts into temporary buffe
+	FOR_EACH_PCI_DATA_REG_RAW(pecb, i, id)
+	{
+		offset_delta = ECB_entries_reg_offset(pecb, i);
+		tmp_value_lo =
+			PCI_MMIO_Read_U32(unc_gt_virtual_address, offset_delta);
+		offset_delta = offset_delta + NEXT_ADDR_OFFSET;
+		tmp_value_hi =
+			PCI_MMIO_Read_U32(unc_gt_virtual_address, offset_delta);
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, i));
+		GT_CTR_NODE_low(gt_ctr_value) = tmp_value_lo;
+		GT_CTR_NODE_high(gt_ctr_value) = tmp_value_hi;
+		*data = GT_CTR_NODE_value(gt_ctr_value);
+		SEP_DRV_LOG_TRACE("DATA offset delta is 0x%x R is 0x%llx.",
+				  offset_delta,
+				  GT_CTR_NODE_value(gt_ctr_value));
+	}
+	END_FOR_EACH_PCI_DATA_REG_RAW;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+static VOID unc_gt_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	// U32 cur_grp;
+	U32 offset_delta;
+	U32 tmp_value_lo = 0;
+	U32 tmp_value_hi = 0;
+	GT_CTR_NODE gt_ctr_value;
+	U32 package_num = 0;
+
+	SEP_DRV_LOG_DEBUG_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_DEBUG_OUT("Early exit (!system_master).");
+		return;
+	}
+
+	package_num = core_to_package_map[this_cpu];
+	// cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+
+	FOR_EACH_PCI_DATA_REG_RAW(pecb, i, dev_idx)
+	{
+		j = EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(
+			package_num,
+			EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+				emon_buffer_driver_helper),
+			ECB_entries_uncore_buffer_offset_in_package(pecb, i));
+		offset_delta = ECB_entries_reg_offset(pecb, i);
+		tmp_value_lo =
+			PCI_MMIO_Read_U32(unc_gt_virtual_address, offset_delta);
+		offset_delta = offset_delta + NEXT_ADDR_OFFSET;
+		tmp_value_hi =
+			PCI_MMIO_Read_U32(unc_gt_virtual_address, offset_delta);
+		GT_CTR_NODE_low(gt_ctr_value) = tmp_value_lo;
+		GT_CTR_NODE_high(gt_ctr_value) = tmp_value_hi;
+		buffer[j] = GT_CTR_NODE_value(gt_ctr_value);
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_PCI_DATA_REG_RAW;
+
+	SEP_DRV_LOG_DEBUG_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+
+DISPATCH_NODE unc_gt_dispatch = { .init = NULL,
+				  .fini = NULL,
+				  .write = unc_gt_Write_PMU,
+				  .freeze = unc_gt_Disable_PMU,
+				  .restart = unc_gt_Enable_PMU,
+				  .read_data = unc_gt_Read_PMU_Data,
+				  .check_overflow = NULL,
+				  .swap_group = NULL,
+				  .read_lbrs = NULL,
+				  .cleanup = NULL,
+				  .hw_errata = NULL,
+				  .read_power = NULL,
+				  .check_overflow_errata = NULL,
+				  .read_counts = unc_gt_Read_Counts,
+				  .check_overflow_gp_errata = NULL,
+				  .read_ro = NULL,
+				  .platform_info = NULL,
+				  .trigger_read = unc_gt_Read_Counts,
+				  .scan_for_uncore = NULL,
+				  .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/unc_mmio.c b/drivers/platform/x86/sepdk/sep/unc_mmio.c
new file mode 100755
index 000000000000..b1d997d0f405
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_mmio.c
@@ -0,0 +1,1083 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include <linux/pci.h>
+
+#include "lwpmudrv_defines.h"
+#include <linux/version.h>
+#include <linux/wait.h>
+#include <linux/fs.h>
+
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "lwpmudrv.h"
+#include "utility.h"
+#include "control.h"
+#include "unc_common.h"
+#include "ecb_iterators.h"
+#include "pebs.h"
+#include "inc/pci.h"
+
+extern U64 *read_counter_info;
+extern U64 *prev_counter_data;
+extern DRV_CONFIG drv_cfg;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+
+#define MASK_32BIT 0xffffffff
+#define MASK_64BIT 0xffffffff00000000ULL
+
+#define IS_MASTER(device_type, cpu)                                            \
+	(((device_type) == DRV_SINGLE_INSTANCE) ?                              \
+		 CPU_STATE_system_master(&pcb[cpu]) :                          \
+		 CPU_STATE_socket_master(&pcb[(cpu)]))
+#define GET_PACKAGE_NUM(device_type, cpu)                                      \
+	(((device_type) == DRV_SINGLE_INSTANCE) ? 0 : core_to_package_map[cpu])
+#define IS_64BIT(mask) (((mask) >> 32) != 0)
+
+#define EVENT_COUNTER_MAX_TRY 30
+
+struct FPGA_CONTROL_NODE_S {
+	union {
+		struct {
+			U64 rst_ctrs : 1;
+			U64 rsvd1 : 7;
+			U64 frz : 1;
+			U64 rsvd2 : 7;
+			U64 event_select : 4;
+			U64 port_id : 2;
+			U64 rsvd3 : 1;
+			U64 port_enable : 1;
+			U64 rsvd4 : 40;
+		} bits;
+		U64 bit_field;
+	} u;
+};
+
+static struct FPGA_CONTROL_NODE_S control_node;
+
+/*!
+ * @fn          static VOID unc_mmio_Write_PMU(VOID*)
+ *
+ * @brief       Initial write of PMU registers
+ *              Walk through the enties and write the value of the register accordingly.
+ *              When current_group = 0, then this is the first time this routine is called,
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_mmio_Write_PMU(VOID *param)
+{
+	U32 dev_idx;
+	U32 offset_delta = 0;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 event_id = 0;
+	U64 tmp_value = 0;
+	U32 this_cpu;
+	U32 package_num = 0;
+	U32 cur_grp;
+	ECB pecb;
+	U64 virtual_addr = 0;
+	U32 idx_w = 0;
+	U32 event_code = 0;
+	U32 counter = 0;
+	U32 entry = 0;
+	U32 dev_node = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	if (!IS_MASTER(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!is_master).");
+		return;
+	}
+
+	package_num =
+		GET_PACKAGE_NUM(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[(cur_grp)];
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+	entry = package_num;
+	if (!IS_MMIO_MAP_VALID(dev_node, entry)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (!IS_MMIO_MAP_VALID).");
+		return;
+	}
+
+	virtual_addr = virtual_address_table(dev_node, entry);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_WRITE)
+	{
+		PCI_MMIO_Write_U64(virtual_addr, ECB_entries_reg_id(pecb, idx),
+				   ECB_entries_reg_value(pecb, idx));
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	if (DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!event_based_counts).");
+		return;
+	}
+
+	idx_w = ECB_operations_register_start(pecb, PMU_OPERATION_WRITE);
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_reg_offset(pecb, idx) >
+		    DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+			    &ECB_pcidev_entry_node(pecb))) {
+			offset_delta =
+				ECB_entries_reg_offset(pecb, idx) -
+				DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+					&ECB_pcidev_entry_node(pecb));
+		} else {
+			offset_delta = ECB_entries_reg_offset(pecb, idx);
+		}
+
+		if ((DEV_UNC_CONFIG_device_type(pcfg_unc) ==
+		     DRV_SINGLE_INSTANCE) &&
+		    (GET_NUM_MAP_ENTRIES(dev_node) > 1)) {
+			// multiple MMIO mapping per <dev_no, func_no> device, find virtual_addr per mapping.
+			entry = ECB_entries_unit_id(pecb, idx);
+			virtual_addr = virtual_address_table(dev_node, entry);
+		}
+
+		if ((ECB_entries_counter_type(pecb, idx) ==
+		     PROG_FREERUN_COUNTER) &&
+		    (ECB_entries_unit_id(pecb, idx) == 0)) {
+			//Write event code before reading
+			PCI_MMIO_Write_U64(virtual_addr,
+					   ECB_entries_reg_id(pecb, idx_w),
+					   ECB_entries_reg_value(pecb, idx_w));
+			event_code = (U32)control_node.u.bits.event_select;
+			idx_w++;
+		}
+
+		// this is needed for overflow detection of the accumulators.
+		if (IS_64BIT((U64)(ECB_entries_max_bits(pecb, idx)))) {
+			if (ECB_entries_counter_type(pecb, idx) ==
+			    PROG_FREERUN_COUNTER) {
+				do {
+					if (counter > EVENT_COUNTER_MAX_TRY) {
+						break;
+					}
+					tmp_value = SYS_MMIO_Read64(
+						virtual_addr, offset_delta);
+					counter++;
+				} while (event_code != (tmp_value >> 60));
+			}
+			tmp_value = SYS_MMIO_Read64(virtual_addr, offset_delta);
+		} else {
+			tmp_value = SYS_MMIO_Read32(virtual_addr, offset_delta);
+		}
+		tmp_value &= (U64)ECB_entries_max_bits(pecb, idx);
+
+		LWPMU_DEVICE_prev_value(
+			&devices[dev_idx])[package_num][event_id] = tmp_value;
+		SEP_DRV_LOG_TRACE(
+			"unc_mmio_Write_PMU: cpu[%d], device[%d], package[%d], entry %d, event_id %d, value %llu\n",
+			this_cpu, dev_idx, package_num, entry, event_id,
+			tmp_value);
+		event_id++;
+
+		if (LWPMU_DEVICE_counter_mask(&devices[dev_idx]) == 0) {
+			LWPMU_DEVICE_counter_mask(&devices[dev_idx]) =
+				(U64)ECB_entries_max_bits(pecb, idx);
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+	SEP_DRV_LOG_TRACE(
+		"BAR address is 0x%llx and virt is 0x%llx.",
+		DRV_PCI_DEVICE_ENTRY_bar_address(&ECB_pcidev_entry_node(pecb)),
+		virtual_addr);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID unc_mmio_Enable_PMU(PVOID)
+ *
+ * @brief      Capture the previous values to calculate delta later.
+ *
+ * @param      None
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static void unc_mmio_Enable_PMU(PVOID param)
+{
+	U32 j;
+	U64 *buffer = prev_counter_data;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 package_num;
+	U32 offset_delta;
+	U32 cur_grp;
+	ECB pecb;
+	U64 virtual_addr = 0;
+	U64 reg_val = 0;
+	U32 idx_w = 0;
+	U32 event_code = 0;
+	U32 counter = 0;
+	// U32 num_events = 0;
+	U32 entry = 0;
+	// U32 num_pkgs = num_packages;
+	U32 dev_node = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	if (!IS_MASTER(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!IS_MASTER).");
+		return;
+	}
+
+	package_num =
+		GET_PACKAGE_NUM(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[(cur_grp)];
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+	entry = package_num;
+	if (!IS_MMIO_MAP_VALID(dev_node, entry)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (!IS_MMIO_MAP_VALID).");
+		return;
+	}
+
+	// if (DEV_UNC_CONFIG_device_type(pcfg_unc) == DRV_SINGLE_INSTANCE) {
+	// 	num_pkgs = 1;
+	// }
+
+	virtual_addr = virtual_address_table(dev_node, entry);
+
+	// NOTE THAT the enable function currently captures previous values
+	// for EMON collection to avoid unnecessary memory copy.
+	if (DRV_CONFIG_emon_mode(drv_cfg)) {
+		// num_events = ECB_num_events(pecb);
+		idx_w = ECB_operations_register_start(pecb,
+						      PMU_OPERATION_WRITE);
+		FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx,
+					   PMU_OPERATION_READ)
+		{
+			if (ECB_entries_reg_offset(pecb, idx) >
+			    DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+				    &ECB_pcidev_entry_node(pecb))) {
+				offset_delta =
+					ECB_entries_reg_offset(pecb, idx) -
+					DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+						&ECB_pcidev_entry_node(pecb));
+			} else {
+				offset_delta =
+					ECB_entries_reg_offset(pecb, idx);
+			}
+
+			if ((DEV_UNC_CONFIG_device_type(pcfg_unc) ==
+			     DRV_SINGLE_INSTANCE) &&
+			    (GET_NUM_MAP_ENTRIES(dev_node) > 1)) {
+				// multiple MMIO mapping per <dev_no, func_no> device, find virtual_addr per mapping.
+				entry = ECB_entries_unit_id(pecb, idx);
+				virtual_addr =
+					virtual_address_table(dev_node, entry);
+			}
+
+			if ((ECB_entries_counter_type(pecb, idx) ==
+			     PROG_FREERUN_COUNTER) &&
+			    (ECB_entries_unit_id(pecb, idx) == 0)) {
+				PCI_MMIO_Write_U64(
+					virtual_addr,
+					ECB_entries_reg_id(pecb, idx_w),
+					ECB_entries_reg_value(pecb, idx_w));
+				control_node.u.bit_field =
+					ECB_entries_reg_value(pecb, idx_w);
+				event_code =
+					(U32)control_node.u.bits.event_select;
+				idx_w++;
+			}
+
+			if ((ECB_entries_event_scope(pecb, idx) ==
+			     PACKAGE_EVENT) ||
+			    (ECB_entries_event_scope(pecb, idx) ==
+			     SYSTEM_EVENT)) {
+				if (ECB_entries_event_scope(pecb, idx) ==
+				    SYSTEM_EVENT) {
+					j = ECB_entries_uncore_buffer_offset_in_system(
+						pecb, idx);
+				} else {
+					j = EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(
+						package_num,
+						EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+							emon_buffer_driver_helper),
+						ECB_entries_uncore_buffer_offset_in_package(
+							pecb, idx));
+				}
+
+				if (IS_64BIT((U64)(
+					    ECB_entries_max_bits(pecb, idx)))) {
+					if (ECB_entries_counter_type(pecb,
+								     idx) ==
+					    PROG_FREERUN_COUNTER) {
+						do {
+							if (counter >
+							    EVENT_COUNTER_MAX_TRY) {
+								break;
+							}
+							buffer[j] = SYS_MMIO_Read64(
+								virtual_addr,
+								offset_delta);
+							counter++;
+						} while (event_code !=
+							 (buffer[j] >> 60));
+					}
+					buffer[j] = SYS_MMIO_Read64(
+						virtual_addr, offset_delta);
+				} else {
+					buffer[j] = SYS_MMIO_Read32(
+						virtual_addr, offset_delta);
+				}
+				buffer[j] &=
+					(U64)ECB_entries_max_bits(pecb, idx);
+				SEP_DRV_LOG_TRACE(
+					"j=%u, value=%llu, cpu=%u, MSR=0x%x", j,
+					buffer[j], this_cpu,
+					ECB_entries_reg_id(pecb, idx));
+			}
+		}
+		END_FOR_EACH_REG_UNC_OPERATION;
+	}
+	virtual_addr = virtual_address_table(dev_node, entry);
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_ENABLE)
+	{
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = PCI_MMIO_Read_U64(
+				virtual_addr, ECB_entries_reg_id(pecb, idx));
+			reg_val &= ECB_entries_reg_value(pecb, idx);
+			PCI_MMIO_Write_U64(virtual_addr,
+					   ECB_entries_reg_id(pecb, idx),
+					   reg_val);
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID unc_mmio_Disable_PMU(PVOID)
+ *
+ * @brief      Unmap the virtual address when you stop sampling.
+ *
+ * @param      None
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static void unc_mmio_Disable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	U64 virtual_addr = 0;
+	U64 reg_val = 0;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 package_num;
+	U32 dev_node = 0;
+	U32 cur_grp = 0;
+	ECB pecb;
+	U32 entry = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	if (!IS_MASTER(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!IS_MASTER).");
+		return;
+	}
+
+	package_num =
+		GET_PACKAGE_NUM(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[dev_idx])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[(cur_grp)];
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+	entry = package_num;
+	if (!IS_MMIO_MAP_VALID(dev_node, entry)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (!IS_MMIO_MAP_VALID).");
+		return;
+	}
+
+	virtual_addr = virtual_address_table(dev_node, entry);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_DISABLE)
+	{
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = PCI_MMIO_Read_U64(
+				virtual_addr, ECB_entries_reg_id(pecb, idx));
+			reg_val |= ECB_entries_reg_value(pecb, idx);
+			PCI_MMIO_Write_U64(virtual_addr,
+					   ECB_entries_reg_id(pecb, idx),
+					   reg_val);
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       void unc_mmio_Trigger_Read(id)
+ *
+ * @param    id       Device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore data from counters and store into buffer
+ */
+static VOID unc_mmio_Trigger_Read(PVOID param, U32 id)
+{
+	U32 this_cpu;
+	U32 cur_grp;
+	ECB pecb;
+	U32 index = 0;
+	U64 diff = 0;
+	U32 offset_delta = 0;
+	U64 value = 0ULL;
+	U64 *data;
+	U64 virtual_addr = 0;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 package_num;
+	U32 idx_w = 0;
+	U32 event_code = 0;
+	U32 counter = 0;
+	U32 entry = 0;
+	U32 dev_node = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[id]);
+	if (!IS_MASTER(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!IS_MASTER).");
+		return;
+	}
+
+	package_num =
+		GET_PACKAGE_NUM(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[(cur_grp)];
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+	entry = package_num;
+	if (!IS_MMIO_MAP_VALID(dev_node, entry)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (!IS_MMIO_MAP_VALID).");
+		return;
+	}
+
+	virtual_addr = virtual_address_table(dev_node, entry);
+
+	// Write GroupID
+	data = (U64 *)((S8 *)param + ECB_group_offset(pecb));
+	*data = cur_grp + 1;
+	//Read in the counts into temporary buffer
+	idx_w = ECB_operations_register_start(pecb, PMU_OPERATION_WRITE);
+	FOR_EACH_REG_UNC_OPERATION(pecb, id, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_reg_offset(pecb, idx) >
+		    DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+			    &ECB_pcidev_entry_node(pecb))) {
+			offset_delta =
+				ECB_entries_reg_offset(pecb, idx) -
+				DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+					&ECB_pcidev_entry_node(pecb));
+		} else {
+			offset_delta = ECB_entries_reg_offset(pecb, idx);
+		}
+
+		if ((DEV_UNC_CONFIG_device_type(pcfg_unc) ==
+		     DRV_SINGLE_INSTANCE) &&
+		    (GET_NUM_MAP_ENTRIES(dev_node) > 1)) {
+			// multiple MMIO mapping per <dev_no, func_no> device
+			entry = ECB_entries_unit_id(pecb, idx);
+			virtual_addr = virtual_address_table(dev_node, entry);
+		}
+
+		if ((ECB_entries_counter_type(pecb, idx) ==
+		     PROG_FREERUN_COUNTER) &&
+		    (ECB_entries_unit_id(pecb, idx) == 0)) {
+			PCI_MMIO_Write_U64(virtual_addr,
+					   ECB_entries_reg_id(pecb, idx_w),
+					   ECB_entries_reg_value(pecb, idx_w));
+			control_node.u.bit_field =
+				ECB_entries_reg_value(pecb, idx_w);
+			event_code = (U32)control_node.u.bits.event_select;
+			idx_w++;
+		}
+
+		if (IS_64BIT((U64)(ECB_entries_max_bits(pecb, idx)))) {
+			if (ECB_entries_counter_type(pecb, idx) ==
+			    PROG_FREERUN_COUNTER) {
+				do {
+					if (counter > EVENT_COUNTER_MAX_TRY) {
+						break;
+					}
+					value = SYS_MMIO_Read64(virtual_addr,
+								offset_delta);
+					counter++;
+				} while (event_code != (value >> 60));
+			}
+			value = SYS_MMIO_Read64(virtual_addr, offset_delta);
+		} else {
+			value = SYS_MMIO_Read32((volatile unsigned int *)virtual_addr, offset_delta);
+		}
+		value &= (U64)ECB_entries_max_bits(pecb, idx);
+
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, idx));
+		//check for overflow if not a static counter
+		if (ECB_entries_counter_type(pecb, idx) == STATIC_COUNTER) {
+			*data = value;
+		} else {
+			if (value < LWPMU_DEVICE_prev_value(
+					    &devices[id])[package_num][index]) {
+				diff = LWPMU_DEVICE_counter_mask(&devices[id]) -
+				       LWPMU_DEVICE_prev_value(
+					       &devices[id])[package_num][index];
+				diff += value;
+			} else {
+				diff = value -
+				       LWPMU_DEVICE_prev_value(
+					       &devices[id])[package_num][index];
+			}
+			LWPMU_DEVICE_acc_value(
+				&devices[id])[package_num][cur_grp][index] +=
+				diff;
+			LWPMU_DEVICE_prev_value(
+				&devices[id])[package_num][index] = value;
+			*data = LWPMU_DEVICE_acc_value(
+				&devices[id])[package_num][cur_grp][index];
+		}
+		index++;
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_mmio_Read_PMU_Data(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read all the data MSR's into a buffer.  Called by the interrupt handler.
+ *
+ */
+static VOID unc_mmio_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U64 *prev_buffer = prev_counter_data;
+	U32 this_cpu;
+	U32 dev_idx;
+	DEV_UNC_CONFIG pcfg_unc;
+	U32 offset_delta;
+	U32 cur_grp;
+	ECB pecb;
+	U64 tmp_value = 0ULL;
+	U64 virtual_addr = 0;
+	U32 idx_w = 0;
+	U32 event_code = 0;
+	U32 counter = 0;
+	// U32 num_events = 0;
+	U32 package_num;
+	U32 entry = 0;
+	U32 dev_node = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcfg_unc = (DEV_UNC_CONFIG)LWPMU_DEVICE_pcfg(&devices[dev_idx]);
+	if (!IS_MASTER(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!IS_MASTER).");
+		return;
+	}
+
+	package_num =
+		GET_PACKAGE_NUM(DEV_UNC_CONFIG_device_type(pcfg_unc), this_cpu);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[(cur_grp)];
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+	entry = package_num;
+	if (!IS_MMIO_MAP_VALID(dev_node, entry)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (!IS_MMIO_MAP_VALID).");
+		return;
+	}
+
+	virtual_addr = virtual_address_table(dev_node, entry);
+
+	// num_events = ECB_num_events(pecb);
+
+	idx_w = ECB_operations_register_start(pecb, PMU_OPERATION_WRITE);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_reg_offset(pecb, idx) >
+		    DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+			    &ECB_pcidev_entry_node(pecb))) {
+			offset_delta =
+				ECB_entries_reg_offset(pecb, idx) -
+				DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+					&ECB_pcidev_entry_node(pecb));
+		} else {
+			offset_delta = ECB_entries_reg_offset(pecb, idx);
+		}
+
+		if ((DEV_UNC_CONFIG_device_type(pcfg_unc) ==
+		     DRV_SINGLE_INSTANCE) &&
+		    (GET_NUM_MAP_ENTRIES(dev_node) > 1)) {
+			// multiple MMIO mapping per <dev_no, func_no> device, find virtual_addr per mapping.
+			entry = ECB_entries_unit_id(pecb, idx);
+			virtual_addr = virtual_address_table(dev_node, entry);
+		}
+
+		if ((ECB_entries_counter_type(pecb, idx) ==
+		     PROG_FREERUN_COUNTER) &&
+		    (ECB_entries_unit_id(pecb, idx) == 0)) {
+			PCI_MMIO_Write_U64(virtual_addr,
+					   ECB_entries_reg_id(pecb, idx_w),
+					   ECB_entries_reg_value(pecb, idx_w));
+			control_node.u.bit_field =
+				ECB_entries_reg_value(pecb, idx_w);
+			event_code = (U32)control_node.u.bits.event_select;
+			idx_w++;
+		}
+
+		if ((ECB_entries_event_scope(pecb, idx) == PACKAGE_EVENT) ||
+		    (ECB_entries_event_scope(pecb, idx) == SYSTEM_EVENT)) {
+			if (ECB_entries_event_scope(pecb, idx) ==
+			    SYSTEM_EVENT) {
+				j = ECB_entries_uncore_buffer_offset_in_system(
+					pecb, idx);
+			} else {
+				j = EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(
+					package_num,
+					EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+						emon_buffer_driver_helper),
+					ECB_entries_uncore_buffer_offset_in_package(
+						pecb, idx));
+			}
+
+			if (IS_64BIT((U64)(ECB_entries_max_bits(pecb, idx)))) {
+				if (ECB_entries_counter_type(pecb, idx) ==
+				    PROG_FREERUN_COUNTER) {
+					do {
+						if (counter >
+						    EVENT_COUNTER_MAX_TRY) {
+							break;
+						}
+						tmp_value = SYS_MMIO_Read64(
+							virtual_addr,
+							offset_delta);
+						counter++;
+					} while (event_code !=
+						 (tmp_value >> 60));
+				}
+				tmp_value = SYS_MMIO_Read64(virtual_addr,
+							    offset_delta);
+			} else {
+				tmp_value = SYS_MMIO_Read32(virtual_addr,
+							    offset_delta);
+			}
+			tmp_value &= (U64)ECB_entries_max_bits(pecb, idx);
+			if (ECB_entries_counter_type(pecb, idx) ==
+			    STATIC_COUNTER) {
+				buffer[j] = tmp_value;
+			} else {
+				if (tmp_value >= prev_buffer[j]) {
+					buffer[j] = tmp_value - prev_buffer[j];
+				} else {
+					buffer[j] = tmp_value +
+						    (ECB_entries_max_bits(pecb,
+									  idx) -
+						     prev_buffer[j]);
+				}
+			}
+			SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u, MSR=0x%x",
+					  j, buffer[j], this_cpu,
+					  ECB_entries_reg_id(pecb, idx));
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_mmio_Initialize(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Do the mapping of the physical address (to do the invalidates in the TLB)
+ *           NOTE: this should never be done with SMP call
+ *
+ */
+static VOID unc_mmio_Initialize(PVOID param)
+{
+	DRV_PCI_DEVICE_ENTRY_NODE dpden;
+
+	U64 bar;
+
+	U64 physical_address;
+	U32 dev_idx = 0;
+	U32 cur_grp = 0;
+	ECB pecb = NULL;
+	U32 dev_node;
+	U32 i = 0;
+	U32 page_len = 4096; // 4K
+
+	U32 use_default_busno = 0;
+	U32 entries = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[0];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+	dev_node = ECB_dev_node(pecb);
+
+	if (IS_MMIO_MAP_VALID(dev_node, 0)) {
+		SEP_DRV_LOG_INIT_TRACE_OUT(
+			"Early exit (device[%d] node %d already mapped).",
+			dev_idx, dev_node);
+		return;
+	}
+
+	dpden = ECB_pcidev_entry_node(pecb);
+
+	// use busno found from topology scan if available
+	// otherwise use the default one
+	entries = GET_NUM_MAP_ENTRIES(dev_node);
+	if (entries == 0) {
+		use_default_busno = 1;
+		entries = 1; // this could the client, does not through the scan
+		UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node])) = 1;
+		UNC_PCIDEV_max_entries(&(unc_pcidev_map[dev_node])) = 1;
+	}
+	if (!UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node]))) {
+		// it is better to allocate space in the beginning
+		UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])) =
+			CONTROL_Allocate_Memory(entries *
+						sizeof(SEP_MMIO_NODE));
+		if (UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])) == NULL) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (No Memory).");
+			return;
+		}
+		memset(UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])), 0,
+		       entries * sizeof(U64));
+	}
+	for (i = 0; i < entries; i++) {
+		if (!use_default_busno) {
+			if (IS_BUS_MAP_VALID(dev_node, i)) {
+				DRV_PCI_DEVICE_ENTRY_bus_no(&dpden) =
+					UNC_PCIDEV_busno_entry(
+						&(unc_pcidev_map[dev_node]), i);
+			}
+		}
+
+		bar = PCI_Read_U64(DRV_PCI_DEVICE_ENTRY_bus_no(&dpden),
+				   DRV_PCI_DEVICE_ENTRY_dev_no(&dpden),
+				   DRV_PCI_DEVICE_ENTRY_func_no(&dpden),
+				   DRV_PCI_DEVICE_ENTRY_bar_offset(&dpden));
+
+		bar &= DRV_PCI_DEVICE_ENTRY_bar_mask(&dpden);
+
+		DRV_PCI_DEVICE_ENTRY_bar_address(&ECB_pcidev_entry_node(pecb)) =
+			bar;
+		physical_address = DRV_PCI_DEVICE_ENTRY_bar_address(
+					   &ECB_pcidev_entry_node(pecb)) +
+				   DRV_PCI_DEVICE_ENTRY_base_offset_for_mmio(
+					   &ECB_pcidev_entry_node(pecb));
+
+		PCI_Map_Memory(&UNC_PCIDEV_mmio_map_entry(
+				       &(unc_pcidev_map[dev_node]), i),
+			       physical_address, page_len);
+	}
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_mmio_fpga_Initialize(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Do the mapping of the physical address (to do the invalidates in the TLB)
+ *           NOTE: this should never be done with SMP call
+ *
+ */
+static VOID unc_mmio_fpga_Initialize(PVOID param)
+{
+#if defined(DRV_EM64T)
+	U64 phys_addr;
+	SEP_MMIO_NODE tmp_map = { 0 };
+	U64 virt_addr;
+	U64 dfh;
+	U32 id;
+	U32 offset = 0;
+	S32 next_offset = -1;
+	U32 dev_idx;
+	U32 cur_grp;
+	ECB pecb;
+	U32 bus_list[2] = { 0x5e, 0xbe };
+	U32 busno;
+	U32 page_len = 4096;
+	U32 package_num = 0;
+	U32 dev_node = 0;
+	U32 entries = 0;
+	DRV_PCI_DEVICE_ENTRY_NODE dpden;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[0];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	dev_node = ECB_dev_node(pecb);
+
+	entries = GET_NUM_MAP_ENTRIES(dev_node);
+	if (entries == 0) {
+		entries = num_packages;
+	}
+
+	if (!UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node]))) {
+		// it is better to allocate space in the beginning
+		UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])) =
+			CONTROL_Allocate_Memory(entries *
+						sizeof(SEP_MMIO_NODE));
+		if (UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])) == NULL) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT("Early exit (No Memory).");
+			return;
+		}
+		memset(UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node])), 0,
+		       (entries * sizeof(SEP_MMIO_NODE)));
+		UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node])) = 0;
+		UNC_PCIDEV_max_entries(&(unc_pcidev_map[dev_node])) = entries;
+	} else {
+		if (virtual_address_table(dev_node, 0) != 0) {
+			SEP_DRV_LOG_INIT_TRACE_OUT(
+				"Early exit (device[%d] node %d already mapped).",
+				dev_idx, dev_node);
+			return;
+		}
+	}
+
+	dpden = ECB_pcidev_entry_node(pecb);
+
+	for (package_num = 0; package_num < num_packages; package_num++) {
+		if (package_num < 2) {
+			busno = bus_list[package_num];
+		} else {
+			busno = 0;
+		}
+		phys_addr =
+			PCI_Read_U64(busno, DRV_PCI_DEVICE_ENTRY_dev_no(&dpden),
+				     DRV_PCI_DEVICE_ENTRY_func_no(&dpden),
+				     DRV_PCI_DEVICE_ENTRY_bar_offset(&dpden));
+		phys_addr &= DRV_PCI_DEVICE_ENTRY_bar_mask(&dpden);
+		if (package_num == 0) {
+			PCI_Map_Memory(&tmp_map, phys_addr, 8 * page_len);
+			virt_addr = SEP_MMIO_NODE_virtual_address(&tmp_map);
+			while (next_offset != 0) {
+				dfh = SYS_MMIO_Read64((U64)virt_addr, offset);
+				next_offset = (U32)((dfh >> 16) & 0xffffff);
+				id = (U32)(dfh & 0xfff);
+				if (offset &&
+				    (id ==
+				     DRV_PCI_DEVICE_ENTRY_feature_id(&dpden))) {
+					break;
+				}
+				offset += next_offset;
+			}
+			PCI_Unmap_Memory(&tmp_map);
+		}
+		phys_addr += offset;
+		PCI_Map_Memory(
+			&UNC_PCIDEV_mmio_map_entry(&(unc_pcidev_map[dev_node]),
+						   package_num),
+			phys_addr, 8 * page_len);
+		UNC_PCIDEV_num_entries(&(unc_pcidev_map[dev_node]))++;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_mmio_Destroy(param)
+ *
+ * @param    param    dummy parameter which is not used
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Invalidate the entry in TLB of the physical address
+ *           NOTE: this should never be done with SMP call
+ *
+ */
+static VOID unc_mmio_Destroy(PVOID param)
+{
+	U32 dev_idx;
+	U32 i;
+	U64 addr = 0;
+	U32 cur_grp = 0;
+	U32 dev_node = 0;
+	U32 entries = 0;
+	ECB pecb;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[0];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[dev_idx])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+	dev_node = ECB_dev_node(pecb);
+
+	if (!UNC_PCIDEV_mmio_map(&(unc_pcidev_map[dev_node]))) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (no mapping).");
+		return;
+	}
+
+	entries = GET_NUM_MAP_ENTRIES(dev_node);
+
+	for (i = 0; i < entries; i++) {
+		addr = virtual_address_table(dev_node, i);
+		if (addr) {
+			PCI_Unmap_Memory(&UNC_PCIDEV_mmio_map_entry(
+				&(unc_pcidev_map[dev_node]), i));
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+DISPATCH_NODE unc_mmio_dispatch = { .init = unc_mmio_Initialize,
+				    .fini = unc_mmio_Destroy,
+				    .write = unc_mmio_Write_PMU,
+				    .freeze = unc_mmio_Disable_PMU,
+				    .restart = unc_mmio_Enable_PMU,
+				    .read_data = unc_mmio_Read_PMU_Data,
+				    .check_overflow = NULL,
+				    .swap_group = NULL,
+				    .read_lbrs = NULL,
+				    .cleanup = UNC_COMMON_Dummy_Func,
+				    .hw_errata = NULL,
+				    .read_power = NULL,
+				    .check_overflow_errata = NULL,
+				    .read_counts = NULL,
+				    .check_overflow_gp_errata = NULL,
+				    .read_ro = NULL,
+				    .platform_info = NULL,
+				    .trigger_read = unc_mmio_Trigger_Read,
+				    .scan_for_uncore = NULL,
+				    .read_metrics = NULL };
+
+DISPATCH_NODE unc_mmio_fpga_dispatch = { .init = unc_mmio_fpga_Initialize,
+					 .fini = unc_mmio_Destroy,
+					 .write = unc_mmio_Write_PMU,
+					 .freeze = unc_mmio_Disable_PMU,
+					 .restart = unc_mmio_Enable_PMU,
+					 .read_data = unc_mmio_Read_PMU_Data,
+					 .check_overflow = NULL,
+					 .swap_group = NULL,
+					 .read_lbrs = NULL,
+					 .cleanup = UNC_COMMON_Dummy_Func,
+					 .hw_errata = NULL,
+					 .read_power = NULL,
+					 .check_overflow_errata = NULL,
+					 .read_counts = NULL,
+					 .check_overflow_gp_errata = NULL,
+					 .read_ro = NULL,
+					 .platform_info = NULL,
+					 .trigger_read = unc_mmio_Trigger_Read,
+					 .scan_for_uncore = NULL,
+					 .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/unc_msr.c b/drivers/platform/x86/sepdk/sep/unc_msr.c
new file mode 100755
index 000000000000..ce144203dc39
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_msr.c
@@ -0,0 +1,347 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/unc_common.h"
+#include "inc/utility.h"
+
+extern U64 *read_counter_info;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+extern DRV_CONFIG drv_cfg;
+
+/*!
+ * @fn          static VOID UNC_COMMON_MSR_Write_PMU(VOID*)
+ *
+ * @brief       Initial write of PMU registers
+ * Walk through the enties and write the value of the register accordingly.
+ * When current_group = 0, then this is the first time this routine is called,
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID UNC_MSR_Write_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_WRITE)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, idx),
+			      ECB_entries_reg_value(pecb, idx));
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, idx), 0ULL);
+		if (LWPMU_DEVICE_counter_mask(&devices[dev_idx]) == 0) {
+			LWPMU_DEVICE_counter_mask(&devices[dev_idx]) =
+				(U64)ECB_entries_max_bits(pecb, idx);
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID UNC_MSR_Enable_PMU(PVOID)
+ *
+ * @brief      Set the enable bit for all the evsel registers
+ *
+ * @param      None
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID UNC_MSR_Enable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U64 reg_val = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_ENABLE)
+	{
+		reg_val = ECB_entries_reg_value(pecb, idx);
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+			if (ECB_entries_reg_type(pecb, idx) ==
+			    PMU_REG_UNIT_CTRL) {
+				reg_val &= ECB_entries_reg_value(pecb, idx);
+			} else {
+				reg_val |= ECB_entries_reg_value(pecb, idx);
+			}
+		}
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, idx), reg_val);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID UNC_MSR_Disable_PMU(PVOID)
+ *
+ * @brief      Set the enable bit for all the evsel registers
+ *
+ * @param      None
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID UNC_MSR_Disable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U64 reg_val = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_DISABLE)
+	{
+		reg_val = ECB_entries_reg_value(pecb, idx);
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+			if (ECB_entries_reg_type(pecb, idx) ==
+			    PMU_REG_UNIT_CTRL) {
+				reg_val |= ECB_entries_reg_value(pecb, idx);
+			} else {
+				reg_val &= ECB_entries_reg_value(pecb, idx);
+			}
+		}
+		SYS_Write_MSR(ECB_entries_reg_id(pecb, idx), reg_val);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn static VOID UNC_MSR_Read_PMU_Data(param)
+ *
+ * @param    param    The read thread node to process
+ * @param    id       The id refers to the device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer
+ * Let us say we have 2 core events in a dual socket JKTN;
+ * The start_index will be at 32 as it will 2 events in 16 CPU per socket
+ * The position for first event of QPI will be computed based on its event
+ *
+ */
+static VOID UNC_MSR_Read_PMU_Data(PVOID param)
+{
+	U32 j = 0;
+	U32 dev_idx;
+	U32 this_cpu;
+	U32 package_num = 0;
+	U64 *buffer;
+	CPU_STATE pcpu;
+	U32 cur_grp;
+	ECB pecb;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	buffer = read_counter_info;
+	pcpu = &pcb[this_cpu];
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[cur_grp];
+
+	// NOTE THAT the read_pmu function on for EMON collection.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	//Read in the counts into temporary buffer
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_event_scope(pecb, idx) == SYSTEM_EVENT) {
+			j = ECB_entries_uncore_buffer_offset_in_system(pecb,
+								       idx);
+		} else {
+			j = EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(
+				package_num,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				ECB_entries_uncore_buffer_offset_in_package(
+					pecb, idx));
+		}
+
+		buffer[j] = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u, event_id=%u", j,
+				  buffer[j], this_cpu,
+				  ECB_entries_core_event_id(pecb, idx));
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       static VOID UNC_MSR_Trigger_Read(id)
+ *
+ * @param    id       Device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore data from counters and store into buffer
+ */
+static VOID UNC_MSR_Trigger_Read(PVOID param, U32 id)
+{
+	U32 this_cpu;
+	U32 package_num;
+	U32 cur_grp;
+	ECB pecb;
+	U32 index = 0;
+	U64 diff = 0;
+	U64 value;
+	U64 *data;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	// Write GroupID
+	data = (U64 *)((S8 *)param + ECB_group_offset(pecb));
+	*data = cur_grp + 1;
+	//Read in the counts into uncore buffer
+	FOR_EACH_REG_UNC_OPERATION(pecb, id, idx, PMU_OPERATION_READ)
+	{
+		value = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+		//check for overflow
+		if (value <
+		    LWPMU_DEVICE_prev_value(&devices[id])[package_num][index]) {
+			diff = LWPMU_DEVICE_counter_mask(&devices[id]) -
+			       LWPMU_DEVICE_prev_value(
+				       &devices[id])[package_num][index];
+			diff += value;
+		} else {
+			diff = value -
+			       LWPMU_DEVICE_prev_value(
+				       &devices[id])[package_num][index];
+		}
+		LWPMU_DEVICE_acc_value(
+			&devices[id])[package_num][cur_grp][index] += diff;
+		LWPMU_DEVICE_prev_value(&devices[id])[package_num][index] =
+			value;
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, idx));
+		*data = LWPMU_DEVICE_acc_value(
+			&devices[id])[package_num][cur_grp][index];
+		index++;
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+
+DISPATCH_NODE unc_msr_dispatch = { .init = NULL,
+				   .fini = NULL,
+				   .write = UNC_MSR_Write_PMU,
+				   .freeze = UNC_MSR_Disable_PMU,
+				   .restart = UNC_MSR_Enable_PMU,
+				   .read_data = UNC_MSR_Read_PMU_Data,
+				   .check_overflow = NULL,
+				   .swap_group = NULL,
+				   .read_lbrs = NULL,
+				   .cleanup = UNC_COMMON_MSR_Clean_Up,
+				   .hw_errata = NULL,
+				   .read_power = NULL,
+				   .check_overflow_errata = NULL,
+				   .read_counts = NULL,
+				   .check_overflow_gp_errata = NULL,
+				   .read_ro = NULL,
+				   .platform_info = NULL,
+				   .trigger_read = UNC_MSR_Trigger_Read,
+				   .scan_for_uncore = NULL,
+				   .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/unc_pci.c b/drivers/platform/x86/sepdk/sep/unc_pci.c
new file mode 100755
index 000000000000..e338556f8b34
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_pci.c
@@ -0,0 +1,491 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/unc_common.h"
+#include "inc/utility.h"
+#include "inc/pci.h"
+
+extern U64 *read_counter_info;
+extern UNCORE_TOPOLOGY_INFO_NODE uncore_topology;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+extern DRV_CONFIG drv_cfg;
+
+/*!
+ * @fn          static VOID unc_pci_Write_PMU(VOID*)
+ *
+ * @brief       Initial write of PMU registers
+ *              Walk through the enties and write the value of the register accordingly.
+ *              When current_group = 0, then this is the first time this routine is called,
+ *
+ * @param       None
+ *
+ * @return      None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_pci_Write_PMU(PVOID param)
+{
+	U32 device_id;
+	U32 dev_idx;
+	U32 value;
+	U32 vendor_id;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_num = 0;
+	U32 dev_node = 0;
+	U32 cur_grp;
+	ECB pecb;
+	U32 busno;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[cur_grp];
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	// first, figure out which package maps to which bus
+	dev_node = ECB_dev_node(pecb);
+	if (!IS_BUS_MAP_VALID(dev_node, package_num)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("No UNC_PCIDEV bus map for %u!",
+					    dev_node);
+		return;
+	}
+
+	busno = GET_BUS_MAP(dev_node, package_num);
+
+	LWPMU_DEVICE_pci_dev_node_index(&devices[dev_idx]) = dev_node;
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_WRITE)
+	{
+		if (ECB_entries_reg_type(pecb, idx) == PMU_REG_GLOBAL_CTRL) {
+			//Check if we need to zero this MSR out
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, idx), 0LL);
+			continue;
+		}
+
+		// otherwise, we have a valid entry
+		// now we just need to find the corresponding bus #
+		ECB_entries_bus_no(pecb, idx) = busno;
+		value = PCI_Read_U32(busno, ECB_entries_dev_no(pecb, idx),
+				     ECB_entries_func_no(pecb, idx), 0);
+
+		CONTINUE_IF_NOT_GENUINE_INTEL_DEVICE(value, vendor_id,
+						     device_id);
+		SEP_DRV_LOG_TRACE("Uncore device ID = 0x%x.",
+				  device_id);
+
+		if (ECB_entries_reg_type(pecb, idx) == PMU_REG_UNIT_CTRL) {
+			// busno can not be stored in ECB because different sockets have different bus no.
+			PCI_Write_U32(busno, ECB_entries_dev_no(pecb, idx),
+				      ECB_entries_func_no(pecb, idx),
+				      ECB_entries_reg_id(pecb, idx),
+				      (U32)ECB_entries_reg_value(pecb, idx));
+			continue;
+		}
+
+		// now program at the corresponding offset
+		PCI_Write_U32(busno, ECB_entries_dev_no(pecb, idx),
+			      ECB_entries_func_no(pecb, idx),
+			      ECB_entries_reg_id(pecb, idx),
+			      (U32)ECB_entries_reg_value(pecb, idx));
+
+		if ((ECB_entries_reg_value(pecb, idx) >> NEXT_ADDR_SHIFT) !=
+		    0) {
+			PCI_Write_U32(busno, ECB_entries_dev_no(pecb, idx),
+				      ECB_entries_func_no(pecb, idx),
+				      ECB_entries_reg_id(pecb, idx) +
+					      NEXT_ADDR_OFFSET,
+				      (U32)(ECB_entries_reg_value(pecb, idx) >>
+					    NEXT_ADDR_SHIFT));
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		PCI_Write_U64(busno, ECB_entries_dev_no(pecb, idx),
+			      ECB_entries_func_no(pecb, idx),
+			      ECB_entries_reg_id(pecb, idx), 0);
+
+		// this is needed for overflow detection of the accumulators.
+		if (LWPMU_DEVICE_counter_mask(&devices[dev_idx]) == 0) {
+			LWPMU_DEVICE_counter_mask(&devices[dev_idx]) =
+				(U64)ECB_entries_max_bits(pecb, idx);
+		}
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID unc_pci_Enable_PMU(PVOID)
+ *
+ * @brief      Set the enable bit for all the EVSEL registers
+ *
+ * @param      Device Index of this PMU unit
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_pci_Enable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_num = 0;
+	U32 dev_node;
+	U32 reg_val = 0;
+	U32 busno;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_node = LWPMU_DEVICE_pci_dev_node_index(&devices[dev_idx]);
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+
+	package_num = core_to_package_map[this_cpu];
+
+	if (!IS_BUS_MAP_VALID(dev_node, package_num)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("No UNC_PCIDEV bus map for %u!",
+					    dev_node);
+		return;
+	}
+
+	busno = GET_BUS_MAP(dev_node, package_num);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_ENABLE)
+	{
+		if (ECB_entries_reg_type(pecb, idx) == PMU_REG_GLOBAL_CTRL) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, idx),
+				      ECB_entries_reg_value(pecb, idx));
+			continue;
+		}
+		reg_val = (U32)ECB_entries_reg_value(pecb, idx);
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = PCI_Read_U32(busno,
+					       ECB_entries_dev_no(pecb, idx),
+					       ECB_entries_func_no(pecb, idx),
+					       ECB_entries_reg_id(pecb, idx));
+			reg_val &= ECB_entries_reg_value(pecb, idx);
+		}
+		PCI_Write_U32(busno, ECB_entries_dev_no(pecb, idx),
+			      ECB_entries_func_no(pecb, idx),
+			      ECB_entries_reg_id(pecb, idx), reg_val);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn           static VOID unc_pci_Disable_PMU(PVOID)
+ *
+ * @brief        Disable the per unit global control to stop the PMU counters.
+ *
+ * @param        Device Index of this PMU unit
+ * @control_msr  Control MSR address
+ * @enable_val   If counter freeze bit does not work, counter enable bit should be cleared
+ * @disable_val  Disable collection
+ *
+ * @return       None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID unc_pci_Disable_PMU(PVOID param)
+{
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_num = 0;
+	U32 dev_node;
+	U32 reg_val = 0;
+	U32 busno;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	dev_node = LWPMU_DEVICE_pci_dev_node_index(&devices[dev_idx]);
+
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+
+	package_num = core_to_package_map[this_cpu];
+
+	if (!IS_BUS_MAP_VALID(dev_node, package_num)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("No UNC_PCIDEV bus map for %u!",
+					    dev_node);
+		return;
+	}
+
+	busno = GET_BUS_MAP(dev_node, package_num);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_DISABLE)
+	{
+		if (ECB_entries_reg_type(pecb, idx) == PMU_REG_GLOBAL_CTRL) {
+			SYS_Write_MSR(ECB_entries_reg_id(pecb, idx),
+				      ECB_entries_reg_value(pecb, idx));
+			continue;
+		}
+		reg_val = (U32)ECB_entries_reg_value(pecb, idx);
+		if (ECB_entries_reg_rw_type(pecb, idx) ==
+		    PMU_REG_RW_READ_WRITE) {
+			reg_val = PCI_Read_U32(busno,
+					       ECB_entries_dev_no(pecb, idx),
+					       ECB_entries_func_no(pecb, idx),
+					       ECB_entries_reg_id(pecb, idx));
+			reg_val |= ECB_entries_reg_value(pecb, idx);
+		}
+		PCI_Write_U32(busno, ECB_entries_dev_no(pecb, idx),
+			      ECB_entries_func_no(pecb, idx),
+			      ECB_entries_reg_id(pecb, idx), reg_val);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       static VOID unc_pci_Trigger_Read(id)
+ *
+ * @param    id       Device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore data from counters and store into buffer
+ */
+static VOID unc_pci_Trigger_Read(PVOID param, U32 id)
+{
+	U32 this_cpu = 0;
+	U32 package_num = 0;
+	U32 dev_node = 0;
+	U32 cur_grp = 0;
+	ECB pecb = NULL;
+	U32 index = 0;
+	U64 value_low = 0;
+	U64 value_high = 0;
+	U64 diff = 0;
+	U64 value;
+	U64 *data;
+	U32 busno;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	dev_node = LWPMU_DEVICE_pci_dev_node_index(&devices[id]);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	if (!IS_BUS_MAP_VALID(dev_node, package_num)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("No UNC_PCIDEV bus map for %u!",
+					    dev_node);
+		return;
+	}
+
+	busno = GET_BUS_MAP(dev_node, package_num);
+
+	// Write GroupID
+	data = (U64 *)((S8 *)param + ECB_group_offset(pecb));
+	*data = cur_grp + 1;
+	// Read the counts into uncore buffer
+	FOR_EACH_REG_UNC_OPERATION(pecb, id, idx, PMU_OPERATION_READ)
+	{
+		// read lower 4 bytes
+		value_low = PCI_Read_U32(busno, ECB_entries_dev_no(pecb, idx),
+					 ECB_entries_func_no(pecb, idx),
+					 ECB_entries_reg_id(pecb, idx));
+		value = LOWER_4_BYTES_MASK & value_low;
+
+		// read upper 4 bytes
+		value_high = PCI_Read_U32(
+			busno, ECB_entries_dev_no(pecb, idx),
+			ECB_entries_func_no(pecb, idx),
+			(ECB_entries_reg_id(pecb, idx) + NEXT_ADDR_OFFSET));
+		value |= value_high << NEXT_ADDR_SHIFT;
+		//check for overflow
+		if (value <
+		    LWPMU_DEVICE_prev_value(&devices[id])[package_num][index]) {
+			diff = LWPMU_DEVICE_counter_mask(&devices[id]) -
+			       LWPMU_DEVICE_prev_value(
+				       &devices[id])[package_num][index];
+			diff += value;
+		} else {
+			diff = value -
+			       LWPMU_DEVICE_prev_value(
+				       &devices[id])[package_num][index];
+		}
+		LWPMU_DEVICE_acc_value(
+			&devices[id])[package_num][cur_grp][index] += diff;
+		LWPMU_DEVICE_prev_value(&devices[id])[package_num][index] =
+			value;
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, idx));
+		*data = LWPMU_DEVICE_acc_value(
+			&devices[id])[package_num][cur_grp][index];
+		index++;
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn       static   unc_pci_Read_PMU_Data(param)
+ *
+ * @param    param    The device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer;
+ */
+static VOID unc_pci_Read_PMU_Data(PVOID param)
+{
+	U32 j = 0;
+	U32 dev_idx;
+	U32 this_cpu;
+	U64 *buffer = read_counter_info;
+	CPU_STATE pcpu;
+	U32 cur_grp;
+	ECB pecb;
+	U32 dev_node;
+	U32 package_num = 0;
+	U32 busno;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[cur_grp];
+	dev_node = LWPMU_DEVICE_pci_dev_node_index(&devices[dev_idx]);
+
+	// NOTE THAT the read_pmu function on for EMON collection.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!CPU_STATE_socket_master).");
+		return;
+	}
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	if (!IS_BUS_MAP_VALID(dev_node, package_num)) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT("No UNC_PCIDEV bus map for %u!",
+					    dev_node);
+		return;
+	}
+
+	busno = GET_BUS_MAP(dev_node, package_num);
+
+	//Read in the counts into temporary buffer
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_event_scope(pecb, idx) == SYSTEM_EVENT) {
+			j = ECB_entries_uncore_buffer_offset_in_system(pecb,
+								       idx);
+		} else {
+			j = EMON_BUFFER_UNCORE_PACKAGE_EVENT_OFFSET(
+				package_num,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				ECB_entries_uncore_buffer_offset_in_package(
+					pecb, idx));
+		}
+
+		buffer[j] = PCI_Read_U64(busno, ECB_entries_dev_no(pecb, idx),
+					 ECB_entries_func_no(pecb, idx),
+					 ECB_entries_reg_id(pecb, idx));
+
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+
+DISPATCH_NODE unc_pci_dispatch = { .init = NULL,
+				   .fini = NULL,
+				   .write = unc_pci_Write_PMU,
+				   .freeze = unc_pci_Disable_PMU,
+				   .restart = unc_pci_Enable_PMU,
+				   .read_data = unc_pci_Read_PMU_Data,
+				   .check_overflow = NULL,
+				   .swap_group = NULL,
+				   .read_lbrs = NULL,
+				   .cleanup = NULL,
+				   .hw_errata = NULL,
+				   .read_power = NULL,
+				   .check_overflow_errata = NULL,
+				   .read_counts = NULL,
+				   .check_overflow_gp_errata = NULL,
+				   .read_ro = NULL,
+				   .platform_info = NULL,
+				   .trigger_read = unc_pci_Trigger_Read,
+				   .scan_for_uncore = NULL,
+				   .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/unc_power.c b/drivers/platform/x86/sepdk/sep/unc_power.c
new file mode 100755
index 000000000000..4f7d8ff43744
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_power.c
@@ -0,0 +1,444 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/unc_common.h"
+#include "inc/utility.h"
+
+extern U64 *read_counter_info;
+extern U64 *prev_counter_data;
+extern EMON_BUFFER_DRIVER_HELPER emon_buffer_driver_helper;
+static U64 **prev_val_per_thread;
+static U64 **acc_per_thread;
+extern DRV_CONFIG drv_cfg;
+
+/*!
+ * @fn unc_power_Allocate(param)
+ *
+ * @param    param    device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Allocate arrays required for reading counts
+ */
+static VOID unc_power_Allocate(PVOID param)
+{
+	U32 id;
+	U32 cur_grp;
+	ECB pecb;
+	U32 i;
+	U32 j;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	id = *((U32 *)param);
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[0];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	acc_per_thread = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64 *));
+	if (acc_per_thread == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Unable to allocate memory for acc_per_thread!");
+		return;
+	}
+
+	prev_val_per_thread = CONTROL_Allocate_Memory(
+		GLOBAL_STATE_num_cpus(driver_state) * sizeof(U64 *));
+	if (prev_val_per_thread == NULL) {
+		SEP_DRV_LOG_ERROR_TRACE_OUT(
+			"Unable to allocate memory for prev_val_per_thread!");
+		return;
+	}
+
+	for (i = 0; i < (U32)GLOBAL_STATE_num_cpus(driver_state); i++) {
+		acc_per_thread[i] = CONTROL_Allocate_Memory(
+			ECB_num_events(pecb) * sizeof(U64));
+		if (acc_per_thread[i] == NULL) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Unable to allocate memory for acc_per_thread[%u]!",
+				i);
+			return;
+		}
+
+		prev_val_per_thread[i] = CONTROL_Allocate_Memory(
+			ECB_num_events(pecb) * sizeof(U64));
+		if (prev_val_per_thread[i] == NULL) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Unable to allocate memory for prev_val_per_thread[%u]!",
+				i);
+			return;
+		}
+
+		// initialize all values to 0
+		for (j = 0; j < ECB_num_events(pecb); j++) {
+			acc_per_thread[i][j] = 0LL;
+			prev_val_per_thread[i][j] = 0LL;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn unc_power_Free(param)
+ *
+ * @param    param    device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Free arrays required for reading counts
+ */
+static VOID unc_power_Free(PVOID param)
+{
+	U32 i;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	if (acc_per_thread) {
+		for (i = 0; i < (U32)GLOBAL_STATE_num_cpus(driver_state); i++) {
+			acc_per_thread[i] =
+				CONTROL_Free_Memory(acc_per_thread[i]);
+		}
+		acc_per_thread = CONTROL_Free_Memory(acc_per_thread);
+	}
+
+	if (prev_val_per_thread) {
+		for (i = 0; i < (U32)GLOBAL_STATE_num_cpus(driver_state); i++) {
+			prev_val_per_thread[i] =
+				CONTROL_Free_Memory(prev_val_per_thread[i]);
+		}
+		prev_val_per_thread = CONTROL_Free_Memory(prev_val_per_thread);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn unc_power_Read_Counts(param, id, mask)
+ *
+ * @param    param    pointer to sample buffer
+ * @param    id       device index
+ * @param    mask     The mask bits for value
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param
+ */
+static VOID unc_power_Trigger_Read(PVOID param, U32 id)
+{
+	U64 *data = (U64 *)param;
+	U32 cur_grp;
+	ECB pecb;
+	U32 this_cpu;
+	U32 package_num;
+	U32 index = 0;
+	U64 diff = 0;
+	U64 value;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	// Write GroupID
+	data = (U64 *)((S8 *)data + ECB_group_offset(pecb));
+	*data = cur_grp + 1;
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, id, idx, PMU_OPERATION_READ)
+	{
+		data = (U64 *)((S8 *)param +
+			       ECB_entries_counter_event_offset(pecb, idx));
+		value = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+		if (ECB_entries_max_bits(pecb, idx)) {
+			value &= ECB_entries_max_bits(pecb, idx);
+		}
+		//check for overflow if not a static counter
+		if (ECB_entries_counter_type(pecb, idx) == STATIC_COUNTER) {
+			*data = value;
+		} else {
+			if (value < prev_val_per_thread[this_cpu][index]) {
+				diff = ECB_entries_max_bits(pecb, idx) -
+				       prev_val_per_thread[this_cpu][index];
+				diff += value;
+			} else {
+				diff = value -
+				       prev_val_per_thread[this_cpu][index];
+			}
+			acc_per_thread[this_cpu][index] += diff;
+			prev_val_per_thread[this_cpu][index] = value;
+			*data = acc_per_thread[this_cpu][index];
+		}
+		index++;
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_power_Enable_PMU(param)
+ *
+ * @param    None
+ *
+ * @return   None
+ *
+ * @brief    Capture the previous values to calculate delta later.
+ */
+static VOID unc_power_Enable_PMU(PVOID param)
+{
+	U32 j;
+	U64 *buffer = prev_counter_data;
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_event_count = 0;
+	U32 thread_event_count = 0;
+	U32 module_event_count = 0;
+	U64 tmp_value = 0;
+	U32 package_id = 0;
+	U32 core_id = 0;
+	U32 thread_id = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_id = core_to_package_map[this_cpu];
+	core_id = core_to_phys_core_map[this_cpu];
+	thread_id = core_to_thread_map[this_cpu];
+
+	// NOTE THAT the enable function currently captures previous values
+	// for EMON collection to avoid unnecessary memory copy.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_event_scope(pecb, idx) == PACKAGE_EVENT) {
+			j = EMON_BUFFER_UNCORE_PACKAGE_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				package_event_count);
+			package_event_count++;
+		} else if (ECB_entries_event_scope(pecb, idx) == MODULE_EVENT) {
+			j = EMON_BUFFER_UNCORE_MODULE_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_num_package_events(
+					emon_buffer_driver_helper),
+				CPU_STATE_cpu_module_master(pcpu),
+				EMON_BUFFER_DRIVER_HELPER_power_num_module_events(
+					emon_buffer_driver_helper),
+				module_event_count);
+			module_event_count++;
+		} else {
+			j = EMON_BUFFER_UNCORE_THREAD_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_num_package_events(
+					emon_buffer_driver_helper),
+				GLOBAL_STATE_num_modules(driver_state),
+				EMON_BUFFER_DRIVER_HELPER_power_num_module_events(
+					emon_buffer_driver_helper),
+				core_id, threads_per_core[cpu], thread_id,
+				EMON_BUFFER_DRIVER_HELPER_power_num_thread_events(
+					emon_buffer_driver_helper),
+				thread_event_count);
+			thread_event_count++;
+		}
+
+		tmp_value = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+		if (ECB_entries_max_bits(pecb, idx)) {
+			tmp_value &= ECB_entries_max_bits(pecb, idx);
+		}
+		buffer[j] = tmp_value;
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn unc_power_Read_PMU_Data(param)
+ *
+ * @param    param    The read thread node to process
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param;
+ * Uncore PMU does not support sampling, i.e. ignore the id parameter.
+ */
+static VOID unc_power_Read_PMU_Data(PVOID param)
+{
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U64 *prev_buffer = prev_counter_data;
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_event_count = 0;
+	U32 thread_event_count = 0;
+	U32 module_event_count = 0;
+	U64 tmp_value;
+	U32 package_id = 0;
+	U32 core_id = 0;
+	U32 thread_id = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_id = core_to_package_map[this_cpu];
+	core_id = core_to_phys_core_map[this_cpu];
+	thread_id = core_to_thread_map[this_cpu];
+
+	// NOTE THAT the read_pmu function on for EMON collection.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		if (ECB_entries_event_scope(pecb, idx) == PACKAGE_EVENT) {
+			j = EMON_BUFFER_UNCORE_PACKAGE_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				package_event_count);
+			package_event_count++;
+		} else if (ECB_entries_event_scope(pecb, idx) == MODULE_EVENT) {
+			j = EMON_BUFFER_UNCORE_MODULE_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_num_package_events(
+					emon_buffer_driver_helper),
+				CPU_STATE_cpu_module_master(pcpu),
+				EMON_BUFFER_DRIVER_HELPER_power_num_module_events(
+					emon_buffer_driver_helper),
+				module_event_count);
+			module_event_count++;
+		} else {
+			j = EMON_BUFFER_UNCORE_THREAD_POWER_EVENT_OFFSET(
+				package_id,
+				EMON_BUFFER_DRIVER_HELPER_num_entries_per_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_device_offset_in_package(
+					emon_buffer_driver_helper),
+				EMON_BUFFER_DRIVER_HELPER_power_num_package_events(
+					emon_buffer_driver_helper),
+				GLOBAL_STATE_num_modules(driver_state),
+				EMON_BUFFER_DRIVER_HELPER_power_num_module_events(
+					emon_buffer_driver_helper),
+				core_id, threads_per_core[cpu], thread_id,
+				EMON_BUFFER_DRIVER_HELPER_power_num_thread_events(
+					emon_buffer_driver_helper),
+				thread_event_count);
+			thread_event_count++;
+		}
+
+		tmp_value = SYS_Read_MSR(ECB_entries_reg_id(pecb, idx));
+		if (ECB_entries_max_bits(pecb, idx)) {
+			tmp_value &= ECB_entries_max_bits(pecb, idx);
+		}
+		if (ECB_entries_counter_type(pecb, idx) == STATIC_COUNTER) {
+			buffer[j] = tmp_value;
+		} else {
+			if (tmp_value >= prev_buffer[j]) {
+				buffer[j] = tmp_value - prev_buffer[j];
+			} else {
+				buffer[j] = tmp_value +
+					    (ECB_entries_max_bits(pecb, idx) -
+					     prev_buffer[j]);
+			}
+		}
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*
+ * Initialize the dispatch table
+ */
+
+DISPATCH_NODE unc_power_dispatch = { .init = unc_power_Allocate,
+				     .fini = unc_power_Free,
+				     .write = UNC_COMMON_Dummy_Func,
+				     .freeze = NULL,
+				     .restart = unc_power_Enable_PMU,
+				     .read_data = unc_power_Read_PMU_Data,
+				     .check_overflow = NULL,
+				     .swap_group = NULL,
+				     .read_lbrs = NULL,
+				     .cleanup = NULL,
+				     .hw_errata = NULL,
+				     .read_power = NULL,
+				     .check_overflow_errata = NULL,
+				     .read_counts = NULL,
+				     .check_overflow_gp_errata = NULL,
+				     .read_ro = NULL,
+				     .platform_info = NULL,
+				     .trigger_read = unc_power_Trigger_Read,
+				     .scan_for_uncore = NULL,
+				     .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/unc_sa.c b/drivers/platform/x86/sepdk/sep/unc_sa.c
new file mode 100755
index 000000000000..7345807f9588
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/unc_sa.c
@@ -0,0 +1,173 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/haswellunc_sa.h"
+#include "inc/utility.h"
+
+#if 0
+extern U64 *read_counter_info;
+extern DRV_CONFIG drv_cfg;
+
+extern VOID SOCPERF_Read_Data3(PVOID data_buffer);
+#endif
+
+/*!
+ * @fn         static VOID hswunc_sa_Initialize(PVOID)
+ *
+ * @brief      Initialize any registers or addresses
+ *
+ * @param      param
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID hswunc_sa_Initialize(VOID *param)
+{
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+	SEP_DRV_LOG_TRACE_OUT("Empty function.");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn hswunc_sa_Read_Counts(param, id)
+ *
+ * @param    param    The read thread node to process
+ * @param    id       The id refers to the device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param;
+ *
+ */
+static VOID hswunc_sa_Trigger_Read(PVOID param, U32 id)
+{
+#if 0
+	U64 *data = (U64 *)param;
+	U32 cur_grp;
+	ECB pecb;
+	U32 this_cpu;
+	U32 package_num;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	// group id
+	data = (U64 *)((S8 *)data + ECB_group_offset(pecb));
+	SOCPERF_Read_Data3((void*)data);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn hswunc_sa_Read_PMU_Data(param)
+ *
+ * @param    param    the device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param;
+ *
+ */
+static VOID hswunc_sa_Read_PMU_Data(PVOID param)
+{
+#if 0
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 event_index = 0;
+	U64 counter_buffer[HSWUNC_SA_MAX_COUNTERS + 1];
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	// NOTE THAT the read_pmu function on for EMON collection.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+
+	SOCPERF_Read_Data3((void*)counter_buffer);
+
+	FOR_EACH_PCI_DATA_REG_RAW(pecb, i, dev_idx)
+	{
+		j = ECB_entries_uncore_buffer_offset_in_system(pecb, i);
+		buffer[j] = counter_buffer[event_index + 1];
+		event_index++;
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_PCI_DATA_REG_RAW;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/*
+ * Initialize the dispatch table
+ */
+
+DISPATCH_NODE hswunc_sa_dispatch = { .init = hswunc_sa_Initialize,
+				     .fini = NULL,
+				     .write = NULL,
+				     .freeze = NULL,
+				     .restart = NULL,
+				     .read_data = hswunc_sa_Read_PMU_Data,
+				     .check_overflow = NULL,
+				     .swap_group = NULL,
+				     .read_lbrs = NULL,
+				     .cleanup = NULL,
+				     .hw_errata = NULL,
+				     .read_power = NULL,
+				     .check_overflow_errata = NULL,
+				     .read_counts = NULL,
+				     .check_overflow_gp_errata = NULL,
+				     .read_ro = NULL,
+				     .platform_info = NULL,
+				     .trigger_read = hswunc_sa_Trigger_Read,
+				     .scan_for_uncore = NULL,
+				     .read_metrics = NULL };
diff --git a/drivers/platform/x86/sepdk/sep/utility.c b/drivers/platform/x86/sepdk/sep/utility.c
new file mode 100755
index 000000000000..cc4f0cba5e9e
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/utility.c
@@ -0,0 +1,1157 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include <linux/slab.h>
+#include <linux/version.h>
+#include <linux/fs.h>
+#include <linux/kallsyms.h>
+#include <asm/msr.h>
+#include <linux/ptrace.h>
+#include <linux/time.h>
+#include <linux/vmalloc.h>
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "rise_errors.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv.h"
+#include "control.h"
+#include "core2.h"
+#include "silvermont.h"
+#include "perfver4.h"
+#include "valleyview_sochap.h"
+#include "unc_gt.h"
+#include "haswellunc_sa.h"
+#if defined(BUILD_CHIPSET)
+#include "chap.h"
+#endif
+#include "utility.h"
+#if defined(BUILD_CHIPSET)
+#include "lwpmudrv_chipset.h"
+#include "gmch.h"
+#endif
+
+#include "control.h"
+
+//volatile int config_done;
+
+
+#if defined(BUILD_CHIPSET)
+extern CHIPSET_CONFIG pma;
+#endif
+
+VOID UTILITY_down_read_mm(struct mm_struct *mm)
+{
+	SEP_DRV_LOG_TRACE_IN("Mm: %p.", mm);
+
+	down_read((struct rw_semaphore *)&mm->mmap_sem);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+VOID UTILITY_up_read_mm(struct mm_struct *mm)
+{
+	SEP_DRV_LOG_TRACE_IN("Mm: %p.", mm);
+
+	up_read((struct rw_semaphore *)&mm->mmap_sem);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+// NOT to be instrumented, used inside DRV_LOG!
+VOID UTILITY_Read_TSC(U64 *pTsc)
+{
+	*pTsc = rdtsc_ordered();
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID UTILITY_Read_Cpuid
+ *
+ * @brief    executes the cpuid_function of cpuid and returns values
+ *
+ * @param  IN   cpuid_function
+ *         OUT  rax  - results of the cpuid instruction in the
+ *         OUT  rbx  - corresponding registers
+ *         OUT  rcx
+ *         OUT  rdx
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ *
+ */
+VOID UTILITY_Read_Cpuid(U64 cpuid_function, U64 *rax_value,
+			       U64 *rbx_value, U64 *rcx_value, U64 *rdx_value)
+{
+	U32 function;
+	U32 *eax, *ebx, *ecx, *edx;
+
+	SEP_DRV_LOG_TRACE_IN(
+		"Fn: %llu, rax_p: %p, rbx_p: %p, rcx_p: %p, rdx_p: %p.",
+		cpuid_function, rax_value, rbx_value, rcx_value, rdx_value);
+
+#if defined(DRV_SEP_ACRN_ON)
+	if (cpuid_function != 0x40000000) {
+		struct profiling_pcpuid pcpuid;
+		memset(&pcpuid, 0, sizeof(struct profiling_pcpuid));
+		pcpuid.leaf = (U32)cpuid_function;
+		if (rcx_value != NULL) {
+			pcpuid.subleaf = (U32)*rcx_value;
+		}
+
+		BUG_ON(!virt_addr_valid(&pcpuid));
+
+		acrn_hypercall2(HC_PROFILING_OPS, PROFILING_GET_PCPUID,
+				virt_to_phys(&pcpuid));
+
+		if (rax_value != NULL) {
+			*rax_value = pcpuid.eax;
+		}
+		if (rbx_value != NULL) {
+			*rbx_value = pcpuid.ebx;
+		}
+		if (rcx_value != NULL) {
+			*rcx_value = pcpuid.ecx;
+		}
+		if (rdx_value != NULL) {
+			*rdx_value = pcpuid.edx;
+		}
+		return;
+	}
+#endif
+	function = (U32)cpuid_function;
+	eax = (U32 *)rax_value;
+	ebx = (U32 *)rbx_value;
+	ecx = (U32 *)rcx_value;
+	edx = (U32 *)rdx_value;
+
+	*eax = function;
+
+	__asm__("cpuid"
+		: "=a"(*eax), "=b"(*ebx), "=c"(*ecx), "=d"(*edx)
+		: "a"(function), "b"(*ebx), "c"(*ecx), "d"(*edx));
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID UTILITY_Configure_CPU
+ *
+ * @brief    Reads the CPU information from the hardware
+ *
+ * @param    param   dispatch_id -  The id of the dispatch table.
+ *
+ * @return   Pointer to the correct dispatch table for the CPU architecture
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+DISPATCH UTILITY_Configure_CPU(U32 dispatch_id)
+{
+	DISPATCH dispatch = NULL;
+
+	SEP_DRV_LOG_TRACE_IN("Dispatch_id: %u.", dispatch_id);
+
+	switch (dispatch_id) {
+	case 1:
+		SEP_DRV_LOG_INIT(
+			"Set up the Core(TM)2 processor dispatch table.");
+		dispatch = &core2_dispatch;
+		break;
+	case 6:
+		SEP_DRV_LOG_INIT("Set up the Silvermont dispatch table.");
+		dispatch = &silvermont_dispatch;
+		break;
+	case 7:
+		SEP_DRV_LOG_INIT(
+			"Set up the perfver4 HTON dispatch table such as Skylake.");
+		dispatch = &perfver4_dispatch;
+		break;
+	case 8:
+		SEP_DRV_LOG_INIT(
+			"Set up the perfver4 HTOFF dispatch table such as Skylake.");
+		dispatch = &perfver4_dispatch_htoff_mode;
+		break;
+	case 11:
+		SEP_DRV_LOG_INIT(
+			"Set up the perfver4 NONHT dispatch table such as Icelake.");
+		dispatch = &perfver4_dispatch_nonht_mode;
+		break;
+	case 700:
+	case 701:
+	case 1100:
+		SEP_DRV_LOG_INIT("Set up the Valleyview SA dispatch table.");
+		dispatch = &valleyview_visa_dispatch;
+		break;
+	case 2:
+		SEP_DRV_LOG_INIT(
+			"Set up the Core i7(TM) processor dispatch table.");
+		dispatch = &corei7_dispatch;
+		break;
+	case 3:
+		SEP_DRV_LOG_INIT("Set up the Core i7(TM) dispatch table.");
+		dispatch = &corei7_dispatch_htoff_mode;
+		break;
+	case 4:
+		SEP_DRV_LOG_INIT(
+			"Set up the Sandybridge processor dispatch table.");
+		dispatch = &corei7_dispatch_2;
+		break;
+	case 5:
+		SEP_DRV_LOG_INIT("Set up the Sandybridge dispatch table.");
+		dispatch = &corei7_dispatch_htoff_mode_2;
+		break;
+	case 9:
+		SEP_DRV_LOG_INIT(
+			"Set up the Nehalem, Westemere dispatch table.");
+		dispatch = &corei7_dispatch_nehalem;
+		break;
+	case 10:
+		SEP_DRV_LOG_INIT("Set up the Knights family dispatch table.");
+		dispatch = &knights_dispatch;
+		break;
+	case 100:
+		SEP_DRV_LOG_INIT("Set up the MSR based uncore dispatch table.");
+		dispatch = &unc_msr_dispatch;
+		break;
+	case 110:
+		SEP_DRV_LOG_INIT("Set up the PCI Based Uncore dispatch table.");
+		dispatch = &unc_pci_dispatch;
+		break;
+	case 120:
+		SEP_DRV_LOG_INIT(
+			"Set up the MMIO based uncore dispatch table.");
+		dispatch = &unc_mmio_dispatch;
+		break;
+	case 121:
+		SEP_DRV_LOG_INIT(
+			"Set up the MMIO based uncore dispatch table for FPGA.");
+		dispatch = &unc_mmio_fpga_dispatch;
+		break;
+	case 130:
+		SEP_DRV_LOG_INIT("Set up the Uncore Power dispatch table.");
+		dispatch = &unc_power_dispatch;
+		break;
+	case 230:
+		SEP_DRV_LOG_INIT("Set up the Haswell SA dispatch table.");
+		dispatch = &hswunc_sa_dispatch;
+		break;
+	case 400:
+		SEP_DRV_LOG_INIT("Set up the GT dispatch table.");
+		dispatch = &unc_gt_dispatch;
+		break;
+	default:
+		dispatch = NULL;
+		SEP_DRV_LOG_ERROR(
+			"Architecture not supported (dispatch_id: %d).",
+			dispatch_id);
+		break;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %p.", dispatch);
+	return dispatch;
+}
+
+U64 SYS_MMIO_Read64(U64 baseAddress, U64 offset)
+{
+	U64 res = 0;
+#if defined(DRV_EM64T)
+	SEP_DRV_LOG_REGISTER_IN("Will read MMIO *(0x%llx + 0x%llx).",
+				baseAddress, offset);
+
+	if (baseAddress) {
+		volatile U64 *p =
+			(U64 *)(baseAddress + offset); // offset is in bytes
+		res = *p;
+	} else {
+		SEP_DRV_LOG_ERROR("BaseAddress is NULL!");
+		res = (U64)-1; // typical value for undefined CSR
+	}
+
+	SEP_DRV_LOG_REGISTER_OUT("Has read MMIO *(0x%llx + 0x%llx): 0x%llx.",
+				 baseAddress, offset, res);
+#endif
+	return res;
+}
+
+U64 SYS_Read_MSR(U32 msr)
+{
+	U64 val = 0;
+
+#if defined(DRV_DEBUG_MSR)
+	int error;
+	SEP_DRV_LOG_REGISTER_IN("Will safely read MSR 0x%x.", msr);
+	error = rdmsrl_safe(msr, &val);
+	if (error) {
+		SEP_DRV_LOG_ERROR("Failed to read MSR 0x%x.", msr);
+	}
+	SEP_DRV_LOG_REGISTER_OUT("Has read MSR 0x%x: 0x%llx (error: %d).", msr,
+				 val, error);
+#else
+	SEP_DRV_LOG_REGISTER_IN("Will read MSR 0x%x.", msr);
+	rdmsrl(msr, val);
+	SEP_DRV_LOG_REGISTER_OUT("Has read MSR 0x%x: 0x%llx.", msr, val);
+#endif
+
+	return val;
+}
+
+void SYS_Write_MSR(U32 msr, U64 val)
+{
+#if defined(DRV_DEBUG_MSR)
+	int error;
+	SEP_DRV_LOG_REGISTER_IN("Will safely write MSR 0x%x: 0x%llx.", msr,
+				val);
+	error = wrmsr_safe(msr, (U32)val, (U32)(val >> 32));
+	if (error) {
+		SEP_DRV_LOG_ERROR("Failed to write MSR 0x%x: 0x%llx.", msr,
+				  val);
+	}
+	SEP_DRV_LOG_REGISTER_OUT("Wrote MSR 0x%x: 0x%llx (error: %d).", msr,
+				 val, error);
+
+#else // !DRV_DEBUG_MSR
+	SEP_DRV_LOG_REGISTER_IN("Will write MSR 0x%x: 0x%llx.", msr, val);
+#if defined(DRV_IA32)
+	wrmsr(msr, (U32)val, (U32)(val >> 32));
+#endif
+#if defined(DRV_EM64T)
+	wrmsrl(msr, val);
+#endif
+	SEP_DRV_LOG_REGISTER_OUT("Wrote MSR 0x%x: 0x%llx.", msr, val);
+
+#endif // !DRV_DEBUG_MSR
+}
+
+#if defined(BUILD_CHIPSET)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID UTILITY_Configure_Chipset
+ *
+ * @brief    Configures the chipset information
+ *
+ * @param    none
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ *              <NONE>
+ */
+CS_DISPATCH UTILITY_Configure_Chipset(void)
+{
+	SEP_DRV_LOG_TRACE_IN("");
+
+	if (CHIPSET_CONFIG_gmch_chipset(pma)) {
+		cs_dispatch = &gmch_dispatch;
+		SEP_DRV_LOG_INIT("Using GMCH dispatch table.");
+	} else if (CHIPSET_CONFIG_mch_chipset(pma) ||
+		   CHIPSET_CONFIG_ich_chipset(pma)) {
+		cs_dispatch = &chap_dispatch;
+		SEP_DRV_LOG_INIT("Using CHAP dispatch table.");
+	} else {
+		SEP_DRV_LOG_ERROR("Unable to map chipset dispatch table!");
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %p.", cs_dispatch);
+	return cs_dispatch;
+}
+
+#endif
+
+#if LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 32)
+static unsigned long utility_Compare_Symbol_Names_Return_Value;
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       static int utility_Compare_Symbol_Names (void* ref_name,
+ * const char* symbol_name, struct module* dummy, unsigned long symbol_address)
+ *
+ * @brief    Comparator for kallsyms_on_each_symbol.
+ *
+ * @param
+ * void         * ref_name       : Symbol we are looking for
+ * const char   * symbol_name    : Name of the current symbol being evaluated
+ * struct module* dummy          : Pointer to the module structure. Not needed.
+ * unsigned long  symbol_address : Address of the current symbol being evaluated
+ *
+ * @return   1 if ref_name matches symbol_name, 0 otherwise.
+ * Fills utility_Compare_Symbol_Names_Return_Value with the symbol's address
+ * on success.
+ *
+ * <I>Special Notes:</I>
+ *           Only used as a callback comparator for kallsyms_on_each_symbol.
+ */
+static int utility_Compare_Symbol_Names(void *ref_name, const char *symbol_name,
+					struct module *dummy,
+					unsigned long symbol_address)
+{
+	int res = 0;
+
+	SEP_DRV_LOG_TRACE_IN(
+		"Ref_name: %p, symbol_name: %p, dummy: %p, symbol_address: %u.",
+		ref_name, symbol_name, dummy, symbol_address);
+
+	if (strcmp((char *)ref_name, symbol_name) == 0) {
+		utility_Compare_Symbol_Names_Return_Value = symbol_address;
+		res = 1;
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", res);
+	return res;
+}
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern unsigned long UTILITY_Find_Symbol (const char* name)
+ *
+ * @brief    Finds the address of the specified kernel symbol.
+ *
+ * @param    const char* name - name of the symbol to look for
+ *
+ * @return   Symbol address (0 if could not find)
+ *
+ * <I>Special Notes:</I>
+ * This wrapper is needed due to kallsyms_lookup_name not being exported
+ * in kernel version 2.6.32.*.
+ * Careful! This code is *NOT* multithread-safe or reentrant! Should only
+ * be called from 1 context at a time!
+ */
+unsigned long UTILITY_Find_Symbol(const char *name)
+{
+	unsigned long res = 0;
+
+	SEP_DRV_LOG_TRACE_IN("Name: %p.", name);
+	// Not printing the name to follow the log convention: *must not*
+	// dereference any pointer in an 'IN' message
+
+#if LINUX_VERSION_CODE == KERNEL_VERSION(2, 6, 32)
+	if (kallsyms_on_each_symbol(utility_Compare_Symbol_Names,
+				    (void *)name)) {
+		res = utility_Compare_Symbol_Names_Return_Value;
+	}
+#else
+	res = kallsyms_lookup_name(name);
+#endif
+
+	SEP_DRV_LOG_INIT("Name: '%s': 0x%llx.", name ? name : "NULL",
+			 (unsigned long long)res);
+	// Printing here instead. (Paranoia in case of corrupt pointer.)
+
+	SEP_DRV_LOG_TRACE_OUT("Res: 0x%llx.", (unsigned long long)res);
+	return res;
+}
+
+/*
+ ************************************
+ *  DRIVER LOG BUFFER DECLARATIONS  *
+ ************************************
+ */
+
+volatile U8 active_ioctl;
+
+DRV_LOG_BUFFER driver_log_buffer;
+
+static const char *drv_log_categories[DRV_NB_LOG_CATEGORIES] = {
+	"load",  "init",     "detection",    "error",  "state change",
+	"mark",  "debug",    "flow",	 "alloc",  "interrupt",
+	"trace", "register", "notification", "warning"
+};
+
+#define DRV_LOG_NB_DRIVER_STATES 9
+static const char *drv_log_states[DRV_LOG_NB_DRIVER_STATES] = {
+	"Uninitialized", "Reserved", "Idle",	 "Paused",     "Stopped",
+	"Running",       "Pausing",  "Prepare_Stop", "Terminating"
+};
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn static VOID utility_Driver_Log_Kprint_Helper
+ *                      (U8 category, char**  category_string,
+ *                       U8 secondary, char** secondary_string_1,
+ *                       char**  secondary_string_2, char**  secondary_string_3,
+ *                       char**  secondary_string_4)
+ *
+ * @brief    Helper function for printing log messages to the system log.
+ *
+ * @param    IN     category            -  message category
+ *           IN/OUT category_string     -  location where to place a pointer
+ *	to the category's name
+ *           IN     secondary           -  secondary field value for the message
+ *           IN/OUT secondary_string_1  -  location where to place a pointer to
+ *	the 1st part of the secondary info's decoded information
+ *           IN/OUT secondary_string_2  -  location where to place a pointer to
+ * the 2nd part of the secondary info's decoded information
+ *           IN/OUT secondary_string_3  -  location where to place a pointer to
+ *	the 3rd part of the secondary info's decoded information
+ *           IN/OUT secondary_string_4  -  location where to place a pointer to
+ *	the 4th part of the secondary info's decoded information
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ * Allows a single format string to be used for all categories (instead of
+ * category-specific format strings) when calling printk, simplifying the
+ * print routine and reducing potential errors. There is a performance cost to
+ * this approach (forcing printk to process empty strings), but it
+ * should be dwarved by the cost of calling printk in the first place.
+ * NB: none of the input string pointers may be NULL!
+ */
+static VOID utility_Driver_Log_Kprint_Helper(
+	U8 category, char **category_string, U8 secondary,
+	char **secondary_string_1, char **secondary_string_2,
+	char **secondary_string_3, char **secondary_string_4)
+{
+	if (category >= DRV_NB_LOG_CATEGORIES) {
+		*category_string = "Unknown category";
+	} else {
+		*category_string = (char *)drv_log_categories[category];
+	}
+
+	*secondary_string_1 = "";
+	*secondary_string_2 = "";
+	*secondary_string_3 = "";
+	*secondary_string_4 = "";
+
+	switch (category) {
+	case DRV_LOG_CATEGORY_FLOW:
+	case DRV_LOG_CATEGORY_TRACE:
+	case DRV_LOG_CATEGORY_INTERRUPT:
+		// we should *never* be kprinting from an interrupt context...
+		if (secondary != DRV_LOG_NOTHING) {
+			*secondary_string_1 = ", ";
+			if (secondary == DRV_LOG_FLOW_IN) {
+				*secondary_string_2 = "Entering";
+			} else if (secondary == DRV_LOG_FLOW_OUT) {
+				*secondary_string_2 = "Leaving";
+			}
+		}
+		break;
+	case DRV_LOG_CATEGORY_STATE_CHANGE: {
+		U8 orig_state, dest_state;
+
+		orig_state = (secondary & 0xF0) >> 4;
+		dest_state = secondary & 0x0F;
+
+		*secondary_string_1 = ", ";
+
+		if (orig_state < DRV_LOG_NB_DRIVER_STATES) {
+			*secondary_string_2 =
+				(char *)drv_log_states[orig_state];
+		} else {
+			*secondary_string_2 = "Unknown_state";
+		}
+
+		*secondary_string_3 = " -> ";
+
+		if (dest_state < DRV_LOG_NB_DRIVER_STATES) {
+			*secondary_string_4 =
+				(char *)drv_log_states[dest_state];
+		} else {
+			*secondary_string_4 = "Unknown_state";
+		}
+	} break;
+
+	default:
+		break;
+	}
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       static inline VOID utility_Log_Write (
+ *                U8 destination, U8 category, U8 secondary,
+ *                const char* function_name, U32 func_name_len,
+ *                U32 line_number, U64 tsc, U8 ioctl, U16 processor_id,
+ *                U8 driver_state, U16 nb_active_interrupts,
+ *                                    U16 nb_active_notifications,
+ *                                    const char* format_string, ...)
+ *
+ * @brief    Checks whether and where the message should be logged, and logs
+ * it as appropriate.
+ *
+ * @param
+ *  U8          destination    - whether to write to the primary (0)
+ *  or the auxiliary log buffer (1)
+ *  U8          category       - message category
+ *  U8          secondary      - secondary information field for the message
+ *  const char* function_name  - name of the calling function
+ *  U32         func_name_len  - length of the name of the calling function
+ *  (more efficient to pass it as parameter than finding it back at runtime)
+ *  U32         line_number    - line number of the call site
+ *  U64         tsc            - time stamp value to use
+ *  U8          ioctl          - current active ioctl
+ *  U16         processor_id   - id of the active core/thread
+ *  U8          driver_state   - current driver state
+ *  U16    nb_active_interrupts    - number of interrupts currently being
+ * processed
+ *  U16    nb_active_notifications - number of notifications currently being
+ * processed
+ *  const char* format_string - classical format string for printf-like funcs
+ *  ...                       - elements to print
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ * Writes the specified message to the specified log buffer.
+ * The order of writes (integrity tag at the beginning, overflow tag at
+ * the very end) matters to ensure the logged information can be detected
+ * to be only partially written if applicable). Much of the needed information
+ * (active core, driver state, tsc..) is passed through the stack (instead of
+ * obtained inside utility_Log_Write) to guarantee entries representing the
+ * same message (or log call) in different channels use consistent information,
+ * letting the decoder reliably identify duplicates.
+ */
+static inline VOID utility_Log_Write(U8 destination, U8 category, U8 secondary,
+				     const char *function_name,
+				     U32 function_name_length, U32 line_number,
+				     U64 tsc, U8 ioctl, U16 processor_id,
+				     U8 driver_state, U16 nb_active_interrupts,
+				     U16 nb_active_notifications,
+				     const char *format_string, va_list args)
+{
+	U32 entry_id;
+	U16 overflow_tag;
+	DRV_LOG_ENTRY entry;
+	char *target_func_buffer;
+	U32 local_func_name_length;
+	U32 i;
+
+	if (destination == 0) { // primary buffer
+		entry_id = __sync_add_and_fetch(
+			&DRV_LOG_BUFFER_pri_entry_index(DRV_LOG()), 1);
+		overflow_tag = (U16)(entry_id / DRV_LOG_MAX_NB_PRI_ENTRIES);
+		entry = DRV_LOG_BUFFER_entries(DRV_LOG()) +
+			entry_id % DRV_LOG_MAX_NB_PRI_ENTRIES;
+	} else {
+		entry_id = __sync_add_and_fetch(
+			&DRV_LOG_BUFFER_aux_entry_index(DRV_LOG()), 1);
+		overflow_tag = (U16)(entry_id / DRV_LOG_MAX_NB_AUX_ENTRIES);
+		entry = DRV_LOG_BUFFER_entries(DRV_LOG()) +
+			DRV_LOG_MAX_NB_PRI_ENTRIES +
+			entry_id % DRV_LOG_MAX_NB_AUX_ENTRIES;
+	}
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_ENTRY_integrity_tag(entry) = overflow_tag;
+	DRV_LOG_COMPILER_MEM_BARRIER();
+
+	if (format_string &&
+	    *format_string) { // setting this one first to try to increase MLP
+		DRV_VSNPRINTF(DRV_LOG_ENTRY_message(entry),
+			      DRV_LOG_MESSAGE_LENGTH, DRV_LOG_MESSAGE_LENGTH,
+			      format_string, args);
+	} else {
+		DRV_LOG_ENTRY_message(entry)[0] = 0;
+	}
+
+	target_func_buffer = DRV_LOG_ENTRY_function_name(entry);
+	local_func_name_length =
+		function_name_length < DRV_LOG_FUNCTION_NAME_LENGTH ?
+			function_name_length :
+			DRV_LOG_FUNCTION_NAME_LENGTH;
+	for (i = 0; i < local_func_name_length - 1; i++) {
+		target_func_buffer[i] = function_name[i];
+	}
+	target_func_buffer[i] = 0;
+
+	DRV_LOG_ENTRY_category(entry) = category;
+	DRV_LOG_ENTRY_secondary_info(entry) = secondary;
+	DRV_LOG_ENTRY_line_number(entry) = line_number;
+	DRV_LOG_ENTRY_active_drv_operation(entry) = ioctl;
+	DRV_LOG_ENTRY_processor_id(entry) = processor_id;
+	DRV_LOG_ENTRY_driver_state(entry) = driver_state;
+	DRV_LOG_ENTRY_nb_active_interrupts(entry) = nb_active_interrupts;
+	DRV_LOG_ENTRY_nb_active_notifications(entry) = nb_active_notifications;
+	DRV_LOG_ENTRY_tsc(entry) = tsc;
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_ENTRY_temporal_tag(entry) = overflow_tag;
+	DRV_LOG_COMPILER_MEM_BARRIER();
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn extern void UTILITY_Log (U8 category, U8 in_notification, U8 secondary,
+ *                          const char* function_name, U32 func_name_len,
+ *                          U32 line_number, const char* format_string, ...)
+ *
+ * @brief    Checks whether and where the message should be logged,
+ * and logs it as appropriate.
+ *
+ * @param
+ * U8          category        - message category
+ * U8          in_notification - whether or not we are in a notification/OS
+ * callback context (this information cannot be reliably obtained without
+ * passing it through the stack)
+ * U8          secondary       - secondary information field for the message
+ * const char* function_name   - name of the calling function
+ * U32         func_name_len   - length of the name of the calling function
+ * (more efficient to pass it as parameter than finding it back at runtime)
+ * U32         line_number     - line number of the call site
+ * const char* format_string   - classical format string for printf-like
+ * ...                         functions elements to print
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ * Takes a snapshot of various elements (TSC, driver state, etc.) to ensure
+ * a single log call writes consistent information to all applicable channels
+ * (i.e. favoring consistency over instantaneous accuracy).
+ * See utility_Log_Write for details.
+ */
+VOID UTILITY_Log(U8 category, U8 in_notification, U8 secondary,
+			const char *function_name, U32 func_name_len,
+			U32 line_number, const char *format_string, ...)
+{
+	U64 tsc_snapshot;
+	U8 ioctl_snapshot;
+	U8 driver_state_snapshot;
+	U16 processor_id_snapshot;
+	U16 nb_active_interrupts_snapshot;
+	U16 nb_active_notifications_snapshot;
+	U8 category_verbosity;
+	U8 in_interrupt;
+	U8 is_enabled;
+	va_list args;
+	U32 i;
+
+	category_verbosity = DRV_LOG_VERBOSITY(category);
+	processor_id_snapshot = raw_smp_processor_id();
+	in_interrupt = ((pcb && atomic_read(&CPU_STATE_in_interrupt(
+					&pcb[processor_id_snapshot]))) +
+			(category == DRV_LOG_CATEGORY_INTERRUPT));
+	is_enabled =
+		in_interrupt * !!(category_verbosity & LOG_CONTEXT_INTERRUPT) +
+		in_notification *
+			!!(category_verbosity & LOG_CONTEXT_NOTIFICATION) +
+		(!in_interrupt * !in_notification) *
+			!!(category_verbosity & LOG_CONTEXT_REGULAR);
+
+	if (!is_enabled) {
+		return;
+	}
+
+	ioctl_snapshot = active_ioctl;
+	driver_state_snapshot = GET_DRIVER_STATE();
+	nb_active_interrupts_snapshot =
+		DRV_LOG_BUFFER_nb_active_interrupts(DRV_LOG());
+	nb_active_notifications_snapshot =
+		DRV_LOG_BUFFER_nb_active_notifications(DRV_LOG());
+	UTILITY_Read_TSC(&tsc_snapshot);
+
+	va_start(args, format_string);
+
+	for (i = 0; i < 2; i++) {
+		if (category_verbosity & (1 << i)) {
+			va_list args_copy;
+
+			va_copy(args_copy, args);
+			utility_Log_Write(
+				i, category, secondary, function_name,
+				func_name_len, line_number,
+				tsc_snapshot, ioctl_snapshot,
+				processor_id_snapshot,
+				driver_state_snapshot,
+				nb_active_interrupts_snapshot,
+				nb_active_notifications_snapshot,
+				format_string, args_copy);
+			va_end(args_copy);
+		}
+	}
+	if (category_verbosity & LOG_CHANNEL_PRINTK ||
+	    category_verbosity & LOG_CHANNEL_TRACEK) {
+#define DRV_LOG_DEBUG_ARRAY_SIZE 512
+		char tmp_array[DRV_LOG_DEBUG_ARRAY_SIZE];
+		U32 nb_written_characters;
+		char *category_s, *sec1_s, *sec2_s, *sec3_s, *sec4_s;
+		va_list args_copy;
+
+		utility_Driver_Log_Kprint_Helper(category, &category_s,
+						 secondary, &sec1_s,
+						 &sec2_s, &sec3_s,
+						 &sec4_s);
+
+		nb_written_characters = DRV_SNPRINTF(
+			tmp_array, DRV_LOG_DEBUG_ARRAY_SIZE - 1,
+			DRV_LOG_DEBUG_ARRAY_SIZE - 1,
+			SEP_MSG_PREFIX " [%s%s%s%s%s] [%s@%d]: ",
+			category_s, sec1_s, sec2_s, sec3_s, sec4_s,
+			function_name, line_number);
+
+		if (nb_written_characters > 0) {
+			va_copy(args_copy, args);
+			nb_written_characters += DRV_VSNPRINTF(
+				tmp_array + nb_written_characters,
+				DRV_LOG_DEBUG_ARRAY_SIZE -
+					nb_written_characters - 1,
+				DRV_LOG_DEBUG_ARRAY_SIZE -
+					nb_written_characters - 1,
+				format_string, args_copy);
+			va_end(args_copy);
+#undef DRV_LOG_DEBUG_ARRAY_SIZE
+
+			tmp_array[nb_written_characters++] = '\n';
+			tmp_array[nb_written_characters++] = '\0';
+
+			if ((category_verbosity & LOG_CHANNEL_PRINTK) *
+			    !in_interrupt * !in_notification) {
+				if (!in_atomic()) {
+					switch (category) {
+					case DRV_LOG_CATEGORY_ERROR:
+					 	pr_err("%s", tmp_array);
+						break;
+					case DRV_LOG_CATEGORY_WARNING:
+						pr_debug("%s", tmp_array);
+						break;
+					default:
+						pr_info("%s", tmp_array);
+						break;
+					}
+				}
+			}
+
+			if (category_verbosity & LOG_CHANNEL_TRACEK) {
+				trace_printk("%s", tmp_array);
+			}
+		}
+	}
+
+	va_end(args);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern DRV_STATUS UTILITY_Driver_Log_Init (void)
+ *
+ * @brief    Allocates and initializes the driver log buffer.
+ *
+ * @param    none
+ *
+ * @return   OS_SUCCESS on success, OS_NO_MEM on error.
+ *
+ * <I>Special Notes:</I>
+ *  Should be (successfully) run before any non-LOAD log calls.
+ *  Allocates memory without going through CONTROL_Allocate (to avoid
+ *  complicating the instrumentation of CONTROL_* functions): calling
+ *  UTILITY_Driver_Log_Free is necessary to free the log structure.
+ *  Falls back to vmalloc when contiguous physical memory cannot be
+ *  allocated. This does not impact runtime behavior, but may impact
+ *  the easiness of retrieving the log from a core dump if the system
+ *  crashes.
+ */
+DRV_STATUS UTILITY_Driver_Log_Init(void)
+{
+	struct timespec cur_time;
+	U32 size = sizeof(*driver_log_buffer);
+	U8 using_contiguous_physical_memory;
+	U32 bitness;
+
+	if (size < MAX_KMALLOC_SIZE) {
+		// allocating outside regular func to restrict area of driver
+		driver_log_buffer = (PVOID)kmalloc(
+			size,
+			GFP_KERNEL); // where the log might not be initialized
+	} else {
+		driver_log_buffer =
+			(PVOID)__get_free_pages(GFP_KERNEL, get_order(size));
+	}
+
+	if (driver_log_buffer) {
+		using_contiguous_physical_memory = 1;
+	} else {
+		driver_log_buffer = vmalloc(size);
+
+		if (!driver_log_buffer) {
+			return OS_NO_MEM;
+		}
+
+		using_contiguous_physical_memory = 0;
+	}
+
+	memset(driver_log_buffer, DRV_LOG_FILLER_BYTE,
+	       sizeof(*driver_log_buffer));
+	// we don't want zero-filled pages
+	// (so that the buffer's pages don't get omitted in some crash dumps)
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[0] =
+		DRV_LOG_SIGNATURE_0;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[0] =
+		DRV_LOG_SIGNATURE_6;
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[3] =
+		DRV_LOG_SIGNATURE_3;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[3] =
+		DRV_LOG_SIGNATURE_3;
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[2] =
+		DRV_LOG_SIGNATURE_2;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[2] =
+		DRV_LOG_SIGNATURE_4;
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[1] =
+		DRV_LOG_SIGNATURE_1;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[1] =
+		DRV_LOG_SIGNATURE_5;
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[7] =
+		DRV_LOG_SIGNATURE_7;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[7] =
+		DRV_LOG_SIGNATURE_7;
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[5] =
+		DRV_LOG_SIGNATURE_5;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[5] =
+		DRV_LOG_SIGNATURE_1;
+
+	DRV_LOG_COMPILER_MEM_BARRIER();
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[6] =
+		DRV_LOG_SIGNATURE_6;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[6] =
+		DRV_LOG_SIGNATURE_0;
+	DRV_LOG_BUFFER_header_signature(driver_log_buffer)[4] =
+		DRV_LOG_SIGNATURE_4;
+	DRV_LOG_BUFFER_footer_signature(driver_log_buffer)[4] =
+		DRV_LOG_SIGNATURE_2;
+
+	DRV_LOG_BUFFER_log_size(driver_log_buffer) = sizeof(*driver_log_buffer);
+	DRV_LOG_BUFFER_max_nb_pri_entries(driver_log_buffer) =
+		DRV_LOG_MAX_NB_PRI_ENTRIES;
+	DRV_LOG_BUFFER_max_nb_aux_entries(driver_log_buffer) =
+		DRV_LOG_MAX_NB_AUX_ENTRIES;
+	getnstimeofday(&cur_time);
+	DRV_LOG_BUFFER_init_time(driver_log_buffer) = cur_time.tv_sec;
+	DRV_LOG_BUFFER_disambiguator(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_log_version(driver_log_buffer) = DRV_LOG_VERSION;
+	DRV_LOG_BUFFER_pri_entry_index(driver_log_buffer) = (U32)((S32)-1);
+	DRV_LOG_BUFFER_aux_entry_index(driver_log_buffer) = (U32)((S32)-1);
+
+#if defined(DRV_EM64T)
+	bitness = 64;
+#else
+	bitness = 32;
+#endif
+
+	DRV_SNPRINTF(DRV_LOG_BUFFER_driver_version(driver_log_buffer),
+		     DRV_LOG_DRIVER_VERSION_SIZE, DRV_LOG_DRIVER_VERSION_SIZE,
+		     "[%u-bit Linux] SEP v%d.%d (update %d). API %d.", bitness,
+		     SEP_MAJOR_VERSION, SEP_MINOR_VERSION, SEP_UPDATE_VERSION,
+		     SEP_API_VERSION);
+
+	DRV_LOG_BUFFER_driver_state(driver_log_buffer) = GET_DRIVER_STATE();
+	DRV_LOG_BUFFER_active_drv_operation(driver_log_buffer) = active_ioctl;
+	DRV_LOG_BUFFER_nb_drv_operations(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_nb_interrupts(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_nb_active_interrupts(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_nb_notifications(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_nb_active_notifications(driver_log_buffer) = 0;
+	DRV_LOG_BUFFER_nb_driver_state_transitions(driver_log_buffer) = 0;
+
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_LOAD) =
+		DRV_LOG_DEFAULT_LOAD_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_INIT) =
+		DRV_LOG_DEFAULT_INIT_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_DETECTION) =
+		DRV_LOG_DEFAULT_DETECTION_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_ERROR) =
+		DRV_LOG_DEFAULT_ERROR_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_STATE_CHANGE) =
+		DRV_LOG_DEFAULT_STATE_CHANGE_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_MARK) =
+		DRV_LOG_DEFAULT_MARK_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_DEBUG) =
+		DRV_LOG_DEFAULT_DEBUG_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_FLOW) =
+		DRV_LOG_DEFAULT_FLOW_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_ALLOC) =
+		DRV_LOG_DEFAULT_ALLOC_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_INTERRUPT) =
+		DRV_LOG_DEFAULT_INTERRUPT_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_TRACE) =
+		DRV_LOG_DEFAULT_TRACE_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_REGISTER) =
+		DRV_LOG_DEFAULT_REGISTER_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_NOTIFICATION) =
+		DRV_LOG_DEFAULT_NOTIFICATION_VERBOSITY;
+	DRV_LOG_VERBOSITY(DRV_LOG_CATEGORY_WARNING) =
+		DRV_LOG_DEFAULT_WARNING_VERBOSITY;
+
+	DRV_LOG_BUFFER_contiguous_physical_memory(driver_log_buffer) =
+		using_contiguous_physical_memory;
+
+	SEP_DRV_LOG_LOAD(
+		"Initialized driver log using %scontiguous physical memory.",
+		DRV_LOG_BUFFER_contiguous_physical_memory(driver_log_buffer) ?
+			"" :
+			"non-");
+
+	return OS_SUCCESS;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern DRV_STATUS UTILITY_Driver_Log_Free (void)
+ *
+ * @brief    Frees the driver log buffer.
+ *
+ * @param    none
+ *
+ * @return   OS_SUCCESS on success, OS_NO_MEM on error.
+ *
+ * <I>Special Notes:</I>
+ *           Should be done before unloading the driver.
+ *           See UTILITY_Driver_Log_Init for details.
+ */
+void UTILITY_Driver_Log_Free(void)
+{
+	U32 size = sizeof(*driver_log_buffer);
+
+	if (driver_log_buffer) {
+		if (DRV_LOG_BUFFER_contiguous_physical_memory(
+			    driver_log_buffer)) {
+			if (size < MAX_KMALLOC_SIZE) {
+				kfree(driver_log_buffer);
+			} else {
+				free_pages((unsigned long)driver_log_buffer,
+					   get_order(size));
+			}
+		} else {
+			vfree(driver_log_buffer);
+		}
+
+		driver_log_buffer = NULL;
+	}
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern void UTILITY_Driver_Set_Active_Ioctl (U32 ioctl)
+ *
+ * @brief    Sets the 'active_ioctl' global to the specified value.
+ *
+ * @param    U32 ioctl - ioctl/drvop code to use
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ * Used to keep track of the IOCTL operation currently being processed.
+ * This information is saved in the log buffer (globally), as well as
+ * in every log entry.
+ * NB: only IOCTLs for which grabbing the ioctl mutex is necessary
+ * should be kept track of this way.
+ */
+void UTILITY_Driver_Set_Active_Ioctl(U32 ioctl)
+{
+	active_ioctl = ioctl;
+	if (ioctl) {
+		DRV_LOG_BUFFER_nb_drv_operations(driver_log_buffer)++;
+	}
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern const char** UTILITY_Log_Category_Strings (void)
+ *
+ * @brief    Accessor function for the log category string array
+ *
+ * @param    none
+ *
+ * @return   none
+ *
+ * <I>Special Notes:</I>
+ *  Only needed for cosmetic purposes when adjusting category verbosities.
+ */
+const char **UTILITY_Log_Category_Strings(void)
+{
+	return drv_log_categories;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       extern U32 UTILITY_Change_Driver_State (U32 allowed_prior_states,
+ *		U32 state, const char* func, U32 line_number)
+ *
+ * @brief    Updates the driver state (if the transition is legal).
+ *
+ * @param    U32 allowed_prior_states   - the bitmask representing the states
+ *			from which the transition is allowed to occur
+ *           U32 state                  - the destination state
+ *           const char* func           - the callsite's function's name
+ *           U32 line_number            - the callsite's line number
+ *
+ * @return   1 in case of success, 0 otherwise
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+U32 UTILITY_Change_Driver_State(U32 allowed_prior_states, U32 state,
+				       const char *func, U32 line_number)
+{
+	U32 res = 1;
+	U32 previous_state;
+	U32 current_state = GET_DRIVER_STATE();
+	U32 nb_attempts = 0;
+
+	SEP_DRV_LOG_TRACE_IN(
+		"Prior states: 0x%x, state: %u, func: %p, line: %u.",
+		allowed_prior_states, state, func, line_number);
+
+	if (state >= DRV_LOG_NB_DRIVER_STATES) {
+		SEP_DRV_LOG_ERROR("Illegal destination state %d (%s@%u)!",
+				  state, func, line_number);
+		res = 0;
+		goto clean_return;
+	}
+
+	do {
+		previous_state = current_state;
+		nb_attempts++;
+		SEP_DRV_LOG_TRACE("Attempt #%d to transition to state %s.",
+				  nb_attempts, drv_log_states[state]);
+
+		if (DRIVER_STATE_IN(current_state, allowed_prior_states)) {
+			current_state = cmpxchg(&GET_DRIVER_STATE(),
+						previous_state, state);
+		} else {
+			SEP_DRV_LOG_ERROR(
+				"Invalid transition [%s -> %s] (%s@%u)!",
+				drv_log_states[previous_state],
+				drv_log_states[state], func, line_number);
+			res = 0;
+			goto clean_return;
+		}
+
+	} while (previous_state != current_state);
+
+	SEP_DRV_LOG_STATE_TRANSITION(previous_state, state, "From %s@%u.", func,
+				     line_number);
+
+clean_return:
+	SEP_DRV_LOG_TRACE_OUT("Res: %u.", res);
+	return res;
+}
diff --git a/drivers/platform/x86/sepdk/sep/valleyview_sochap.c b/drivers/platform/x86/sepdk/sep/valleyview_sochap.c
new file mode 100755
index 000000000000..7e1e5eb9c65f
--- /dev/null
+++ b/drivers/platform/x86/sepdk/sep/valleyview_sochap.c
@@ -0,0 +1,301 @@
+/* ****************************************************************************
+ *  Copyright(C) 2009-2018 Intel Corporation.  All Rights Reserved.
+ *
+ *  This file is part of SEP Development Kit
+ *
+ *  SEP Development Kit is free software; you can redistribute it
+ *  and/or modify it under the terms of the GNU General Public License
+ *  version 2 as published by the Free Software Foundation.
+ *
+ *  SEP Development Kit is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  As a special exception, you may use this file as part of a free software
+ *  library without restriction.  Specifically, if other files instantiate
+ *  templates or use macros or inline functions from this file, or you
+ *  compile this file and link it with other files to produce an executable
+ *  this file does not by itself cause the resulting executable to be
+ *  covered by the GNU General Public License.  This exception does not
+ *  however invalidate any other reasons why the executable file might be
+ *  covered by the GNU General Public License.
+ * ****************************************************************************
+ */
+
+#include "lwpmudrv_defines.h"
+#include "lwpmudrv_types.h"
+#include "lwpmudrv_ecb.h"
+#include "lwpmudrv_struct.h"
+
+#include "inc/ecb_iterators.h"
+#include "inc/control.h"
+#include "inc/utility.h"
+#include "inc/valleyview_sochap.h"
+
+static U64 *uncore_current_data;
+static U64 *uncore_to_read_data;
+extern DRV_CONFIG drv_cfg;
+
+#if 0
+extern U64 *read_counter_info;
+extern VOID SOCPERF_Read_Data3(PVOID data_buffer);
+#endif
+
+/*!
+ * @fn         static VOID valleyview_VISA_Initialize(PVOID)
+ *
+ * @brief      Initialize any registers or addresses
+ *
+ * @param      param
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID valleyview_VISA_Initialize(VOID *param)
+{
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	// Allocate memory for reading GMCH counter values + the group id
+	if (!uncore_current_data) {
+		uncore_current_data = CONTROL_Allocate_Memory(
+			(VLV_CHAP_MAX_COUNTERS + 1) * sizeof(U64));
+		if (!uncore_current_data) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Early exit (uncore_current_data is NULL!).");
+			return;
+		}
+	}
+	if (!uncore_to_read_data) {
+		uncore_to_read_data = CONTROL_Allocate_Memory(
+			(VLV_CHAP_MAX_COUNTERS + 1) * sizeof(U64));
+		if (!uncore_to_read_data) {
+			SEP_DRV_LOG_ERROR_TRACE_OUT(
+				"Early exit (uncore_to_read_data is NULL!).");
+			return;
+		}
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID valleyview_VISA_Enable_PMU(PVOID)
+ *
+ * @brief      Start counting
+ *
+ * @param      param - device index
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID valleyview_VISA_Enable_PMU(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+
+	SEP_DRV_LOG_TRACE("Starting the counters...");
+	if (uncore_current_data) {
+		memset(uncore_current_data, 0,
+		       (VLV_CHAP_MAX_COUNTERS + 1) * sizeof(U64));
+	}
+	if (uncore_to_read_data) {
+		memset(uncore_to_read_data, 0,
+		       (VLV_CHAP_MAX_COUNTERS + 1) * sizeof(U64));
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID valleyview_VISA_Disable_PMU(PVOID)
+ *
+ * @brief      Unmap the virtual address when sampling/driver stops
+ *
+ * @param      param - device index
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID valleyview_VISA_Disable_PMU(PVOID param)
+{
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 cur_driver_state;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	cur_driver_state = GET_DRIVER_STATE();
+
+	if (!CPU_STATE_system_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!system_master).");
+		return;
+	}
+	SEP_DRV_LOG_TRACE("Stopping the counters...");
+	if (cur_driver_state == DRV_STATE_PREPARE_STOP ||
+	    cur_driver_state == DRV_STATE_STOPPED ||
+	    cur_driver_state == DRV_STATE_TERMINATING) {
+		uncore_current_data = CONTROL_Free_Memory(uncore_current_data);
+		uncore_to_read_data = CONTROL_Free_Memory(uncore_to_read_data);
+	}
+
+	SEP_DRV_LOG_TRACE_OUT("");
+}
+
+/*!
+ * @fn         static VOID valleyview_VISA_Clean_Up(PVOID)
+ *
+ * @brief      Reset any registers or addresses
+ *
+ * @param      param
+ *
+ * @return     None
+ *
+ * <I>Special Notes:</I>
+ */
+static VOID valleyview_VISA_Clean_Up(VOID *param)
+{
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+	SEP_DRV_LOG_TRACE_OUT("Empty function.");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn valleyview_VISA_Read_PMU_Data(param)
+ *
+ * @param    param    The device index
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the Uncore count data and store into the buffer param;
+ *
+ */
+static VOID valleyview_VISA_Read_PMU_Data(PVOID param)
+{
+#if 0
+	U32 j;
+	U64 *buffer = read_counter_info;
+	U32 dev_idx;
+	U32 this_cpu;
+	CPU_STATE pcpu;
+	U32 package_num;
+	U32 event_index = 0;
+	U32 cur_grp;
+	ECB pecb;
+	U64 counter_buffer[VLV_CHAP_MAX_COUNTERS + 1];
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p.", param);
+
+	dev_idx = *((U32 *)param);
+	this_cpu = CONTROL_THIS_CPU();
+	pcpu = &pcb[this_cpu];
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[(dev_idx)])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[(dev_idx)])[cur_grp];
+
+	// NOTE THAT the read_pmu function on for EMON collection.
+	if (!DRV_CONFIG_emon_mode(drv_cfg)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!emon_mode).");
+		return;
+	}
+	if (!CPU_STATE_socket_master(pcpu)) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!socket_master).");
+		return;
+	}
+	if (!pecb) {
+		SEP_DRV_LOG_TRACE_OUT("Early exit (!pecb).");
+		return;
+	}
+
+	SOCPERF_Read_Data3((void*)counter_buffer);
+
+	FOR_EACH_REG_UNC_OPERATION(pecb, dev_idx, idx, PMU_OPERATION_READ)
+	{
+		//the buffer index for this PMU needs to account for each event
+		j = ECB_entries_uncore_buffer_offset_in_system(pecb, idx);
+		buffer[j] = counter_buffer[event_index + 1];
+		event_index++;
+		SEP_DRV_LOG_TRACE("j=%u, value=%llu, cpu=%u", j, buffer[j],
+				  this_cpu);
+	}
+	END_FOR_EACH_REG_UNC_OPERATION;
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn valleyview_Trigger_Read()
+ *
+ * @param    None
+ *
+ * @return   None     No return needed
+ *
+ * @brief    Read the SoCHAP counters when timer is triggered
+ *
+ */
+static VOID valleyview_Trigger_Read(PVOID param, U32 id)
+{
+#if 0
+	U64 *data = (U64 *)param;
+	U32 cur_grp;
+	ECB pecb;
+	U32 this_cpu;
+	U32 package_num;
+
+	SEP_DRV_LOG_TRACE_IN("Param: %p, , id: %u.", param, id);
+
+	this_cpu = CONTROL_THIS_CPU();
+	package_num = core_to_package_map[this_cpu];
+	cur_grp = LWPMU_DEVICE_cur_group(&devices[id])[package_num];
+	pecb = LWPMU_DEVICE_PMU_register_data(&devices[id])[cur_grp];
+
+	// group id
+	data = (U64 *)((S8 *)data + ECB_group_offset(pecb));
+	SOCPERF_Read_Data3((void*)data);
+
+	SEP_DRV_LOG_TRACE_OUT("");
+#endif
+}
+
+/*
+ * Initialize the dispatch table
+ */
+DISPATCH_NODE valleyview_visa_dispatch = {
+	.init = valleyview_VISA_Initialize,
+	.fini = NULL,
+	.write = NULL,
+	.freeze = valleyview_VISA_Disable_PMU,
+	.restart = valleyview_VISA_Enable_PMU,
+	.read_data = valleyview_VISA_Read_PMU_Data,
+	.check_overflow = NULL,
+	.swap_group = NULL,
+	.read_lbrs = NULL,
+	.cleanup = valleyview_VISA_Clean_Up,
+	.hw_errata = NULL,
+	.read_power = NULL,
+	.check_overflow_errata = NULL,
+	.read_counts = NULL,
+	.check_overflow_gp_errata = NULL,
+	.read_ro = NULL,
+	.platform_info = NULL,
+	.trigger_read = valleyview_Trigger_Read,
+	.scan_for_uncore = NULL,
+	.read_metrics = NULL
+};
diff --git a/drivers/platform/x86/socwatch/Kconfig b/drivers/platform/x86/socwatch/Kconfig
new file mode 100644
index 000000000000..87a7ae205f2d
--- /dev/null
+++ b/drivers/platform/x86/socwatch/Kconfig
@@ -0,0 +1,6 @@
+menuconfig INTEL_SOCWATCH
+	depends on X86
+	tristate "SocWatch Driver Support"
+	default m
+	help
+	  Say Y here to enable SocWatch driver
diff --git a/drivers/platform/x86/socwatch/Makefile b/drivers/platform/x86/socwatch/Makefile
new file mode 100644
index 000000000000..15ac18fcfdc0
--- /dev/null
+++ b/drivers/platform/x86/socwatch/Makefile
@@ -0,0 +1,22 @@
+#
+# Makefile for the socwatch driver.
+#
+
+DRIVER_BASE=socwatch
+DRIVER_MAJOR=2
+DRIVER_MINOR=6
+# basic name of driver
+DRIVER_NAME=${DRIVER_BASE}${DRIVER_MAJOR}_${DRIVER_MINOR}
+
+DO_DRIVER_PROFILING=0
+
+ccflags-y +=	-Idrivers/platform/x86/socwatch/inc/ \
+		-DDO_DRIVER_PROFILING=$(DO_DRIVER_PROFILING)
+
+obj-$(CONFIG_INTEL_SOCWATCH)    += $(DRIVER_NAME).o
+
+$(DRIVER_NAME)-objs	:= sw_driver.o sw_hardware_io.o \
+			sw_output_buffer.o sw_tracepoint_handlers.o \
+			sw_mem.o sw_collector.o sw_telem.o \
+			sw_file_ops.o sw_internal.o sw_ops_provider.o \
+			sw_reader.o sw_trace_notifier_provider.o
diff --git a/drivers/platform/x86/socwatch/inc/sw_collector.h b/drivers/platform/x86/socwatch/inc/sw_collector.h
new file mode 100644
index 000000000000..b771ab936b26
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_collector.h
@@ -0,0 +1,136 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_COLLECTOR_H__
+
+#include "sw_internal.h"
+
+/*
+ * Forward declaration
+ */
+struct sw_hw_ops;
+
+// TODO: convert from 'list_head' to 'hlist_head'
+/**
+ * struct - sw_collector_data
+ * Information about the collector to be invoked at collection time.
+ *
+ * The collector_lists array holds linked lists of collectors to
+ * be exercised at specific points in time during the collection
+ * (e.g. begin, poll, end, etc.).  At a trigger time, the driver walks
+ * that time's list of nodes, and exercises the collectors on that list.
+ *
+ * @list:                   List/link implementation
+ * @cpumask:                Collect if cpu matches mask
+ * @info:                   Ptr to metric info
+ * @ops:                    Ptr to collector's operations
+ * @last_update_jiffies:    Indicates when this node was last exercised.
+ * @per_msg_payload_size:   Data size
+ * @msg:                    Ptr to collected data
+ */
+typedef struct sw_collector_data {
+	SW_LIST_ENTRY(list, sw_collector_data);
+	struct cpumask cpumask;
+	struct sw_driver_interface_info *info;
+	const struct sw_hw_ops **ops;
+	size_t per_msg_payload_size;
+	u64 last_update_jiffies;
+	struct sw_driver_msg *msg;
+} sw_collector_data_t;
+#define GET_MSG_SLOT_FOR_CPU(msgs, cpu, size)                                  \
+	((struct sw_driver_msg *)&(                                            \
+		((char *)(msgs))[(cpu) *                                       \
+				 (sizeof(struct sw_driver_msg) + (size))]))
+
+struct sw_collector_data *sw_alloc_collector_node(void);
+void sw_free_collector_node(struct sw_collector_data *node);
+int sw_handle_collector_node(struct sw_collector_data *data);
+int sw_handle_collector_node_on_cpu(struct sw_collector_data *data, int cpu);
+int sw_write_collector_node(struct sw_collector_data *data);
+
+void sw_init_collector_list(void *list_head);
+void sw_destroy_collector_list(void *list_head);
+int sw_handle_collector_list(void *list_head,
+			     int (*func)(struct sw_collector_data *data));
+int sw_handle_collector_list_on_cpu(void *list_head,
+				    int (*func)(struct sw_collector_data *data,
+						int cpu),
+				    int cpu);
+
+int sw_handle_driver_io_descriptor(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	const struct sw_hw_ops *hw_ops);
+int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
+int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor);
+
+int sw_add_driver_info(void *list_head,
+		       const struct sw_driver_interface_info *info);
+
+void sw_handle_per_cpu_msg(void *info);
+void sw_handle_per_cpu_msg_no_sched(void *info);
+void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info);
+
+void sw_set_collector_ops(const struct sw_hw_ops *hw_ops);
+
+/**
+ * Process all messages for the given time.
+ * @param[in]   when    The time period e.g. 'BEGIN' or 'END'
+ *
+ * @returns     0   on success, non-zero on error
+ */
+extern int sw_process_snapshot(enum sw_when_type when);
+extern int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu);
+#endif // __SW_COLLECTOR_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_defines.h b/drivers/platform/x86/socwatch/inc/sw_defines.h
new file mode 100644
index 000000000000..15ccca1efed6
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_defines.h
@@ -0,0 +1,156 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_DEFINES_H_
+#define _PW_DEFINES_H_ 1
+
+#include "sw_version.h"
+
+/* ***************************************************
+ * Common to kernel and userspace.
+ * ***************************************************
+ */
+#define PW_SUCCESS 0
+#define PW_ERROR 1
+#define PW_SUCCESS_NO_COLLECT 2
+
+/*
+ * Helper macro to convert 'u64' to 'unsigned long long' to avoid gcc warnings.
+ */
+#define TO_ULL(x) (unsigned long long)(x)
+/*
+* Convert an arg to 'long long'
+*/
+#define TO_LL(x) (long long)(x)
+/*
+ * Convert an arg to 'unsigned long'
+ */
+#define TO_UL(x) (unsigned long)(x)
+/*
+ * Helper macro for string representation of a boolean value.
+ */
+#define GET_BOOL_STRING(b) ((b) ? "TRUE" : "FALSE")
+
+/*
+ * Circularly increment 'i' MODULO 'l'.
+ * ONLY WORKS IF 'l' is (power of 2 - 1) ie.
+ * l == (2 ^ x) - 1
+ */
+#define CIRCULAR_INC(index, mask) (((index) + 1) & (mask))
+#define CIRCULAR_ADD(index, val, mask) (((index) + (val)) & (mask))
+/*
+ * Circularly decrement 'i'.
+ */
+#define CIRCULAR_DEC(i, m)                                                     \
+	({                                                                     \
+		int __tmp1 = (i);                                              \
+		if (--__tmp1 < 0)                                              \
+			__tmp1 = (m);                                          \
+		__tmp1;                                                        \
+	})
+/*
+ * Retrieve size of an array.
+ */
+#define SW_ARRAY_SIZE(array) (sizeof(array) / sizeof((array)[0]))
+/*
+ * Should the driver count number of dropped samples?
+ */
+#define DO_COUNT_DROPPED_SAMPLES 1
+/*
+ * Extract F/W major, minor versions.
+ * Assumes version numbers are 8b unsigned ints.
+ */
+#define SW_GET_SCU_FW_VERSION_MAJOR(ver) (((ver) >> 8) & 0xff)
+#define SW_GET_SCU_FW_VERSION_MINOR(ver) ((ver)&0xff)
+/*
+ * Max size of process name retrieved from kernel.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+
+/*
+ * Number of SOCPERF counters.
+ * Needed by both Ring-0 and Ring-3
+ */
+#define SW_NUM_SOCPERF_COUNTERS 9
+
+/*
+ * Max size of process name retrieved from kernel space.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+/*
+ * Max size of kernel wakelock name.
+ */
+#define SW_MAX_KERNEL_WAKELOCK_NAME_SIZE 100
+
+/* Data value read when a telemetry data read fails. */
+#define SW_TELEM_READ_FAIL_VALUE 0xF00DF00DF00DF00DUL
+
+#ifdef SWW_MERGE
+typedef enum {
+	SW_STOP_EVENT = 0,
+	SW_CS_EXIT_EVENT,
+	SW_COUNTER_RESET_EVENT,
+	SW_COUNTER_HOTKEY_EVENT,
+	SW_MAX_COLLECTION_EVENT
+} collector_stop_event_t;
+#endif // SWW_MERGE
+
+#define MAX_UNSIGNED_16_BIT_VALUE 0xFFFF
+#define MAX_UNSIGNED_24_BIT_VALUE 0xFFFFFF
+#define MAX_UNSIGNED_32_BIT_VALUE 0xFFFFFFFF
+#define MAX_UNSIGNED_64_BIT_VALUE 0xFFFFFFFFFFFFFFFF
+
+#endif // _PW_DEFINES_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_file_ops.h b/drivers/platform/x86/socwatch/inc/sw_file_ops.h
new file mode 100644
index 000000000000..7c5705cf942c
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_file_ops.h
@@ -0,0 +1,70 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_FILE_OPS_H__
+#define __SW_FILE_OPS_H__
+
+enum sw_driver_collection_cmd;
+struct sw_file_ops {
+	long (*ioctl_handler)(unsigned int ioctl_num, void *local_args);
+	int (*stop_handler)(void);
+	enum sw_driver_collection_cmd (*get_current_cmd)(void);
+	bool (*should_flush)(void);
+};
+
+int sw_register_dev(struct sw_file_ops *ops);
+void sw_unregister_dev(void);
+
+#endif // __SW_FILE_OPS_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_hardware_io.h b/drivers/platform/x86/socwatch/inc/sw_hardware_io.h
new file mode 100644
index 000000000000..f93fa6b10d7a
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_hardware_io.h
@@ -0,0 +1,118 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_HARDWARE_IO_H__
+#define __SW_HARDWARE_IO_H__
+
+#include "sw_structs.h"
+
+typedef int (*sw_io_desc_init_func_t)(
+	struct sw_driver_io_descriptor *descriptor);
+typedef void (*sw_hardware_op_func_t)(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	u16 counter_size_in_bytes);
+typedef int (*sw_io_desc_print_func_t)(
+	const struct sw_driver_io_descriptor *descriptor);
+typedef int (*sw_io_desc_reset_func_t)(
+	const struct sw_driver_io_descriptor *descriptor);
+typedef bool (*sw_io_desc_available_func_t)(void);
+typedef bool (*sw_hw_op_post_config_func_t)(void);
+
+/**
+ * struct sw_hw_ops - Operations for each of the HW collection mechanisms
+ *                    in swkernelcollector.
+ * @name:           A descriptive name used to identify this particular operation.
+ * @init:           Initialize a metric's collection.
+ * @read:           Read a metric's data.
+ * @write:          Write to the HW for the metric(?).
+ * @print:          Print out the data.
+ * @reset:          Opposite of init--called after we're done collecting.
+ * @available:      Decide whether this H/W op is available on the current platform.
+ * @post_config:    Perform any post-configuration steps.
+ */
+struct sw_hw_ops {
+	const char *name;
+	sw_io_desc_init_func_t init;
+	sw_hardware_op_func_t read;
+	sw_hardware_op_func_t write;
+	sw_io_desc_print_func_t print;
+	sw_io_desc_reset_func_t reset;
+	sw_io_desc_available_func_t available;
+	sw_hw_op_post_config_func_t post_config;
+};
+
+bool sw_is_valid_hw_op_id(int id);
+int sw_get_hw_op_id(const struct sw_hw_ops *op);
+const struct sw_hw_ops *sw_get_hw_ops_for(int id);
+const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op);
+
+int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
+		      void *priv, bool return_on_error);
+
+/**
+ * Add an operation to the list of providers.
+ */
+int sw_register_hw_op(const struct sw_hw_ops *ops);
+/**
+ * Register all H/W operations.
+ */
+int sw_register_hw_ops(void);
+/**
+ * Unregister previously registered H/W operations.
+ */
+void sw_free_hw_ops(void);
+
+#endif // __SW_HARDWARE_IO_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_internal.h b/drivers/platform/x86/socwatch/inc/sw_internal.h
new file mode 100644
index 000000000000..8e88d5d5ea54
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_internal.h
@@ -0,0 +1,138 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_DATA_STRUCTS_H__
+#define __SW_DATA_STRUCTS_H__
+
+/*
+ * Taken from 'sw_driver'
+ * TODO: move to separate file?
+ */
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/cpumask.h>
+#include <linux/hrtimer.h>
+#include <linux/fs.h> // inode
+#include <linux/device.h> // class_create
+#include <linux/cdev.h> // cdev_alloc
+#include <linux/vmalloc.h> // vmalloc
+#include <linux/sched.h> // TASK_INTERRUPTIBLE
+#include <linux/wait.h> // wait_event_interruptible
+#include <linux/pci.h> // pci_get_bus_and_slot
+#include <linux/version.h> // LINUX_VERSION_CODE
+#include <linux/sfi.h> // For SFI F/W version
+#include <asm/hardirq.h>
+#include <linux/cpufreq.h>
+#include <asm/local.h> // local_t
+#include <linux/hardirq.h> // "in_atomic"
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
+#include <asm/uaccess.h> // copy_to_user
+#else
+#include <linux/uaccess.h> // copy_to_user
+#endif // LINUX_VERSION_CODE
+
+#ifdef CONFIG_X86_WANT_INTEL_MID
+#include <asm/intel-mid.h>
+#endif // CONFIG_X86_WANT_INTEL_MID
+/*
+ * End taken from sw_driver
+ */
+
+#include "sw_structs.h"
+#include "sw_ioctl.h"
+#include "sw_list.h"
+
+/* ******************************************
+ * Compile time constants
+ * ******************************************
+ */
+#define GET_POLLED_CPU() (sw_max_num_cpus)
+
+/* ******************************************
+ * Function declarations.
+ * ******************************************
+ */
+/*
+ * Output to user.
+ */
+unsigned long sw_copy_to_user(char __user *dst,
+			      char *src, size_t bytes_to_copy);
+bool sw_check_output_buffer_params(void __user *buffer, size_t bytes_to_read,
+				   size_t buff_size);
+/*
+ * smp call function.
+ */
+void sw_schedule_work(const struct cpumask *mask, void (*work)(void *),
+		      void *data);
+/*
+ * Save IRQ flags and retrieve cpu number.
+ */
+int sw_get_cpu(unsigned long *flags);
+/*
+ * Restore IRQ flags.
+ */
+void sw_put_cpu(unsigned long flags);
+/*
+ * Set module scope for cpu frequencies.
+ */
+int sw_set_module_scope_for_cpus(void);
+/*
+ * reset module scope for cpu frequencies.
+ */
+int sw_reset_module_scope_for_cpus(void);
+
+#endif // __SW_DATA_STRUCTS_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_ioctl.h b/drivers/platform/x86/socwatch/inc/sw_ioctl.h
new file mode 100644
index 000000000000..baf93058c5c5
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_ioctl.h
@@ -0,0 +1,303 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_IOCTL_H__
+#define __SW_IOCTL_H__ 1
+
+#if defined(__linux__) || defined(__QNX__)
+#if __KERNEL__
+#include <linux/ioctl.h>
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#include <asm/compat.h>
+#include <linux/compat.h>
+#endif // COMPAT && x64
+#else // !__KERNEL__
+#include <sys/ioctl.h>
+#endif // __KERNEL__
+#endif // __linux__
+/*
+ * Ensure we pull in definition of 'DO_COUNT_DROPPED_SAMPLES'!
+ */
+#include "sw_defines.h"
+
+#ifdef ONECORE
+#ifndef __KERNEL__
+#include <winioctl.h>
+#endif //__KERNEL__
+#endif // ONECORE
+
+/*
+ * The APWR-specific IOCTL magic
+ * number -- used to ensure IOCTLs
+ * are delivered to the correct
+ * driver.
+ */
+// #define APWR_IOCTL_MAGIC_NUM 0xdead
+#define APWR_IOCTL_MAGIC_NUM 100
+
+/*
+ * The name of the device file
+ */
+// #define DEVICE_FILE_NAME "/dev/pw_driver_char_dev"
+#define PW_DEVICE_FILE_NAME "/dev/apwr_driver_char_dev"
+#define PW_DEVICE_NAME "apwr_driver_char_dev"
+
+enum sw_ioctl_cmd {
+	sw_ioctl_cmd_none = 0,
+	sw_ioctl_cmd_config,
+	sw_ioctl_cmd_cmd,
+	sw_ioctl_cmd_poll,
+	sw_ioctl_cmd_immediate_io,
+	sw_ioctl_cmd_scu_version,
+	sw_ioctl_cmd_read_immediate,
+	sw_ioctl_cmd_driver_version,
+	sw_ioctl_cmd_avail_trace,
+	sw_ioctl_cmd_avail_notify,
+	sw_ioctl_cmd_avail_collect,
+	sw_ioctl_cmd_topology_changes,
+};
+/*
+ * The actual IOCTL commands.
+ *
+ * From the kernel documentation:
+ * "_IOR" ==> Read IOCTL
+ * "_IOW" ==> Write IOCTL
+ * "_IOWR" ==> Read/Write IOCTL
+ *
+ * Where "Read" and "Write" are from the user's perspective
+ * (similar to the file "read" and "write" calls).
+ */
+#ifdef SWW_MERGE // Windows
+//
+// Device type           -- in the "User Defined" range."
+//
+#define POWER_I_CONF_TYPE 40000
+
+// List assigned tracepoint id
+#define CSIR_TRACEPOINT_ID_MASK 1
+#define DEVICE_STATE_TRACEPOINT_ID_MASK 2
+#define CSIR_SEPARATE_TRACEPOINT_ID_MASK 3
+#define RESET_TRACEPOINT_ID_MASK 4
+#define DISPLAY_ON_TRACEPOINT_ID_MASK 5
+
+#ifdef SWW_MERGE
+//
+// TELEM BAR CONFIG
+//
+#define MAX_TELEM_BAR_CFG 3
+#define TELEM_MCHBAR_CFG 0
+#define TELEM_IPC1BAR_CFG 1
+#define TELEM_SSRAMBAR_CFG 2
+#endif
+
+//
+// The IOCTL function codes from 0x800 to 0xFFF are for customer use.
+//
+#define PW_IOCTL_CONFIG                                                        \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x900, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_START_COLLECTION                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x901, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_STOP_COLLECTION                                               \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x902, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+// TODO: pause, resume, cancel not supported yet
+#define PW_IOCTL_PAUSE_COLLECTION                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x903, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_RESUME_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x904, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_CANCEL_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x905, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+#define PW_IOCTL_GET_PROCESSOR_GROUP_TOPOLOGY                                  \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x906, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_TOPOLOGY                                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x907, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x908, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x909, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DRV_CLEANUP                                                   \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90A, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_COLLECTION_EVENT                                          \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90B, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_TRY_STOP_EVENT                                                \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90C, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_PCH_ACTIVE_INTERVAL                                       \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90D, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_TELEM_BAR                                                 \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90E, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_METADATA                                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90F, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_GBE_INTERVAL                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x910, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_ENABLE_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x911, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DISABLE_COLLECTION                                            \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x912, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DRIVER_BUILD_DATE                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x913, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+#elif !defined(__APPLE__)
+#define PW_IOCTL_CONFIG                                                        \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config,                        \
+	     struct sw_driver_ioctl_arg *)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD                                                           \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                          \
+	      struct sw_driver_ioctl_arg *)
+#else
+#define PW_IOCTL_CMD                                                           \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                           \
+	     struct sw_driver_ioctl_arg *)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io,                 \
+	      struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_SCU_FW_VERSION                                            \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version,                   \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_READ_IMMEDIATE                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate,               \
+	      struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_DRIVER_VERSION                                            \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version,                \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS                                     \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace,                   \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS                                       \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify,                  \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect,                 \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes,              \
+	     struct sw_driver_ioctl_arg *)
+#else // __APPLE__
+#define PW_IOCTL_CONFIG                                                        \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config,                        \
+	     struct sw_driver_ioctl_arg)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD                                                           \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                          \
+	      struct sw_driver_ioctl_arg)
+#else
+#define PW_IOCTL_CMD                                                           \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io,                 \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_SCU_FW_VERSION                                            \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version,                  \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_READ_IMMEDIATE                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate,               \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_DRIVER_VERSION                                            \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version,               \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS                                     \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace,                  \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS                                       \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify,                 \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect,                \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES                                          \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes,             \
+	      struct sw_driver_ioctl_arg)
+#endif // __APPLE__
+
+/*
+ * 32b-compatible version of the above
+ * IOCTL numbers. Required ONLY for
+ * 32b compatibility on 64b systems,
+ * and ONLY by the driver.
+ */
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#define PW_IOCTL_CONFIG32                                                      \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, compat_uptr_t)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD32                                                         \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#else
+#define PW_IOCTL_CMD32                                                         \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL32 _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO32                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, compat_uptr_t)
+#define PW_IOCTL_GET_SCU_FW_VERSION32                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, compat_uptr_t)
+#define PW_IOCTL_READ_IMMEDIATE32                                              \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, compat_uptr_t)
+#define PW_IOCTL_GET_DRIVER_VERSION32                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS32                                   \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS32                                     \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS32                                    \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, compat_uptr_t)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES32                                        \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, compat_uptr_t)
+#endif // defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#endif // __SW_IOCTL_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h b/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
new file mode 100644
index 000000000000..275b1bdfc25e
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_kernel_defines.h
@@ -0,0 +1,164 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef _SW_KERNEL_DEFINES_H_
+#define _SW_KERNEL_DEFINES_H_ 1
+
+#include "sw_defines.h"
+
+#if defined(__APPLE__)
+#define likely(x) (x)
+#define unlikely(x) (x)
+#endif // __APPLE__
+
+#if !defined(__APPLE__)
+#define CPU() (raw_smp_processor_id())
+#define RAW_CPU() (raw_smp_processor_id())
+#else
+#define CPU() (cpu_number())
+#define RAW_CPU() (cpu_number())
+#endif // __APPLE__
+
+#define TID() (current->pid)
+#define PID() (current->tgid)
+#define NAME() (current->comm)
+#define PKG(c) (cpu_data(c).phys_proc_id)
+#define IT_REAL_INCR() (current->signal->it_real_incr.tv64)
+
+#define ATOMIC_CAS(ptr, old_val, new_val)                                      \
+	(cmpxchg((ptr), (old_val), (new_val)) == (old_val))
+
+/*
+ * Should we measure overheads?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_OVERHEAD_MEASUREMENTS 0
+/*
+ * Should we track memory usage?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_TRACK_MEMORY_USAGE 0
+/*
+ * Are we compiling with driver profiling support
+ * turned ON? If YES then force 'DO_OVERHEAD_MEASUREMENTS'
+ * and 'DO_TRACK_MEMORY_USAGE' to be TRUE.
+ */
+#if DO_DRIVER_PROFILING
+#if !DO_OVERHEAD_MEASUREMENTS
+#undef DO_OVERHEAD_MEASUREMENTS
+#define DO_OVERHEAD_MEASUREMENTS 1
+#endif // DO_OVERHEAD_MEASUREMENTS
+#if !DO_TRACK_MEMORY_USAGE
+#undef DO_TRACK_MEMORY_USAGE
+#define DO_TRACK_MEMORY_USAGE 1
+#endif // DO_TRACK_MEMORY_USAGE
+#endif // DO_DRIVER_PROFILING
+/*
+ * Should we allow debug output.
+ * Set to: "1" ==> 'OUTPUT' is enabled.
+ *         "0" ==> 'OUTPUT' is disabled.
+ */
+#define DO_DEBUG_OUTPUT 0
+/*
+ * Control whether to output driver ERROR messages.
+ * These are independent of the 'OUTPUT' macro
+ * (which controls debug messages).
+ * Set to '1' ==> Print driver error messages (to '/var/log/messages')
+ *        '0' ==> Do NOT print driver error messages
+ */
+#define DO_PRINT_DRIVER_ERROR_MESSAGES 1
+/*
+ * Macros to control output printing.
+ */
+#if !defined(__APPLE__)
+#if DO_DEBUG_OUTPUT
+#define pw_pr_debug(...) printk(KERN_INFO __VA_ARGS__)
+#define pw_pr_warn(...) printk(KERN_WARNING __VA_ARGS__)
+#else
+#define pw_pr_debug(...)
+#define pw_pr_warn(...)
+#endif
+#define pw_pr_force(...) printk(KERN_INFO __VA_ARGS__)
+#else
+#if DO_DEBUG_OUTPUT
+#define pw_pr_debug(...) IOLog(__VA_ARGS__)
+#define pw_pr_warn(...) IOLog(__VA_ARGS__)
+#else
+#define pw_pr_debug(...)
+#define pw_pr_warn(...)
+#endif
+#define pw_pr_force(...) IOLog(__VA_ARGS__)
+#endif // __APPLE__
+
+/*
+ * Macro for driver error messages.
+ */
+#if !defined(__APPLE__)
+#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+#define pw_pr_error(...) printk(KERN_ERR __VA_ARGS__)
+#else
+#define pw_pr_error(...)
+#endif
+#else
+#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+#define pw_pr_error(...) IOLog(__VA_ARGS__)
+#else
+#define pw_pr_error(...)
+#endif
+#endif // __APPLE__
+
+#endif // _SW_KERNEL_DEFINES_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_list.h b/drivers/platform/x86/socwatch/inc/sw_list.h
new file mode 100644
index 000000000000..ecc646a99caa
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_list.h
@@ -0,0 +1,76 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_LIST_H__
+#define __SW_LIST_H__
+
+#include <linux/list.h>
+
+#define SW_DEFINE_LIST_HEAD(name, dummy) struct list_head name
+#define SW_DECLARE_LIST_HEAD(name, dummy) extern struct list_head name
+#define SW_LIST_ENTRY(name, dummy) struct list_head name
+#define SW_LIST_HEAD_VAR(dummy) struct list_head
+#define SW_LIST_HEAD_INIT(head) INIT_LIST_HEAD(head)
+#define SW_LIST_ENTRY_INIT(node, field) INIT_LIST_HEAD(&node->field)
+#define SW_LIST_ADD(head, node, field) list_add_tail(&node->field, head)
+#define SW_LIST_GET_HEAD_ENTRY(head, type, field)                              \
+	list_first_entry(head, struct type, field)
+#define SW_LIST_UNLINK(node, field) list_del(&node->field)
+#define SW_LIST_FOR_EACH_ENTRY(node, head, field)                              \
+	list_for_each_entry(node, head, field)
+#define SW_LIST_EMPTY(head) list_empty(head)
+#define SW_LIST_HEAD_INITIALIZER(head) LIST_HEAD_INIT(head)
+
+#endif // __SW_LIST_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_lock_defs.h b/drivers/platform/x86/socwatch/inc/sw_lock_defs.h
new file mode 100644
index 000000000000..7c9d68c02f58
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_lock_defs.h
@@ -0,0 +1,98 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+/*
+ * Description: file containing locking routines
+ * used by the power driver.
+ */
+
+#ifndef __SW_LOCK_DEFS_H__
+#define __SW_LOCK_DEFS_H__
+
+#define SW_DEFINE_SPINLOCK(s) DEFINE_SPINLOCK(s)
+#define SW_DECLARE_SPINLOCK(s) static spinlock_t s
+
+#define SW_INIT_SPINLOCK(s) spin_lock_init(&s)
+#define SW_DESTROY_SPINLOCK(s) /* NOP */
+
+#define LOCK(l)                                                                \
+	{                                                                      \
+		unsigned long _tmp_l_flags;                                    \
+		spin_lock_irqsave(&(l), _tmp_l_flags);
+
+#define UNLOCK(l)                                                              \
+	spin_unlock_irqrestore(&(l), _tmp_l_flags);                            \
+	}
+
+#define READ_LOCK(l)                                                           \
+	{                                                                      \
+		unsigned long _tmp_l_flags;                                    \
+		read_lock_irqsave(&(l), _tmp_l_flags);
+
+#define READ_UNLOCK(l)                                                         \
+	read_unlock_irqrestore(&(l), _tmp_l_flags);                            \
+	}
+
+#define WRITE_LOCK(l)                                                          \
+	{                                                                      \
+		unsigned long _tmp_l_flags;                                    \
+		write_lock_irqsave(&(l), _tmp_l_flags);
+
+#define WRITE_UNLOCK(l)                                                        \
+	write_unlock_irqrestore(&(l), _tmp_l_flags);                           \
+	}
+
+#endif // __SW_LOCK_DEFS_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_mem.h b/drivers/platform/x86/socwatch/inc/sw_mem.h
new file mode 100644
index 000000000000..600b8881262c
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_mem.h
@@ -0,0 +1,82 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+/*
+ * Description: file containing memory management routines
+ * used by the power driver.
+ */
+
+#ifndef _SW_MEM_H_
+#define _SW_MEM_H_ 1
+
+#include "sw_types.h"
+
+void *sw_kmalloc(size_t size, gfp_t flags);
+void sw_kfree(const void *obj);
+/*
+ * Allocate free pages.
+ */
+unsigned long sw_allocate_pages(gfp_t flags,
+				unsigned int alloc_size_in_bytes);
+/*
+ * Free up previously allocated pages.
+ */
+void sw_release_pages(unsigned long addr, unsigned int alloc_size_in_bytes);
+
+u64 sw_get_total_bytes_alloced(void);
+u64 sw_get_max_bytes_alloced(void);
+u64 sw_get_curr_bytes_alloced(void);
+#endif // _SW_MEM_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_ops_provider.h b/drivers/platform/x86/socwatch/inc/sw_ops_provider.h
new file mode 100644
index 000000000000..43bd73fd3445
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_ops_provider.h
@@ -0,0 +1,62 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_OPS_PROVIDER_H__
+#define __SW_OPS_PROVIDER_H__
+
+int sw_register_ops_providers(void);
+void sw_free_ops_providers(void);
+
+#endif // __SW_OPS_PROVIDER_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_output_buffer.h b/drivers/platform/x86/socwatch/inc/sw_output_buffer.h
new file mode 100644
index 000000000000..17e59445ce85
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_output_buffer.h
@@ -0,0 +1,136 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _SW_OUTPUT_BUFFER_H_
+#define _SW_OUTPUT_BUFFER_H_ 1
+/*
+ * Special mask for the case where all buffers have been flushed.
+ */
+// #define sw_ALL_WRITES_DONE_MASK 0xffffffff
+#define SW_ALL_WRITES_DONE_MASK ((u32)-1)
+/*
+ * Special mask for the case where no data is available to be read.
+ */
+#define SW_NO_DATA_AVAIL_MASK ((u32)-2)
+
+/*
+ * Forward declarations.
+ */
+struct sw_driver_msg;
+
+/*
+ * Data structures.
+ */
+enum sw_wakeup_action {
+	SW_WAKEUP_ACTION_DIRECT,
+	SW_WAKEUP_ACTION_TIMER,
+	SW_WAKEUP_ACTION_NONE,
+};
+
+/*
+ * Variable declarations.
+ */
+extern u64 sw_num_samples_produced, sw_num_samples_dropped;
+extern unsigned long sw_buffer_alloc_size;
+extern int sw_max_num_cpus;
+extern wait_queue_head_t sw_reader_queue;
+
+/*
+ * Public API.
+ */
+int sw_init_per_cpu_buffers(void);
+void sw_destroy_per_cpu_buffers(void);
+void sw_reset_per_cpu_buffers(void);
+
+void sw_count_samples_produced_dropped(void);
+
+int sw_produce_polled_msg(struct sw_driver_msg *, enum sw_wakeup_action);
+int sw_produce_generic_msg(struct sw_driver_msg *, enum sw_wakeup_action);
+
+bool sw_any_seg_full(u32 *val, bool is_flush_mode);
+size_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read);
+
+unsigned int sw_get_output_buffer_size(void);
+
+void sw_wait_once(void);
+void sw_wakeup(void);
+
+void sw_print_output_buffer_overheads(void);
+
+/*
+ * Init reader queue.
+ */
+int sw_init_reader_queue(void);
+/*
+ * Destroy reader queue.
+ */
+void sw_destroy_reader_queue(void);
+/*
+ * Wakeup client waiting for a full buffer.
+ */
+void sw_wakeup_reader(enum sw_wakeup_action);
+/*
+ * Wakeup client waiting for a full buffer, and
+ * cancel any timers initialized by the reader
+ * subsys.
+ */
+void sw_cancel_reader(void);
+/*
+ * Print some stats about the reader subsys.
+ */
+void sw_print_reader_stats(void);
+
+#endif // _SW_OUTPUT_BUFFER_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h b/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
new file mode 100644
index 000000000000..7d9dc683119b
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_overhead_measurements.h
@@ -0,0 +1,189 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+/*
+ * Description: file containing overhead measurement
+ * routines used by the power driver.
+ */
+
+#ifndef _PW_OVERHEAD_MEASUREMENTS_H_
+#define _PW_OVERHEAD_MEASUREMENTS_H_
+
+/*
+ * Helper macro to declare variables required
+ * for conducting overhead measurements.
+ */
+/*
+ * For each function that you want to profile,
+ * do the following (e.g. function 'foo'):
+ * **************************************************
+ * DECLARE_OVERHEAD_VARS(foo);
+ * **************************************************
+ * This will declare the two variables required
+ * to keep track of overheads incurred in
+ * calling/servicing 'foo'. Note that the name
+ * that you declare here *MUST* match the function name!
+ */
+
+#if DO_OVERHEAD_MEASUREMENTS
+
+#ifndef __get_cpu_var
+/*
+     * Kernels >= 3.19 don't include a definition
+     * of '__get_cpu_var'. Create one now.
+     */
+#define __get_cpu_var(var) (*this_cpu_ptr(&var))
+#endif // __get_cpu_var
+#ifndef __raw_get_cpu_var
+/*
+     * Kernels >= 3.19 don't include a definition
+     * of '__raw_get_cpu_var'. Create one now.
+     */
+#define __raw_get_cpu_var(var) (*raw_cpu_ptr(&var))
+#endif // __get_cpu_var
+
+extern u64 sw_timestamp(void);
+
+#define DECLARE_OVERHEAD_VARS(name)					\
+	static DEFINE_PER_CPU(u64, name##_elapsed_time);		\
+	static DEFINE_PER_CPU(local_t, name##_num_iters) = LOCAL_INIT(0); \
+									\
+	static inline u64 get_my_cumulative_elapsed_time_##name(void)	\
+	{								\
+		return *(&__get_cpu_var(name##_elapsed_time));		\
+	}								\
+	static inline int get_my_cumulative_num_iters_##name(void)	\
+	{								\
+		return local_read(&__get_cpu_var(name##_num_iters));	\
+	}								\
+									\
+	static inline u64 name##_get_cumulative_elapsed_time_for(int cpu) \
+	{								\
+		return *(&per_cpu(name##_elapsed_time, cpu));		\
+	}								\
+									\
+	static inline int name##_get_cumulative_num_iters_for(int cpu)	\
+	{								\
+		return local_read(&per_cpu(name##_num_iters, cpu));	\
+	}								\
+									\
+	static inline void name##_get_cumulative_overhead_params(u64 *time, \
+								 int *iters) \
+	{								\
+		int cpu = 0;						\
+		*time = 0;						\
+		*iters = 0;						\
+		for_each_online_cpu(cpu) {				\
+			*iters += name##_get_cumulative_num_iters_for(cpu); \
+			*time += name##_get_cumulative_elapsed_time_for(cpu); \
+		}							\
+		return;							\
+	}								\
+									\
+	static inline void name##_print_cumulative_overhead_params(	\
+		const char *str)					\
+	{								\
+		int num = 0;						\
+		u64 time = 0;						\
+		name##_get_cumulative_overhead_params(&time, &num);	\
+		printk(KERN_INFO "%s: %d iters took %llu nano seconds!\n", \
+		       str, num, time);					\
+	}
+
+#define DO_PER_CPU_OVERHEAD_FUNC(func, ...)				\
+	do {								\
+		u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);	\
+		u64 tmp_1 = 0, tmp_2 = 0;				\
+		local_inc(&__raw_get_cpu_var(func##_num_iters));	\
+		tmp_1 = sw_timestamp();					\
+		{							\
+			func(__VA_ARGS__);				\
+		}							\
+		tmp_2 = sw_timestamp();					\
+		*(__v) += (tmp_2 - tmp_1);				\
+	} while (0)
+
+#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...)			\
+	({								\
+		type __ret;						\
+		u64 *__v = &__raw_get_cpu_var(func##_elapsed_time);	\
+		u64 tmp_1 = 0, tmp_2 = 0;				\
+		local_inc(&__raw_get_cpu_var(func##_num_iters));	\
+		tmp_1 = sw_timestamp();					\
+		{							\
+			__ret = func(__VA_ARGS__);			\
+		}							\
+		tmp_2 = sw_timestamp();					\
+		*(__v) += (tmp_2 - tmp_1);				\
+		__ret;							\
+	})
+
+#else // !DO_OVERHEAD_MEASUREMENTS
+#define DECLARE_OVERHEAD_VARS(name)					\
+	static inline void name##_print_cumulative_overhead_params(	\
+		const char *str)					\
+	{ /* NOP */							\
+	}
+
+#define DO_PER_CPU_OVERHEAD_FUNC(func, ...) func(__VA_ARGS__)
+#define DO_PER_CPU_OVERHEAD_FUNC_RET(type, func, ...) func(__VA_ARGS__)
+
+#endif // DO_OVERHEAD_MEASUREMENTS
+
+#define PRINT_CUMULATIVE_OVERHEAD_PARAMS(name, str)	\
+	name##_print_cumulative_overhead_params(str)
+
+#endif // _PW_OVERHEAD_MEASUREMENTS_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_structs.h b/drivers/platform/x86/socwatch/inc/sw_structs.h
new file mode 100644
index 000000000000..7f53a9e2984c
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_structs.h
@@ -0,0 +1,500 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_STRUCTS_H__
+#define __SW_STRUCTS_H__ 1
+
+#include "sw_types.h"
+
+/*
+ * An enumeration of MSR types.
+ * Required if we want to differentiate
+ * between different types of MSRs.
+ */
+enum sw_msr_type {
+	SW_MSR_TYPE_THREAD,
+	SW_MSR_TYPE_CORE,
+	SW_MSR_TYPE_MODULE,
+	SW_MSR_TYPE_PACKAGE,
+	SW_MSR_TYPE_SOC,
+	SW_MSR_TYPE_MAX,
+};
+
+/*
+ * Convenience for a 'string' data type.
+ * Not strictly required.
+ */
+#pragma pack(push, 1)
+typedef struct sw_string_type {
+	pw_u16_t len;
+	char data[1];
+} sw_string_type_t;
+#pragma pack(pop)
+#define SW_STRING_TYPE_HEADER_SIZE()                                           \
+	(sizeof(struct sw_string_type) - sizeof(char[1]))
+
+#pragma pack(push, 1)
+struct sw_key_value_payload {
+	pw_u16_t m_numKeyValuePairs;
+	char data[1];
+};
+#pragma pack(pop)
+#define SW_KEY_VALUE_PAYLOAD_HEADER_SIZE()                                     \
+	(sizeof(struct sw_key_value_payload) - sizeof(char[1]))
+
+typedef enum sw_kernel_wakelock_type {
+	SW_WAKE_LOCK = 0, // A kernel wakelock was acquired
+	SW_WAKE_UNLOCK = 1, // A kernel wakelock was released
+	SW_WAKE_LOCK_TIMEOUT =
+		2, // A kernel wakelock was acquired with a timeout
+	SW_WAKE_LOCK_INITIAL = 3, // A kernel wakelock was acquired before the
+	//   collection started
+	SW_WAKE_UNLOCK_ALL = 4, // All previously held kernel wakelocks were
+	//   released -- used in ACPI S3 notifications
+} sw_kernel_wakelock_type_t;
+
+typedef enum sw_when_type {
+	SW_WHEN_TYPE_BEGIN = 0, /* Start snapshot */
+	SW_WHEN_TYPE_POLL,
+	SW_WHEN_TYPE_NOTIFIER,
+	SW_WHEN_TYPE_TRACEPOINT,
+	SW_WHEN_TYPE_END, /* Stop snapshot */
+	SW_WHEN_TYPE_NONE
+} sw_when_type_t;
+
+/**
+ * trigger_bits is defined to use type pw_u8_t that makes only upto 8 types possible
+ */
+#define SW_TRIGGER_BEGIN_MASK() (1U << SW_WHEN_TYPE_BEGIN)
+#define SW_TRIGGER_END_MASK() (1U << SW_WHEN_TYPE_END)
+#define SW_TRIGGER_POLL_MASK() (1U << SW_WHEN_TYPE_POLL)
+#define SW_TRIGGER_TRACEPOINT_MASK() (1U << SW_WHEN_TYPE_TRACEPOINT)
+#define SW_TRIGGER_NOTIFIER_MASK() (1U << SW_WHEN_TYPE_NOTIFIER)
+#define SW_GET_TRIGGER_MASK_VALUE(m) (1U << (m))
+#define SW_TRIGGER_MASK_ALL() (0xFF)
+
+enum sw_io_cmd { SW_IO_CMD_READ = 0, SW_IO_CMD_WRITE, SW_IO_CMD_MAX };
+
+#pragma pack(push, 1)
+struct sw_driver_msr_io_descriptor {
+	pw_u64_t address;
+	enum sw_msr_type type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_ipc_mmio_io_descriptor {
+	union {
+#ifdef SWW_MERGE
+#pragma warning(push)
+#pragma warning(                                                               \
+	disable : 4201) // disable C4201: nonstandard extension used: nameless struct/union
+#endif
+		struct {
+			pw_u16_t command;
+			pw_u16_t sub_command;
+		};
+#ifdef SWW_MERGE
+#pragma warning(pop) // enable C4201
+#endif
+		union {
+			pw_u32_t ipc_command; // (sub_command << 12) | (command)
+			pw_u8_t is_gbe; // Used only for GBE MMIO
+		};
+	};
+	// TODO: add a section for 'ctrl_address' and 'ctrl_remapped_address'
+	union {
+		pw_u64_t data_address; // Will be "io_remapped"
+		pw_u64_t data_remapped_address;
+	};
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pci_io_descriptor {
+	pw_u32_t bus;
+	pw_u32_t device;
+	pw_u32_t function;
+#ifdef __QNX__
+	union {
+		pw_u32_t offset;
+		pw_u32_t index;
+	};
+#else /* __QNX__ */
+	pw_u32_t offset;
+#endif /* __QNX__ */
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_configdb_io_descriptor {
+	// pw_u32_t port;
+	// pw_u32_t offset;
+	pw_u32_t address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_trace_args_io_descriptor {
+	pw_u8_t num_args; // Number of valid entries in the 'args' array, below; 1 <= num_args <= 7
+	pw_u8_t args[7]; // Max of 7 args can be recorded
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+/**
+ * struct - sw_driver_telem_io_descriptor - Telemetry Metric descriptor
+ *
+ * @id:    (Client & Driver) Telemetry ID of the counter to read.
+ * @idx:   (Driver only) index into telem array to read, or the row
+ *            of the telem_indirect table to lookup the telem array index.
+ * @unit:  Unit from which to collect:  0 = PMC, 1 = PUNIT
+ *              Values come from the telemetry_unit enum.
+ * @scale_op:  When there are multiple instances of a telem value (e.g.
+ *              module C-states) the operation to use when scaling the CPU ID
+ *              and adding it to the telemetry data ID.
+ * @scale_val: Amount to scale an ID (when scaling one.)
+ *
+ * Like all hardware mechanism descriptors, the client uses this to pass
+ * metric hardware properties (unit and ID) to the driver.  The driver
+ * uses it to program the telemetry unit.
+ *
+ * Users can specify that IDs should be scaled based on the CPU id, using
+ * the equation: ID = ID_value + (cpuid <scaling_op> <scaling_val>)
+ * where <scaling_op> is one of +, *, /, or %, and scaling_val is an integer
+ * value.  This gives you:
+ *            Operation             scale_op     scale_val
+ *       Single instance of an ID       *            0
+ *       Sequentially increasing
+ *          CPU-specific values         *            1
+ *       Per module cpu-specific
+ *          values (2 cores/module)     /            2
+ *       Round Robin assignment         %         cpu_count
+ *
+ * Note that scaling_value of 0 implies that no scaling should be
+ * applied.  While (*, 1) is equivalent to (+, 0), the scaling value of 0
+ * is reserved/defined to mean "no scaling", and is disallowed.
+ *
+ * If you're really tight on space, you could always fold unit and
+ * scale_op into a single byte without a lot of pain or even effort.
+ */
+struct sw_driver_telem_io_descriptor {
+	union {
+		pw_u16_t id;
+		pw_u8_t idx;
+	};
+	pw_u8_t unit;
+	pw_u8_t scale_op;
+	pw_u16_t scale_val;
+};
+#pragma pack(pop)
+enum telemetry_unit { TELEM_PUNIT = 0, TELEM_PMC, TELEM_UNIT_NONE };
+#define TELEM_MAX_ID 0xFFFF /* Maximum value of a Telemtry event ID. */
+#define TELEM_MAX_SCALE 0xFFFF /* Maximum ID scaling value. */
+#define TELEM_OP_ADD '+' /* Addition operator */
+#define TELEM_OP_MULT '*' /* Multiplication operator */
+#define TELEM_OP_DIV '/' /* Division operator */
+#define TELEM_OP_MOD '%' /* Modulus operator */
+#define TELEM_OP_NONE 'X' /* No operator--Not a scaled ID */
+
+#pragma pack(push, 1)
+struct sw_driver_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t interface_address;
+		pw_u64_t interface_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t data_address;
+		pw_u64_t data_remapped_address;
+	};
+	pw_u64_t command;
+	pw_u64_t command_mask;
+	pw_u16_t run_busy_bit;
+	pw_u16_t is_msr_type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pch_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mtpmc_address;
+		pw_u64_t mtpmc_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t msg_full_sts_address;
+		pw_u64_t msg_full_sts_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mfpmc_address;
+		pw_u64_t mfpmc_remapped_address;
+	};
+	pw_u32_t data_address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_io_descriptor {
+	pw_u16_t collection_type;
+	// TODO: specify READ/WRITE
+	pw_s16_t collection_command; // One of 'enum sw_io_cmd'
+	pw_u16_t counter_size_in_bytes; // The number of bytes to READ or WRITE
+	union {
+		struct sw_driver_msr_io_descriptor msr_descriptor;
+		struct sw_driver_ipc_mmio_io_descriptor ipc_descriptor;
+		struct sw_driver_ipc_mmio_io_descriptor mmio_descriptor;
+		struct sw_driver_pci_io_descriptor pci_descriptor;
+		struct sw_driver_configdb_io_descriptor configdb_descriptor;
+		struct sw_driver_trace_args_io_descriptor trace_args_descriptor;
+		struct sw_driver_telem_io_descriptor telem_descriptor;
+		struct sw_driver_pch_mailbox_io_descriptor
+			pch_mailbox_descriptor;
+		struct sw_driver_mailbox_io_descriptor mailbox_descriptor;
+	};
+	pw_u64_t write_value; // The value to WRITE
+} sw_driver_io_descriptor_t;
+#pragma pack(pop)
+
+/**
+ * sw_driver_interface_info is used to map data collected by kernel-level
+ * collectors to metrics.  The client passes one of these structs to the
+ * driver for each metric the driver should collect.  The driver tags the
+ * collected data (messages) using info from this struct. When processing
+ * data from the driver, the client uses its copy of this data to
+ * identify the plugin, metric, and message IDs of each message.
+ */
+#pragma pack(push, 1)
+struct sw_driver_interface_info {
+	pw_u64_t tracepoint_id_mask;
+	pw_u64_t notifier_id_mask;
+	pw_s16_t cpu_mask; // On which CPU(s) should the driver read the data?
+		// Currently:  -2 ==> read on ALL CPUs,
+		//             -1 ==> read on ANY CPU,
+		//           >= 0 ==> the specific CPU to read on
+	pw_s16_t plugin_id; // Metric Plugin SID
+	pw_s16_t metric_id; // Domain-specific ID assigned by each Metric Plugin
+	pw_s16_t msg_id; // Msg ID retrieved from the SoC Watch config file
+	pw_u16_t num_io_descriptors; // Number of descriptors in the array, below.
+	pw_u8_t trigger_bits; // Mask of 'when bits' to fire this collector.
+	pw_u16_t sampling_interval_msec; // Sampling interval, in msecs
+	pw_u8_t descriptors[1]; // Array of sw_driver_io_descriptor structs.
+};
+#pragma pack(pop)
+
+#define SW_DRIVER_INTERFACE_INFO_HEADER_SIZE()                                 \
+	(sizeof(struct sw_driver_interface_info) - sizeof(pw_u8_t[1]))
+
+#pragma pack(push, 1)
+struct sw_driver_interface_msg {
+	pw_u16_t num_infos; // Number of 'sw_driver_interface_info' structs contained within the 'infos' variable, below
+	pw_u16_t min_polling_interval_msecs; // Min time to wait before polling; used exclusively
+		// with the low overhead, context-switch based
+		// polling mode
+	// pw_u16_t infos_size_bytes; // Size of data inlined within the 'infos' variable, below
+	pw_u8_t infos[1];
+};
+#pragma pack(pop)
+#define SW_DRIVER_INTERFACE_MSG_HEADER_SIZE()                                  \
+	(sizeof(struct sw_driver_interface_msg) - sizeof(pw_u8_t[1]))
+
+typedef enum sw_name_id_type {
+	SW_NAME_TYPE_TRACEPOINT,
+	SW_NAME_TYPE_NOTIFIER,
+	SW_NAME_TYPE_COLLECTOR,
+	SW_NAME_TYPE_MAX,
+} sw_name_id_type_t;
+
+#pragma pack(push, 1)
+struct sw_name_id_pair {
+	pw_u16_t id;
+	pw_u16_t type; // One of 'sw_name_id_type'
+	struct sw_string_type name;
+};
+#pragma pack(pop)
+#define SW_NAME_ID_HEADER_SIZE()                                               \
+	(sizeof(struct sw_name_id_pair) - sizeof(struct sw_string_type))
+
+#pragma pack(push, 1)
+struct sw_name_info_msg {
+	pw_u16_t num_name_id_pairs;
+	pw_u16_t payload_len;
+	pw_u8_t pairs[1];
+};
+#pragma pack(pop)
+
+/**
+ * This is the basic data structure for passing data collected by the
+ * kernel-level collectors up to the client.  In addition to the data
+ * (payload), it contains the minimum metadata required for the client
+ * to identify the source of that data.
+ */
+#pragma pack(push, 1)
+typedef struct sw_driver_msg {
+	pw_u64_t tsc;
+	pw_u16_t cpuidx;
+	pw_u8_t plugin_id; // Cannot have more than 256 plugins
+	pw_u8_t metric_id; // Each plugin cannot handle more than 256 metrics
+	pw_u8_t msg_id; // Each metric cannot have more than 256 components
+	pw_u16_t payload_len;
+	// pw_u64_t p_payload;  // Ptr to payload
+	union {
+		pw_u64_t __dummy; // Ensure size of struct is consistent on x86, x64
+		char *p_payload; // Ptr to payload (collected data values).
+	};
+} sw_driver_msg_t;
+#pragma pack(pop)
+#define SW_DRIVER_MSG_HEADER_SIZE()                                            \
+	(sizeof(struct sw_driver_msg) - sizeof(pw_u64_t))
+
+typedef enum sw_driver_collection_cmd {
+	SW_DRIVER_START_COLLECTION = 1,
+	SW_DRIVER_STOP_COLLECTION = 2,
+	SW_DRIVER_PAUSE_COLLECTION = 3,
+	SW_DRIVER_RESUME_COLLECTION = 4,
+	SW_DRIVER_CANCEL_COLLECTION = 5,
+} sw_driver_collection_cmd_t;
+
+#pragma pack(push, 1)
+struct sw_driver_version_info {
+	pw_u16_t major;
+	pw_u16_t minor;
+	pw_u16_t other;
+};
+#pragma pack(pop)
+
+enum cpu_action {
+	SW_CPU_ACTION_NONE,
+	SW_CPU_ACTION_OFFLINE,
+	SW_CPU_ACTION_ONLINE_PREPARE,
+	SW_CPU_ACTION_ONLINE,
+	SW_CPU_ACTION_MAX,
+};
+#pragma pack(push, 1)
+struct sw_driver_topology_change {
+	pw_u64_t timestamp; // timestamp
+	enum cpu_action type; // One of 'enum cpu_action'
+	pw_u16_t cpu; // logical cpu
+	pw_u16_t core; // core id
+	pw_u16_t pkg; // pkg/physical id
+};
+struct sw_driver_topology_msg {
+	pw_u16_t num_entries;
+	pw_u8_t topology_entries[1];
+};
+#pragma pack(pop)
+
+/**
+ * An enumeration of possible pm states that
+ * SoC Watch is interested in
+ */
+enum sw_pm_action {
+	SW_PM_ACTION_NONE,
+	SW_PM_ACTION_SUSPEND_ENTER,
+	SW_PM_ACTION_SUSPEND_EXIT,
+	SW_PM_ACTION_HIBERNATE_ENTER,
+	SW_PM_ACTION_HIBERNATE_EXIT,
+	SW_PM_ACTION_MAX,
+};
+
+/**
+ * An enumeration of possible actions that trigger
+ * the power notifier
+ */
+enum sw_pm_mode {
+	SW_PM_MODE_FIRMWARE,
+	SW_PM_MODE_NONE,
+};
+
+#define SW_PM_VALUE(mode, action) ((mode) << 16 | (action))
+
+/*
+ * Wrapper for ioctl arguments.
+ * EVERY ioctl MUST use this struct!
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	// pw_u64_t p_in_arg; // Pointer to input arg
+	// pw_u64_t p_out_arg; // Pointer to output arg
+	char *in_arg;
+	char *out_arg;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_msg_interval {
+	pw_u8_t plugin_id; // Cannot have more than 256 plugins
+	pw_u8_t metric_id; // Each plugin cannot handle more than 256 metrics
+	pw_u8_t msg_id; // Each metric cannot have more than 256 components
+	pw_u16_t interval; // collection interval
+} sw_driver_msg_interval_t;
+#pragma pack(pop)
+
+#endif // __SW_STRUCTS_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_telem.h b/drivers/platform/x86/socwatch/inc/sw_telem.h
new file mode 100644
index 000000000000..52e5119b557e
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_telem.h
@@ -0,0 +1,74 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _SW_TELEM_H_
+#define _SW_TELEM_H_ 1
+
+#include "sw_structs.h" // sw_driver_io_descriptor
+#include "sw_types.h" // u8 and other types
+
+int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor);
+void sw_read_telem_info(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+void sw_write_telem_info(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes);
+int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor);
+bool sw_telem_available(void);
+bool sw_telem_post_config(void);
+
+#endif /* SW_TELEM_H */
diff --git a/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h b/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
new file mode 100644
index 000000000000..3834a16d7ae8
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_trace_notifier_provider.h
@@ -0,0 +1,82 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_TRACE_NOTIFIER_PROVIDER_H__
+#define __SW_TRACE_NOTIFIER_PROVIDER_H__
+
+u64 sw_timestamp(void);
+/*
+ * Some architectures and OS versions require a "discovery"
+ * phase for tracepoints and/or notifiers. Allow for that here.
+ */
+int sw_extract_trace_notifier_providers(void);
+/*
+ * Reset trace/notifier providers at the end
+ * of a collection.
+ */
+void sw_reset_trace_notifier_providers(void);
+/*
+ * Print statistics on trace/notifier provider overheads.
+ */
+void sw_print_trace_notifier_provider_overheads(void);
+/*
+ * Add all trace/notifier providers.
+ */
+int sw_add_trace_notifier_providers(void);
+/*
+ * Remove previously added providers.
+ */
+void sw_remove_trace_notifier_providers(void);
+#endif // __SW_TRACE_NOTIFIER_PROVIDER_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h b/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
new file mode 100644
index 000000000000..db8294a9a137
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_tracepoint_handlers.h
@@ -0,0 +1,142 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_TRACEPOINT_HANDLERS_H__
+#define __SW_TRACEPOINT_HANDLERS_H__
+
+#include "sw_internal.h"
+
+extern pw_u16_t sw_min_polling_interval_msecs;
+
+enum sw_trace_data_type {
+	SW_TRACE_COLLECTOR_TRACEPOINT,
+	SW_TRACE_COLLECTOR_NOTIFIER
+};
+
+struct sw_trace_notifier_name {
+	const char *
+		kernel_name; // The tracepoint name; used by the kernel to identify tracepoints
+	const char *
+		abstract_name; // An abstract name used by plugins to specify tracepoints-of-interest; shared with Ring-3
+};
+
+typedef struct sw_trace_notifier_data sw_trace_notifier_data_t;
+typedef int (*sw_trace_notifier_register_func)(
+	struct sw_trace_notifier_data *node);
+typedef int (*sw_trace_notifier_unregister_func)(
+	struct sw_trace_notifier_data *node);
+
+struct sw_trace_notifier_data {
+	enum sw_trace_data_type type; // Tracepoint or Notifier
+	const struct sw_trace_notifier_name *name; // Tracepoint name(s)
+	sw_trace_notifier_register_func probe_register; // probe register function
+	sw_trace_notifier_unregister_func probe_unregister; // probe unregister function
+	struct tracepoint *tp;
+	bool always_register; // Set to TRUE if this tracepoint/notifier must ALWAYS be registered, regardless
+		// of whether the user has specified anything to collect
+	bool was_registered;
+	SW_DEFINE_LIST_HEAD(
+		list,
+		sw_collector_data); // List of 'sw_collector_data' instances for this tracepoint or notifier
+};
+
+struct sw_topology_node {
+	struct sw_driver_topology_change change;
+
+	SW_LIST_ENTRY(list, sw_topology_node);
+};
+SW_DECLARE_LIST_HEAD(
+	sw_topology_list,
+	sw_topology_node); // List of entries tracking changes in CPU topology
+extern size_t sw_num_topology_entries; // Size of the 'sw_topology_list'
+
+int sw_extract_tracepoints(void);
+int sw_register_trace_notifiers(void);
+int sw_unregister_trace_notifiers(void);
+
+/*
+ * Register a single TRACE/NOTIFY provider.
+ */
+int sw_register_trace_notify_provider(struct sw_trace_notifier_data *tnode);
+/*
+ * Add all TRACE/NOTIFY providers.
+ */
+int sw_add_trace_notify(void);
+void sw_remove_trace_notify(void);
+
+void sw_reset_trace_notifier_lists(void);
+
+void sw_print_trace_notifier_overheads(void);
+
+int sw_for_each_tracepoint_node(int (*func)(struct sw_trace_notifier_data *node,
+					    void *priv),
+				void *priv, bool return_on_error);
+int sw_for_each_notifier_node(int (*func)(struct sw_trace_notifier_data *node,
+					  void *priv),
+			      void *priv, bool return_on_error);
+
+int sw_get_trace_notifier_id(struct sw_trace_notifier_data *node);
+
+const char *
+sw_get_trace_notifier_kernel_name(struct sw_trace_notifier_data *node);
+const char *
+sw_get_trace_notifier_abstract_name(struct sw_trace_notifier_data *node);
+
+/*
+ * Clear out the topology list.
+ */
+void sw_clear_topology_list(void);
+
+#endif // __SW_TRACEPOINT_HANDLERS_H__
diff --git a/drivers/platform/x86/socwatch/inc/sw_types.h b/drivers/platform/x86/socwatch/inc/sw_types.h
new file mode 100644
index 000000000000..914ce9806965
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_types.h
@@ -0,0 +1,152 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_TYPES_H_
+#define _PW_TYPES_H_
+
+#if defined(__linux__) || defined(__APPLE__) || defined(__QNX__)
+
+#ifndef __KERNEL__
+/*
+ * Called from Ring-3.
+ */
+#include <stdint.h> // Grab 'uint64_t' etc.
+#include <unistd.h> // Grab 'pid_t'
+/*
+ * UNSIGNED types...
+ */
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+ * SIGNED types...
+ */
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#else // __KERNEL__
+#if !defined(__APPLE__)
+#include <linux/types.h>
+#else // __APPLE__
+#include <sys/types.h>
+#include <stdint.h> // Grab 'uint64_t' etc.
+
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+* SIGNED types...
+*/
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+#endif // __APPLE__
+#endif // __KERNEL__
+
+#elif defined(_WIN32)
+typedef __int32 int32_t;
+typedef unsigned __int32 uint32_t;
+typedef __int64 int64_t;
+typedef unsigned __int64 uint64_t;
+
+/*
+ * UNSIGNED types...
+ */
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long long u64;
+
+/*
+ * SIGNED types...
+ */
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long long s64;
+typedef s32 pid_t;
+typedef s32 ssize_t;
+
+#endif // _WIN32
+
+/* ************************************
+ * Common to both operating systems.
+ * ************************************
+ */
+/*
+ * UNSIGNED types...
+ */
+typedef u8 pw_u8_t;
+typedef u16 pw_u16_t;
+typedef u32 pw_u32_t;
+typedef u64 pw_u64_t;
+
+/*
+ * SIGNED types...
+ */
+typedef s8 pw_s8_t;
+typedef s16 pw_s16_t;
+typedef s32 pw_s32_t;
+typedef s64 pw_s64_t;
+
+typedef pid_t pw_pid_t;
+
+#endif // _PW_TYPES_H_
diff --git a/drivers/platform/x86/socwatch/inc/sw_version.h b/drivers/platform/x86/socwatch/inc/sw_version.h
new file mode 100644
index 000000000000..5797edffa64d
--- /dev/null
+++ b/drivers/platform/x86/socwatch/inc/sw_version.h
@@ -0,0 +1,74 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef __SW_VERSION_H__
+#define __SW_VERSION_H__ 1
+
+/*
+ * SOCWatch driver version
+ */
+#define SW_DRIVER_VERSION_MAJOR 2
+#define SW_DRIVER_VERSION_MINOR 6
+#define SW_DRIVER_VERSION_OTHER 2
+
+/*
+ * Every SOC Watch userspace component shares the same version number.
+ */
+#define SOCWATCH_VERSION_MAJOR 2
+#define SOCWATCH_VERSION_MINOR 8
+#define SOCWATCH_VERSION_OTHER 0
+
+#endif // __SW_VERSION_H__
diff --git a/drivers/platform/x86/socwatch/sw_collector.c b/drivers/platform/x86/socwatch/sw_collector.c
new file mode 100644
index 000000000000..a6c8a9cec48b
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_collector.c
@@ -0,0 +1,706 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.	 When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.	See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+  * Redistributions of source code must retain the above copyright
+  notice, this list of condiions and the following disclaimer.
+  * Redistributions in binary form must reproduce the above copyright
+  notice, this list of conditions and the following disclaimer in
+  the documentation and/or other materials provided with the
+  distribution.
+  * Neither the name of Intel Corporation nor the names of its
+  contributors may be used to endorse or promote products derived
+  from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_internal.h"
+#include "sw_structs.h"
+#include "sw_collector.h"
+#include "sw_kernel_defines.h"
+#include "sw_mem.h"
+#include "sw_types.h"
+#include "sw_hardware_io.h"
+#include "sw_output_buffer.h"
+
+/* -------------------------------------------------
+ * Local function declarations.
+ * -------------------------------------------------
+ */
+void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info);
+const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors);
+void sw_free_ops_i(const struct sw_hw_ops **ops);
+struct sw_driver_interface_info *
+sw_copy_driver_interface_info_i(const struct sw_driver_interface_info *info);
+int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info);
+int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info);
+int sw_init_ops_i(const struct sw_hw_ops **ops,
+		  const struct sw_driver_interface_info *info);
+sw_driver_msg_t *
+sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info,
+			 size_t per_msg_payload_size);
+void sw_free_collector_msg_i(sw_driver_msg_t *msg);
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
+void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action);
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+const static struct sw_hw_ops *s_hw_ops;
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * Driver interface info functions.
+ */
+
+/**
+ * sw_add_driver_info() - Add a collector node to the list called at this
+ *			"when type".
+ * @head:   The collector node list to add the new node to.
+ * @info:   Driver information to add to the list.
+ *
+ *  This function allocates and links in a "collector node" for each
+ *  collector based on the collector info in the info parameter.
+ *  The function allocates the new node, and links it to a local copy
+ *  of the passed-in driver interface info.  If the collector has an
+ *  init function among its operations, it iterates through the
+ *  descriptors in info, passing each one to the init function.
+ *
+ *  Finally, it allocates and initializes the "collector message" which
+ *  buffers a data sample that this collector gathers during the run.
+ *
+ * Returns:  -PW_ERROR on failure, PW_SUCCESS on success.
+ */
+int sw_add_driver_info(void *list_head,
+		       const struct sw_driver_interface_info *info)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	struct sw_collector_data *node = sw_alloc_collector_node();
+
+	if (!node) {
+		pw_pr_error("ERROR allocating collector node!\n");
+		return -PW_ERROR;
+	}
+
+	node->info = sw_copy_driver_interface_info_i(info);
+	if (!node->info) {
+		pw_pr_error(
+			"ERROR allocating or copying driver_interface_info!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Initialize the collectors in the node's descriptors.
+	 */
+	if (sw_init_driver_interface_info_i(node->info)) {
+		pw_pr_error(
+			"ERROR initializing a driver_interface_info node!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Allocate the ops array. We do this one time as an optimization
+	 * (we could always just repeatedly call 'sw_get_hw_ops_for()'
+	 * during the collection but we want to avoid that overhead)
+	 */
+	node->ops = sw_alloc_ops_i(info->num_io_descriptors);
+	if (!node->ops || sw_init_ops_i(node->ops, info)) {
+		pw_pr_error("ERROR initializing the ops array!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	/*
+	 * Allocate and initialize the "collector message".
+	 */
+	node->per_msg_payload_size = sw_get_payload_size_i(info);
+	pw_pr_debug("Debug: Per msg payload size = %u\n",
+		    (unsigned int)node->per_msg_payload_size);
+	node->msg = sw_alloc_collector_msg_i(info, node->per_msg_payload_size);
+	if (!node->msg) {
+		pw_pr_error("ERROR allocating space for a collector msg!\n");
+		sw_free_collector_node(node);
+		return -PW_ERROR;
+	}
+	pw_pr_debug("NODE = %p, NODE->MSG = %p\n", node, node->msg);
+	cpumask_clear(&node->cpumask);
+	{
+		/*
+		 * For now, use following protocol:
+		 * cpu_mask == -2 ==> Collect on ALL CPUs
+		 * cpu_mask == -1 ==> Collect on ANY CPU
+		 * cpu_mask >= 0 ==> Collect on a specific CPU
+		 */
+		if (node->info->cpu_mask >= 0) {
+			/*
+			 * Collect data on 'node->info->cpu_mask'
+			 */
+			cpumask_set_cpu(node->info->cpu_mask, &node->cpumask);
+			pw_pr_debug("OK: set CPU = %d\n", node->info->cpu_mask);
+		} else if (node->info->cpu_mask == -1) {
+			/*
+			 * Collect data on ANY CPU. Leave empty as a flag
+			 * to signify user wishes to collect data on 'ANY' cpu.
+			 */
+			pw_pr_debug("OK: set ANY CPU\n");
+		} else {
+			/*
+			 * Collect data on ALL cpus.
+			 */
+			cpumask_copy(&node->cpumask, cpu_present_mask);
+			pw_pr_debug("OK: set ALL CPUs\n");
+		}
+	}
+	SW_LIST_ADD(head, node, list);
+	return PW_SUCCESS;
+}
+
+const struct sw_hw_ops **sw_alloc_ops_i(pw_u16_t num_io_descriptors)
+{
+	size_t size = num_io_descriptors * sizeof(struct sw_hw_ops *);
+	const struct sw_hw_ops **ops = sw_kmalloc(size, GFP_KERNEL);
+
+	if (ops) {
+		memset(ops, 0, size);
+	}
+	return ops;
+}
+
+void sw_free_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	if (info) {
+		sw_kfree(info);
+	}
+}
+
+void sw_free_ops_i(const struct sw_hw_ops **ops)
+{
+	if (ops) {
+		sw_kfree(ops);
+	}
+}
+
+/**
+ * sw_copy_driver_interface_info_i - Allocate and copy the passed-in "info".
+ *
+ * @info: Information about the metric and collection properties
+ *
+ * Returns: a pointer to the newly allocated sw_driver_interface_info,
+ *	    which is a copy of the version passed in via the info pointer.
+ */
+struct sw_driver_interface_info *
+sw_copy_driver_interface_info_i(const struct sw_driver_interface_info *info)
+{
+	size_t size;
+	struct sw_driver_interface_info *node = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: NULL sw_driver_interface_info in alloc!\n");
+		return node;
+	}
+
+	size = SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+	       (info->num_io_descriptors *
+		sizeof(struct sw_driver_io_descriptor));
+	node = (struct sw_driver_interface_info *)sw_kmalloc(size, GFP_KERNEL);
+	if (!node) {
+		pw_pr_error("ERROR allocating driver interface info!\n");
+		return node;
+	}
+	memcpy((char *)node, (const char *)info, size);
+
+	/*
+	 * Do debug dump.
+	 */
+	pw_pr_debug("DRIVER info has plugin_ID = %d, metric_ID = %d, "
+		    "msg_ID = %d\n",
+		    node->plugin_id, node->metric_id, node->msg_id);
+
+	return node;
+}
+int sw_init_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	/*
+	 * Do any initialization here.
+	 * For now, only IPC/MMIO descriptors need to be initialized.
+	 */
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: no info!\n");
+		return -PW_ERROR;
+	}
+	for (i = 0,
+	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	     i < info->num_io_descriptors; ++i, ++descriptor) {
+		if (sw_init_driver_io_descriptor(descriptor)) {
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+int sw_reset_driver_interface_info_i(struct sw_driver_interface_info *info)
+{
+	/*
+	 * Do any finalization here.
+	 * For now, only IPC/MMIO descriptors need to be finalized.
+	 */
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!info) {
+		pw_pr_error("ERROR: no info!\n");
+		return -PW_ERROR;
+	}
+	for (i = 0,
+	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	     i < info->num_io_descriptors; ++i, ++descriptor) {
+		if (sw_reset_driver_io_descriptor(descriptor)) {
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+int sw_init_ops_i(const struct sw_hw_ops **ops,
+		  const struct sw_driver_interface_info *info)
+{
+	int i = 0;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+
+	if (!ops || !info) {
+		return -PW_ERROR;
+	}
+	for (i = 0,
+	    descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	     i < info->num_io_descriptors; ++i, ++descriptor) {
+		ops[i] = sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops[i] == NULL) {
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * If this descriptor's collector has an init function, call it passing in
+ * this descriptor.  That allows the collector to perform any initialization
+ * or registration specific to this metric.
+ */
+int sw_init_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+	sw_io_desc_init_func_t init_func = NULL;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+	if (ops == NULL) {
+		pw_pr_error("NULL ops found in init_driver_io_desc: type %d\n",
+			    descriptor->collection_type);
+		return -PW_ERROR;
+	}
+	init_func = ops->init;
+
+	if (init_func) {
+		int retval = (*init_func)(descriptor);
+
+		if (retval) {
+			pw_pr_error("(*init) return value for type %d: %d\n",
+				    descriptor->collection_type, retval);
+		}
+		return retval;
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * If this descriptor's collector has a finalize function, call it passing in
+ * this descriptor. This allows the collector to perform any finalization
+ * specific to this metric.
+ */
+int sw_reset_driver_io_descriptor(struct sw_driver_io_descriptor *descriptor)
+{
+	sw_io_desc_reset_func_t reset_func = NULL;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+	if (ops == NULL) {
+		pw_pr_error("NULL ops found in reset_driver_io_desc: type %d\n",
+			    descriptor->collection_type);
+		return -PW_ERROR;
+	}
+	pw_pr_debug("calling reset on descriptor of type %d\n",
+		    descriptor->collection_type);
+	reset_func = ops->reset;
+
+	if (reset_func) {
+		int retval = (*reset_func)(descriptor);
+
+		if (retval) {
+			pw_pr_error("(*reset) return value for type %d: %d\n",
+				    descriptor->collection_type, retval);
+		}
+		return retval;
+	}
+	return PW_SUCCESS;
+}
+
+int sw_handle_driver_io_descriptor(
+	char *dst_vals, int cpu,
+	const struct sw_driver_io_descriptor *descriptor,
+	const struct sw_hw_ops *hw_ops)
+{
+	typedef void (*sw_hardware_io_func_t)(
+		char *, int, const struct sw_driver_io_descriptor *, u16);
+	sw_hardware_io_func_t hardware_io_func = NULL;
+
+	if (descriptor->collection_command < SW_IO_CMD_READ ||
+	    descriptor->collection_command > SW_IO_CMD_WRITE) {
+		return -PW_ERROR;
+	}
+	switch (descriptor->collection_command) {
+	case SW_IO_CMD_READ:
+		hardware_io_func = hw_ops->read;
+		break;
+	case SW_IO_CMD_WRITE:
+		hardware_io_func = hw_ops->write;
+		break;
+	default:
+		break;
+	}
+	if (hardware_io_func) {
+		(*hardware_io_func)(dst_vals, cpu, descriptor,
+				    descriptor->counter_size_in_bytes);
+	} else {
+		pw_pr_debug(
+			"NO ops to satisfy %u operation for collection type %u!\n",
+			descriptor->collection_command,
+			descriptor->collection_type);
+	}
+	return PW_SUCCESS;
+}
+
+sw_driver_msg_t *
+sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info,
+			 size_t per_msg_payload_size)
+{
+	size_t per_msg_size = 0, total_size = 0;
+	sw_driver_msg_t *msg = NULL;
+
+	if (!info) {
+		return NULL;
+	}
+	per_msg_size = sizeof(struct sw_driver_msg) + per_msg_payload_size;
+	total_size = per_msg_size * num_possible_cpus();
+	msg = (sw_driver_msg_t *)sw_kmalloc(total_size, GFP_KERNEL);
+	if (msg) {
+		int cpu = -1;
+
+		memset(msg, 0, total_size);
+		for_each_possible_cpu(cpu) {
+			sw_driver_msg_t *__msg = GET_MSG_SLOT_FOR_CPU(
+				msg, cpu, per_msg_payload_size);
+			char *__payload =
+				(char *)__msg + sizeof(struct sw_driver_msg);
+			__msg->cpuidx = (pw_u16_t)cpu;
+			__msg->plugin_id = (pw_u8_t)info->plugin_id;
+			__msg->metric_id = (pw_u8_t)info->metric_id;
+			__msg->msg_id = (pw_u8_t)info->msg_id;
+			__msg->payload_len = per_msg_payload_size;
+			__msg->p_payload = __payload;
+			pw_pr_debug(
+				"[%d]: per_msg_payload_size = %zx, msg = %p, payload = %p\n",
+				cpu, per_msg_payload_size, __msg, __payload);
+		}
+	}
+	return msg;
+}
+
+void sw_free_collector_msg_i(sw_driver_msg_t *msg)
+{
+	if (msg) {
+		sw_kfree(msg);
+	}
+}
+
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info)
+{
+	size_t size = 0;
+	int i = 0;
+
+	if (info) {
+		for (i = 0; i < info->num_io_descriptors;
+		     size +=
+		     ((struct sw_driver_io_descriptor *)info->descriptors)[i]
+			     .counter_size_in_bytes,
+		    ++i)
+			;
+	}
+	return size;
+}
+
+void sw_handle_per_cpu_msg_i(void *info, enum sw_wakeup_action action)
+{
+	/*
+	 * Basic algo:
+	 * For each descriptor in 'node->info->descriptors'; do:
+	 * 1. Perform H/W read; use 'descriptor->collection_type'
+	 * to determine type of read; use 'descriptor->counter_size_in_bytes'
+	 * for read size. Use msg->p_payload[dst_idx] as dst address
+	 * 2. Increment dst idx by 'descriptor->counter_size_in_bytes'
+	 */
+	struct sw_collector_data *node = (struct sw_collector_data *)info;
+	int cpu = RAW_CPU();
+	u16 num_descriptors = node->info->num_io_descriptors, i = 0;
+	struct sw_driver_io_descriptor *descriptors =
+		(struct sw_driver_io_descriptor *)node->info->descriptors;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+	const struct sw_hw_ops **ops = node->ops;
+	bool wasAnyWrite = false;
+
+	// msg TSC assigned when msg is written to buffer
+	msg->cpuidx = cpu;
+
+	for (i = 0; i < num_descriptors; ++i,
+	    dst_vals += descriptors->counter_size_in_bytes, ++descriptors) {
+		if (unlikely(ops[i] == NULL)) {
+			pw_pr_debug("NULL OPS!\n");
+			continue;
+		}
+		if (descriptors->collection_command == SW_IO_CMD_WRITE) {
+			wasAnyWrite = true;
+		}
+		if (sw_handle_driver_io_descriptor(dst_vals, cpu, descriptors,
+						   ops[i])) {
+			pw_pr_error("ERROR reading descriptor with type %d\n",
+				    descriptors->collection_type);
+		}
+	}
+
+	/*
+	 * We produce messages only on READs. Note that SWA prohibits
+	 * messages that contain both READ and WRITE descriptors, so it
+	 * is enough to check if there was ANY WRITE descriptor in this
+	 * message.
+	 */
+	if (likely(wasAnyWrite == false)) {
+		if (sw_produce_generic_msg(msg, action)) {
+			pw_pr_warn("WARNING: could NOT produce message!\n");
+		}
+	}
+
+	return;
+}
+
+/*
+ * Collector list and node functions.
+ */
+struct sw_collector_data *sw_alloc_collector_node(void)
+{
+	struct sw_collector_data *node = (struct sw_collector_data *)sw_kmalloc(
+		sizeof(struct sw_collector_data), GFP_KERNEL);
+	if (node) {
+		node->per_msg_payload_size = 0x0;
+		node->last_update_jiffies = 0x0;
+		node->info = NULL;
+		node->ops = NULL;
+		node->msg = NULL;
+		SW_LIST_ENTRY_INIT(node, list);
+	}
+	return node;
+}
+
+void sw_free_collector_node(struct sw_collector_data *node)
+{
+	if (!node) {
+		return;
+	}
+	if (node->info) {
+		sw_reset_driver_interface_info_i(node->info);
+		sw_free_driver_interface_info_i(node->info);
+		node->info = NULL;
+	}
+	if (node->ops) {
+		sw_free_ops_i(node->ops);
+		node->ops = NULL;
+	}
+	if (node->msg) {
+		sw_free_collector_msg_i(node->msg);
+		node->msg = NULL;
+	}
+	sw_kfree(node);
+	return;
+}
+
+int sw_handle_collector_node(struct sw_collector_data *node)
+{
+	if (!node || !node->info || !node->ops || !node->msg) {
+		return -PW_ERROR;
+	}
+	pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+	sw_schedule_work(&node->cpumask, &sw_handle_per_cpu_msg, node);
+	return PW_SUCCESS;
+}
+
+int sw_handle_collector_node_on_cpu(struct sw_collector_data *node, int cpu)
+{
+	if (!node || !node->info || !node->ops || !node->msg) {
+		return -PW_ERROR;
+	}
+	/*
+	 * Check if this node indicates it should be scheduled
+	 * on the given cpu. If so, clear all other CPUs from the
+	 * mask and schedule the node.
+	 */
+	if (cpumask_test_cpu(cpu, &node->cpumask)) {
+		struct cpumask tmp_mask;
+
+		cpumask_clear(&tmp_mask);
+		cpumask_set_cpu(cpu, &tmp_mask);
+		pw_pr_debug("Calling SMP_CALL_FUNCTION_MANY!\n");
+		sw_schedule_work(&tmp_mask, &sw_handle_per_cpu_msg, node);
+	}
+	return PW_SUCCESS;
+}
+
+void sw_init_collector_list(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	SW_LIST_HEAD_INIT(head);
+}
+
+void sw_destroy_collector_list(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_collector_data *curr =
+			SW_LIST_GET_HEAD_ENTRY(head, sw_collector_data, list);
+		BUG_ON(!curr->info);
+		SW_LIST_UNLINK(curr, list);
+		sw_free_collector_node(curr);
+	}
+}
+
+/**
+ * sw_handle_collector_list - Iterate through the collector list, calling
+ *			      func() upon each element.
+ * @list_head:	The collector list head.
+ * @func:  The function to call for each collector.
+ *
+ * This function is called when one of the "when types" fires, since the
+ * passed-in collector node list is the list of collections to do at that time.
+ *
+ * Returns: PW_SUCCESS on success, -PW_ERROR on error.
+ */
+int sw_handle_collector_list(void *list_head,
+			     int (*func)(struct sw_collector_data *data))
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	int retVal = PW_SUCCESS;
+	struct sw_collector_data *curr = NULL;
+
+	if (!head || !func) {
+		return -PW_ERROR;
+	}
+	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
+	{
+		pw_pr_debug("HANDLING\n");
+		if ((*func)(curr)) {
+			retVal = -PW_ERROR;
+		}
+	}
+	return retVal;
+}
+
+int sw_handle_collector_list_on_cpu(void *list_head,
+				    int (*func)(struct sw_collector_data *data,
+						int cpu),
+				    int cpu)
+{
+	SW_LIST_HEAD_VAR(sw_collector_data) * head = list_head;
+	int retVal = PW_SUCCESS;
+	struct sw_collector_data *curr = NULL;
+	if (!head || !func) {
+		return -PW_ERROR;
+	}
+	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
+	{
+		pw_pr_debug("HANDLING\n");
+		if ((*func)(curr, cpu)) {
+			retVal = -PW_ERROR;
+		}
+	}
+	return retVal;
+}
+
+void sw_handle_per_cpu_msg(void *info)
+{
+	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_handle_per_cpu_msg_no_sched(void *info)
+{
+	sw_handle_per_cpu_msg_i(info, SW_WAKEUP_ACTION_TIMER);
+}
+
+void sw_handle_per_cpu_msg_on_cpu(int cpu, void *info)
+{
+	if (unlikely(cpu == RAW_CPU())) {
+		sw_handle_per_cpu_msg_no_sched(info);
+	} else {
+		pw_pr_debug("[%d] is handling for %d\n", RAW_CPU(), cpu);
+		/*
+		 * No need to disable preemption -- 'smp_call_function_single'
+		 * does that for us.
+		 */
+		smp_call_function_single(
+			cpu, &sw_handle_per_cpu_msg_no_sched, info,
+			false /* false ==> do NOT wait for function
+				 completion */);
+	}
+}
+
+void sw_set_collector_ops(const struct sw_hw_ops *hw_ops)
+{
+	s_hw_ops = hw_ops;
+}
diff --git a/drivers/platform/x86/socwatch/sw_driver.c b/drivers/platform/x86/socwatch/sw_driver.c
new file mode 100644
index 000000000000..35b516cfb26a
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_driver.c
@@ -0,0 +1,1472 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#define MOD_AUTHOR "Gautam Upadhyaya <gautam.upadhyaya@intel.com>"
+#define MOD_DESC "SoC Watch kernel module"
+
+#include "sw_internal.h"
+#include "sw_structs.h"
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_mem.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+#include "sw_hardware_io.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_collector.h"
+#include "sw_file_ops.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Number of entries in the 'sw_collector_lists' array
+ */
+#define NUM_COLLECTOR_MODES (SW_WHEN_TYPE_END - SW_WHEN_TYPE_BEGIN + 1)
+#define PW_OUTPUT_BUFFER_SIZE                                                  \
+	256 /* Number of output messages in each per-cpu buffer */
+/*
+ * Check if tracepoint/notifier ID is in (user-supplied) mask
+ */
+#define IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)                                 \
+	((id) >= 0 && (((mask) >> (id)) & 0x1))
+
+/* -------------------------------------------------
+ *  Local function declarations.
+ * -------------------------------------------------
+ */
+int sw_load_driver_i(void);
+void sw_unload_driver_i(void);
+int sw_init_collector_lists_i(void);
+void sw_destroy_collector_lists_i(void);
+int sw_init_data_structures_i(void);
+void sw_destroy_data_structures_i(void);
+int sw_get_arch_details_i(void);
+void sw_iterate_driver_info_lists_i(void);
+void sw_handle_immediate_request_i(void *request);
+int sw_print_collector_node_i(struct sw_collector_data *data);
+int sw_collection_start_i(void);
+int sw_collection_stop_i(void);
+int sw_collection_poll_i(void);
+size_t sw_get_payload_size_i(const struct sw_driver_interface_info *info);
+sw_driver_msg_t *
+sw_alloc_collector_msg_i(const struct sw_driver_interface_info *info,
+			 size_t per_msg_payload_size);
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+				       void *p_local_args);
+static long
+sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg,
+		      int local_len);
+static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd,
+			    u64 __user *remote_out_args);
+static void sw_do_extract_scu_fw_version(void);
+static long
+sw_get_available_name_id_mappings_i(enum sw_name_id_type type,
+				    struct sw_name_info_msg __user *remote_info,
+				    size_t local_len);
+static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void);
+static bool sw_should_flush_buffer_i(void);
+
+/* -------------------------------------------------
+ * Data structures.
+ * -------------------------------------------------
+ */
+/*
+ * Structure to hold current CMD state
+ * of the device driver. Constantly evolving, but
+ * that's OK -- this is internal to the driver
+ * and is NOT exported.
+ */
+struct swa_internal_state {
+	sw_driver_collection_cmd_t
+		cmd; // indicates which command was specified
+		     // last e.g. START, STOP etc.
+	/*
+	 * Should we write to our per-cpu output buffers?
+	 * YES if we're actively collecting.
+	 * NO if we're not.
+	 */
+	bool write_to_buffers;
+	/*
+	 * Should we "drain/flush" the per-cpu output buffers?
+	 * (See "device_read" for an explanation)
+	 */
+	bool drain_buffers;
+	// Others...
+};
+
+/* -------------------------------------------------
+ * Variables.
+ * -------------------------------------------------
+ */
+static bool do_force_module_scope_for_cpu_frequencies;
+module_param(do_force_module_scope_for_cpu_frequencies, bool, S_IRUSR);
+MODULE_PARM_DESC(
+	do_force_module_scope_for_cpu_frequencies,
+	"Toggle module scope for cpu frequencies. Sets \"affected_cpus\" and \"related_cpus\" of cpufreq_policy.");
+
+static unsigned short sw_buffer_num_pages = 16;
+module_param(sw_buffer_num_pages, ushort, S_IRUSR);
+MODULE_PARM_DESC(
+	sw_buffer_num_pages,
+	"Specify number of 4kB pages to use for each per-cpu buffer. MUST be a power of 2! Default value = 16 (64 kB)");
+
+/* TODO: convert from 'list_head' to 'hlist_head' */
+/*
+ * sw_collector_lists is an array of linked lists of "collector nodes"
+ * (sw_collector_data structs).  It is indexed by the sw_when_type_t's.
+ * Each list holds the collectors to "execute" at a specific time,
+ * e.g. the beginning of the run, at a poll interval, tracepoint, etc.
+ */
+static SW_DEFINE_LIST_HEAD(sw_collector_lists,
+			   sw_collector_data)[NUM_COLLECTOR_MODES];
+static __read_mostly u16 sw_scu_fw_major_minor;
+
+static struct swa_internal_state s_internal_state;
+static struct sw_file_ops s_ops = {
+	.ioctl_handler = &sw_unlocked_handle_ioctl_i,
+	.stop_handler = &sw_collection_stop_i,
+	.get_current_cmd = &sw_get_collection_cmd_i,
+	.should_flush = &sw_should_flush_buffer_i,
+};
+
+/*
+ * For each function that you want to profile,
+ * do the following (e.g. function 'foo'):
+ * **************************************************
+ * DECLARE_OVERHEAD_VARS(foo);
+ * **************************************************
+ * This will declare the two variables required
+ * to keep track of overheads incurred in
+ * calling/servicing 'foo'. Note that the name
+ * that you declare here *MUST* match the function name!
+ */
+
+DECLARE_OVERHEAD_VARS(sw_collection_poll_i); // for POLL
+DECLARE_OVERHEAD_VARS(sw_any_seg_full);
+
+/*
+ * String representation of the various 'SW_WHEN_TYPE_XYZ' enum values.
+ * Debugging ONLY!
+ */
+#if DO_DEBUG_OUTPUT
+static const char *s_when_type_names[] = { "BEGIN", "POLL", "NOTIFIER",
+					   "TRACEPOINT", "END" };
+#endif // DO_DEBUG_OUTPUT
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * External functions.
+ */
+int sw_process_snapshot(enum sw_when_type when)
+{
+	if (when > SW_WHEN_TYPE_END) {
+		pw_pr_error("invalid snapshot time %d specified!\n", when);
+		return -EINVAL;
+	}
+	if (sw_handle_collector_list(&sw_collector_lists[when],
+				     &sw_handle_collector_node)) {
+		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
+			    when);
+		return -EIO;
+	}
+	return 0;
+}
+
+int sw_process_snapshot_on_cpu(enum sw_when_type when, int cpu)
+{
+	if (when > SW_WHEN_TYPE_END) {
+		pw_pr_error("invalid snapshot time %d specified!\n", when);
+		return -EINVAL;
+	}
+	if (sw_handle_collector_list_on_cpu(&sw_collector_lists[when],
+					    &sw_handle_collector_node_on_cpu,
+					    cpu)) {
+		pw_pr_error("ERROR: could NOT handle snapshot for time %d!\n",
+			    when);
+		return -EIO;
+	}
+	return 0;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+int sw_print_collector_node_i(struct sw_collector_data *curr)
+{
+	pw_u16_t num_descriptors = 0;
+	sw_io_desc_print_func_t print_func = NULL;
+	struct sw_driver_io_descriptor *descriptor = NULL;
+	struct sw_driver_interface_info *info = NULL;
+
+	if (!curr) {
+		return -PW_ERROR;
+	}
+	info = curr->info;
+	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	pw_pr_debug(
+		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
+	     --num_descriptors, ++descriptor) {
+		const struct sw_hw_ops *ops =
+			sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops == NULL) {
+			return -PW_ERROR;
+		}
+		print_func = ops->print;
+		if (print_func && (*print_func)(descriptor)) {
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * Driver interface info and collector list functions.
+ */
+
+/**
+ * sw_reset_collector_node_i - Call the reset op on all of the descriptors
+ *                             in coll that have one.
+ * @coll: The data structure containing an array of collector descriptors.
+ *
+ * Return: PW_SUCCESS if all of the resets succeeded, -PW_ERROR if any failed.
+ */
+static int sw_reset_collector_node_i(struct sw_collector_data *coll)
+{
+	struct sw_driver_io_descriptor *descriptor = NULL;
+	struct sw_driver_interface_info *info = NULL;
+	int num_descriptors;
+	int retcode = PW_SUCCESS;
+
+	if (!coll) {
+		return -PW_ERROR;
+	}
+	info = coll->info;
+
+	descriptor = (struct sw_driver_io_descriptor *)info->descriptors;
+	pw_pr_debug(
+		"cpu-mask = %d, Plugin-ID = %d, Metric-ID = %d, MSG-ID = %d\n",
+		info->cpu_mask, info->plugin_id, info->metric_id, info->msg_id);
+	for (num_descriptors = info->num_io_descriptors; num_descriptors > 0;
+	     --num_descriptors, ++descriptor) {
+		const struct sw_hw_ops *ops =
+			sw_get_hw_ops_for(descriptor->collection_type);
+		if (ops && ops->reset && (*ops->reset)(descriptor)) {
+			retcode = -PW_ERROR;
+		}
+	}
+	return retcode;
+}
+
+static int sw_iterate_trace_notifier_list_i(struct sw_trace_notifier_data *node,
+				     void *dummy)
+{
+	return sw_handle_collector_list(&node->list,
+					&sw_print_collector_node_i);
+}
+
+void sw_iterate_driver_info_lists_i(void)
+{
+	sw_when_type_t which;
+
+	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+		if (sw_handle_collector_list(
+			    &sw_collector_lists[which],
+			    &sw_print_collector_node_i)) { // Should NEVER happen!
+			pw_pr_error(
+				"WARNING: error occurred while printing values!\n");
+		}
+	}
+
+	if (sw_for_each_tracepoint_node(&sw_iterate_trace_notifier_list_i, NULL,
+					false /*return-on-error*/)) {
+		pw_pr_error(
+			"WARNING: error occurred while printing tracepoint values!\n");
+	}
+	if (sw_for_each_notifier_node(&sw_iterate_trace_notifier_list_i, NULL,
+				      false /*return-on-error*/)) {
+		pw_pr_error(
+			"WARNING: error occurred while printing notifier values!\n");
+	}
+}
+
+static void sw_reset_collectors_i(void)
+{
+	sw_when_type_t which;
+
+	for (which = SW_WHEN_TYPE_BEGIN; which <= SW_WHEN_TYPE_END; ++which) {
+		pw_pr_debug("ITERATING list %s\n", s_when_type_names[which]);
+		if (sw_handle_collector_list(&sw_collector_lists[which],
+					     &sw_reset_collector_node_i)) {
+			pw_pr_error(
+				"WARNING: error occurred while resetting a collector!\n");
+		}
+	}
+}
+
+int sw_init_data_structures_i(void)
+{
+	/*
+	 * Find the # CPUs in this system.
+	 * Update: use 'num_possible' instead of 'num_present' in case
+	 * the cpus aren't numbered contiguously
+	 */
+	sw_max_num_cpus = num_possible_cpus();
+
+	/*
+	 * Initialize our trace subsys: MUST be called
+	 * BEFORE 'sw_init_collector_lists_i()!
+	 */
+	if (sw_add_trace_notify()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_init_collector_lists_i()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_init_per_cpu_buffers()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	if (sw_register_hw_ops()) {
+		sw_destroy_data_structures_i();
+		return -PW_ERROR;
+	}
+	return PW_SUCCESS;
+}
+
+void sw_destroy_data_structures_i(void)
+{
+	sw_free_hw_ops();
+	sw_destroy_per_cpu_buffers();
+	sw_destroy_collector_lists_i();
+	sw_remove_trace_notify();
+}
+
+int sw_get_arch_details_i(void)
+{
+	/*
+	 * SCU F/W version (if applicable)
+	 */
+	sw_do_extract_scu_fw_version();
+	return PW_SUCCESS;
+}
+
+#define INIT_FLAG ((void *)0)
+#define DESTROY_FLAG ((void *)1)
+
+static int
+sw_init_destroy_trace_notifier_lists_i(struct sw_trace_notifier_data *node,
+				       void *is_init)
+{
+	if (is_init == INIT_FLAG) {
+		sw_init_collector_list(&node->list);
+	} else {
+		sw_destroy_collector_list(&node->list);
+	}
+	node->was_registered = false;
+
+	return PW_SUCCESS;
+}
+
+int sw_init_collector_lists_i(void)
+{
+	int i = 0;
+
+	for (i = 0; i < NUM_COLLECTOR_MODES; ++i) {
+		sw_init_collector_list(&sw_collector_lists[i]);
+	}
+	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
+				    INIT_FLAG, false /*return-on-error*/);
+	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
+				  INIT_FLAG, false /*return-on-error*/);
+
+	return PW_SUCCESS;
+}
+
+void sw_destroy_collector_lists_i(void)
+{
+	int i = 0;
+
+	for (i = 0; i < NUM_COLLECTOR_MODES; ++i) {
+		sw_destroy_collector_list(&sw_collector_lists[i]);
+	}
+	sw_for_each_tracepoint_node(&sw_init_destroy_trace_notifier_lists_i,
+				    DESTROY_FLAG, false /*return-on-error*/);
+	sw_for_each_notifier_node(&sw_init_destroy_trace_notifier_lists_i,
+				  DESTROY_FLAG, false /*return-on-error*/);
+}
+
+/*
+ * Used for {READ,WRITE}_IMMEDIATE requests.
+ */
+typedef struct sw_immediate_request_info sw_immediate_request_info_t;
+struct sw_immediate_request_info {
+	struct sw_driver_io_descriptor *local_descriptor;
+	char *dst_vals;
+	int *retVal;
+};
+void sw_handle_immediate_request_i(void *request)
+{
+	struct sw_immediate_request_info *info =
+		(struct sw_immediate_request_info *)request;
+	struct sw_driver_io_descriptor *descriptor = info->local_descriptor;
+	char *dst_vals = info->dst_vals;
+	const struct sw_hw_ops *ops =
+		sw_get_hw_ops_for(descriptor->collection_type);
+	if (likely(ops != NULL)) {
+		*(info->retVal) = sw_handle_driver_io_descriptor(
+			dst_vals, RAW_CPU(), descriptor, ops);
+	} else {
+		pw_pr_error(
+			"No operations found to satisfy collection type %u!\n",
+			descriptor->collection_type);
+	}
+	return;
+}
+
+static int num_times_polled;
+
+int sw_collection_start_i(void)
+{
+	/*
+	 * Reset the poll tick counter.
+	 */
+	num_times_polled = 0;
+	/*
+	 * Update the output buffers.
+	 */
+	sw_reset_per_cpu_buffers();
+	/*
+	 * Ensure clients don't think we're in 'flush' mode.
+	 */
+	s_internal_state.drain_buffers = false;
+	/*
+	 * Set the 'command'
+	 */
+	s_internal_state.cmd = SW_DRIVER_START_COLLECTION;
+	/*
+	 * Clear out the topology list
+	 */
+	sw_clear_topology_list();
+	/*
+	 * Handle 'START' snapshots, if any.
+	 */
+	{
+		if (sw_handle_collector_list(
+			    &sw_collector_lists[SW_WHEN_TYPE_BEGIN],
+			    &sw_handle_collector_node)) {
+			pw_pr_error(
+				"ERROR: could NOT handle START collector list!\n");
+			return -PW_ERROR;
+		}
+	}
+	/*
+	 * Register any required tracepoints and notifiers.
+	 */
+	{
+		if (sw_register_trace_notifiers()) {
+			pw_pr_error("ERROR registering trace_notifiers!\n");
+			sw_unregister_trace_notifiers();
+			return -PW_ERROR;
+		}
+	}
+	pw_pr_debug("OK, STARTED collection!\n");
+	return PW_SUCCESS;
+}
+
+int sw_collection_stop_i(void)
+{
+	/*
+	 * Unregister any registered tracepoints and notifiers.
+	 */
+	if (sw_unregister_trace_notifiers()) {
+		pw_pr_warn(
+			"Warning: some trace_notifier probe functions could NOT be unregistered!\n");
+	}
+	/*
+	 * Handle 'STOP' snapshots, if any.
+	 */
+	if (sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_END],
+				     &sw_handle_collector_node)) {
+		pw_pr_error("ERROR: could NOT handle STOP collector list!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Set the 'command'
+	 */
+	s_internal_state.cmd = SW_DRIVER_STOP_COLLECTION;
+	/*
+	 * Tell consumers to 'flush' all buffers. We need to
+	 * defer this as long as possible because it needs to be
+	 * close to the 'wake_up_interruptible', below.
+	 */
+	s_internal_state.drain_buffers = true;
+	smp_mb();
+	/*
+	 * Wakeup any sleeping readers, and cleanup any
+	 * timers in the reader subsys.
+	 */
+	sw_cancel_reader();
+	/*
+	 * Collect stats on samples produced and dropped.
+	 * TODO: call from 'device_read()' instead?
+	 */
+	sw_count_samples_produced_dropped();
+#if DO_OVERHEAD_MEASUREMENTS
+	pw_pr_force(
+		"DEBUG: there were %llu samples produced and %llu samples dropped in buffer v5!\n",
+		sw_num_samples_produced, sw_num_samples_dropped);
+#endif // DO_OVERHEAD_MEASUREMENTS
+	/*
+	 * DEBUG: iterate over collection lists.
+	 */
+	sw_iterate_driver_info_lists_i();
+	/*
+	 * Shut down any collectors that need shutting down.
+	 */
+	sw_reset_collectors_i();
+	/*
+	 * Clear out the collector lists.
+	 */
+	sw_destroy_collector_lists_i();
+	pw_pr_debug("OK, STOPPED collection!\n");
+#if DO_OVERHEAD_MEASUREMENTS
+	pw_pr_force("There were %d poll ticks!\n", num_times_polled);
+#endif // DO_OVERHEAD_MEASUREMENTS
+	return PW_SUCCESS;
+}
+
+int sw_collection_poll_i(void)
+{
+	/*
+	 * Handle 'POLL' timer expirations.
+	 */
+	if (SW_LIST_EMPTY(&sw_collector_lists[SW_WHEN_TYPE_POLL])) {
+		pw_pr_debug("DEBUG: EMPTY POLL LIST\n");
+	}
+	++num_times_polled;
+	return sw_handle_collector_list(&sw_collector_lists[SW_WHEN_TYPE_POLL],
+					&sw_handle_collector_node);
+}
+
+/*
+ * Private data for the 'sw_add_trace_notifier_driver_info_i' function.
+ */
+struct tn_data {
+	struct sw_driver_interface_info *info;
+	u64 mask;
+};
+
+static int
+sw_add_trace_notifier_driver_info_i(struct sw_trace_notifier_data *node,
+				    void *priv)
+{
+	struct tn_data *data = (struct tn_data *)priv;
+	struct sw_driver_interface_info *local_info = data->info;
+	u64 mask = data->mask;
+	int id = sw_get_trace_notifier_id(node);
+
+	if (IS_TRACE_NOTIFIER_ID_IN_MASK(id, mask)) {
+		pw_pr_debug("TRACEPOINT ID = %d is IN mask 0x%llx\n", id, mask);
+		if (sw_add_driver_info(&node->list, local_info)) {
+			pw_pr_error(
+				"WARNING: could NOT add driver info to list!\n");
+			return -PW_ERROR;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+static int sw_post_config_i(const struct sw_hw_ops *op, void *priv)
+{
+	if (!op->available || !(*op->available)()) {
+		/* op not available */
+		return 0;
+	}
+	if (!op->post_config || (*op->post_config)()) {
+		return 0;
+	}
+	return -EIO;
+}
+
+/**
+ * sw_set_driver_infos_i - Process the collection config data passed down
+ *                         from the client.
+ * @remote_msg: The user space address of our ioctl data.
+ * @local_len:  The number of bytes of remote_msg we should copy.
+ *
+ * This function copies the ioctl data from user space to kernel
+ * space.  That data is an array of sw_driver_interface_info structs,
+ * which hold information about tracepoints, notifiers, and collector
+ * configuration info for this collection run..  For each driver_info
+ * struct, it calls the appropriate "add info" (registration/
+ * configuration) function for each of the "when types" (begin, poll,
+ * notifier, tracepoint, end) which should trigger a collection
+ * operation for that collector.
+ *
+ * When this function is done, the data structures corresponding to
+ * collection should be configured and initialized.
+ *
+ *
+ * Returns: PW_SUCCESS on success, or a non-zero on an error.
+ */
+static long
+sw_set_driver_infos_i(struct sw_driver_interface_msg __user *remote_msg,
+		      int local_len)
+{
+	struct sw_driver_interface_info *local_info = NULL;
+	struct sw_driver_interface_msg *local_msg = vmalloc(local_len);
+	pw_u8_t read_triggers = 0x0;
+	pw_u16_t num_infos = 0;
+	sw_when_type_t i = SW_WHEN_TYPE_BEGIN;
+	char *__data = (char *)local_msg->infos;
+	size_t dst_idx = 0;
+
+	if (!local_msg) {
+		pw_pr_error("ERROR allocating space for local message!\n");
+		return -EFAULT;
+	}
+	if (copy_from_user(local_msg, (struct sw_driver_interface_msg __user *)
+			   remote_msg, local_len)) {
+		pw_pr_error("ERROR copying message from user space!\n");
+		vfree(local_msg);
+		return -EFAULT;
+	}
+	/*
+	 * We aren't allowed to config the driver multiple times between
+	 * collections. Clear out any previous config values.
+	 */
+	sw_destroy_collector_lists_i();
+
+	/*
+	 * Did the user specify a min polling interval?
+	 */
+	sw_min_polling_interval_msecs = local_msg->min_polling_interval_msecs;
+	pw_pr_debug("min_polling_interval_msecs = %u\n",
+		    sw_min_polling_interval_msecs);
+
+	num_infos = local_msg->num_infos;
+	pw_pr_debug("LOCAL NUM INFOS = %u\n", num_infos);
+	for (; num_infos > 0; --num_infos) {
+		local_info =
+			(struct sw_driver_interface_info *)&__data[dst_idx];
+		dst_idx += (SW_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+			    local_info->num_io_descriptors *
+				    sizeof(struct sw_driver_io_descriptor));
+		read_triggers = local_info->trigger_bits;
+		pw_pr_debug(
+			"read_triggers = %u, # msrs = %u, new dst_idx = %u\n",
+			(unsigned int)read_triggers,
+			(unsigned int)local_info->num_io_descriptors,
+			(unsigned int)dst_idx);
+		for (i = SW_WHEN_TYPE_BEGIN; i <= SW_WHEN_TYPE_END;
+		     ++i, read_triggers >>= 1) {
+			if (read_triggers & 0x1) { // Bit 'i' is set
+				pw_pr_debug("BIT %d is SET!\n", i);
+				if (i == SW_WHEN_TYPE_TRACEPOINT) {
+					struct tn_data tn_data = {
+						local_info,
+						local_info->tracepoint_id_mask
+					};
+					pw_pr_debug(
+						"TRACEPOINT, MASK = 0x%llx\n",
+						local_info->tracepoint_id_mask);
+					sw_for_each_tracepoint_node(
+						&sw_add_trace_notifier_driver_info_i,
+						&tn_data,
+						false /*return-on-error*/);
+				} else if (i == SW_WHEN_TYPE_NOTIFIER) {
+					struct tn_data tn_data = {
+						local_info,
+						local_info->notifier_id_mask
+					};
+					pw_pr_debug(
+						"NOTIFIER, MASK = 0x%llx\n",
+						local_info->notifier_id_mask);
+					sw_for_each_notifier_node(
+						&sw_add_trace_notifier_driver_info_i,
+						&tn_data,
+						false /*return-on-error*/);
+				} else {
+					if (sw_add_driver_info(
+						    &sw_collector_lists[i],
+						    local_info)) {
+						pw_pr_error(
+							"WARNING: could NOT add driver info to list for 'when type' %d!\n",
+							i);
+					}
+				}
+			}
+		}
+	}
+	if (sw_for_each_hw_op(&sw_post_config_i, NULL,
+			      false /*return-on-error*/)) {
+		pw_pr_error("POST-CONFIG error!\n");
+	}
+	vfree(local_msg);
+	memset(&s_internal_state, 0, sizeof(s_internal_state));
+	/*
+	 * DEBUG: iterate over collection lists.
+	 */
+	sw_iterate_driver_info_lists_i();
+	return PW_SUCCESS;
+}
+
+static long sw_handle_cmd_i(sw_driver_collection_cmd_t cmd,
+			    u64 __user *remote_out_args)
+{
+	/*
+	 * First, handle the command.
+	 */
+	if (cmd < SW_DRIVER_START_COLLECTION ||
+	    cmd > SW_DRIVER_CANCEL_COLLECTION) {
+		pw_pr_error("ERROR: invalid cmd = %d\n", cmd);
+		return -PW_ERROR;
+	}
+	switch (cmd) {
+	case SW_DRIVER_START_COLLECTION:
+		if (sw_collection_start_i()) {
+			return -PW_ERROR;
+		}
+		break;
+	case SW_DRIVER_STOP_COLLECTION:
+		if (sw_collection_stop_i()) {
+			return -PW_ERROR;
+		}
+		break;
+	default:
+		pw_pr_error("WARNING: unsupported command %d\n", cmd);
+		break;
+	}
+	/*
+	 * Then retrieve sample stats.
+	 */
+#if DO_COUNT_DROPPED_SAMPLES
+	if (cmd == SW_DRIVER_STOP_COLLECTION) {
+		u64 local_args[2] = { sw_num_samples_produced,
+				      sw_num_samples_dropped };
+		if (copy_to_user(remote_out_args, local_args,
+				 sizeof(local_args))) {
+			pw_pr_error(
+				"couldn't copy collection stats to user space!\n");
+			return -PW_ERROR;
+		}
+	}
+#endif // DO_COUNT_DROPPED_SAMPLES
+	return PW_SUCCESS;
+}
+
+#ifdef SFI_SIG_OEMB
+static int sw_do_parse_sfi_oemb_table(struct sfi_table_header *header)
+{
+#ifdef CONFIG_X86_WANT_INTEL_MID
+	struct sfi_table_oemb *oemb = (struct sfi_table_oemb *)
+		header; // 'struct sfi_table_oemb' defined in 'intel-mid.h'
+	if (!oemb) {
+		pw_pr_error("ERROR: NULL sfi table header!\n");
+		return -PW_ERROR;
+	}
+	sw_scu_fw_major_minor = (oemb->scu_runtime_major_version << 8) |
+				(oemb->scu_runtime_minor_version);
+	pw_pr_debug("DEBUG: major = %u, minor = %u\n",
+		    oemb->scu_runtime_major_version,
+		    oemb->scu_runtime_minor_version);
+#endif // CONFIG_X86_WANT_INTEL_MID
+	return PW_SUCCESS;
+}
+#endif // SFI_SIG_OEMB
+
+static void sw_do_extract_scu_fw_version(void)
+{
+	sw_scu_fw_major_minor = 0x0;
+#ifdef SFI_SIG_OEMB
+	if (sfi_table_parse(SFI_SIG_OEMB, NULL, NULL,
+			    &sw_do_parse_sfi_oemb_table)) {
+		pw_pr_force("WARNING: NO SFI information!\n");
+	}
+#endif // SFI_SIG_OEMB
+}
+
+static int sw_gather_trace_notifier_i(struct sw_trace_notifier_data *node,
+				      struct sw_name_info_msg *msg,
+				      enum sw_name_id_type type)
+{
+	pw_u16_t *idx = &msg->payload_len;
+	char *buffer = (char *)&msg->pairs[*idx];
+	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+	int id = sw_get_trace_notifier_id(node);
+	struct sw_string_type *str = &pair->name;
+	const char *abstract_name = sw_get_trace_notifier_abstract_name(node);
+
+	if (likely(abstract_name && id >= 0)) {
+		++msg->num_name_id_pairs;
+		pair->type = type;
+		pair->id = (u16)id;
+		str->len = strlen(abstract_name) + 1; // "+1" for trailing '\0'
+		memcpy(&str->data[0], abstract_name, str->len);
+
+		pw_pr_debug("TP[%d] = %s (%u)\n",
+			    sw_get_trace_notifier_id(node), abstract_name,
+			    (unsigned int)strlen(abstract_name));
+
+		*idx += SW_NAME_ID_HEADER_SIZE() +
+			SW_STRING_TYPE_HEADER_SIZE() + str->len;
+	}
+
+	return PW_SUCCESS;
+}
+
+static int sw_gather_tracepoint_i(struct sw_trace_notifier_data *node,
+				  void *priv)
+{
+	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
+					  SW_NAME_TYPE_TRACEPOINT);
+}
+
+static int sw_gather_notifier_i(struct sw_trace_notifier_data *node, void *priv)
+{
+	return sw_gather_trace_notifier_i(node, (struct sw_name_info_msg *)priv,
+					  SW_NAME_TYPE_NOTIFIER);
+}
+
+static long
+sw_get_available_trace_notifiers_i(enum sw_name_id_type type,
+				   struct sw_name_info_msg *local_info)
+{
+	long retVal = PW_SUCCESS;
+
+	if (type == SW_NAME_TYPE_TRACEPOINT) {
+		retVal = sw_for_each_tracepoint_node(&sw_gather_tracepoint_i,
+						     local_info,
+						     false /*return-on-error*/);
+	} else {
+		retVal = sw_for_each_notifier_node(&sw_gather_notifier_i,
+						   local_info,
+						   false /*return-on-error*/);
+	}
+	pw_pr_debug(
+		"There are %u extracted traces/notifiers for a total of %u bytes!\n",
+		local_info->num_name_id_pairs, local_info->payload_len);
+	return retVal;
+}
+
+static int sw_gather_hw_op_i(const struct sw_hw_ops *op, void *priv)
+{
+	struct sw_name_info_msg *msg = (struct sw_name_info_msg *)priv;
+	pw_u16_t *idx = &msg->payload_len;
+	char *buffer = (char *)&msg->pairs[*idx];
+	struct sw_name_id_pair *pair = (struct sw_name_id_pair *)buffer;
+	struct sw_string_type *str = &pair->name;
+	const char *abstract_name = sw_get_hw_op_abstract_name(op);
+	int id = sw_get_hw_op_id(op);
+
+	pw_pr_debug("Gather Collector[%d] = %s\n", id, abstract_name);
+	if (likely(abstract_name && id >= 0)) {
+		/*
+		 * Final check: is this operation available on the
+		 * target platform? If 'available' function doesn't
+		 * exist then YES. Else call 'available'
+		 * function to decide.
+		 */
+		pw_pr_debug("%s has available = %p\n", abstract_name,
+			    op->available);
+		if (!op->available || (*op->available)()) {
+			++msg->num_name_id_pairs;
+			pair->type = SW_NAME_TYPE_COLLECTOR;
+			pair->id = (u16)id;
+			str->len = strlen(abstract_name) +
+				   1; // "+1" for trailing '\0'
+			memcpy(&str->data[0], abstract_name, str->len);
+
+			*idx += SW_NAME_ID_HEADER_SIZE() +
+				SW_STRING_TYPE_HEADER_SIZE() + str->len;
+		}
+	}
+
+	return PW_SUCCESS;
+}
+
+static long sw_get_available_collectors_i(struct sw_name_info_msg *local_info)
+{
+	return sw_for_each_hw_op(&sw_gather_hw_op_i, local_info,
+				 false /*return-on-error*/);
+}
+
+static long
+sw_get_available_name_id_mappings_i(enum sw_name_id_type type,
+				    struct sw_name_info_msg __user *remote_info,
+				    size_t local_len)
+{
+	char *buffer = vmalloc(local_len);
+	struct sw_name_info_msg *local_info = NULL;
+	long retVal = PW_SUCCESS;
+
+	if (!buffer) {
+		pw_pr_error("ERROR: couldn't alloc temp buffer!\n");
+		return -PW_ERROR;
+	}
+	memset(buffer, 0, local_len);
+	local_info = (struct sw_name_info_msg *)buffer;
+
+	if (type == SW_NAME_TYPE_COLLECTOR) {
+		retVal = sw_get_available_collectors_i(local_info);
+	} else {
+		retVal = sw_get_available_trace_notifiers_i(type, local_info);
+	}
+	if (retVal == PW_SUCCESS) {
+		retVal = copy_to_user(remote_info, local_info, local_len);
+		if (retVal) {
+			pw_pr_error(
+				"ERROR: couldn't copy tracepoint info to user space!\n");
+		}
+	}
+	vfree(buffer);
+	return retVal;
+}
+
+static long
+sw_get_topology_changes_i(struct sw_driver_topology_msg __user *remote_msg,
+			  size_t local_len)
+{
+	char *buffer = NULL;
+	struct sw_driver_topology_msg *local_msg = NULL;
+	size_t buffer_len = sizeof(struct sw_driver_topology_msg) +
+			    sw_num_topology_entries *
+				    sizeof(struct sw_driver_topology_change);
+	long retVal = PW_SUCCESS;
+	struct sw_driver_topology_change *dst = NULL;
+	size_t dst_idx = 0;
+
+	SW_LIST_HEAD_VAR(sw_topology_node) *head = (void *)&sw_topology_list;
+	struct sw_topology_node *tnode = NULL;
+
+	if (local_len < buffer_len) {
+		pw_pr_error(
+			"ERROR: insufficient buffer space to encode topology changes! Requires %zu, output space = %zu\n",
+			buffer_len, local_len);
+		return -EIO;
+	}
+
+	buffer = vmalloc(buffer_len);
+	if (!buffer) {
+		pw_pr_error(
+			"ERROR: couldn't allocate buffer for topology transfer!\n");
+		return -EIO;
+	}
+	memset(buffer, 0, buffer_len);
+
+	local_msg = (struct sw_driver_topology_msg *)buffer;
+	local_msg->num_entries = sw_num_topology_entries;
+	dst = (struct sw_driver_topology_change *)&local_msg
+		      ->topology_entries[0];
+	SW_LIST_FOR_EACH_ENTRY(tnode, head, list)
+	{
+		struct sw_driver_topology_change *change = &tnode->change;
+
+		memcpy(&dst[dst_idx++], change, sizeof(*change));
+	}
+	retVal = copy_to_user(remote_msg, local_msg, buffer_len);
+	if (retVal) {
+		pw_pr_error(
+			"ERROR: couldn't copy topology changes to user space!\n");
+	}
+	vfree(buffer);
+	return retVal;
+}
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#define MATCH_IOCTL(num, pred) ((num) == (pred) || (num) == (pred##32))
+#else
+#define MATCH_IOCTL(num, pred) ((num) == (pred))
+#endif
+
+static long sw_unlocked_handle_ioctl_i(unsigned int ioctl_num,
+				       void *p_local_args)
+{
+	struct sw_driver_ioctl_arg local_args;
+	int local_in_len, local_out_len;
+
+	if (!p_local_args) {
+		pw_pr_error("ERROR: NULL p_local_args value?!\n");
+		return -PW_ERROR;
+	}
+
+	/*
+	 * (1) Sanity check:
+	 * Before doing anything, double check to
+	 * make sure this IOCTL was really intended
+	 * for us!
+	 */
+	if (_IOC_TYPE(ioctl_num) != APWR_IOCTL_MAGIC_NUM) {
+		pw_pr_error(
+			"ERROR: requested IOCTL TYPE (%d) != APWR_IOCTL_MAGIC_NUM (%d)\n",
+			_IOC_TYPE(ioctl_num), APWR_IOCTL_MAGIC_NUM);
+		return -PW_ERROR;
+	}
+	/*
+	 * (2) Extract arg lengths.
+	 */
+	local_args = *((struct sw_driver_ioctl_arg *)p_local_args);
+
+	local_in_len = local_args.in_len;
+	local_out_len = local_args.out_len;
+	pw_pr_debug("GU: local_in_len = %d, local_out_len = %d\n", local_in_len,
+		    local_out_len);
+	/*
+	 * (3) Service individual IOCTL requests.
+	 */
+	if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CONFIG)) {
+		pw_pr_debug("PW_IOCTL_CONFIG\n");
+		return sw_set_driver_infos_i(
+			(struct sw_driver_interface_msg __user *)
+				local_args.in_arg,
+			local_in_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_CMD)) {
+		sw_driver_collection_cmd_t local_cmd;
+
+		pw_pr_debug("PW_IOCTL_CMD\n");
+		if (get_user(local_cmd, (sw_driver_collection_cmd_t __user *)
+						local_args.in_arg)) {
+			pw_pr_error("ERROR: could NOT extract cmd value!\n");
+			return -PW_ERROR;
+		}
+		return sw_handle_cmd_i(local_cmd,
+				       (u64 __user *)local_args.out_arg);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_POLL)) {
+		pw_pr_debug("PW_IOCTL_POLL\n");
+		return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_collection_poll_i);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_IMMEDIATE_IO)) {
+		struct sw_driver_interface_info *local_info;
+		struct sw_driver_io_descriptor *local_descriptor = NULL;
+		int retVal = PW_SUCCESS;
+		char *src_vals = NULL;
+		char *dst_vals = NULL;
+
+		pw_pr_debug("PW_IOCTL_IMMEDIATE_IO\n");
+		pw_pr_debug("local_in_len = %u\n", local_in_len);
+
+		src_vals = vmalloc(local_in_len);
+		if (!src_vals) {
+			pw_pr_error(
+				"ERROR allocating space for immediate IO\n");
+			return -PW_ERROR;
+		}
+		if (local_out_len) {
+			dst_vals = vmalloc(local_out_len);
+			if (!dst_vals) {
+				vfree(src_vals);
+				pw_pr_error(
+					"ERROR allocating space for immediate IO\n");
+				return -PW_ERROR;
+			}
+		}
+		if (copy_from_user(src_vals, (char __user *)local_args.in_arg,
+				   local_in_len)) {
+			pw_pr_error(
+				"ERROR copying in immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		local_info = (struct sw_driver_interface_info *)src_vals;
+		pw_pr_debug(
+			"OK, asked to perform immediate IO on cpu(s) %d, # descriptors = %d\n",
+			local_info->cpu_mask, local_info->num_io_descriptors);
+		/*
+		 * For now, require only a single descriptor.
+		 */
+		if (local_info->num_io_descriptors != 1) {
+			pw_pr_error(
+				"ERROR: told to perform immediate IO with %d descriptors -- MAX of 1 descriptor allowed!\n",
+				local_info->num_io_descriptors);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		local_descriptor = ((struct sw_driver_io_descriptor *)
+					    local_info->descriptors);
+		pw_pr_debug("Collection type after %d\n",
+			    local_descriptor->collection_type);
+		/*
+		 * Check cpu mask for correctness here. For now, we do NOT allow
+		 * reading on ALL cpus.
+		 */
+		if ((int)local_info->cpu_mask < -1 ||
+		    (int)local_info->cpu_mask >= (int)sw_max_num_cpus) {
+			pw_pr_error(
+				"ERROR: invalid cpu mask %d specified in immediate IO; valid values are: -1, [0 -- %d]!\n",
+				local_info->cpu_mask, sw_max_num_cpus - 1);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Check collection type for correctness here
+		 */
+		pw_pr_debug(
+			"Asked to perform immediate IO with descriptor with type = %d, on cpu = %d\n",
+			local_descriptor->collection_type,
+			local_info->cpu_mask);
+		if (sw_is_valid_hw_op_id(local_descriptor->collection_type) ==
+		    false) {
+			pw_pr_error(
+				"ERROR: invalid collection type %d specified for immediate IO\n",
+				(int)local_descriptor->collection_type);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Check collection cmd for correctness here
+		 */
+		if (local_descriptor->collection_command < SW_IO_CMD_READ ||
+		    local_descriptor->collection_command > SW_IO_CMD_WRITE) {
+			pw_pr_error(
+				"ERROR: invalid collection command %d specified for immediate IO\n",
+				local_descriptor->collection_command);
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * Initialize the descriptor -- 'MMIO' and 'IPC' reads may need
+		 * an "ioremap_nocache"
+		 */
+		if (sw_init_driver_io_descriptor(local_descriptor)) {
+			pw_pr_error(
+				"ERROR initializing immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+		/*
+		 * OK, perform the actual IO.
+		 */
+		{
+			struct sw_immediate_request_info request_info = {
+				local_descriptor, dst_vals, &retVal
+			};
+			struct cpumask cpumask;
+
+			cpumask_clear(&cpumask);
+			switch (local_info->cpu_mask) {
+			case -1: // IO on ANY CPU (assume current CPU)
+				cpumask_set_cpu(RAW_CPU(), &cpumask);
+				pw_pr_debug("ANY CPU\n");
+				break;
+			default: // IO on a particular CPU
+				cpumask_set_cpu(local_info->cpu_mask, &cpumask);
+				pw_pr_debug("[%d] setting for %d\n", RAW_CPU(),
+					    local_info->cpu_mask);
+				break;
+			}
+			sw_schedule_work(&cpumask,
+					 &sw_handle_immediate_request_i,
+					 &request_info);
+		}
+		if (retVal != PW_SUCCESS) {
+			pw_pr_error(
+				"ERROR performing immediate IO on one (or more) CPUs!\n");
+			goto ret_immediate_io_reset;
+		}
+		/*
+		 * OK, all done.
+		 */
+		if (local_descriptor->collection_command == SW_IO_CMD_READ) {
+			if (copy_to_user(local_args.out_arg, dst_vals,
+					 local_out_len)) {
+				pw_pr_error(
+					"ERROR copying %u bytes of value to userspace!\n",
+					local_out_len);
+				retVal = -PW_ERROR;
+				goto ret_immediate_io_reset;
+			}
+			pw_pr_debug(
+				"OK, copied %u bytes of value to userspace addr %p!\n",
+				local_out_len, local_args.out_arg);
+		}
+ret_immediate_io_reset:
+		/*
+		 * Reset the descriptor -- 'MMIO' and 'IPC' reads may have
+		 * performed an "ioremap_nocache" which now needs to be
+		 * unmapped.
+		 */
+		if (sw_reset_driver_io_descriptor(local_descriptor)) {
+			pw_pr_error(
+				"ERROR resetting immediate IO descriptor\n");
+			retVal = -PW_ERROR;
+			goto ret_immediate_io;
+		}
+ret_immediate_io:
+		vfree(src_vals);
+		if (dst_vals) {
+			vfree(dst_vals);
+		}
+		return retVal;
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_SCU_FW_VERSION)) {
+		u32 local_data = (u32)sw_scu_fw_major_minor;
+
+		if (put_user(local_data, (u32 __user *)local_args.out_arg)) {
+			pw_pr_error(
+				"ERROR copying scu fw version to userspace!\n");
+			return -PW_ERROR;
+		}
+		return PW_SUCCESS;
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_DRIVER_VERSION)) {
+		pw_u64_t local_version =
+			(pw_u64_t)SW_DRIVER_VERSION_MAJOR << 32 |
+			(pw_u64_t)SW_DRIVER_VERSION_MINOR << 16 |
+			(pw_u64_t)SW_DRIVER_VERSION_OTHER;
+		if (put_user(local_version, (u64 __user *)local_args.out_arg)) {
+			pw_pr_error(
+				"ERROR copying driver version to userspace!\n");
+			return -PW_ERROR;
+		}
+		return PW_SUCCESS;
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_TRACEPOINTS)) {
+		pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n",
+			    local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_TRACEPOINT,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_NOTIFIERS)) {
+		pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n",
+			    local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_NOTIFIER,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_AVAILABLE_COLLECTORS)) {
+		pw_pr_debug("DEBUG: AVAIL tracepoints! local_out_len = %u\n",
+			    local_out_len);
+		return sw_get_available_name_id_mappings_i(
+			SW_NAME_TYPE_COLLECTOR,
+			(struct sw_name_info_msg __user *)local_args.out_arg,
+			local_out_len);
+	} else if (MATCH_IOCTL(ioctl_num, PW_IOCTL_GET_TOPOLOGY_CHANGES)) {
+		pw_pr_debug("DEBUG: TOPOLOGY changes! local_out_len = %u\n",
+			    local_out_len);
+		return sw_get_topology_changes_i(
+			(struct sw_driver_topology_msg __user *)
+				local_args.out_arg,
+			local_out_len);
+	} else {
+		pw_pr_error("ERROR: invalid ioctl num: %u\n",
+			    _IOC_NR(ioctl_num));
+	}
+	return -PW_ERROR;
+}
+
+static enum sw_driver_collection_cmd sw_get_collection_cmd_i(void)
+{
+	return s_internal_state.cmd;
+};
+
+static bool sw_should_flush_buffer_i(void)
+{
+	return s_internal_state.drain_buffers;
+};
+
+int sw_load_driver_i(void)
+{
+	/*
+	 * Set per-cpu buffer size.
+	 * First, Perform sanity checking of per-cpu buffer size.
+	 */
+	/*
+	 * 1. Num pages MUST be pow-of-2.
+	 */
+	{
+		if (sw_buffer_num_pages & (sw_buffer_num_pages - 1)) {
+			pw_pr_error(
+				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be a power of 2!\n",
+				sw_buffer_num_pages);
+			return -PW_ERROR;
+		}
+	}
+	/*
+	 * 2. Num pages MUST be <= 16 (i.e. per-cpu buffer size
+	 * MUST be <= 64 kB)
+	 */
+	{
+		if (sw_buffer_num_pages > 16) {
+			pw_pr_error(
+				"Invalid value (%u) for number of pages in each per-cpu buffer; MUST be <= 16!\n",
+				sw_buffer_num_pages);
+			return -PW_ERROR;
+		}
+	}
+	sw_buffer_alloc_size = sw_buffer_num_pages * PAGE_SIZE;
+	/*
+	 * Retrieve any arch details here.
+	 */
+	if (sw_get_arch_details_i()) {
+		pw_pr_error("ERROR retrieving arch details!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Check to see if the user wants us to force
+	 * software coordination of CPU frequencies.
+	 */
+	if (do_force_module_scope_for_cpu_frequencies) {
+		pw_pr_force(
+			"DEBUG: FORCING MODULE SCOPE FOR CPU FREQUENCIES!\n");
+		if (sw_set_module_scope_for_cpus()) {
+			pw_pr_force("ERROR setting affected cpus\n");
+			return -PW_ERROR;
+		} else {
+			pw_pr_debug("OK, setting worked\n");
+		}
+	}
+	if (sw_init_data_structures_i()) {
+		pw_pr_error("ERROR initializing data structures!\n");
+		goto err_ret_init_data;
+	}
+	if (sw_register_dev(&s_ops)) {
+		goto err_ret_register_dev;
+	}
+	/*
+	 * Retrieve a list of tracepoint structs to use when
+	 * registering probe functions.
+	 */
+	{
+		if (sw_extract_tracepoints()) {
+			pw_pr_error(
+				"ERROR: could NOT retrieve a complete list of valid tracepoint structs!\n");
+			goto err_ret_tracepoint;
+		}
+	}
+	pw_pr_force("-----------------------------------------\n");
+	pw_pr_force("OK: LOADED SoC Watch Driver\n");
+#ifdef CONFIG_X86_WANT_INTEL_MID
+	pw_pr_force("SOC Identifier = %u, Stepping = %u\n",
+		    intel_mid_identify_cpu(), intel_mid_soc_stepping());
+#endif // CONFIG_X86_WANT_INTEL_MID
+	pw_pr_force("-----------------------------------------\n");
+	return PW_SUCCESS;
+
+err_ret_tracepoint:
+	sw_unregister_dev();
+err_ret_register_dev:
+	sw_destroy_data_structures_i();
+err_ret_init_data:
+	if (do_force_module_scope_for_cpu_frequencies) {
+		if (sw_reset_module_scope_for_cpus()) {
+			pw_pr_force("ERROR resetting affected cpus\n");
+		} else {
+			pw_pr_debug("OK, resetting worked\n");
+		}
+	}
+	return -PW_ERROR;
+}
+
+void sw_unload_driver_i(void)
+{
+	sw_iterate_driver_info_lists_i();
+
+	sw_unregister_dev();
+
+	sw_destroy_data_structures_i();
+
+	if (do_force_module_scope_for_cpu_frequencies) {
+		if (sw_reset_module_scope_for_cpus()) {
+			pw_pr_force("ERROR resetting affected cpus\n");
+		} else {
+			pw_pr_debug("OK, resetting worked\n");
+		}
+	}
+
+	pw_pr_force("-----------------------------------------\n");
+	pw_pr_force("OK: UNLOADED SoC Watch Driver\n");
+
+	sw_print_trace_notifier_overheads();
+	sw_print_output_buffer_overheads();
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_collection_poll_i, "POLL");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_any_seg_full, "ANY_SEG_FULL");
+#if DO_TRACK_MEMORY_USAGE
+	{
+		/*
+		 * Dump memory stats.
+		 */
+		pw_pr_force(
+			"TOTAL # BYTES ALLOCED = %llu, CURR # BYTES ALLOCED = %llu, MAX # BYTES ALLOCED = %llu\n",
+			sw_get_total_bytes_alloced(),
+			sw_get_curr_bytes_alloced(),
+			sw_get_max_bytes_alloced());
+		if (unlikely(sw_get_curr_bytes_alloced())) {
+			pw_pr_force(
+				"***********************************************************************\n");
+			pw_pr_force(
+				"WARNING: possible memory leak: there are %llu bytes still allocated!\n",
+				sw_get_curr_bytes_alloced());
+			pw_pr_force(
+				"***********************************************************************\n");
+		}
+	}
+#endif // DO_TRACK_MEMORY_USAGE
+	pw_pr_force("-----------------------------------------\n");
+}
+
+module_init(sw_load_driver_i);
+module_exit(sw_unload_driver_i);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR(MOD_AUTHOR);
+MODULE_DESCRIPTION(MOD_DESC);
diff --git a/drivers/platform/x86/socwatch/sw_file_ops.c b/drivers/platform/x86/socwatch/sw_file_ops.c
new file mode 100644
index 000000000000..06c88801a9ec
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_file_ops.c
@@ -0,0 +1,364 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/module.h> // try_module_get
+#include <linux/fs.h> // inode
+#include <linux/device.h> // class_create
+#include <linux/cdev.h> // cdev_alloc
+#include <linux/version.h> // LINUX_VERSION_CODE
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 12, 0)
+#include <asm/uaccess.h> // copy_to_user
+#else
+#include <linux/uaccess.h> // copy_to_user
+#endif // LINUX_VERSION_CODE
+#include <linux/wait.h> // wait_event_interruptible
+#include <linux/sched.h> // TASK_INTERRUPTIBLE
+
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_structs.h"
+#include "sw_file_ops.h"
+#include "sw_ioctl.h"
+#include "sw_output_buffer.h"
+
+/* -------------------------------------------------
+ * Compile time constants.
+ * -------------------------------------------------
+ */
+/*
+ * Get current command.
+ */
+#define GET_CMD() ((*s_file_ops->get_current_cmd)())
+/*
+ * Check if we're currently collecting data.
+ */
+#define IS_COLLECTING()                                                        \
+	({                                                                     \
+		sw_driver_collection_cmd_t __cmd = GET_CMD();                  \
+		bool __val = (__cmd == SW_DRIVER_START_COLLECTION ||           \
+			      __cmd == SW_DRIVER_RESUME_COLLECTION);           \
+		__val;                                                         \
+	})
+/*
+ * Check if we're currently paused.
+ */
+#define IS_SLEEPING()                                                          \
+	({                                                                     \
+		sw_driver_collection_cmd_t __cmd = GET_CMD();                  \
+		bool __val = __cmd == SW_DRIVER_PAUSE_COLLECTION;              \
+		__val;                                                         \
+	})
+/* -------------------------------------------------
+ * Typedefs
+ * -------------------------------------------------
+ */
+typedef unsigned long sw_bits_t;
+
+/* -------------------------------------------------
+ *  Local function declarations.
+ * -------------------------------------------------
+ */
+static int sw_device_open_i(struct inode *inode, struct file *file);
+static int sw_device_release_i(struct inode *inode, struct file *file);
+static ssize_t sw_device_read_i(struct file *file, char __user *buffer,
+				size_t length, loff_t *offset);
+static long sw_device_unlocked_ioctl_i(struct file *filp,
+				       unsigned int ioctl_num,
+				       unsigned long ioctl_param);
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+static long sw_device_compat_ioctl_i(struct file *file, unsigned int ioctl_num,
+				     unsigned long ioctl_param);
+#endif
+
+/*
+ * File operations exported by the driver.
+ */
+static struct file_operations s_fops = {
+	.open = &sw_device_open_i,
+	.read = &sw_device_read_i,
+	.unlocked_ioctl = &sw_device_unlocked_ioctl_i,
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+	.compat_ioctl = &sw_device_compat_ioctl_i,
+#endif // COMPAT && x64
+	.release = &sw_device_release_i,
+};
+/*
+ * Character device file MAJOR
+ * number -- we're now obtaining
+ * this dynamically.
+ */
+static int apwr_dev_major_num = -1;
+/*
+ * Variables to create the character device file
+ */
+static dev_t apwr_dev;
+static struct cdev *apwr_cdev;
+static struct class *apwr_class;
+/*
+ * Operations exported by the main driver.
+ */
+static struct sw_file_ops *s_file_ops;
+/*
+ * Is the device open right now? Used to prevent
+ * concurent access into the same device.
+ */
+#define DEV_IS_OPEN 0 // see if device is in use
+static volatile sw_bits_t dev_status;
+
+/*
+ * File operations.
+ */
+/*
+ * Service an "open(...)" call from user-space.
+ */
+static int sw_device_open_i(struct inode *inode, struct file *file)
+{
+	/*
+	 * We don't want to talk to two processes at the same time
+	 */
+	if (test_and_set_bit(DEV_IS_OPEN, &dev_status)) {
+		// Device is busy
+		return -EBUSY;
+	}
+
+	if (!try_module_get(THIS_MODULE)) {
+		// No such device
+		return -ENODEV;
+	}
+	pw_pr_debug("OK, allowed client open!\n");
+	return PW_SUCCESS;
+}
+
+/*
+ * Service a "close(...)" call from user-space.
+ */
+static int sw_device_release_i(struct inode *inode, struct file *file)
+{
+	/*
+	 * Did the client just try to zombie us?
+	 */
+	int retVal = PW_SUCCESS;
+
+	if (IS_COLLECTING()) {
+		pw_pr_error(
+			"ERROR: Detected ongoing collection on a device release!\n");
+		retVal = (*s_file_ops->stop_handler)();
+	}
+	module_put(THIS_MODULE);
+	/*
+	 * We're now ready for our next caller
+	 */
+	clear_bit(DEV_IS_OPEN, &dev_status);
+	return retVal;
+}
+
+static ssize_t sw_device_read_i(struct file *file, char __user *user_buffer,
+				size_t length, loff_t *offset)
+{
+	size_t bytes_read = 0;
+	u32 val = 0;
+
+	if (!user_buffer) {
+		pw_pr_error(
+			"ERROR: \"read\" called with an empty user_buffer?!\n");
+		return -PW_ERROR;
+	}
+	do {
+		val = SW_ALL_WRITES_DONE_MASK;
+		if (wait_event_interruptible(
+			    sw_reader_queue,
+			    (sw_any_seg_full(&val,
+					     (*s_file_ops->should_flush)()) ||
+			     (!IS_COLLECTING() && !IS_SLEEPING())))) {
+			pw_pr_error("wait_event_interruptible error\n");
+			return -ERESTARTSYS;
+		}
+		pw_pr_debug(KERN_INFO "After wait: val = %u\n", val);
+	} while (val == SW_NO_DATA_AVAIL_MASK);
+	/*
+	 * Are we done producing/consuming?
+	 */
+	if (val == SW_ALL_WRITES_DONE_MASK) {
+		return 0; // "0" ==> EOF
+	}
+	/*
+	 * Copy the buffer contents into userspace.
+	 */
+	bytes_read = sw_consume_data(
+		val, user_buffer,
+		length); // 'read' returns # of bytes actually read
+	if (unlikely(bytes_read == 0)) {
+		/* Cannot be EOF since that has already been checked above */
+		return -EIO;
+	}
+	return bytes_read;
+}
+
+/*
+ * (1) Handle 32b IOCTLs in 32b kernel-space.
+ * (2) Handle 64b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_unlocked_ioctl_i(struct file *filp,
+				       unsigned int ioctl_num,
+				       unsigned long ioctl_param)
+{
+	struct sw_driver_ioctl_arg __user *remote_args =
+		(struct sw_driver_ioctl_arg __user *)ioctl_param;
+	struct sw_driver_ioctl_arg local_args;
+
+	if (copy_from_user(&local_args, remote_args, sizeof(local_args))) {
+		pw_pr_error("ERROR copying ioctl args from userspace\n");
+		return -PW_ERROR;
+	}
+	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+};
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+/*
+ * Helper struct for use in translating
+ * IOCTLs from 32b user programs in 64b
+ * kernels.
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg32 {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	compat_caddr_t in_arg;
+	compat_caddr_t out_arg;
+};
+#pragma pack(pop)
+
+/*
+ * Handle 32b IOCTLs in 64b kernel-space.
+ */
+static long sw_device_compat_ioctl_i(struct file *file, unsigned int ioctl_num,
+				     unsigned long ioctl_param)
+{
+	struct sw_driver_ioctl_arg32 __user *remote_args32 =
+		compat_ptr(ioctl_param);
+	struct sw_driver_ioctl_arg local_args;
+	u32 data;
+
+	if (get_user(local_args.in_len, &remote_args32->in_len)) {
+		return -PW_ERROR;
+	}
+	if (get_user(local_args.out_len, &remote_args32->out_len)) {
+		return -PW_ERROR;
+	}
+	if (get_user(data, &remote_args32->in_arg)) {
+		return -PW_ERROR;
+	}
+	local_args.in_arg = (char *)(unsigned long)data;
+	if (get_user(data, &remote_args32->out_arg)) {
+		return -PW_ERROR;
+	}
+	local_args.out_arg = (char *)(unsigned long)data;
+	return (*s_file_ops->ioctl_handler)(ioctl_num, &local_args);
+}
+#endif
+
+/*
+ * Device creation, deletion operations.
+ */
+int sw_register_dev(struct sw_file_ops *ops)
+{
+	int ret;
+	/*
+	 * Ensure we have valid handlers!
+	 */
+	if (!ops) {
+		pw_pr_error("NULL file ops?!\n");
+		return -PW_ERROR;
+	}
+
+	/*
+	 * Create the character device
+	 */
+	ret = alloc_chrdev_region(&apwr_dev, 0, 1, PW_DEVICE_NAME);
+	apwr_dev_major_num = MAJOR(apwr_dev);
+	apwr_class = class_create(THIS_MODULE, "apwr");
+	if (IS_ERR(apwr_class)) {
+		printk(KERN_ERR "Error registering apwr class\n");
+	}
+
+	device_create(apwr_class, NULL, apwr_dev, NULL, PW_DEVICE_NAME);
+	apwr_cdev = cdev_alloc();
+	if (apwr_cdev == NULL) {
+		printk("Error allocating character device\n");
+		return ret;
+	}
+	apwr_cdev->owner = THIS_MODULE;
+	apwr_cdev->ops = &s_fops;
+	if (cdev_add(apwr_cdev, apwr_dev, 1) < 0) {
+		printk("Error registering device driver\n");
+		return ret;
+	}
+	s_file_ops = ops;
+
+	return ret;
+}
+
+void sw_unregister_dev(void)
+{
+	/*
+	 * Remove the device
+	 */
+	unregister_chrdev(apwr_dev_major_num, PW_DEVICE_NAME);
+	device_destroy(apwr_class, apwr_dev);
+	class_destroy(apwr_class);
+	unregister_chrdev_region(apwr_dev, 1);
+	cdev_del(apwr_cdev);
+}
diff --git a/drivers/platform/x86/socwatch/sw_hardware_io.c b/drivers/platform/x86/socwatch/sw_hardware_io.c
new file mode 100644
index 000000000000..759288ac546e
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_hardware_io.c
@@ -0,0 +1,188 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include "sw_types.h"
+#include "sw_kernel_defines.h"
+#include "sw_ops_provider.h"
+#include "sw_mem.h"
+#include "sw_internal.h"
+#include "sw_hardware_io.h"
+
+struct sw_ops_node {
+	const struct sw_hw_ops *op;
+	int id;
+
+	SW_LIST_ENTRY(list, sw_ops_node);
+};
+
+static SW_DEFINE_LIST_HEAD(s_ops,
+sw_in			   sw_ops_node) = SW_LIST_HEAD_INITIALIZER(s_ops);
+
+static int s_op_idx = -1;
+
+/*
+ * Function definitions.
+ */
+int sw_get_hw_op_id(const struct sw_hw_ops *ops)
+{
+	if (ops && ops->name) {
+		struct sw_ops_node *node = NULL;
+
+		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list)
+		{
+			if (node->op->name &&
+			    !strcmp(node->op->name, ops->name)) {
+				return node->id;
+			}
+		}
+	}
+	return -1;
+}
+
+const struct sw_hw_ops *sw_get_hw_ops_for(int id)
+{
+	struct sw_ops_node *node = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list)
+	{
+		if (node->id == id) {
+			return node->op;
+		}
+	}
+	return NULL;
+}
+
+bool sw_is_valid_hw_op_id(int id)
+{
+	struct sw_ops_node *node = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list)
+	{
+		if (node->id == id) {
+			return true;
+		}
+	}
+	return false;
+}
+
+const char *sw_get_hw_op_abstract_name(const struct sw_hw_ops *op)
+{
+	if (op) {
+		return op->name;
+	}
+	return NULL;
+}
+
+int sw_for_each_hw_op(int (*func)(const struct sw_hw_ops *op, void *priv),
+		      void *priv, bool return_on_error) {
+	int retval = PW_SUCCESS;
+	struct sw_ops_node *node = NULL;
+
+	if (func) {
+		SW_LIST_FOR_EACH_ENTRY(node, &s_ops, list)
+		{
+			if ((*func)(node->op, priv)) {
+				retval = -EIO;
+				if (return_on_error) {
+					break;
+				}
+			}
+		}
+	}
+	return retval;
+}
+
+int sw_register_hw_op(const struct sw_hw_ops *op)
+{
+	struct sw_ops_node *node = NULL;
+
+	if (!op) {
+		pw_pr_error("NULL input node in \"sw_register_hw_op\"");
+		return -EIO;
+	}
+	node = sw_kmalloc(sizeof(struct sw_ops_node), GFP_KERNEL);
+	if (!node) {
+		pw_pr_error("sw_kmalloc error in \"sw_register_hw_op\"");
+		return -ENOMEM;
+	}
+	node->op = op;
+	node->id = ++s_op_idx;
+	SW_LIST_ENTRY_INIT(node, list);
+	SW_LIST_ADD(&s_ops, node, list);
+	return PW_SUCCESS;
+}
+
+int sw_register_hw_ops(void)
+{
+	return sw_register_ops_providers();
+}
+
+void sw_free_hw_ops(void)
+{
+	/*
+	 * Free all nodes.
+	 */
+	while (!SW_LIST_EMPTY(&s_ops)) {
+		struct sw_ops_node *node =
+			SW_LIST_GET_HEAD_ENTRY(&s_ops, sw_ops_node, list);
+		SW_LIST_UNLINK(node, list);
+		sw_kfree(node);
+	}
+	/*
+	 * Call our providers to deallocate resources.
+	 */
+	sw_free_ops_providers();
+}
diff --git a/drivers/platform/x86/socwatch/sw_internal.c b/drivers/platform/x86/socwatch/sw_internal.c
new file mode 100644
index 000000000000..a4a4dca9dc53
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_internal.c
@@ -0,0 +1,238 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_hardware_io.h"
+#include "sw_mem.h"
+#include "sw_kernel_defines.h"
+#include "sw_internal.h"
+
+bool sw_check_output_buffer_params(void __user *buffer, size_t bytes_to_read,
+				   size_t buff_size)
+{
+	if (!buffer) {
+		pw_pr_error("ERROR: NULL ptr in sw_consume_data!\n");
+		return false;
+	}
+	if (bytes_to_read != buff_size) {
+		pw_pr_error("Error: bytes_to_read = %zu, required to be %zu\n",
+			    bytes_to_read, buff_size);
+		return false;
+	}
+	return true;
+}
+
+unsigned long sw_copy_to_user(char __user *dst, char *src, size_t bytes_to_copy)
+{
+	return copy_to_user(dst, src, bytes_to_copy);
+}
+
+void sw_schedule_work(const struct cpumask *mask, void (*work)(void *),
+		      void *data)
+{
+	/*
+	 * Did the user ask us to run on 'ANY' CPU?
+	 */
+	if (cpumask_empty(mask)) {
+		(*work)(data); // Call on current CPU
+	} else {
+		preempt_disable();
+		{
+			/*
+			 * Did the user ask to run on this CPU?
+			 */
+			if (cpumask_test_cpu(RAW_CPU(), mask)) {
+				(*work)(data); // Call on current CPU
+			}
+			/*
+			 * OK, now check other CPUs.
+			 */
+			smp_call_function_many(
+				mask, work, data,
+				true /* Wait for all funcs to complete */);
+		}
+		preempt_enable();
+	}
+}
+
+int sw_get_cpu(unsigned long *flags)
+{
+	local_irq_save(*flags);
+	return get_cpu();
+}
+
+void sw_put_cpu(unsigned long flags)
+{
+	put_cpu();
+	local_irq_restore(flags);
+}
+
+#ifndef CONFIG_NR_CPUS_PER_MODULE
+#define CONFIG_NR_CPUS_PER_MODULE 2
+#endif // CONFIG_NR_CPUS_PER_MODULE
+
+static void sw_get_cpu_sibling_mask(int cpu, struct cpumask *sibling_mask)
+{
+	unsigned int base =
+		(cpu / CONFIG_NR_CPUS_PER_MODULE) * CONFIG_NR_CPUS_PER_MODULE;
+	unsigned int i;
+
+	cpumask_clear(sibling_mask);
+	for (i = base; i < (base + CONFIG_NR_CPUS_PER_MODULE); ++i) {
+		cpumask_set_cpu(i, sibling_mask);
+	}
+}
+
+struct pw_cpufreq_node {
+	int cpu;
+	struct cpumask cpus, related_cpus;
+	unsigned int shared_type;
+	struct list_head list;
+};
+static struct list_head pw_cpufreq_policy_lists;
+
+int sw_set_module_scope_for_cpus(void)
+{
+	/*
+	 * Warning: no support for cpu hotplugging!
+	 */
+	int cpu = 0;
+
+	INIT_LIST_HEAD(&pw_cpufreq_policy_lists);
+
+	for_each_online_cpu(cpu) {
+		struct cpumask sibling_mask;
+		struct pw_cpufreq_node *node = NULL;
+		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+
+		if (!policy) {
+			continue;
+		}
+		/*
+		 * Get siblings for this cpu.
+		 */
+		sw_get_cpu_sibling_mask(cpu, &sibling_mask);
+		/*
+		 * Check if affected_cpus already contains sibling_mask
+		 */
+		if (cpumask_subset(&sibling_mask, policy->cpus)) {
+			/*
+			 * 'sibling_mask' is already a subset of affected_cpus -- nothing
+			 * to do on this CPU.
+			 */
+			cpufreq_cpu_put(policy);
+			continue;
+		}
+
+		node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+		if (node) {
+			cpumask_clear(&node->cpus);
+			cpumask_clear(&node->related_cpus);
+
+			node->cpu = cpu;
+			cpumask_copy(&node->cpus, policy->cpus);
+			cpumask_copy(&node->related_cpus, policy->related_cpus);
+			node->shared_type = policy->shared_type;
+		}
+
+		policy->shared_type = CPUFREQ_SHARED_TYPE_ALL;
+		/*
+		 * Set siblings. Don't worry about online/offline, that's
+		 * handled below.
+		 */
+		cpumask_copy(policy->cpus, &sibling_mask);
+		/*
+		 * Ensure 'related_cpus' is a superset of 'cpus'
+		 */
+		cpumask_or(policy->related_cpus, policy->related_cpus,
+			   policy->cpus);
+		/*
+		 * Ensure 'cpus' only contains online cpus.
+		 */
+		cpumask_and(policy->cpus, policy->cpus, cpu_online_mask);
+
+		cpufreq_cpu_put(policy);
+
+		if (node) {
+			INIT_LIST_HEAD(&node->list);
+			list_add_tail(&node->list, &pw_cpufreq_policy_lists);
+		}
+	}
+	return PW_SUCCESS;
+}
+
+int sw_reset_module_scope_for_cpus(void)
+{
+	struct list_head *head = &pw_cpufreq_policy_lists;
+
+	while (!list_empty(head)) {
+		struct pw_cpufreq_node *node =
+			list_first_entry(head, struct pw_cpufreq_node, list);
+		int cpu = node->cpu;
+		struct cpufreq_policy *policy = cpufreq_cpu_get(cpu);
+		if (!policy) {
+			continue;
+		}
+		policy->shared_type = node->shared_type;
+		cpumask_copy(policy->related_cpus, &node->related_cpus);
+		cpumask_copy(policy->cpus, &node->cpus);
+
+		cpufreq_cpu_put(policy);
+
+		pw_pr_debug("OK, reset cpufreq_policy for cpu %d\n", cpu);
+		list_del(&node->list);
+		sw_kfree(node);
+	}
+	return PW_SUCCESS;
+}
diff --git a/drivers/platform/x86/socwatch/sw_mem.c b/drivers/platform/x86/socwatch/sw_mem.c
new file mode 100644
index 000000000000..0d1231c2e3a8
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_mem.c
@@ -0,0 +1,331 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/slab.h>
+
+#include "sw_kernel_defines.h"
+#include "sw_lock_defs.h"
+#include "sw_mem.h"
+
+/*
+ * How do we behave if we ever
+ * get an allocation error?
+ * (a) Setting to '1' REFUSES ANY FURTHER
+ * allocation requests.
+ * (b) Setting to '0' treats each
+ * allocation request as separate, and
+ * handles them on an on-demand basis
+ */
+#define DO_MEM_PANIC_ON_ALLOC_ERROR 0
+
+#if DO_MEM_PANIC_ON_ALLOC_ERROR
+/*
+ * If we ever run into memory allocation errors then
+ * stop (and drop) everything.
+ */
+static atomic_t pw_mem_should_panic = ATOMIC_INIT(0);
+/*
+ * Macro to check if PANIC is on.
+ */
+#define MEM_PANIC()                                                            \
+	do {                                                                   \
+		atomic_set(&pw_mem_should_panic, 1);                           \
+		smp_mb();                                                      \
+	} while (0)
+#define SHOULD_TRACE()                                                         \
+	({                                                                     \
+		bool __tmp = false;                                            \
+		smp_mb();                                                      \
+		__tmp = (atomic_read(&pw_mem_should_panic) == 0);              \
+		__tmp;                                                         \
+	})
+
+#else // if !DO_MEM_PANIC_ON_ALLOC_ERROR
+
+#define MEM_PANIC()
+#define SHOULD_TRACE() (true)
+
+#endif
+
+/*
+ * Variables to track memory usage.
+ */
+/*
+ * TOTAL num bytes allocated.
+ */
+static u64 total_num_bytes_alloced;
+/*
+ * Num of allocated bytes that have
+ * not yet been freed.
+ */
+static u64 curr_num_bytes_alloced;
+/*
+ * Max # of allocated bytes that
+ * have not been freed at any point
+ * in time.
+ */
+static u64 max_num_bytes_alloced;
+
+u64 sw_get_total_bytes_alloced(void)
+{
+	return total_num_bytes_alloced;
+};
+
+u64 sw_get_max_bytes_alloced(void)
+{
+	return max_num_bytes_alloced;
+};
+
+u64 sw_get_curr_bytes_alloced(void)
+{
+	return curr_num_bytes_alloced;
+};
+
+/*
+ * Allocate free pages.
+ * TODO: add memory tracker?
+ */
+unsigned long sw_allocate_pages(gfp_t flags,
+				unsigned int alloc_size_in_bytes)
+{
+	return __get_free_pages(flags, get_order(alloc_size_in_bytes));
+}
+/*
+ * Free up previously allocated pages.
+ * TODO: add memory tracker?
+ */
+void sw_release_pages(unsigned long addr, unsigned int alloc_size_in_bytes)
+{
+	free_pages(addr, get_order(alloc_size_in_bytes));
+}
+
+#if DO_TRACK_MEMORY_USAGE
+
+/*
+ * Lock to guard access to memory
+ * debugging stats.
+ */
+static SW_DEFINE_SPINLOCK(sw_kmalloc_lock);
+
+/*
+ * Helper macros to print out
+ * mem debugging stats.
+ */
+#define TOTAL_NUM_BYTES_ALLOCED() total_num_bytes_alloced
+#define CURR_NUM_BYTES_ALLOCED() curr_num_bytes_alloced
+#define MAX_NUM_BYTES_ALLOCED() max_num_bytes_alloced
+
+/*
+ * MAGIC number based memory tracker. Relies on
+ * storing (a) a MAGIC marker and (b) the requested
+ * size WITHIN the allocated block of memory. Standard
+ * malloc-tracking stuff, really.
+ *
+ * Overview:
+ * (1) ALLOCATION:
+ * When asked to allocate a block of 'X' bytes, allocate
+ * 'X' + 8 bytes. Then, in the FIRST 4 bytes, write the
+ * requested size. In the NEXT 4 bytes, write a special
+ * (i.e. MAGIC) number to let our deallocator know that
+ * this block of memory was allocated using this technique.
+ * Also, keep track of the number of bytes allocated.
+ *
+ * (2) DEALLOCATION:
+ * When given an object to deallocate, we first check
+ * the MAGIC number by decrementing the pointer by
+ * 4 bytes and reading the (integer) stored there.
+ * After ensuring the pointer was, in fact, allocated
+ * by us, we then read the size of the allocated
+ * block (again, by decrementing the pointer by 4
+ * bytes and reading the integer size). We
+ * use this size argument to decrement # of bytes
+ * allocated.
+ */
+#define PW_MEM_MAGIC 0xdeadbeef
+
+#define PW_ADD_MAGIC(x)                                                        \
+	({                                                                     \
+		char *__tmp1 = (char *)(x);                                    \
+		*((int *)__tmp1) = PW_MEM_MAGIC;                               \
+		__tmp1 += sizeof(int);                                         \
+		__tmp1;                                                        \
+	})
+#define PW_ADD_SIZE(x, s)                                                      \
+	({                                                                     \
+		char *__tmp1 = (char *)(x);                                    \
+		*((int *)__tmp1) = (s);                                        \
+		__tmp1 += sizeof(int);                                         \
+		__tmp1;                                                        \
+	})
+#define PW_ADD_STAMP(x, s) PW_ADD_MAGIC(PW_ADD_SIZE((x), (s)))
+
+#define PW_IS_MAGIC(x)                                                         \
+	({                                                                     \
+		int *__tmp1 = (int *)((char *)(x) - sizeof(int));              \
+		*__tmp1 == PW_MEM_MAGIC;                                       \
+	})
+#define PW_REMOVE_STAMP(x)                                                     \
+	({                                                                     \
+		char *__tmp1 = (char *)(x);                                    \
+		__tmp1 -= sizeof(int) * 2;                                     \
+		__tmp1;                                                        \
+	})
+#define PW_GET_SIZE(x) (*((int *)(x)))
+
+void *sw_kmalloc(size_t size, gfp_t flags)
+{
+	size_t act_size = 0;
+	void *retVal = NULL;
+	/*
+	 * No point in allocating if
+	 * we were unable to allocate
+	 * previously!
+	 */
+	{
+		if (!SHOULD_TRACE()) {
+			return NULL;
+		}
+	}
+	/*
+	 * (1) Allocate requested block.
+	 */
+	act_size = size + sizeof(int) * 2;
+	retVal = kmalloc(act_size, flags);
+	if (!retVal) {
+		/*
+		 * Panic if we couldn't allocate
+		 * requested memory.
+		 */
+		printk(KERN_INFO "ERROR: could NOT allocate memory!\n");
+		MEM_PANIC();
+		return NULL;
+	}
+	/*
+	 * (2) Update memory usage stats.
+	 */
+	LOCK(sw_kmalloc_lock);
+	{
+		total_num_bytes_alloced += size;
+		curr_num_bytes_alloced += size;
+		if (curr_num_bytes_alloced > max_num_bytes_alloced)
+			max_num_bytes_alloced = curr_num_bytes_alloced;
+	}
+	UNLOCK(sw_kmalloc_lock);
+	/*
+	 * (3) And finally, add the 'size'
+	 * and 'magic' stamps.
+	 */
+	return PW_ADD_STAMP(retVal, size);
+};
+
+void sw_kfree(const void *obj)
+{
+	void *tmp = NULL;
+	size_t size = 0;
+
+	/*
+	 * (1) Check if this block was allocated
+	 * by us.
+	 */
+	if (!PW_IS_MAGIC(obj)) {
+		printk(KERN_INFO "ERROR: %p is NOT a PW_MAGIC ptr!\n", obj);
+		return;
+	}
+	/*
+	 * (2) Strip the magic num...
+	 */
+	tmp = PW_REMOVE_STAMP(obj);
+	/*
+	 * ...and retrieve size of block.
+	 */
+	size = PW_GET_SIZE(tmp);
+	/*
+	 * (3) Update memory usage stats.
+	 */
+	LOCK(sw_kmalloc_lock);
+	{
+		curr_num_bytes_alloced -= size;
+	}
+	UNLOCK(sw_kmalloc_lock);
+	/*
+	 * And finally, free the block.
+	 */
+	kfree(tmp);
+};
+
+#else // !DO_TRACK_MEMORY_USAGE
+
+void *sw_kmalloc(size_t size, gfp_t flags)
+{
+	void *ret = NULL;
+
+	if (SHOULD_TRACE()) {
+		if (!(ret = kmalloc(size, flags))) {
+			/*
+			 * Panic if we couldn't allocate
+			 * requested memory.
+			 */
+			MEM_PANIC();
+		}
+	}
+	return ret;
+};
+
+void sw_kfree(const void *mem)
+{
+	kfree(mem);
+};
+
+#endif // DO_TRACK_MEMORY_USAGE
diff --git a/drivers/platform/x86/socwatch/sw_ops_provider.c b/drivers/platform/x86/socwatch/sw_ops_provider.c
new file mode 100644
index 000000000000..1eb60d12b701
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_ops_provider.c
@@ -0,0 +1,1225 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/pci.h> // "pci_get_domain_bus_and_slot"
+#include <linux/delay.h> // "udelay"
+#include <asm/msr.h>
+#ifdef CONFIG_RPMSG_IPC
+#include <asm/intel_mid_rpmsg.h>
+#endif // CONFIG_RPMSG_IPC
+
+#include "sw_types.h"
+#include "sw_kernel_defines.h"
+#include "sw_hardware_io.h"
+#include "sw_telem.h"
+#include "sw_ops_provider.h"
+
+/*
+ * Compile time constants.
+ */
+/*
+ * Should we be doing 'direct' PCI reads and writes?
+ * '1' ==> YES, call "pci_{read,write}_config_dword()" directly
+ * '0' ==> NO, Use the "intel_mid_msgbus_{read32,write32}_raw()" API (defined in 'intel_mid_pcihelpers.c')
+ */
+#define DO_DIRECT_PCI_READ_WRITE 0
+#if !IS_ENABLED(CONFIG_ANDROID) || !defined(CONFIG_X86_WANT_INTEL_MID)
+/*
+ * 'intel_mid_pcihelpers.h' is probably not present -- force
+ * direct PCI calls in this case.
+ */
+#undef DO_DIRECT_PCI_READ_WRITE
+#define DO_DIRECT_PCI_READ_WRITE 1
+#endif
+#if !DO_DIRECT_PCI_READ_WRITE
+#include <asm/intel_mid_pcihelpers.h>
+#endif
+
+#define SW_PCI_MSG_CTRL_REG 0x000000D0
+#define SW_PCI_MSG_DATA_REG 0x000000D4
+
+/*
+ *  NUM_RETRY & USEC_DELAY are used in PCH Mailbox (sw_read_pch_mailbox_info_i).
+ *  Tested on KBL + SPT-LP. May need to revisit.
+ */
+#define NUM_RETRY 100
+#define USEC_DELAY 100
+
+#define EXTCNF_CTRL 0xF00 // offset for hw semaphore.
+#define FWSM_CTRL 0x5B54 // offset for fw semaphore
+#define GBE_CTRL_OFFSET 0x34 // GBE LPM offset
+
+#define IS_HW_SEMAPHORE_SET(data) (data & (pw_u64_t)(0x1 << 6))
+#define IS_FW_SEMAPHORE_SET(data) (data & (pw_u64_t)0x1)
+/*
+ * Number of retries for mailbox configuration
+ */
+#define MAX_MAILBOX_ITERS 100
+
+/*
+ * Local data structures.
+ */
+/*
+ * TODO: separate into H/W and S/W IO?
+ */
+typedef enum sw_io_type {
+	SW_IO_MSR = 0,
+	SW_IO_IPC = 1,
+	SW_IO_MMIO = 2,
+	SW_IO_PCI = 3,
+	SW_IO_CONFIGDB = 4,
+	SW_IO_TRACE_ARGS = 5,
+	SW_IO_WAKEUP = 6,
+	SW_IO_SOCPERF = 7,
+	SW_IO_PROC_NAME = 8,
+	SW_IO_IRQ_NAME = 9,
+	SW_IO_WAKELOCK = 10,
+	SW_IO_TELEM = 11,
+	SW_IO_PCH_MAILBOX = 12,
+	SW_IO_MAILBOX = 13,
+	SW_IO_MAX = 14,
+} sw_io_type_t;
+
+/*
+ * "io_remapped" values for HW and FW semaphores
+ */
+static struct {
+	volatile void __iomem *hw_semaphore;
+	volatile void __iomem *fw_semaphore;
+} s_gbe_semaphore = { NULL, NULL };
+
+/*
+ * Function declarations.
+ */
+/*
+ * Exported by the SOCPERF driver.
+ */
+extern void SOCPERF_Read_Data2(void *data_buffer);
+
+/*
+ * Init functions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Read functions.
+ */
+void sw_read_msr_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+void sw_read_ipc_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+void sw_read_mmio_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes);
+void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+				u16 counter_size_in_bytes);
+void sw_read_mailbox_info_i(char *dst_vals, int cpu,
+			    const struct sw_driver_io_descriptor *descriptor,
+			    u16 counter_size_in_bytes);
+void sw_read_pci_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+			u16 counter_size_in_bytes);
+void sw_read_configdb_info_i(char *dst_vals, int cpu,
+			     const struct sw_driver_io_descriptor *descriptor,
+			     u16 counter_size_in_bytes);
+void sw_read_socperf_info_i(char *dst_vals, int cpu,
+			    const struct sw_driver_io_descriptor *descriptor,
+			    u16 counter_size_in_bytes);
+
+/*
+ * Write functions.
+ */
+void sw_write_msr_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes);
+void sw_write_ipc_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes);
+void sw_write_mmio_info_i(char *dst_vals, int cpu,
+			  const struct sw_driver_io_descriptor *descriptor,
+			  u16 counter_size_in_bytes);
+void sw_write_mailbox_info_i(char *dst_vals, int cpu,
+			     const struct sw_driver_io_descriptor *descriptor,
+			     u16 counter_size_in_bytes);
+void sw_write_pci_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes);
+void sw_write_configdb_info_i(char *dst_vals, int cpu,
+			      const struct sw_driver_io_descriptor *descriptor,
+			      u16 counter_size_in_bytes);
+void sw_write_trace_args_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptor,
+				u16 counter_size_in_bytes);
+void sw_write_wakeup_info_i(char *dst_vals, int cpu,
+			    const struct sw_driver_io_descriptor *descriptor,
+			    u16 counter_size_in_bytes);
+void sw_write_socperf_info_i(char *dst_vals, int cpu,
+			     const struct sw_driver_io_descriptor *descriptor,
+			     u16 counter_size_in_bytes);
+
+/*
+ * Print functions.
+ */
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor
+			       *descriptor);
+
+/*
+ * Reset functions -- equal but opposite of init.
+ */
+int sw_ipc_mmio_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor);
+int sw_pch_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor);
+int sw_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor);
+
+/*
+ * Available functions.
+ */
+bool sw_socperf_available_i(void);
+
+/*
+ * Helper functions.
+ */
+u32 sw_platform_configdb_read32(u32 address);
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 ctrl_offset,
+			   u32 ctrl_value, u32 data_offset);
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 ctrl_offset,
+			   u32 ctrl_value, u32 data_offset);
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function,
+			     u32 write_offset, u32 data_value);
+
+/*
+ * Table of collector operations.
+ */
+static const struct sw_hw_ops s_hw_ops[] = {
+	[SW_IO_MSR] = { .name = "MSR",
+			.init = NULL,
+			.read = &sw_read_msr_info_i,
+			.write = &sw_write_msr_info_i,
+			.print = &sw_print_msr_io_descriptor,
+			.reset = NULL,
+			.available = NULL },
+	[SW_IO_IPC] = {
+			.name = "IPC",
+			.init = &sw_ipc_mmio_descriptor_init_func_i,
+			.read = &sw_read_ipc_info_i,
+			.reset = &sw_ipc_mmio_descriptor_reset_func_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_MMIO] = {
+			.name = "MMIO",
+			.init = &sw_ipc_mmio_descriptor_init_func_i,
+			.read = &sw_read_mmio_info_i,
+			.write = &sw_write_mmio_info_i,
+			.reset = &sw_ipc_mmio_descriptor_reset_func_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_PCI] = {
+			.name = "PCI",
+			.read = &sw_read_pci_info_i,
+			.write = &sw_write_pci_info_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_CONFIGDB] = {
+			.name = "CONFIGDB",
+			.read = &sw_read_configdb_info_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_WAKEUP] = {
+			.name = "WAKEUP",
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_SOCPERF] = {
+			.name = "SOCPERF",
+			.read = &sw_read_socperf_info_i,
+			.available = &sw_socperf_available_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_PROC_NAME] = {
+			.name = "PROC-NAME",
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_IRQ_NAME] = {
+			.name = "IRQ-NAME",
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_WAKELOCK] = {
+			.name = "WAKELOCK",
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_TELEM] = {
+			.name = "TELEM",
+			.init = &sw_telem_init_func,
+			.read = &sw_read_telem_info,
+			.reset = &sw_reset_telem,
+			.available = &sw_telem_available,
+			.post_config = &sw_telem_post_config,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_PCH_MAILBOX] = {
+			.name = "PCH-MAILBOX",
+			.init = &sw_pch_mailbox_descriptor_init_func_i,
+			.read = &sw_read_pch_mailbox_info_i,
+			.reset = &sw_pch_mailbox_descriptor_reset_func_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_MAILBOX] = {
+			.name = "MAILBOX",
+			.init = &sw_mailbox_descriptor_init_func_i,
+			.read = &sw_read_mailbox_info_i,
+			.write = &sw_write_mailbox_info_i,
+			.reset = &sw_mailbox_descriptor_reset_func_i,
+			/* Other fields are don't care (will be set to NULL) */
+		},
+	[SW_IO_MAX] = {
+			.name = NULL,
+			/* Other fields are don't care (will be set to NULL) */
+		}
+};
+
+/*
+ * Function definitions.
+ */
+int sw_ipc_mmio_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor)
+{
+	// Perform any required 'io_remap' calls here
+	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+	u64 data_address = 0;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	if (descriptor->collection_type == SW_IO_IPC) {
+		__ipc_mmio = &descriptor->ipc_descriptor;
+	} else {
+		__ipc_mmio = &descriptor->mmio_descriptor;
+	}
+	pw_pr_debug("cmd = %u, sub-cmd = %u, data_addr = 0x%llx\n",
+		    __ipc_mmio->command, __ipc_mmio->sub_command,
+		    __ipc_mmio->data_address);
+	data_address = __ipc_mmio->data_address;
+	/*
+	  if (__ipc_mmio->command || __ipc_mmio->sub_command) {
+	  __ipc_mmio->ipc_command =
+	  ((pw_u32_t)__ipc_mmio->sub_command << 12)
+	  | (pw_u32_t)__ipc_mmio->command;
+	  }
+	*/
+	if (data_address) {
+		__ipc_mmio->data_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)data_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)__ipc_mmio->data_remapped_address ==
+		    NULL) {
+			return -EIO;
+		}
+		pw_pr_debug("mapped addr 0x%llx\n",
+			    __ipc_mmio->data_remapped_address);
+		if (__ipc_mmio->is_gbe) {
+			if (!s_gbe_semaphore.hw_semaphore ||
+			    !s_gbe_semaphore.fw_semaphore) {
+				pw_pr_debug("Initializing GBE semaphore\n");
+				if (data_address >= GBE_CTRL_OFFSET) {
+					u64 hw_addr = (data_address -
+						       GBE_CTRL_OFFSET) +
+						      EXTCNF_CTRL;
+					u64 fw_addr = (data_address -
+						       GBE_CTRL_OFFSET) +
+						      FWSM_CTRL;
+					s_gbe_semaphore.hw_semaphore =
+						ioremap_nocache(
+							(unsigned long)hw_addr,
+							descriptor
+								->counter_size_in_bytes);
+					s_gbe_semaphore.fw_semaphore =
+						ioremap_nocache(
+							(unsigned long)fw_addr,
+							descriptor
+								->counter_size_in_bytes);
+					if (s_gbe_semaphore.hw_semaphore ==
+						    NULL ||
+					    s_gbe_semaphore.fw_semaphore ==
+						    NULL) {
+						pw_pr_error(
+							"couldn't mmap hw/fw semaphores for GBE MMIO op!\n");
+						return -EIO;
+					}
+					pw_pr_debug(
+						"GBE has hw_sem = 0x%llx, fw_sem = 0x%llx, size = %u\n",
+						(unsigned long long)
+							s_gbe_semaphore
+								.hw_semaphore,
+						(unsigned long long)
+							s_gbe_semaphore
+								.fw_semaphore,
+						descriptor
+							->counter_size_in_bytes);
+				}
+			}
+		}
+	}
+	return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_init_func_i(
+	struct sw_driver_io_descriptor *descriptor)
+{
+	// Perform any required 'io_remap' calls here
+	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	__pch_mailbox = &descriptor->pch_mailbox_descriptor;
+	pw_pr_debug("pch_mailbox data_addr = 0x%llx\n",
+		    (unsigned long long)__pch_mailbox->data_address);
+	if (__pch_mailbox->mtpmc_address) {
+		__pch_mailbox->mtpmc_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)__pch_mailbox->mtpmc_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)
+			    __pch_mailbox->mtpmc_remapped_address == NULL) {
+			return -PW_ERROR;
+		}
+		pw_pr_debug("mtpmc_mapped addr 0x%llx\n",
+			    __pch_mailbox->mtpmc_remapped_address);
+	}
+	if (__pch_mailbox->msg_full_sts_address) {
+		__pch_mailbox->msg_full_sts_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)
+					__pch_mailbox->msg_full_sts_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)__pch_mailbox
+			    ->msg_full_sts_remapped_address == NULL) {
+			return -PW_ERROR;
+		}
+		pw_pr_debug("msg_full_sts_mapped addr 0x%llx\n",
+			    __pch_mailbox->msg_full_sts_address);
+	}
+	if (__pch_mailbox->mfpmc_address) {
+		__pch_mailbox->mfpmc_remapped_address =
+			(pw_u64_t)(unsigned long)ioremap_nocache(
+				(unsigned long)__pch_mailbox->mfpmc_address,
+				descriptor->counter_size_in_bytes);
+		if ((void *)(unsigned long)
+			    __pch_mailbox->mfpmc_remapped_address == NULL) {
+			return -PW_ERROR;
+		}
+		pw_pr_debug("mfpmc_mapped addr 0x%llx\n",
+			    __pch_mailbox->mfpmc_remapped_address);
+	}
+	return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_init_func_i(struct sw_driver_io_descriptor *descriptor)
+{
+	// Perform any required 'io_remap' calls here
+	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	__mailbox = &descriptor->mailbox_descriptor;
+
+	pw_pr_debug(
+		"type = %u, interface_address = 0x%llx, data_address = 0x%llx\n",
+		__mailbox->is_msr_type, __mailbox->interface_address,
+		__mailbox->data_address);
+
+	if (!__mailbox->is_msr_type) {
+		if (__mailbox->interface_address) {
+			__mailbox->interface_remapped_address =
+				(pw_u64_t)(unsigned long)ioremap_nocache(
+					(unsigned long)
+						__mailbox->interface_address,
+					descriptor->counter_size_in_bytes);
+			if ((void *)(unsigned long)__mailbox
+				    ->interface_remapped_address == NULL) {
+				pw_pr_error(
+					"Couldn't iomap interface_address = 0x%llx\n",
+					__mailbox->interface_address);
+				return -PW_ERROR;
+			}
+		}
+		if (__mailbox->data_address) {
+			__mailbox->data_remapped_address =
+				(pw_u64_t)(unsigned long)ioremap_nocache(
+					(unsigned long)__mailbox->data_address,
+					descriptor->counter_size_in_bytes);
+			if ((void *)(unsigned long)
+				    __mailbox->data_remapped_address == NULL) {
+				pw_pr_error(
+					"Couldn't iomap data_address = 0x%llx\n",
+					__mailbox->data_address);
+				return -PW_ERROR;
+			}
+		}
+		pw_pr_debug("OK, mapped addr 0x%llx, 0x%llx\n",
+			    __mailbox->interface_remapped_address,
+			    __mailbox->data_remapped_address);
+	}
+	return PW_SUCCESS;
+}
+
+void sw_read_msr_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptors,
+			u16 counter_size_in_bytes)
+{
+	u64 address = descriptors->msr_descriptor.address;
+	u32 l = 0, h = 0;
+
+	if (likely(cpu == RAW_CPU())) {
+		if (rdmsr_safe((unsigned long)address, &l, &h)) {
+			pw_pr_warn("Failed to read MSR address = 0x%llx\n", address);
+		}
+	} else {
+		if (rdmsr_safe_on_cpu(cpu, (unsigned long)address, &l, &h)) {
+			pw_pr_warn("Failed to read MSR address = 0x%llx\n", address);
+		}
+	}
+	switch (counter_size_in_bytes) {
+	case 4:
+		*((u32 *)dst_vals) = l;
+		break;
+	case 8:
+		*((u64 *)dst_vals) = ((u64)h << 32) | l;
+		break;
+	default:
+		break;
+	}
+	return;
+}
+
+#ifdef CONFIG_RPMSG_IPC
+#define SW_DO_IPC(cmd, sub_cmd) rpmsg_send_generic_simple_command(cmd, sub_cmd)
+#else
+#define SW_DO_IPC(cmd, sub_cmd) (-ENODEV)
+#endif // CONFIG_RPMSG_IPC
+
+void sw_read_ipc_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptors,
+			u16 counter_size_in_bytes)
+{
+	u16 cmd = descriptors->ipc_descriptor.command,
+	    sub_cmd = descriptors->ipc_descriptor.sub_command;
+	unsigned long remapped_address =
+		(unsigned long)descriptors->ipc_descriptor.data_remapped_address;
+
+	if (cmd || sub_cmd) {
+		pw_pr_debug("EXECUTING IPC Cmd = %u, %u\n", cmd, sub_cmd);
+		if (SW_DO_IPC(cmd, sub_cmd)) {
+			pw_pr_error("ERROR running IPC command(s)\n");
+			return;
+		}
+	}
+
+	if (remapped_address) {
+		// memcpy(&value, (void *)remapped_address, counter_size_in_bytes);
+		pw_pr_debug("COPYING MMIO size %u\n", counter_size_in_bytes);
+		memcpy(dst_vals, (void *)remapped_address,
+		       counter_size_in_bytes);
+	}
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+static void
+sw_read_gbe_mmio_info_i(char *dst_vals,
+			const struct sw_driver_io_descriptor *descriptors,
+			u16 counter_size_in_bytes)
+{
+	u32 hw_val = 0, fw_val = 0;
+	unsigned long remapped_address =
+		(unsigned long)
+			descriptors->mmio_descriptor.data_remapped_address;
+	u64 write_value = descriptors->write_value;
+
+	memset(dst_vals, 0, counter_size_in_bytes);
+
+	pw_pr_debug(
+		"hw_sem = 0x%llx, fw_sem = 0x%llx, addr = 0x%lx, dst_vals = 0x%lx, size = %u\n",
+		(unsigned long long)s_gbe_semaphore.hw_semaphore,
+		(unsigned long long)s_gbe_semaphore.fw_semaphore,
+		remapped_address, (unsigned long)dst_vals,
+		counter_size_in_bytes);
+	if (!s_gbe_semaphore.hw_semaphore || !s_gbe_semaphore.fw_semaphore ||
+	    !remapped_address) {
+		return;
+	}
+
+	memcpy_fromio(&hw_val, s_gbe_semaphore.hw_semaphore, sizeof(hw_val));
+	memcpy_fromio(&fw_val, s_gbe_semaphore.fw_semaphore, sizeof(fw_val));
+	pw_pr_debug("HW_VAL = 0x%lx, FW_VAL = 0x%lx\n", (unsigned long)hw_val,
+		    (unsigned long)fw_val);
+	if (!IS_HW_SEMAPHORE_SET(hw_val) && !IS_FW_SEMAPHORE_SET(fw_val)) {
+		memcpy_toio((volatile void __iomem *)remapped_address,
+			    &write_value,
+			    4 /* counter_size_in_bytes*/);
+		memcpy_fromio(dst_vals,
+			      (volatile void __iomem *)remapped_address,
+			      counter_size_in_bytes);
+	}
+}
+void sw_read_mmio_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptors,
+			 u16 counter_size_in_bytes)
+{
+	unsigned long remapped_address =
+		(unsigned long)
+			descriptors->mmio_descriptor.data_remapped_address;
+	if (descriptors->mmio_descriptor.is_gbe) {
+		/* MMIO for GBE requires a mailbox-like operation */
+		sw_read_gbe_mmio_info_i(dst_vals, descriptors,
+					counter_size_in_bytes);
+	} else {
+		if (remapped_address) {
+			memcpy_fromio(dst_vals,
+				      (volatile void __iomem *)remapped_address,
+				      counter_size_in_bytes);
+		}
+	}
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+}
+
+void sw_read_pch_mailbox_info_i(char *dst_vals, int cpu,
+				const struct sw_driver_io_descriptor *descriptor,
+				u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_pch_mailbox_io_descriptor *pch_mailbox =
+		&descriptor->pch_mailbox_descriptor;
+	u32 address = pch_mailbox->data_address;
+	u64 mtpmc_remapped_address = pch_mailbox->mtpmc_remapped_address;
+	u64 msg_full_sts_remapped_address =
+		pch_mailbox->msg_full_sts_remapped_address;
+	u64 mfpmc_remapped_address = pch_mailbox->mfpmc_remapped_address;
+
+	/*
+	 * write address of desired device counter to request
+	 * from PMC (shift and add 2 to format device offset)
+	 */
+	if (mtpmc_remapped_address) {
+		int iter = 0;
+		u32 written_val = 0;
+		u32 write_value =
+			(address << 16) +
+			2; /* shift and add 2 to format device offset */
+		memcpy_toio(
+			(volatile void __iomem *)
+			(unsigned long)mtpmc_remapped_address,
+			&write_value, 4 /*counter_size_in_bytes*/);
+		/*
+		 * Check if address has been written using a while loop in
+		 * order to wait for the PMC to consume that address
+		 * and to introduce sufficient delay so that the message full
+		 * status bit has time to flip. This should ensure
+		 * all is ready when begin the wait loop for it to turn 0,
+		 * which indicates the value is available to be read.
+		 * (This fixes problem where values being read were huge.)
+		 */
+		do {
+			memcpy_fromio(&written_val,
+				      (volatile void __iomem *)(unsigned long)
+					      mtpmc_remapped_address,
+				      4 /*counter_size_in_bytes*/);
+			pw_pr_debug(
+				"DEBUG: written_val = 0x%x, address = 0x%x\n",
+				written_val, address);
+			udelay(USEC_DELAY);
+		} while ((written_val >> 16) != address && ++iter < NUM_RETRY);
+	}
+
+	/*
+	 * wait for PMC to set status indicating that device counter
+	 * is available for read.
+	 */
+	if (msg_full_sts_remapped_address) {
+		u32 status_wait = 0;
+		int iter = 0;
+
+		do {
+			memcpy_fromio(&status_wait,
+				      (volatile void __iomem *)(unsigned long)
+					      msg_full_sts_remapped_address,
+				      4 /*counter_size_in_bytes*/);
+			pw_pr_debug("DEBUG: status_wait = 0x%x\n", status_wait);
+			udelay(USEC_DELAY);
+		} while ((status_wait & 0x01000000) >> 24 &&
+			 ++iter < NUM_RETRY);
+	}
+
+	/*
+	 * read device counter
+	 */
+	if (mfpmc_remapped_address) {
+		memcpy_fromio(
+			dst_vals,
+			(volatile void __iomem *)
+			(unsigned long)mfpmc_remapped_address,
+			4 /*counter_size_in_bytes*/);
+		pw_pr_debug("DEBUG: read value = 0x%x\n",
+			    *((pw_u32_t *)dst_vals));
+	}
+}
+
+void sw_read_mailbox_info_i(char *dst_vals, int cpu,
+			    const struct sw_driver_io_descriptor *descriptor,
+			    u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_mailbox_io_descriptor *mailbox =
+		&descriptor->mailbox_descriptor;
+	unsigned long interface_address = mailbox->interface_address;
+	unsigned long interface_remapped_address =
+		mailbox->interface_remapped_address;
+	unsigned long data_address = mailbox->data_address;
+	size_t iter = 0;
+
+	if (mailbox->is_msr_type) {
+		u64 command = 0;
+
+		if (rdmsrl_safe(interface_address, &command)) {
+			pw_pr_warn("Failed to read MSR address = 0x%llx\n",
+				   interface_address);
+		}
+		command &= mailbox->command_mask;
+		command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
+		wrmsrl_safe(interface_address, command);
+		do {
+			udelay(1);
+			if (rdmsrl_safe(interface_address, &command)) {
+				pw_pr_warn("Failed to read MSR address = 0x%llx\n",
+					   interface_address);
+			}
+		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
+			 ++iter < MAX_MAILBOX_ITERS);
+		if (iter >= MAX_MAILBOX_ITERS) {
+			pw_pr_error("Couldn't write to BIOS mailbox\n");
+			command = 0;
+		} else {
+			if (rdmsrl_safe(data_address, &command)) {
+				pw_pr_warn("Failed to read MSR address = 0x%llx\n",
+					   data_address);
+			}
+		}
+		*((u64 *)dst_vals) = command;
+	} else {
+		u32 command = 0;
+		const size_t counter_size =
+			4; /* Always use 4 bytes, regardless of
+			    *'counter_size_in_bytes'
+			    */
+		memcpy_fromio(&command,
+			      (volatile void __iomem *)(unsigned long)
+				      interface_remapped_address,
+			      sizeof(command));
+		command &= mailbox->command_mask;
+		command |= (u32)mailbox->command |
+			   (u32)0x1 << mailbox->run_busy_bit;
+		memcpy_toio((volatile void __iomem *)(unsigned long)
+				    interface_remapped_address,
+			    &command, sizeof(command));
+		do {
+			udelay(1);
+			memcpy_fromio(&command,
+				      (volatile void __iomem *)(unsigned long)
+					      interface_remapped_address,
+				      sizeof(command));
+		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
+			 ++iter < MAX_MAILBOX_ITERS);
+		if (iter >= MAX_MAILBOX_ITERS) {
+			pw_pr_error("Couldn't write to BIOS mailbox\n");
+			command = 0;
+		} else {
+			memcpy_fromio(&command,
+				      (volatile void __iomem *)(unsigned long)
+					      mailbox->data_remapped_address,
+				      counter_size);
+		}
+		*((u32 *)dst_vals) = command;
+	}
+}
+
+void sw_read_pci_info_i(char *dst_vals, int cpu,
+			const struct sw_driver_io_descriptor *descriptors,
+			u16 counter_size_in_bytes)
+{
+	u32 bus = descriptors->pci_descriptor.bus,
+	    device = descriptors->pci_descriptor.device;
+	u32 function = descriptors->pci_descriptor.function,
+	    offset = descriptors->pci_descriptor.offset;
+	u32 data32 = 0;
+	u64 data64 = 0;
+
+	switch (counter_size_in_bytes) {
+	case 4:
+		data32 = sw_platform_pci_read32(bus, device, function,
+						0 /* CTRL-OFFSET */,
+						0 /* CTRL-DATA, don't care */,
+						offset /* DATA-OFFSET */);
+		*((u32 *)dst_vals) = data32;
+		break;
+	case 8:
+		data64 = sw_platform_pci_read64(bus, device, function,
+						0 /* CTRL-OFFSET */,
+						0 /* CTRL-DATA, don't care */,
+						offset /* DATA-OFFSET */);
+		*((u64 *)dst_vals) = data64;
+		break;
+	default:
+		pw_pr_error("ERROR: invalid read size = %u\n",
+			    counter_size_in_bytes);
+		return;
+	}
+	return;
+}
+void sw_read_configdb_info_i(char *dst_vals, int cpu,
+			     const struct sw_driver_io_descriptor *descriptors,
+			     u16 counter_size_in_bytes)
+{
+	{
+		pw_u32_t address = descriptors->configdb_descriptor.address;
+		u32 data = sw_platform_configdb_read32(address);
+
+		pw_pr_debug(
+			"ADDRESS = 0x%x, CPU = %d, dst_vals = %p, counter size = %u, data = %u\n",
+			address, cpu, dst_vals, counter_size_in_bytes, data);
+		/*
+		 * 'counter_size_in_bytes' is ignored, for now.
+		 */
+		*((u32 *)dst_vals) = data;
+	}
+	return;
+}
+void sw_read_socperf_info_i(char *dst_vals, int cpu,
+			    const struct sw_driver_io_descriptor *descriptors,
+			    u16 counter_size_in_bytes)
+{
+#if IS_ENABLED(CONFIG_INTEL_SOCPERF)
+	u64 *socperf_buffer = (u64 *)dst_vals;
+
+	memset(socperf_buffer, 0, counter_size_in_bytes);
+	SOCPERF_Read_Data2(socperf_buffer);
+#endif // IS_ENABLED(CONFIG_INTEL_SOCPERF)
+	return;
+}
+
+/**
+ * Decide if the socperf interface is available for use
+ * @returns     true if available
+ */
+bool sw_socperf_available_i(void)
+{
+	bool retVal = false;
+#if IS_ENABLED(CONFIG_INTEL_SOCPERF)
+	retVal = true;
+#endif // IS_ENABLED(CONFIG_INTEL_SOCPERF)
+	return retVal;
+}
+
+/**
+ * sw_platform_configdb_read32 - for reading PCI space through config registers
+ *                               of the platform.
+ * @address: An address in the PCI space
+ *
+ * Returns: the value read from address.
+ */
+u32 sw_platform_configdb_read32(u32 address)
+{
+	u32 read_value = 0;
+#if DO_DIRECT_PCI_READ_WRITE
+	read_value =
+		sw_platform_pci_read32(0 /*bus*/,
+				       0 /*device*/,
+				       0 /*function*/,
+				       SW_PCI_MSG_CTRL_REG /*ctrl-offset*/,
+				       address /*ctrl-value*/,
+				       SW_PCI_MSG_DATA_REG /*data-offset*/);
+#else // !DO_DIRECT_PCI_READ_WRITE
+	read_value = intel_mid_msgbus_read32_raw(address);
+#endif // if DO_DIRECT_PCI_READ_WRITE
+	pw_pr_debug("address = %u, value = %u\n", address, read_value);
+	return read_value;
+}
+
+u32 sw_platform_pci_read32(u32 bus, u32 device, u32 function, u32 write_offset,
+			   u32 write_value, u32 read_offset)
+{
+	u32 read_value = 0;
+	struct pci_dev *pci_root = pci_get_domain_bus_and_slot(
+		0, bus, PCI_DEVFN(device, function)); // 0, PCI_DEVFN(0, 0));
+	if (!pci_root) {
+		return 0; /* Application will verify the data */
+	}
+	if (write_offset) {
+		pci_write_config_dword(
+			pci_root, write_offset,
+			write_value); // SW_PCI_MSG_CTRL_REG, address);
+	}
+	pci_read_config_dword(
+		pci_root, read_offset,
+		&read_value); // SW_PCI_MSG_DATA_REG, &read_value);
+	return read_value;
+}
+
+u64 sw_platform_pci_read64(u32 bus, u32 device, u32 function, u32 write_offset,
+			   u32 write_value, u32 read_offset)
+{
+	u32 lo = sw_platform_pci_read32(bus, device, function,
+					0 /* CTRL-OFFSET */,
+					0 /* CTRL-DATA, don't care */,
+					read_offset /* DATA-OFFSET */);
+	u32 hi = sw_platform_pci_read32(bus, device, function,
+					0 /* CTRL-OFFSET */,
+					0 /* CTRL-DATA, don't care */,
+					read_offset + 4 /* DATA-OFFSET */);
+	return ((u64)hi << 32) | lo;
+}
+
+void sw_write_msr_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes)
+{
+	u64 write_value = descriptor->write_value;
+	u64 address = descriptor->msr_descriptor.address;
+
+	pw_pr_debug(
+		"ADDRESS = 0x%llx, CPU = %d, counter size = %u, value = %llu\n",
+		address, cpu, counter_size_in_bytes, write_value);
+	if (likely(cpu == RAW_CPU())) {
+		wrmsrl_safe((unsigned long)address, write_value);
+	} else {
+		u32 l = write_value & 0xffffffff,
+		    h = (write_value >> 32) & 0xffffffff;
+		wrmsr_safe_on_cpu(cpu, (u32)address, l, h);
+	}
+	return;
+};
+
+void sw_write_mmio_info_i(char *dst_vals, int cpu,
+			  const struct sw_driver_io_descriptor *descriptor,
+			  u16 counter_size_in_bytes)
+{
+	unsigned long remapped_address =
+		(unsigned long)
+		descriptor->mmio_descriptor.data_remapped_address;
+	u64 write_value = descriptor->write_value;
+
+	if (remapped_address) {
+		memcpy_toio((volatile void __iomem *)remapped_address,
+			    &write_value,
+			    counter_size_in_bytes);
+	}
+	pw_pr_debug("Value = %llu\n", *((u64 *)dst_vals));
+};
+
+void sw_write_mailbox_info_i(char *dst_vals, int cpu,
+			     const struct sw_driver_io_descriptor *descriptor,
+			     u16 counter_size_in_bytes)
+{
+	/*
+	 * TODO: spinlock?
+	 */
+	const struct sw_driver_mailbox_io_descriptor *mailbox =
+		&descriptor->mailbox_descriptor;
+	unsigned long interface_address = mailbox->interface_address;
+	unsigned long interface_remapped_address =
+		mailbox->interface_remapped_address;
+	unsigned long data_address = mailbox->data_address;
+	u64 data = descriptor->write_value;
+	size_t iter = 0;
+
+	if (mailbox->is_msr_type) {
+		u64 command = 0;
+
+		if (rdmsrl_safe(interface_address, &command)) {
+			pw_pr_warn("Failed to read MSR address = 0x%llx\n",
+				   interface_address);
+		}
+		command &= mailbox->command_mask;
+		command |= mailbox->command | (u64)0x1 << mailbox->run_busy_bit;
+		wrmsrl_safe(data_address, data);
+		wrmsrl_safe(interface_address, command);
+		do {
+			if (rdmsrl_safe(interface_address, &command)) {
+				pw_pr_warn("Failed to read MSR address = 0x%llx\n",
+					   interface_address);
+			}
+		} while ((command & ((u64)0x1 << mailbox->run_busy_bit)) &&
+			 ++iter < MAX_MAILBOX_ITERS);
+	} else {
+		u32 command = 0;
+
+		memcpy_fromio(&command,
+			      (volatile void __iomem *)(unsigned long)
+				      interface_remapped_address,
+			      sizeof(command));
+		command &= mailbox->command_mask;
+		command |= (u32)mailbox->command |
+			   (u32)0x1 << mailbox->run_busy_bit;
+		memcpy_toio((volatile void __iomem *)(unsigned long)
+				    mailbox->data_remapped_address,
+			    &data, sizeof(data));
+		memcpy_toio((volatile void __iomem *)(unsigned long)
+				    interface_remapped_address,
+			    &command, sizeof(command));
+		do {
+			memcpy_fromio(&command,
+				      (volatile void __iomem *)(unsigned long)
+					      interface_remapped_address,
+				      sizeof(command));
+		} while ((command & ((u32)0x1 << mailbox->run_busy_bit)) &&
+			 ++iter < MAX_MAILBOX_ITERS);
+	}
+}
+
+void sw_write_pci_info_i(char *dst_vals, int cpu,
+			 const struct sw_driver_io_descriptor *descriptor,
+			 u16 counter_size_in_bytes)
+{
+	u32 bus = descriptor->pci_descriptor.bus,
+	    device = descriptor->pci_descriptor.device;
+	u32 function = descriptor->pci_descriptor.function,
+	    offset = descriptor->pci_descriptor.offset;
+	u32 write_value = (u32)descriptor->write_value;
+	/*
+	 * 'counter_size_in_bytes' is ignored for now.
+	 */
+	if (!sw_platform_pci_write32(bus, device, function, offset,
+				     write_value)) {
+		pw_pr_error("ERROR writing to PCI B/D/F/O %u/%u/%u/%u\n", bus,
+			    device, function, offset);
+	} else {
+		pw_pr_debug(
+			"OK, successfully wrote to PCI B/D/F/O %u/%u/%u/%u\n",
+			bus, device, function, offset);
+	}
+	return;
+};
+
+/*
+ * Write to PCI space via config registers.
+ */
+bool sw_platform_pci_write32(u32 bus, u32 device, u32 function,
+			     u32 write_offset, u32 data_value)
+{
+	struct pci_dev *pci_root = pci_get_domain_bus_and_slot(
+		0, bus, PCI_DEVFN(device, function)); // 0, PCI_DEVFN(0, 0));
+	if (!pci_root) {
+		return false;
+	}
+
+	pci_write_config_dword(pci_root, write_offset, data_value);
+
+	return true;
+};
+
+int sw_print_msr_io_descriptor(const struct sw_driver_io_descriptor *descriptor)
+{
+	if (!descriptor) {
+		return -PW_ERROR;
+	}
+	pw_pr_debug("MSR address = 0x%llx\n",
+		    descriptor->msr_descriptor.address);
+	return PW_SUCCESS;
+}
+
+int sw_ipc_mmio_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_ipc_mmio_io_descriptor *__ipc_mmio = NULL;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	if (descriptor->collection_type == SW_IO_IPC) {
+		__ipc_mmio =
+			(struct sw_driver_ipc_mmio_io_descriptor *)&descriptor
+				->ipc_descriptor;
+	} else {
+		__ipc_mmio =
+			(struct sw_driver_ipc_mmio_io_descriptor *)&descriptor
+				->mmio_descriptor;
+	}
+	if (__ipc_mmio->data_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			    __ipc_mmio->data_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+				__ipc_mmio->data_remapped_address);
+		__ipc_mmio->data_remapped_address = 0;
+	}
+	/* Uninitialize the GBE, if it wasn't already done */
+	if (s_gbe_semaphore.hw_semaphore || s_gbe_semaphore.fw_semaphore) {
+		pw_pr_debug("Uninitializing gbe!\n");
+		if (s_gbe_semaphore.hw_semaphore) {
+			iounmap(s_gbe_semaphore.hw_semaphore);
+		}
+		if (s_gbe_semaphore.fw_semaphore) {
+			iounmap(s_gbe_semaphore.fw_semaphore);
+		}
+		memset(&s_gbe_semaphore, 0, sizeof(s_gbe_semaphore));
+	}
+	return PW_SUCCESS;
+}
+
+int sw_pch_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_pch_mailbox_io_descriptor *__pch_mailbox = NULL;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	__pch_mailbox =
+		(struct sw_driver_pch_mailbox_io_descriptor *)&descriptor
+			->pch_mailbox_descriptor;
+	if (__pch_mailbox->mtpmc_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			    __pch_mailbox->mtpmc_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+				__pch_mailbox->mtpmc_remapped_address);
+		__pch_mailbox->mtpmc_remapped_address = 0;
+	}
+	if (__pch_mailbox->msg_full_sts_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			    __pch_mailbox->msg_full_sts_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+				__pch_mailbox->msg_full_sts_remapped_address);
+		__pch_mailbox->msg_full_sts_remapped_address = 0;
+	}
+	if (__pch_mailbox->mfpmc_remapped_address) {
+		pw_pr_debug("unmapping addr 0x%llx\n",
+			    __pch_mailbox->mfpmc_remapped_address);
+		iounmap((volatile void __iomem *)(unsigned long)
+				__pch_mailbox->mfpmc_remapped_address);
+		__pch_mailbox->mfpmc_remapped_address = 0;
+	}
+	return PW_SUCCESS;
+}
+
+int sw_mailbox_descriptor_reset_func_i(
+	const struct sw_driver_io_descriptor *descriptor)
+{
+	/* Unmap previously mapped memory here */
+	struct sw_driver_mailbox_io_descriptor *__mailbox = NULL;
+
+	if (!descriptor) { // Should NEVER happen
+		return -PW_ERROR;
+	}
+	__mailbox = (struct sw_driver_mailbox_io_descriptor *)&descriptor
+			    ->mailbox_descriptor;
+	if (!__mailbox->is_msr_type) {
+		if (__mailbox->interface_remapped_address) {
+			pw_pr_debug("unmapping addr 0x%llx\n",
+				    __mailbox->interface_remapped_address);
+			iounmap((volatile void __iomem *)(unsigned long)
+					__mailbox->interface_remapped_address);
+			__mailbox->interface_remapped_address = 0;
+		}
+		if (__mailbox->data_remapped_address) {
+			pw_pr_debug("unmapping addr 0x%llx\n",
+				    __mailbox->data_remapped_address);
+			iounmap((volatile void __iomem *)(unsigned long)
+					__mailbox->data_remapped_address);
+			__mailbox->data_remapped_address = 0;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+#define NUM_HW_OPS SW_ARRAY_SIZE(s_hw_ops)
+#define FOR_EACH_HW_OP(idx, op)                                                \
+	for (idx = 0; idx < NUM_HW_OPS && (op = &s_hw_ops[idx]); ++idx)
+
+int sw_register_ops_providers(void)
+{
+	size_t idx = 0;
+	const struct sw_hw_ops *op = NULL;
+
+	FOR_EACH_HW_OP(idx, op)
+	{
+		if (op->name && sw_register_hw_op(op)) {
+			pw_pr_error("ERROR registering provider %s\n",
+				    op->name);
+			return -EIO;
+		}
+	}
+	return PW_SUCCESS;
+}
+
+void sw_free_ops_providers(void)
+{
+	// NOP
+}
diff --git a/drivers/platform/x86/socwatch/sw_output_buffer.c b/drivers/platform/x86/socwatch/sw_output_buffer.c
new file mode 100644
index 000000000000..d3b8e585595d
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_output_buffer.c
@@ -0,0 +1,598 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_kernel_defines.h"
+#include "sw_mem.h"
+#include "sw_lock_defs.h"
+#include "sw_overhead_measurements.h"
+
+/* -------------------------------------------------
+ * Compile time constants and macros.
+ * -------------------------------------------------
+ */
+#define NUM_SEGS_PER_BUFFER 2 /* MUST be pow 2! */
+#define NUM_SEGS_PER_BUFFER_MASK (NUM_SEGS_PER_BUFFER - 1)
+/*
+ * The size of the 'buffer' data array in each segment.
+ */
+#define SW_SEG_DATA_SIZE (sw_buffer_alloc_size)
+/*
+ * Min size of per-cpu output buffers.
+ */
+#define SW_MIN_SEG_SIZE_BYTES (1 << 10) /* 1kB */
+#define SW_MIN_OUTPUT_BUFFER_SIZE (SW_MIN_SEG_SIZE_BYTES * NUM_SEGS_PER_BUFFER)
+/*
+ * A symbolic constant for an empty buffer index.
+ */
+#define EMPTY_SEG (-1)
+/*
+ * How much space is available in a given segment?
+ */
+#define EMPTY_TSC ((u64)-1)
+#define SEG_IS_FULL(seg)                                                       \
+	({                                                                     \
+		bool __full = false;                                           \
+		smp_mb();                                                      \
+		__full = ((seg)->is_full != EMPTY_TSC);                        \
+		__full;                                                        \
+	})
+#define SEG_SET_FULL(seg, tsc)                                                 \
+	do {                                                                   \
+		(seg)->is_full = (tsc);                                        \
+		smp_mb();                                                      \
+	} while (0)
+#define SEG_SET_EMPTY(seg)                                                     \
+	do {                                                                   \
+		barrier();                                                     \
+		(seg)->bytes_written = 0;                                      \
+		SEG_SET_FULL(seg, EMPTY_TSC);                                  \
+		/*smp_mb(); */                                                 \
+	} while (0)
+#define SPACE_AVAIL(seg) (SW_SEG_DATA_SIZE - (seg)->bytes_written)
+#define SEG_IS_EMPTY(seg) (SPACE_AVAIL(seg) == SW_SEG_DATA_SIZE)
+
+#define GET_OUTPUT_BUFFER(cpu) (&per_cpu_output_buffers[(cpu)])
+/*
+ * Convenience macro: iterate over each segment in a per-cpu output buffer.
+ */
+#define for_each_segment(i) for (i = 0; i < NUM_SEGS_PER_BUFFER; ++i)
+#define for_each_seg(buffer, seg)                                              \
+	for (int i = 0;                                                        \
+	     i < NUM_SEGS_PER_BUFFER && (seg = (buffer)->segments[i]); ++i)
+/*
+ * How many buffers are we using?
+ */
+#define GET_NUM_OUTPUT_BUFFERS() (sw_max_num_cpus + 1)
+/*
+ * Convenience macro: iterate over each per-cpu output buffer.
+ */
+#define for_each_output_buffer(i) for (i = 0; i < GET_NUM_OUTPUT_BUFFERS(); ++i)
+
+/* -------------------------------------------------
+ * Local data structures.
+ * -------------------------------------------------
+ */
+typedef struct sw_data_buffer sw_data_buffer_t;
+typedef struct sw_output_buffer sw_output_buffer_t;
+struct sw_data_buffer {
+	u64 is_full;
+	u32 bytes_written;
+	char *buffer;
+} __attribute__((packed));
+#define SW_SEG_HEADER_SIZE() (sizeof(struct sw_data_buffer) - sizeof(char *))
+
+struct sw_output_buffer {
+	sw_data_buffer_t buffers[NUM_SEGS_PER_BUFFER];
+	int buff_index;
+	u32 produced_samples;
+	u32 dropped_samples;
+	int last_seg_read;
+	unsigned int mem_alloc_size;
+	unsigned long free_pages;
+} ____cacheline_aligned_in_smp;
+
+/* -------------------------------------------------
+ * Function declarations.
+ * -------------------------------------------------
+ */
+extern u64 sw_timestamp(void);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+u64 sw_num_samples_produced = 0, sw_num_samples_dropped = 0;
+int sw_max_num_cpus = -1;
+
+DECLARE_OVERHEAD_VARS(sw_produce_generic_msg_i);
+/*
+ * Per-cpu output buffers.
+ */
+static sw_output_buffer_t *per_cpu_output_buffers = NULL;
+/*
+ * Variables for book keeping.
+ */
+static volatile int sw_last_cpu_read = -1;
+static volatile s32 sw_last_mask = -1;
+/*
+ * Lock for the polled buffer.
+ */
+SW_DECLARE_SPINLOCK(sw_polled_lock);
+/*
+ * Buffer allocation size.
+ */
+unsigned long sw_buffer_alloc_size = (1 << 16); // 64 KB
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+
+static char *reserve_seg_space_i(size_t size, int cpu, bool *should_wakeup,
+				 u64 *reservation_tsc)
+{
+	sw_output_buffer_t *buffer = GET_OUTPUT_BUFFER(cpu);
+	int i = 0;
+	int buff_index = buffer->buff_index;
+	char *dst = NULL;
+
+	if (buff_index < 0 || buff_index >= NUM_SEGS_PER_BUFFER) {
+		goto prod_seg_done;
+	}
+	for_each_segment(i) {
+		sw_data_buffer_t *seg = &buffer->buffers[buff_index];
+
+		if (SEG_IS_FULL(seg) == false) {
+			if (SPACE_AVAIL(seg) >= size) {
+				*reservation_tsc = sw_timestamp();
+				dst = &seg->buffer[seg->bytes_written];
+				seg->bytes_written += size;
+				smp_mb();
+				buffer->buff_index = buff_index;
+				buffer->produced_samples++;
+				goto prod_seg_done;
+			}
+			SEG_SET_FULL(seg, sw_timestamp());
+		}
+		buff_index = CIRCULAR_INC(buff_index, NUM_SEGS_PER_BUFFER_MASK);
+		*should_wakeup = true;
+	}
+prod_seg_done:
+	if (!dst) {
+		buffer->dropped_samples++;
+	}
+	return dst;
+};
+
+static int sw_produce_polled_msg_i(struct sw_driver_msg *msg,
+				enum sw_wakeup_action action)
+{
+	int cpu = GET_POLLED_CPU();
+	bool should_wakeup = false;
+	int retVal = PW_SUCCESS;
+
+	if (!msg) {
+		return -PW_ERROR;
+	}
+	pw_pr_debug("POLLED! cpu = %d\n", cpu);
+	LOCK(sw_polled_lock);
+	{
+		size_t size = SW_DRIVER_MSG_HEADER_SIZE() + msg->payload_len;
+		char *dst = reserve_seg_space_i(size, cpu, &should_wakeup,
+						&msg->tsc);
+		if (dst) {
+			/*
+			 * Assign a special CPU number to this CPU.
+			 * This is OK, because messages enqueued in this buffer
+			 * are always CPU agnostic (otherwise they would
+			 * be invoked from within a preempt_disable()d context
+			 * in 'sw_handle_collector_node_i()', which ensures they
+			 * will be enqueued within the 'sw_produce_generic_msg_on_cpu()'
+			 * function).
+			 */
+			msg->cpuidx = cpu;
+			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
+			dst += SW_DRIVER_MSG_HEADER_SIZE();
+			memcpy(dst, msg->p_payload, msg->payload_len);
+		} else {
+			pw_pr_debug("NO space in polled msg!\n");
+			retVal = -PW_ERROR;
+		}
+	}
+	UNLOCK(sw_polled_lock);
+	if (unlikely(should_wakeup)) {
+		sw_wakeup_reader(action);
+	}
+	return retVal;
+};
+
+static int sw_produce_generic_msg_i(struct sw_driver_msg *msg,
+				    enum sw_wakeup_action action)
+{
+	int retval = PW_SUCCESS;
+	bool should_wakeup = false;
+	int cpu = -1;
+	unsigned long flags = 0;
+
+	if (!msg) {
+		pw_pr_error("ERROR: CANNOT produce a NULL msg!\n");
+		return -PW_ERROR;
+	}
+
+#ifdef CONFIG_PREEMPT_COUNT
+	if (!in_atomic()) {
+		return sw_produce_polled_msg(msg, action);
+	}
+#endif
+
+	cpu = sw_get_cpu(&flags);
+	{
+		size_t size = msg->payload_len + SW_DRIVER_MSG_HEADER_SIZE();
+		char *dst = reserve_seg_space_i(size, cpu, &should_wakeup,
+						&msg->tsc);
+		if (likely(dst)) {
+			memcpy(dst, msg, SW_DRIVER_MSG_HEADER_SIZE());
+			dst += SW_DRIVER_MSG_HEADER_SIZE();
+			memcpy(dst, msg->p_payload, msg->payload_len);
+		} else {
+			retval = -PW_ERROR;
+		}
+	}
+	sw_put_cpu(flags);
+
+	if (unlikely(should_wakeup)) {
+		sw_wakeup_reader(action);
+	}
+
+	return retval;
+};
+
+int sw_produce_polled_msg(struct sw_driver_msg *msg,
+			   enum sw_wakeup_action action)
+{
+	return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_produce_polled_msg_i, msg,
+					    action);
+};
+
+int sw_produce_generic_msg(struct sw_driver_msg *msg,
+			   enum sw_wakeup_action action)
+{
+	return DO_PER_CPU_OVERHEAD_FUNC_RET(int, sw_produce_generic_msg_i, msg,
+					    action);
+};
+
+static int sw_init_per_cpu_buffers_i(unsigned long per_cpu_mem_size)
+{
+	int cpu = -1;
+
+	per_cpu_output_buffers = (sw_output_buffer_t *)sw_kmalloc(
+		sizeof(sw_output_buffer_t) * GET_NUM_OUTPUT_BUFFERS(),
+		GFP_KERNEL | __GFP_ZERO);
+	if (per_cpu_output_buffers == NULL) {
+		pw_pr_error(
+			"ERROR allocating space for per-cpu output buffers!\n");
+		sw_destroy_per_cpu_buffers();
+		return -PW_ERROR;
+	}
+	for_each_output_buffer(cpu) {
+		sw_output_buffer_t *buffer = &per_cpu_output_buffers[cpu];
+		char *buff = NULL;
+		int i = 0;
+
+		buffer->mem_alloc_size = per_cpu_mem_size;
+		buffer->free_pages =
+			sw_allocate_pages(GFP_KERNEL | __GFP_ZERO,
+					  (unsigned int)per_cpu_mem_size);
+		if (buffer->free_pages == 0) {
+			pw_pr_error("ERROR allocating pages for buffer [%d]!\n",
+				    cpu);
+			sw_destroy_per_cpu_buffers();
+			return -PW_ERROR;
+		}
+		buff = (char *)buffer->free_pages;
+		for_each_segment(i) {
+			buffer->buffers[i].buffer = (char *)buff;
+			buff += SW_SEG_DATA_SIZE;
+		}
+	}
+	pw_pr_debug("PER_CPU_MEM_SIZE = %lu, order = %u\n",
+		    (unsigned long)per_cpu_mem_size,
+		    get_order(per_cpu_mem_size));
+	return PW_SUCCESS;
+};
+
+int sw_init_per_cpu_buffers(void)
+{
+	unsigned int per_cpu_mem_size = sw_get_output_buffer_size();
+
+	pw_pr_debug("Buffer alloc size = %ld\n", sw_buffer_alloc_size);
+
+	if (GET_NUM_OUTPUT_BUFFERS() <= 0) {
+		pw_pr_error("ERROR: max # output buffers= %d\n",
+			    GET_NUM_OUTPUT_BUFFERS());
+		return -PW_ERROR;
+	}
+
+	pw_pr_debug("DEBUG: sw_max_num_cpus = %d, num output buffers = %d\n",
+		    sw_max_num_cpus, GET_NUM_OUTPUT_BUFFERS());
+
+	/*
+	 * Try to allocate per-cpu buffers. If allocation fails,
+	 * decrease buffer size and retry. Stop trying if size
+	 * drops below 2KB (which means 1KB for each buffer).
+	 */
+	while (per_cpu_mem_size >= SW_MIN_OUTPUT_BUFFER_SIZE &&
+	       sw_init_per_cpu_buffers_i(per_cpu_mem_size)) {
+		pw_pr_debug(
+			"WARNING: couldn't allocate per-cpu buffers with size %u -- trying smaller size!\n",
+			per_cpu_mem_size);
+		sw_buffer_alloc_size >>= 1;
+		per_cpu_mem_size = sw_get_output_buffer_size();
+	}
+
+	if (unlikely(per_cpu_output_buffers == NULL)) {
+		pw_pr_error(
+			"ERROR: couldn't allocate space for per-cpu output buffers!\n");
+		return -PW_ERROR;
+	}
+	/*
+	 * Initialize our locks.
+	 */
+	SW_INIT_SPINLOCK(sw_polled_lock);
+
+	pw_pr_debug("OK, allocated per-cpu buffers with size = %lu\n",
+		    (unsigned long)per_cpu_mem_size);
+
+	if (sw_init_reader_queue()) {
+		pw_pr_error("ERROR initializing reader subsys\n");
+		return -PW_ERROR;
+	}
+
+	return PW_SUCCESS;
+};
+
+void sw_destroy_per_cpu_buffers(void)
+{
+	int cpu = -1;
+
+	/*
+	 * Perform lock finalization.
+	 */
+	SW_DESTROY_SPINLOCK(sw_polled_lock);
+
+	if (per_cpu_output_buffers != NULL) {
+		for_each_output_buffer(cpu) {
+			sw_output_buffer_t *buffer =
+				&per_cpu_output_buffers[cpu];
+			if (buffer->free_pages != 0) {
+				sw_release_pages(buffer->free_pages,
+						 buffer->mem_alloc_size);
+				buffer->free_pages = 0;
+			}
+		}
+		sw_kfree(per_cpu_output_buffers);
+		per_cpu_output_buffers = NULL;
+	}
+};
+
+void sw_reset_per_cpu_buffers(void)
+{
+	int cpu = 0, i = 0;
+
+	for_each_output_buffer(cpu) {
+		sw_output_buffer_t *buffer = GET_OUTPUT_BUFFER(cpu);
+
+		buffer->buff_index = buffer->dropped_samples =
+			buffer->produced_samples = 0;
+		buffer->last_seg_read = -1;
+
+		for_each_segment(i) {
+			sw_data_buffer_t *seg = &buffer->buffers[i];
+
+			memset(seg->buffer, 0, SW_SEG_DATA_SIZE);
+			SEG_SET_EMPTY(seg);
+		}
+	}
+	sw_last_cpu_read = -1;
+	sw_last_mask = -1;
+	pw_pr_debug("OK, reset per-cpu output buffers!\n");
+};
+
+bool sw_any_seg_full(u32 *val, bool is_flush_mode)
+{
+	int num_visited = 0, i = 0;
+
+	if (!val) {
+		pw_pr_error("ERROR: NULL ptrs in sw_any_seg_full!\n");
+		return false;
+	}
+
+	*val = SW_NO_DATA_AVAIL_MASK;
+	pw_pr_debug("Checking for full seg: val = %u, flush = %s\n", *val,
+		    GET_BOOL_STRING(is_flush_mode));
+	for_each_output_buffer(num_visited) {
+		int min_seg = EMPTY_SEG, non_empty_seg = EMPTY_SEG;
+		u64 min_tsc = EMPTY_TSC;
+		sw_output_buffer_t *buffer = NULL;
+
+		if (++sw_last_cpu_read >= GET_NUM_OUTPUT_BUFFERS()) {
+			sw_last_cpu_read = 0;
+		}
+		buffer = GET_OUTPUT_BUFFER(sw_last_cpu_read);
+		for_each_segment(i) {
+			sw_data_buffer_t *seg = &buffer->buffers[i];
+			u64 seg_tsc = seg->is_full;
+
+			if (SEG_IS_EMPTY(seg)) {
+				continue;
+			}
+			non_empty_seg = i;
+			if (seg_tsc < min_tsc) {
+				/*
+				 * Can only happen if seg was full,
+				 * provided 'EMPTY_TSC' is set to "(u64)-1"
+				 */
+				min_tsc = seg_tsc;
+				min_seg = i;
+			}
+		}
+		if (min_seg != EMPTY_SEG) {
+			*val = (sw_last_cpu_read & 0xffff) << 16 |
+			       (min_seg & 0xffff);
+			return true;
+		} else if (is_flush_mode && non_empty_seg != EMPTY_SEG) {
+			*val = (sw_last_cpu_read & 0xffff) << 16 |
+			       (non_empty_seg & 0xffff);
+			return true;
+		}
+	}
+	/*
+	 * Reaches here only if there's no data to be read.
+	 */
+	if (is_flush_mode) {
+		/*
+		 * We've drained all buffers and need to tell the userspace
+		 * application there isn't any data. Unfortunately, we can't
+		 * just return a 'zero' value for the mask (because that could
+		 * also indicate that segment # 0 of cpu #0 has data).
+		 */
+		*val = SW_ALL_WRITES_DONE_MASK;
+		return true;
+	}
+	return false;
+};
+
+/*
+ * Has semantics of 'copy_to_user()' -- returns # of bytes that could
+ * NOT be copied (On success ==> returns 0).
+ */
+size_t sw_consume_data(u32 mask, void __user *buffer, size_t bytes_to_read)
+{
+	int which_cpu = -1, which_seg = -1;
+	unsigned long bytes_not_copied = 0;
+	sw_output_buffer_t *buff = NULL;
+	sw_data_buffer_t *seg = NULL;
+	size_t bytes_read = 0;
+
+	if (!sw_check_output_buffer_params(buffer, bytes_to_read,
+					   SW_SEG_DATA_SIZE)) {
+		pw_pr_error("ERROR: invalid params to \"sw_consume_data\"!\n");
+		return -PW_ERROR;
+	}
+
+	which_cpu = mask >> 16;
+	which_seg = mask & 0xffff;
+	pw_pr_debug("CONSUME: cpu = %d, seg = %d\n", which_cpu, which_seg);
+	if (which_seg >= NUM_SEGS_PER_BUFFER) {
+		pw_pr_error(
+			"Error: which_seg (%d) >= NUM_SEGS_PER_BUFFER (%d)\n",
+			which_seg, NUM_SEGS_PER_BUFFER);
+		return bytes_to_read;
+	}
+	/*
+	 * OK to access unlocked; either the segment is FULL, or no collection
+	 * is ongoing. In either case, we're GUARANTEED no producer is touching
+	 * this segment.
+	 */
+	buff = GET_OUTPUT_BUFFER(which_cpu);
+	seg = &buff->buffers[which_seg];
+
+	bytes_not_copied = sw_copy_to_user(buffer, seg->buffer,
+					   seg->bytes_written); // dst, src
+
+	// bytes_not_copied =
+	// copy_to_user(buffer, seg->buffer, seg->bytes_written); // dst,src
+	if (likely(bytes_not_copied == 0)) {
+		bytes_read = seg->bytes_written;
+	} else {
+		pw_pr_error("Warning: couldn't copy %lu bytes\n",
+			    bytes_not_copied);
+		bytes_read = 0;
+	}
+	SEG_SET_EMPTY(seg);
+	return bytes_read;
+}
+
+unsigned int sw_get_output_buffer_size(void)
+{
+	return (sw_buffer_alloc_size * NUM_SEGS_PER_BUFFER);
+};
+
+void sw_count_samples_produced_dropped(void)
+{
+	int cpu = 0;
+
+	sw_num_samples_produced = sw_num_samples_dropped = 0;
+
+	if (per_cpu_output_buffers == NULL) {
+		return;
+	}
+	for_each_output_buffer(cpu) {
+		sw_output_buffer_t *buff = GET_OUTPUT_BUFFER(cpu);
+
+		sw_num_samples_dropped += buff->dropped_samples;
+		sw_num_samples_produced += buff->produced_samples;
+	}
+};
+
+void sw_print_output_buffer_overheads(void)
+{
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_produce_generic_msg_i,
+					 "PRODUCE_GENERIC_MSG");
+	sw_print_reader_stats();
+};
diff --git a/drivers/platform/x86/socwatch/sw_reader.c b/drivers/platform/x86/socwatch/sw_reader.c
new file mode 100644
index 000000000000..c94e7e8983db
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_reader.c
@@ -0,0 +1,163 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_internal.h"
+#include "sw_output_buffer.h"
+#include "sw_kernel_defines.h"
+
+#define SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC                                     \
+	1000000 /* delay buffer cleanup by 10^6 nsec i.e. 1 msec */
+
+/*
+ * The alarm queue.
+ */
+wait_queue_head_t sw_reader_queue;
+/*
+ * Reader wakeup timer.
+ */
+static struct hrtimer s_reader_wakeup_timer;
+/*
+ * Variable to track # timer fires.
+ */
+static int s_num_timer_fires;
+
+/*
+ * The alarm callback.
+ */
+static enum hrtimer_restart sw_wakeup_callback_i(struct hrtimer *timer)
+{
+	++s_num_timer_fires;
+	wake_up_interruptible(&sw_reader_queue);
+	return HRTIMER_NORESTART;
+}
+
+/*
+ * Init reader queue.
+ */
+int sw_init_reader_queue(void)
+{
+	init_waitqueue_head(&sw_reader_queue);
+	/*
+	 * Also init wakeup timer (used in low-overhead mode).
+	 */
+	hrtimer_init(&s_reader_wakeup_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	s_reader_wakeup_timer.function = &sw_wakeup_callback_i;
+
+	return PW_SUCCESS;
+}
+/*
+ * Destroy reader queue.
+ */
+void sw_destroy_reader_queue(void)
+{
+	/* NOP */
+}
+/*
+ * Wakeup client waiting for a full buffer.
+ */
+void sw_wakeup_reader(enum sw_wakeup_action action)
+{
+	if (!waitqueue_active(&sw_reader_queue)) {
+		return;
+	}
+	/*
+	 * Direct mode?
+	 */
+	switch (action) {
+	case SW_WAKEUP_ACTION_DIRECT:
+		wake_up_interruptible(&sw_reader_queue);
+		break;
+	case SW_WAKEUP_ACTION_TIMER:
+		if (!hrtimer_active(&s_reader_wakeup_timer)) {
+			ktime_t ktime =
+				ns_to_ktime(SW_BUFFER_CLEANUP_TIMER_DELAY_NSEC);
+			// TODO: possible race here -- introduce locks?
+			hrtimer_start(&s_reader_wakeup_timer, ktime,
+				      HRTIMER_MODE_REL);
+		}
+		break;
+	default:
+		break;
+	}
+	return;
+}
+/*
+ * Wakeup client waiting for a full buffer, and
+ * cancel any timers initialized by the reader
+ * subsys.
+ */
+void sw_cancel_reader(void)
+{
+	/*
+	 * Cancel pending wakeup timer (used in low-overhead mode).
+	 */
+	if (hrtimer_active(&s_reader_wakeup_timer)) {
+		hrtimer_cancel(&s_reader_wakeup_timer);
+	}
+	/*
+	 * There might be a reader thread blocked on a read: wake
+	 * it up to give it a chance to respond to changed
+	 * conditions.
+	 */
+	sw_wakeup_reader(SW_WAKEUP_ACTION_DIRECT);
+}
+
+void sw_print_reader_stats(void)
+{
+#if DO_OVERHEAD_MEASUREMENTS
+	printk(KERN_INFO "# reader queue timer fires = %d\n",
+	       s_num_timer_fires);
+#endif // OVERHEAD
+}
diff --git a/drivers/platform/x86/socwatch/sw_telem.c b/drivers/platform/x86/socwatch/sw_telem.c
new file mode 100644
index 000000000000..9f8beb57da68
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_telem.c
@@ -0,0 +1,493 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/compiler.h> /* Definition of __weak */
+#include <linux/version.h> /* LINUX_VERSION_CODE */
+#include "sw_kernel_defines.h" /* pw_pr_debug */
+#include "sw_mem.h" /* sw_kmalloc/free */
+#include "sw_lock_defs.h" /* Various lock-related definitions */
+#include "sw_telem.h" /* Signatures of fn's exported from here. */
+
+/*
+ * These functions and data structures are exported by the Telemetry
+ * driver.  However, that file may not be available in the kernel for
+ * which this driver is being built, so we re-define many of the same
+ * things here.
+ */
+/**
+ * struct telemetry_evtlog - The "event log" returned by the kernel's
+ *                           full-read telemetry driver.
+ * @telem_evtid:   The 16-bit event ID.
+ * @telem_evtlog:  The actual telemetry data.
+ */
+struct telemetry_evtlog {
+	u32 telem_evtid; /* Event ID of a data item. */
+	u64 telem_evtlog; /* Counter data */
+};
+
+struct telemetry_evtconfig {
+	u32 *evtmap; /* Array of Event-IDs to Enable */
+	u8 num_evts; /* Number of Events (<29) in evtmap */
+	u8 period; /* Sampling period */
+};
+
+#define MAX_TELEM_EVENTS 28 /* Max telem events per unit */
+
+/* The enable bit is set when programming events, but is returned
+ * cleared for queried events requests.
+ */
+#define TELEM_EVENT_ENABLE 0x8000 /* Enabled when Event ID HIGH bit */
+
+/*
+ * Sampling Period values.
+ * The sampling period is encoded in an 7-bit value, where
+ *    Period = (Value * 16^Exponent) usec where:
+ *        bits[6:3] -> Value;
+ *        bits [0:2]-> Exponent;
+ * Here are some of the calculated possible values:
+ * | Value  Val+Exp  | Value | Exponent | Period (usec) | Period (msec) |
+ * |-----------------+-------+----------+---------------+---------------|
+ * | 0xA = 000 1+010 |     1 |        2 |           256 |         0.256 |
+ * | 0x12= 001 0+010 |     2 |        2 |           512 |         0.512 |
+ * | 0x22= 010 0+010 |     4 |        2 |          1024 |         1.024 |
+ * | 0xB = 000 1+011 |     1 |        3 |          4096 |         4.096 |
+ * | 0x13= 001 0+011 |     2 |        3 |          8192 |         8.192 |
+ * | 0x1B= 001 1+011 |     3 |        3 |         12288 |        12.288 |
+ * | 0x0C= 000 1+100 |     1 |        4 |         65536 |        65.536 |
+ * | 0x0D= 000 1+101 |     1 |        5 |       1048576 |      1048.576 |
+ */
+#define TELEM_SAMPLING_1MS 0x22 /* Approximately 1 ms */
+#define TELEM_SAMPLING_1S 0x0D /* Approximately 1 s */
+
+/* These functions make up the main APIs of the telemetry driver.  We
+ * define all of them with weak linkage so that we can still compile
+ * and load into kernels which don't have a telemetry driver.
+ */
+extern int __weak telemetry_raw_read_eventlog(enum telemetry_unit telem_unit,
+					      struct telemetry_evtlog *evtlog,
+					      int evcount);
+extern int __weak telemetry_reset(void);
+extern int __weak telemetry_reset_events(void);
+extern int __weak telemetry_get_sampling_period(u8 *punit_min, u8 *punit_max,
+						u8 *pmc_min, u8 *pmc_max);
+extern int __weak telemetry_set_sampling_period(u8 punit_period, u8 pmc_period);
+extern int __weak telemetry_get_eventconfig(
+	struct telemetry_evtconfig *punit_config,
+	struct telemetry_evtconfig *pmc_config, int punit_len, int pmc_len);
+extern int __weak telemetry_add_events(u8 num_punit_evts, u8 num_pmc_evts,
+				       u32 *punit_evtmap, u32 *pmc_evtmap);
+
+extern int __weak
+telemetry_update_events(struct telemetry_evtconfig punit_config,
+			struct telemetry_evtconfig pmc_config);
+
+/*
+ * Some telemetry IDs have multiple instances, indexed by cpu ID.  We
+ * implement these by defining two types of IDs: 'regular' and 'scaled'.
+ * For Telemetry IDs with a single instance (the majority of them), the
+ * index into the system's telemetry table is stored in the
+ * sw_driver_io_descriptor.idx.  At read time, the driver gets the telemetry
+ * "slot" from sw_driver_io_descriptor.idx, and reads that data.  This case
+ * is illustrated by telem_desc_A in the illustration below, where idx 2
+ * indicates that telem_data[2] contains the telem data for this descriptor.
+ *
+ *   telem_desc_A                            telem_data
+ *    scale_op: X                              |..|[0]
+ *    idx     : 2 --------------------         |..|[1]
+ *                                    \------->|..|[2]
+ *                     Scaled_IDs              |..|[3]
+ *   telem_desc_B     CPU#0 1 2 3       ------>|..|[4]
+ *    scale_op: /     [0]|.|.|.|.|     /
+ *    idx     : 1---->[1]|4|4|5|5|    /
+ *                        +----------/
+ *
+ * Descriptors with scaled IDs contain a scale operation (scale_op) and
+ * value.  They use a 'scaled_ids' table, which is indexed by descriptor
+ * number and CPU id, and stores the telem_data index.  So in the
+ * illustration above, CPU 0 reading from telem_desc_B would fetch row 1
+ * (from telem_desc_B.idx == 1), and column [0] yielding element 4, so
+ * that's the telemetry ID it looks up in the telemetry data.
+ *
+ * The scaled_ids table is populated at telemetry ID initialization time
+ *
+ */
+static unsigned char *sw_telem_scaled_ids; /* Allocate on demand */
+static unsigned int sw_telem_rows_alloced; /* Rows currently allocated */
+static unsigned int sw_telem_rows_avail; /* Available rows */
+
+extern int sw_max_num_cpus; /* SoC Watch's copy of cpu count. */
+
+/* Macro for identifying telemetry IDs with either per-cpu, or per-module
+ * instances.  These IDs need to be 'scaled' as per scale_op and scale_val.
+ */
+#define IS_SCALED_ID(td) ((td)->scale_op != TELEM_OP_NONE)
+/*
+ * Event map that is populated with user-supplied IDs
+ */
+static u32 s_event_map[2][MAX_TELEM_EVENTS];
+/*
+ * Index into event map(s)
+ */
+static size_t s_unit_idx[2] = { 0, 0 };
+/*
+ * Used to decide if telemetry values need refreshing
+ */
+static size_t s_unit_iters[2] = { 0, 0 };
+/*
+ * Spinlock to guard updates to the 'iters' values.
+ */
+static SW_DEFINE_SPINLOCK(sw_telem_lock);
+/*
+ * Macro to determine if socwatch telemetry system has been configured
+ */
+#define SW_TELEM_CONFIGURED() (s_unit_idx[0] > 0 || s_unit_idx[1] > 0)
+
+/**
+ * telemetry_available - Determine if telemetry driver is present
+ *
+ * Returns: 1 if telemetry driver is present, 0 if not.
+ */
+static int telemetry_available(void)
+{
+	int retval = 0;
+	struct telemetry_evtconfig punit_evtconfig;
+	struct telemetry_evtconfig pmc_evtconfig;
+	u32 punit_event_map[MAX_TELEM_EVENTS];
+	u32 pmc_event_map[MAX_TELEM_EVENTS];
+
+	/* The symbol below is weak.  We return 1 if we have a definition
+	 * for this telemetry-driver-supplied symbol, or 0 if only the
+	 * weak definition exists. This test will suffice to detect if
+	 * the telemetry driver is loaded.
+	 */
+	if (telemetry_get_eventconfig == NULL) {
+		return 0;
+	}
+	/* OK, the telemetry driver is loaded. But it's possible it
+	 * hasn't been configured properly. To check that, retrieve
+	 * the number of events currently configured. This should never
+	 * be zero since the telemetry driver reserves some SSRAM slots
+	 * for its own use
+	 */
+	memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
+	memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
+
+	punit_evtconfig.evtmap = (u32 *)&punit_event_map;
+	pmc_evtconfig.evtmap = (u32 *)&pmc_event_map;
+
+	retval = telemetry_get_eventconfig(&punit_evtconfig, &pmc_evtconfig,
+					   MAX_TELEM_EVENTS, MAX_TELEM_EVENTS);
+	return retval == 0 && punit_evtconfig.num_evts > 0 &&
+	       pmc_evtconfig.num_evts > 0;
+}
+
+/**
+ * sw_get_instance_row -- Get the address of a 'row' of instance IDs.
+ * @rownum: The row number of the Instance ID table, whose address to return.
+ * Returns: The address of the appropriate row, or NULL if rownum is bad.
+ */
+static unsigned char *sw_get_instance_row_addr(unsigned int rownum)
+{
+	if (rownum >= (sw_telem_rows_alloced - sw_telem_rows_avail)) {
+		pw_pr_error("ERROR: Cannot retrieve row Instance ID row %d\n",
+			    rownum);
+		return NULL;
+	}
+	return &sw_telem_scaled_ids[rownum * sw_max_num_cpus];
+}
+
+/**
+ * sw_free_telem_scaled_id_table - Free the allocated slots.
+ * Returns: Nothing
+ *
+ * Admittedly, a more symmetrical function name would be nice.
+ */
+static void sw_telem_release_scaled_ids(void)
+{
+	sw_telem_rows_alloced = 0;
+	sw_telem_rows_avail = 0;
+	if (sw_telem_scaled_ids) {
+		sw_kfree(sw_telem_scaled_ids);
+	}
+	sw_telem_scaled_ids = NULL;
+}
+
+/**
+ * sw_telem_init_func - Set up the telemetry unit to retrieve a data item
+ *                        (e.g. counter).
+ * @descriptor:  The IO descriptor containing the unit and ID
+ *                        of the telemetry info to gather.
+ *
+ * Because we don't (currently) control all of the counters, we
+ * economize by seeing if it's already being collected before allocate
+ * a slot for it.
+ *
+ * Returns: PW_SUCCESS  if the telem collector can collect the requested data.
+ *         -PW_ERROR   if the the addition of that item fails.
+ */
+int sw_telem_init_func(struct sw_driver_io_descriptor *descriptor)
+{
+	struct sw_driver_telem_io_descriptor *td =
+		&(descriptor->telem_descriptor);
+	u8 unit = td->unit; /* Telemetry unit to use. */
+	u32 id; /* Event ID we want telemetry to track. */
+	size_t idx; /* Index into telemetry data array of event ID to gather. */
+	const char *unit_str = unit == TELEM_PUNIT ? "PUNIT" : "PMC";
+	size_t *unit_idx = &s_unit_idx[unit];
+
+	if (!telemetry_available()) {
+		return -ENXIO;
+	}
+
+	id = (u32)(td->id);
+
+	/* Check if we've already added this ID */
+	for (idx = 0; idx < *unit_idx && idx < MAX_TELEM_EVENTS; ++idx) {
+		if (s_event_map[unit][idx] == id) {
+			/* Invariant: idx contains the index of the new data item. */
+			/* Save the index for later fast lookup. */
+			td->idx = (u16)idx;
+			return 0;
+		}
+	}
+
+	if (*unit_idx >= MAX_TELEM_EVENTS) {
+		pw_pr_error(
+			"Too many events %s units requested; max of %u available!\n",
+			unit_str, MAX_TELEM_EVENTS);
+		return -E2BIG;
+	}
+	s_event_map[unit][(*unit_idx)++] = id;
+	/* Invariant: idx contains the index of the new data item. */
+	/* Save the index for later fast lookup. */
+	td->idx = (u16)idx;
+	pw_pr_debug(
+		"OK, added id = 0x%x to unit %s at entry %zu; retrieved = 0x%x\n",
+		id, unit_str, *unit_idx - 1, s_event_map[unit][*unit_idx - 1]);
+
+	return 0;
+}
+
+/**
+ * sw_read_telem_info - Read a metric's data from the telemetry driver.
+ * @dest:               Destination (storage for the read data)
+ * @cpu:                Which CPU to read from (not used)
+ * @descriptor:         The descriptor containing the data ID to read
+ * @data_size_in_bytes: The # of bytes in the result (always 8)
+ *
+ * Returns: Nothing, but stores SW_TELEM_READ_FAIL_VALUE to dest if the read fails.
+ */
+void sw_read_telem_info(char *dest, int cpu,
+			const sw_driver_io_descriptor_t *descriptor,
+			u16 data_size_in_bytes)
+{
+	int len;
+	u64 *data_dest = (u64 *)dest;
+	int retry_count;
+	const struct sw_driver_telem_io_descriptor *td =
+		&(descriptor->telem_descriptor);
+	unsigned int idx;
+	u8 unit = td->unit;
+	bool needs_refresh = false;
+
+#define TELEM_PKT_SIZE 16 /* sizeof(struct telemetry_evtlog) + padding */
+	static struct telemetry_evtlog events[MAX_TELEM_EVENTS];
+
+	// Get the event index
+	if (IS_SCALED_ID(td)) {
+		unsigned char *scaled_ids;
+
+		scaled_ids = sw_get_instance_row_addr(td->idx);
+		if (scaled_ids == NULL) {
+			pw_pr_error(
+				"Sw_read_telem_info_i: Illegal row index: *%p = %d",
+				&td->idx, td->idx);
+			*data_dest = SW_TELEM_READ_FAIL_VALUE;
+			return; /* Don't set the dest/data buffer. */
+		}
+		idx = scaled_ids[RAW_CPU()]; /* Get per-cpu entry */
+	} else {
+		idx = td->idx;
+	}
+
+	/*
+	 * Check if we need to refresh the list of values
+	 */
+	LOCK(sw_telem_lock);
+	{
+		if (s_unit_iters[unit] == 0) {
+			needs_refresh = true;
+		}
+		if (++s_unit_iters[unit] == s_unit_idx[unit]) {
+			s_unit_iters[unit] = 0;
+		}
+	}
+	UNLOCK(sw_telem_lock);
+
+	/*
+	 * Because of the enormous overhead of reading telemetry data from
+	 * the current kernel driver, failure to read the data is not
+	 * unheard of.  As such, 3 times, should the read fail.  Once we
+	 * get a higher-performance read routine, we should be able to
+	 * eliminate this retry (or maybe decrease it.)
+	 */
+	retry_count = 3;
+	while (needs_refresh && retry_count--) {
+		len = telemetry_raw_read_eventlog(
+			unit, events, sizeof(events) / TELEM_PKT_SIZE);
+
+		if ((len < 0) || (len < idx)) {
+			pw_pr_error(
+				"sw_read_telem_info_i: read failed: len=%d\n",
+				len);
+		} else {
+			break;
+		}
+	}
+
+	if (retry_count) {
+		// TODO: Resolve if we should return something other than
+		//       SW_TELEM_READ_FAIL_VALUE, if the actual data happens to be that.
+		*data_dest = events[idx].telem_evtlog;
+	} else {
+		*data_dest = SW_TELEM_READ_FAIL_VALUE;
+	}
+}
+
+/**
+ * sw_reset_telem - Stop collecting telemetry info.
+ * @descriptor: Unused in this function
+ *
+ * Stop collecting anything extra, and give the driver back to
+ * debugfs.  Because this driver increases the sampling rate, the
+ * kernel's telemetry driver can't successfully reset the driver unless
+ * we first drop the rate back down to a much slower rate.  This is a
+ * temporary measure, since the reset operation will then reset the
+ * sampling interval to whatever the GMIN driver wants.
+ *
+ * Return: PW_SUCCESS.
+ */
+int sw_reset_telem(const struct sw_driver_io_descriptor *descriptor)
+{
+	if (telemetry_available() && SW_TELEM_CONFIGURED()) {
+		telemetry_set_sampling_period(TELEM_SAMPLING_1S,
+					      TELEM_SAMPLING_1S);
+		telemetry_reset_events();
+		sw_telem_release_scaled_ids();
+		memset(s_unit_idx, 0, sizeof(s_unit_idx));
+		memset(s_unit_iters, 0, sizeof(s_unit_iters));
+	}
+	return PW_SUCCESS;
+}
+
+/**
+ * sw_available_telem -- Decide if the telemetry subsystem is available for use
+ */
+bool sw_telem_available(void)
+{
+	return telemetry_available();
+};
+
+bool sw_telem_post_config(void)
+{
+	bool retval = true;
+	size_t i = 0;
+	struct telemetry_evtconfig punit_evtconfig;
+	struct telemetry_evtconfig pmc_evtconfig;
+
+	if (!SW_TELEM_CONFIGURED()) {
+		return true;
+	}
+
+	memset(&punit_evtconfig, 0, sizeof(punit_evtconfig));
+	memset(&pmc_evtconfig, 0, sizeof(pmc_evtconfig));
+
+	telemetry_set_sampling_period(TELEM_SAMPLING_1S, TELEM_SAMPLING_1S);
+
+	punit_evtconfig.period = TELEM_SAMPLING_1S;
+	pmc_evtconfig.period = TELEM_SAMPLING_1S;
+
+	/* Punit */
+	punit_evtconfig.evtmap = (u32 *)&s_event_map[TELEM_PUNIT];
+	punit_evtconfig.num_evts = s_unit_idx[TELEM_PUNIT];
+	/* PMC */
+	pmc_evtconfig.evtmap = (u32 *)&s_event_map[TELEM_PMC];
+	pmc_evtconfig.num_evts = s_unit_idx[TELEM_PMC];
+
+	for (i = 0; i < punit_evtconfig.num_evts; ++i) {
+		pw_pr_debug("PUNIT[%zu] = 0x%x\n", i,
+			    punit_evtconfig.evtmap[i]);
+	}
+	for (i = 0; i < pmc_evtconfig.num_evts; ++i) {
+		pw_pr_debug("PMC[%zu] = 0x%x\n", i, pmc_evtconfig.evtmap[i]);
+	}
+
+	/*
+	 * OK, everything done. Now update
+	 */
+	if (telemetry_update_events(punit_evtconfig, pmc_evtconfig)) {
+		pw_pr_error("telemetry_update_events error");
+		retval = false;
+	} else {
+		pw_pr_debug("OK, telemetry_update_events success\n");
+	}
+
+	telemetry_set_sampling_period(TELEM_SAMPLING_1MS, TELEM_SAMPLING_1MS);
+
+	return retval;
+}
diff --git a/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c b/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
new file mode 100644
index 000000000000..0c414423de09
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_trace_notifier_provider.c
@@ -0,0 +1,2233 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include <linux/version.h> // "LINUX_VERSION_CODE"
+#include <linux/hrtimer.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 11, 0)
+#include <asm/cputime.h>
+#else
+#include <linux/sched/cputime.h>
+#endif
+#include <asm/hardirq.h>
+#include <asm/local.h>
+
+#include <trace/events/power.h>
+#include <trace/events/irq.h>
+#include <trace/events/timer.h>
+#include <trace/events/power.h>
+#include <trace/events/sched.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+#include <asm/trace/irq_vectors.h> // for the various APIC vector tracepoints (e.g. "thermal_apic", "local_timer" etc.)
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+struct pool_workqueue; // Forward declaration to avoid compiler warnings
+struct cpu_workqueue_struct; // Forward declaration to avoid compiler warnings
+#include <trace/events/workqueue.h>
+#include <linux/suspend.h> // for 'pm_notifier'
+#include <linux/cpufreq.h> // for "cpufreq_notifier"
+#include <linux/cpu.h> // for 'CPU_UP_PREPARE' etc
+
+#include "sw_kernel_defines.h"
+#include "sw_collector.h"
+#include "sw_overhead_measurements.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_output_buffer.h"
+#include "sw_mem.h"
+#include "sw_trace_notifier_provider.h"
+
+/* -------------------------------------------------
+ * Compile time constants and useful macros.
+ * -------------------------------------------------
+ */
+#ifndef __get_cpu_var
+/*
+     * Kernels >= 3.19 don't include a definition
+     * of '__get_cpu_var'. Create one now.
+     */
+#define __get_cpu_var(var) (*this_cpu_ptr(&var))
+#endif // __get_cpu_var
+
+#define BEGIN_LOCAL_IRQ_STATS_READ(p)                                          \
+	do {                                                                   \
+		p = &__get_cpu_var(irq_stat);
+
+#define END_LOCAL_IRQ_STATS_READ(p)                                            \
+	}                                                                      \
+	while (0)
+/*
+ * CAS{32,64}
+ */
+#define CAS32(p, o, n) (cmpxchg((p), (o), (n)) == (o))
+#define CAS64(p, o, n) (cmpxchg64((p), (o), (n)) == (o))
+/*
+ * Timer start pid accessor macros
+ */
+#ifdef CONFIG_TIMER_STATS
+#define GET_TIMER_THREAD_ID(t)                                                 \
+	((t)->start_pid) /* 'start_pid' is actually the thread ID of the thread that initialized the timer */
+#else
+#define GET_TIMER_THREAD_ID(t) (-1)
+#endif // CONFIG_TIMER_STATS
+/*
+ * Tracepoint probe register/unregister functions and
+ * helper macros.
+ */
+#ifdef CONFIG_TRACEPOINTS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 35)
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(register_trace_##name(probe))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	unregister_trace_##name(probe)
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(3, 15, 0)
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(register_trace_##name(probe, NULL))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	unregister_trace_##name(probe, NULL)
+#else
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                     \
+	WARN_ON(tracepoint_probe_register(node->tp, probe, NULL))
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, name, probe)                   \
+	tracepoint_probe_unregister(node->tp, probe, NULL)
+#endif
+#else // CONFIG_TRACEPOINTS
+#define DO_REGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+#define DO_UNREGISTER_SW_TRACEPOINT_PROBE(...) /* NOP */
+#endif // CONFIG_TRACEPOINTS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 35)
+#define _DEFINE_PROBE_FUNCTION(name, ...) static void name(__VA_ARGS__)
+#else
+#define _DEFINE_PROBE_FUNCTION(name, ...)                                      \
+	static void name(void *ignore, __VA_ARGS__)
+#endif
+#define DEFINE_PROBE_FUNCTION(x) _DEFINE_PROBE_FUNCTION(x)
+
+/*
+ * Tracepoint probe function parameters.
+ * These tracepoint signatures depend on kernel version.
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 36)
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_power_start_i, unsigned int type, unsigned int state
+#elif LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_power_start_i, unsigned int type, unsigned int state,         \
+		unsigned int cpu_id
+#else
+#define PROBE_TPS_PARAMS                                                       \
+	sw_probe_cpu_idle_i, unsigned int state, unsigned int cpu_id
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+#define PROBE_TPF_PARAMS                                                       \
+	sw_probe_power_frequency_i, unsigned int type, unsigned int state
+#else
+#define PROBE_TPF_PARAMS                                                       \
+	sw_probe_cpu_frequency_i, unsigned int new_freq, unsigned int cpu
+#endif
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 35)
+#define PROBE_SCHED_WAKEUP_PARAMS                                              \
+	sw_probe_sched_wakeup_i, struct rq *rq, struct task_struct *task,      \
+		int success
+#else
+#define PROBE_SCHED_WAKEUP_PARAMS                                              \
+	sw_probe_sched_wakeup_i, struct task_struct *task, int success
+#endif
+
+#if IS_ENABLED(CONFIG_ANDROID)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+#define PROBE_WAKE_LOCK_PARAMS sw_probe_wake_lock_i, struct wake_lock *lock
+#define PROBE_WAKE_UNLOCK_PARAMS                                               \
+	sw_probe_wake_unlock_i, struct wake_unlock *unlock
+#else
+#define PROBE_WAKE_LOCK_PARAMS                                                 \
+	sw_probe_wakeup_source_activate_i, const char *name, unsigned int state
+#define PROBE_WAKE_UNLOCK_PARAMS                                               \
+	sw_probe_wakeup_source_deactivate_i, const char *name,                 \
+		unsigned int state
+#endif // version
+#endif // CONFIG_ANDROID
+
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 35)
+#define PROBE_WORKQUEUE_PARAMS                                                 \
+	sw_probe_workqueue_execution_i, struct task_struct *wq_thread,         \
+		struct work_struct *work
+#else
+#define PROBE_WORKQUEUE_PARAMS                                                 \
+	sw_probe_workqueue_execute_start_i, struct work_struct *work
+#endif
+
+#define PROBE_SCHED_SWITCH_PARAMS                                              \
+	sw_probe_sched_switch_i, struct task_struct *prev,                     \
+		struct task_struct *next
+/*
+ * These tracepoint signatures are independent of kernel version.
+ */
+#define PROBE_IRQ_PARAMS                                                       \
+	sw_probe_irq_handler_entry_i, int irq, struct irqaction *action
+#define PROBE_TIMER_ARGS sw_probe_timer_expire_entry_i, struct timer_list *t
+#define PROBE_HRTIMER_PARAMS                                                   \
+	sw_probe_hrtimer_expire_entry_i, struct hrtimer *hrt, ktime_t *now
+#define PROBE_PROCESS_FORK_PARAMS                                              \
+	sw_probe_sched_process_fork_i, struct task_struct *parent,             \
+		struct task_struct *child
+#define PROBE_SCHED_PROCESS_EXIT_PARAMS                                        \
+	sw_probe_sched_process_exit_i, struct task_struct *task
+#define PROBE_THERMAL_APIC_ENTRY_PARAMS                                        \
+	sw_probe_thermal_apic_entry_i, int vector
+#define PROBE_THERMAL_APIC_EXIT_PARAMS sw_probe_thermal_apic_exit_i, int vector
+
+#define IS_VALID_WAKEUP_EVENT(cpu)                                             \
+	({                                                                     \
+		bool *per_cpu_event =                                          \
+			&per_cpu(sw_is_valid_wakeup_event, (cpu));             \
+		bool old_value =                                               \
+			CAS32(per_cpu_event, true, sw_wakeup_event_flag);      \
+		old_value;                                                     \
+	})
+#define SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu) (IS_VALID_WAKEUP_EVENT(cpu))
+#define RESET_VALID_WAKEUP_EVENT_COUNTER(cpu)                                  \
+	(per_cpu(sw_is_valid_wakeup_event, (cpu)) = true)
+
+#define NUM_TRACEPOINT_NODES SW_ARRAY_SIZE(s_trace_collector_lists)
+#define NUM_VALID_TRACEPOINTS (NUM_TRACEPOINT_NODES - 1) /* "-1" for IPI */
+#define FOR_EACH_TRACEPOINT_NODE(idx, node)                                    \
+	for (idx = 0; idx < NUM_TRACEPOINT_NODES &&                            \
+		      (node = &s_trace_collector_lists[idx]);                  \
+	     ++idx)
+
+#define FOR_EACH_NOTIFIER_NODE(idx, node)                                      \
+	for (idx = 0; idx < SW_ARRAY_SIZE(s_notifier_collector_lists) &&       \
+		      (node = &s_notifier_collector_lists[idx]);               \
+	     ++idx)
+/*
+ * Use these macros if all tracepoint ID numbers ARE contiguous from 0 -- max tracepoint ID #
+ */
+#if 0
+#define IS_VALID_TRACE_NOTIFIER_ID(id)                                         \
+	((id) >= 0 && (id) < SW_ARRAY_SIZE(s_trace_collector_lists))
+#define GET_COLLECTOR_TRACE_NODE(id) (&s_trace_collector_lists[id])
+#define FOR_EACH_trace_notifier_id(idx)                                        \
+	for (idx = 0; idx < SW_ARRAY_SIZE(s_trace_collector_lists); ++idx)
+#endif // if 0
+/*
+ * Use these macros if all tracepoint ID numbers are NOT contiguous from 0 -- max tracepoint ID #
+ */
+#define GET_COLLECTOR_TRACE_NODE(idx)                                          \
+	({                                                                     \
+		int __idx = 0;                                                 \
+		struct sw_trace_notifier_data *__node = NULL,                  \
+					      *__retVal = NULL;                \
+		FOR_EACH_TRACEPOINT_NODE(__idx, __node)                        \
+		{                                                              \
+			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
+				__retVal = __node;                             \
+				break;                                         \
+			}                                                      \
+		}                                                              \
+		__retVal;                                                      \
+	})
+#define IS_VALID_TRACE_NOTIFIER_ID(idx) (GET_COLLECTOR_TRACE_NODE(idx) != NULL)
+
+#define GET_COLLECTOR_NOTIFIER_NODE(idx)                                       \
+	({                                                                     \
+		int __idx = 0;                                                 \
+		struct sw_trace_notifier_data *__node = NULL,                  \
+					      *__retVal = NULL;                \
+		FOR_EACH_NOTIFIER_NODE(__idx, __node)                          \
+		{                                                              \
+			if ((idx) == GET_TRACE_NOTIFIER_ID(__node)) {          \
+				__retVal = __node;                             \
+				break;                                         \
+			}                                                      \
+		}                                                              \
+		__retVal;                                                      \
+	})
+#define IS_VALID_NOTIFIER_ID(idx) (GET_COLLECTOR_NOTIFIER_NODE(idx) != NULL)
+
+/* -------------------------------------------------
+ * Local function declarations.
+ * -------------------------------------------------
+ */
+/*
+ * The tracepoint registration functions.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_timer_expire_entry_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_hrtimer_expire_entry_i(
+	struct sw_trace_notifier_data *node);
+int sw_unregister_trace_hrtimer_expire_entry_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_fork_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_process_exit_i(
+	struct sw_trace_notifier_data *node);
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_thermal_apic_entry_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node);
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+#if IS_ENABLED(CONFIG_ANDROID)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node);
+int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node);
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+int sw_register_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node);
+int sw_unregister_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node);
+int sw_unregister_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+#endif // CONFIG_ANDROID
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_workqueue_execution_i(
+	struct sw_trace_notifier_data *node);
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node);
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node);
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu,
+			      int target_cpu);
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr,
+				     struct sw_trace_notifier_data *node,
+				     pid_t tid);
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node);
+void sw_handle_workqueue_wakeup_helper_i(int cpu,
+					 struct sw_collector_data *node);
+void sw_handle_sched_switch_helper_i(void);
+void sw_tps_apic_i(int cpu);
+void sw_tps_tps_i(int cpu);
+void sw_tps_wakeup_i(int cpu);
+void sw_tps_i(void);
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node);
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node,
+				   struct task_struct *task, bool is_fork);
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node,
+			       const char *name, int type, u64 timeout, int pid,
+			       int tid, const char *proc_name);
+u64 sw_my_local_arch_irq_stats_cpu_i(void);
+
+/*
+ * The tracepoint probes.
+ */
+/*
+ * The tracepoint handlers.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node);
+void sw_handle_trace_notifier_on_cpu_i(int cpu,
+				       struct sw_trace_notifier_data *node);
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node);
+
+/* -------------------------------------------------
+ * Variable definitions.
+ * -------------------------------------------------
+ */
+/*
+ * For overhead measurements.
+ */
+DECLARE_OVERHEAD_VARS(
+	sw_handle_timer_wakeup_helper_i); // for the "timer_expire" family of probes
+DECLARE_OVERHEAD_VARS(sw_handle_irq_wakeup_i); // for IRQ wakeups
+DECLARE_OVERHEAD_VARS(sw_handle_sched_wakeup_i); // for SCHED
+DECLARE_OVERHEAD_VARS(sw_tps_i); // for TPS
+DECLARE_OVERHEAD_VARS(sw_tpf_i); // for TPF
+DECLARE_OVERHEAD_VARS(sw_process_fork_exit_helper_i);
+#if IS_ENABLED(CONFIG_ANDROID)
+DECLARE_OVERHEAD_VARS(sw_handle_wakelock_i); // for wake lock/unlock
+#endif // CONFIG_ANDROID
+DECLARE_OVERHEAD_VARS(sw_handle_workqueue_wakeup_helper_i);
+DECLARE_OVERHEAD_VARS(sw_handle_sched_switch_helper_i);
+/*
+ * Per-cpu wakeup counters.
+ * Used to decide which wakeup event is the first to occur after a
+ * core wakes up from a C-state.
+ * Set to 'true' in TPS probe
+ */
+static DEFINE_PER_CPU(bool, sw_is_valid_wakeup_event) = { true };
+/*
+ * Per-cpu counts of the number of times the local APIC fired.
+ * We need a separate count because some apic timer fires don't seem
+ * to result in hrtimer/timer expires
+ */
+static DEFINE_PER_CPU(u64, sw_num_local_apic_timer_inters);
+/*
+ * Flag value to use to decide if the event is a valid wakeup event.
+ * Set to 'false' in TPS probe.
+ */
+static bool sw_wakeup_event_flag = true;
+/*
+ * Scheduler-based polling emulation.
+ */
+static DEFINE_PER_CPU(unsigned long, sw_pcpu_polling_jiff);
+pw_u16_t sw_min_polling_interval_msecs;
+
+/*
+ * IDs for supported tracepoints.
+ */
+enum sw_trace_id {
+	SW_TRACE_ID_CPU_IDLE,
+	SW_TRACE_ID_CPU_FREQUENCY,
+	SW_TRACE_ID_IRQ_HANDLER_ENTRY,
+	SW_TRACE_ID_TIMER_EXPIRE_ENTRY,
+	SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY,
+	SW_TRACE_ID_SCHED_WAKEUP,
+	SW_TRACE_ID_IPI,
+	SW_TRACE_ID_SCHED_PROCESS_FORK,
+	SW_TRACE_ID_SCHED_PROCESS_EXIT,
+	SW_TRACE_ID_THERMAL_APIC_ENTRY,
+	SW_TRACE_ID_THERMAL_APIC_EXIT,
+	SW_TRACE_ID_WAKE_LOCK,
+	SW_TRACE_ID_WAKE_UNLOCK,
+	SW_TRACE_ID_WORKQUEUE_EXECUTE_START,
+	SW_TRACE_ID_SCHED_SWITCH,
+};
+/*
+ * IDs for supported notifiers.
+ */
+enum sw_notifier_id {
+	SW_NOTIFIER_ID_SUSPEND, // TODO: change name?
+	SW_NOTIFIER_ID_SUSPEND_ENTER,
+	SW_NOTIFIER_ID_SUSPEND_EXIT,
+	SW_NOTIFIER_ID_HIBERNATE,
+	SW_NOTIFIER_ID_HIBERNATE_ENTER,
+	SW_NOTIFIER_ID_HIBERNATE_EXIT,
+	SW_NOTIFIER_ID_COUNTER_RESET,
+	SW_NOTIFIER_ID_CPUFREQ,
+	SW_NOTIFIER_ID_HOTCPU,
+};
+/*
+ * Names for supported tracepoints. A tracepoint
+ * 'name' consists of two strings: a "kernel" string
+ * that is used to locate the tracepoint within the kernel
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which tracepoints to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_trace_names[] = {
+	[SW_TRACE_ID_CPU_IDLE] = { "cpu_idle", "CPU-IDLE" },
+	[SW_TRACE_ID_CPU_FREQUENCY] = { "cpu_frequency", "CPU-FREQUENCY" },
+	[SW_TRACE_ID_IRQ_HANDLER_ENTRY] = { "irq_handler_entry", "IRQ-ENTRY" },
+	[SW_TRACE_ID_TIMER_EXPIRE_ENTRY] = { "timer_expire_entry",
+					     "TIMER-ENTRY" },
+	[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY] = { "hrtimer_expire_entry",
+					       "HRTIMER-ENTRY" },
+	[SW_TRACE_ID_SCHED_WAKEUP] = { "sched_wakeup", "SCHED-WAKEUP" },
+	[SW_TRACE_ID_IPI] = { NULL, "IPI" },
+	[SW_TRACE_ID_SCHED_PROCESS_FORK] = { "sched_process_fork",
+					     "PROCESS-FORK" },
+	[SW_TRACE_ID_SCHED_PROCESS_EXIT] = { "sched_process_exit",
+					     "PROCESS-EXIT" },
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+	[SW_TRACE_ID_THERMAL_APIC_ENTRY] = { "thermal_apic_entry",
+					     "THERMAL-THROTTLE-ENTRY" },
+	[SW_TRACE_ID_THERMAL_APIC_EXIT] = { "thermal_apic_exit",
+					    "THERMAL-THROTTLE-EXIT" },
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+#if IS_ENABLED(CONFIG_ANDROID)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+	[SW_TRACE_ID_WAKE_LOCK] = { "wake_lock", "WAKE-LOCK" },
+	[SW_TRACE_ID_WAKE_UNLOCK] = { "wake_unlock", "WAKE-UNLOCK" },
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+	[SW_TRACE_ID_WAKE_LOCK] = { "wakeup_source_activate", "WAKE-LOCK" },
+	[SW_TRACE_ID_WAKE_UNLOCK] = { "wakeup_source_deactivate",
+				      "WAKE-UNLOCK" },
+#endif
+#endif
+	[SW_TRACE_ID_WORKQUEUE_EXECUTE_START] = { "workqueue_execute_start",
+						  "WORKQUEUE-START" },
+	[SW_TRACE_ID_SCHED_SWITCH] = { "sched_switch", "CONTEXT-SWITCH" },
+};
+
+/*
+ * Names for supported notifiers. A notifier
+ * 'name' consists of two strings: an unused "kernel" string
+ * and an "abstract" string, that is used by Ring-3 to
+ * specify which notifiers to use during a collection.
+ */
+static const struct sw_trace_notifier_name s_notifier_names[] = {
+	[SW_NOTIFIER_ID_SUSPEND] = { "suspend_notifier" /* don't care */,
+				     "SUSPEND-NOTIFIER" },
+	[SW_NOTIFIER_ID_SUSPEND_ENTER] = { NULL, "SUSPEND-ENTER" },
+	[SW_NOTIFIER_ID_SUSPEND_EXIT] = { NULL, "SUSPEND-EXIT" },
+	[SW_NOTIFIER_ID_HIBERNATE] = { "hibernate_notifier" /* don't care */,
+				       "HIBERNATE-NOTIFIER" },
+	[SW_NOTIFIER_ID_HIBERNATE_ENTER] = { NULL, "HIBERNATE-ENTER" },
+	[SW_NOTIFIER_ID_HIBERNATE_EXIT] = { NULL, "HIBERNATE-EXIT" },
+	[SW_NOTIFIER_ID_COUNTER_RESET] = { NULL, "COUNTER-RESET" },
+	[SW_NOTIFIER_ID_CPUFREQ] = { "cpufreq_notifier" /* don't care */,
+				     "CPUFREQ-NOTIFIER" },
+	[SW_NOTIFIER_ID_HOTCPU] = { "hotcpu_notifier" /* don't care */,
+				    "HOTCPU-NOTIFIER" },
+};
+
+#ifdef CONFIG_TRACEPOINTS
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = {
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_CPU_IDLE],
+	  &sw_register_trace_cpu_idle_i, &sw_unregister_trace_cpu_idle_i,
+	  NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_CPU_FREQUENCY],
+	  &sw_register_trace_cpu_frequency_i,
+	  &sw_unregister_trace_cpu_frequency_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_IRQ_HANDLER_ENTRY],
+	  &sw_register_trace_irq_handler_entry_i,
+	  &sw_unregister_trace_irq_handler_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_TIMER_EXPIRE_ENTRY],
+	  &sw_register_trace_timer_expire_entry_i,
+	  &sw_unregister_trace_timer_expire_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY],
+	  &sw_register_trace_hrtimer_expire_entry_i,
+	  &sw_unregister_trace_hrtimer_expire_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_WAKEUP],
+	  &sw_register_trace_sched_wakeup_i,
+	  &sw_unregister_trace_sched_wakeup_i, NULL },
+	/* Placeholder for IPI -- no tracepoints associated with it! */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_IPI], NULL,
+	  NULL, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_FORK],
+	  &sw_register_trace_sched_process_fork_i,
+	  &sw_unregister_trace_sched_process_fork_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_PROCESS_EXIT],
+	  &sw_register_trace_sched_process_exit_i,
+	  &sw_unregister_trace_sched_process_exit_i, NULL },
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+	/*
+	 * For thermal throttling.
+	 * We probably only need one of either 'entry' or 'exit'. Use
+	 * both, until we decide which one to keep. Note that
+	 * tracepoint IDs for these, and subsequent tracepoints
+	 * (e.g. 'wake_lock') will change once we've picked which
+	 * one to use.
+	 */
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_ENTRY],
+	  &sw_register_trace_thermal_apic_entry_i,
+	  &sw_unregister_trace_thermal_apic_entry_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_THERMAL_APIC_EXIT],
+	  &sw_register_trace_thermal_apic_exit_i,
+	  &sw_unregister_trace_thermal_apic_exit_i, NULL },
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+/* Wakelocks have multiple tracepoints, depending on kernel version */
+#if IS_ENABLED(CONFIG_ANDROID)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
+	  &sw_register_trace_wake_lock_i, &sw_unregister_trace_wake_lock_i,
+	  NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
+	  &sw_register_trace_wake_unlock_i, &sw_unregister_trace_wake_unlock_i,
+	  NULL },
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+	{ SW_TRACE_COLLECTOR_TRACEPOINT, &s_trace_names[SW_TRACE_ID_WAKE_LOCK],
+	  &sw_register_trace_wakeup_source_activate_i,
+	  &sw_unregister_trace_wakeup_source_activate_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WAKE_UNLOCK],
+	  &sw_register_trace_wakeup_source_deactivate_i,
+	  &sw_unregister_trace_wakeup_source_deactivate_i, NULL },
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,14,0)
+#endif // CONFIG_ANDROID
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_WORKQUEUE_EXECUTE_START],
+	  &sw_register_trace_workqueue_execution_i,
+	  &sw_unregister_trace_workqueue_execution_i, NULL },
+	{ SW_TRACE_COLLECTOR_TRACEPOINT,
+	  &s_trace_names[SW_TRACE_ID_SCHED_SWITCH],
+	  &sw_register_trace_sched_switch_i,
+	  &sw_unregister_trace_sched_switch_i, NULL },
+};
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND], &sw_register_pm_notifier_i,
+	  &sw_unregister_pm_notifier_i, NULL, true /* always register */ },
+	/* Placeholder for suspend enter/exit -- these will be called
+	   from within the pm notifier */
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_ENTER], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_SUSPEND_EXIT], NULL, NULL, NULL },
+	/* Placeholder for hibernate enter/exit -- these will be called
+	   from within the pm notifier */
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_ENTER], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_HIBERNATE_EXIT], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_COUNTER_RESET], NULL, NULL, NULL },
+	{ SW_TRACE_COLLECTOR_NOTIFIER,
+	  &s_notifier_names[SW_NOTIFIER_ID_CPUFREQ],
+	  &sw_register_cpufreq_notifier_i, &sw_unregister_cpufreq_notifier_i },
+};
+/*
+ * Special entry for CPU notifier (i.e. "hotplug" notifier)
+ * We don't want these to be visible to the user.
+ */
+static struct sw_trace_notifier_data s_hotplug_notifier_data = {
+	SW_TRACE_COLLECTOR_NOTIFIER,
+	&s_notifier_names[SW_NOTIFIER_ID_HOTCPU],
+	&sw_register_hotcpu_notifier_i,
+	&sw_unregister_hotcpu_notifier_i,
+	NULL,
+	true /* always register */
+};
+
+#else // !CONFIG_TRACEPOINTS
+/*
+ * A list of supported tracepoints.
+ */
+static struct sw_trace_notifier_data s_trace_collector_lists[] = {
+	/* EMPTY */};
+/*
+ * List of supported notifiers.
+ */
+static struct sw_trace_notifier_data s_notifier_collector_lists[] = {
+	/* EMPTY */ };
+
+#endif // CONFIG_TRACEPOINTS
+
+/*
+ * Macros to retrieve tracepoint and notifier IDs.
+ */
+#define GET_TRACE_ID_FROM_NODE(node) ((node)->name - s_trace_names)
+#define GET_NOTIFIER_ID_FROM_NODE(node) ((node)->name - s_notifier_names)
+
+#define GET_TRACE_NOTIFIER_ID(node)                                            \
+	(int)(((node)->type == SW_TRACE_COLLECTOR_TRACEPOINT) ?                \
+		      GET_TRACE_ID_FROM_NODE(node) :                           \
+		      GET_NOTIFIER_ID_FROM_NODE(node))
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+/*
+ * Retrieve a TSC value
+ */
+static inline u64 sw_tscval(void)
+{
+	unsigned int low, high;
+
+	asm volatile("rdtsc" : "=a"(low), "=d"(high));
+	return low | ((unsigned long long)high) << 32;
+};
+u64 sw_timestamp(void)
+{
+	struct timespec ts;
+
+	getnstimeofday(&ts);
+	return (ts.tv_sec * 1000000000ULL + ts.tv_nsec);
+}
+/*
+ * Basically the same as arch/x86/kernel/irq.c --> "arch_irq_stat_cpu(cpu)"
+ */
+u64 sw_my_local_arch_irq_stats_cpu_i(void)
+{
+	u64 sum = 0;
+	irq_cpustat_t *stats;
+#ifdef __arm__
+	int i = 0;
+#endif
+	BEGIN_LOCAL_IRQ_STATS_READ(stats);
+	{
+#ifndef __arm__
+		sum += stats->__nmi_count;
+		// #ifdef CONFIG_X86_LOCAL_APIC
+		sum += stats->apic_timer_irqs;
+// #endif
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 34)
+		sum += stats->x86_platform_ipis;
+#endif // 2,6,34
+		sum += stats->apic_perf_irqs;
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 5, 0)
+		sum += stats->apic_irq_work_irqs;
+#endif // 3,5,0
+#ifdef CONFIG_SMP
+		sum += stats->irq_call_count;
+		sum += stats->irq_resched_count;
+		sum += stats->irq_tlb_count;
+#endif
+#ifdef CONFIG_X86_THERMAL_VECTOR
+		sum += stats->irq_thermal_count;
+#endif
+		sum += stats->irq_spurious_count; // should NEVER be non-zero!!!
+#else
+		sum += stats->__softirq_pending;
+#ifdef CONFIG_SMP
+		for (i = 0; i < NR_IPI; ++i) {
+			sum += stats->ipi_irqs[i];
+		}
+#endif
+#ifdef CONFIG_X86_MCE
+		sum += stats->mce_exception_count;
+		sum += stats->mce_poll_count;
+#endif
+#endif
+	}
+	END_LOCAL_IRQ_STATS_READ(stats);
+	return sum;
+};
+
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_i(struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		pw_pr_debug("DEBUG: handling message\n");
+		sw_handle_per_cpu_msg(curr);
+	}
+};
+/*
+ * Generic tracepoint/notifier handling function.
+ */
+void sw_handle_trace_notifier_on_cpu_i(int cpu,
+				       struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		sw_handle_per_cpu_msg_on_cpu(cpu, curr);
+	}
+};
+void sw_handle_reset_messages_i(struct sw_trace_notifier_data *node)
+{
+	struct sw_collector_data *curr = NULL;
+
+	if (!node) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		pw_pr_debug("Handling message of unknown cpumask on cpu %d\n",
+			    RAW_CPU());
+		sw_schedule_work(&curr->cpumask, &sw_handle_per_cpu_msg, curr);
+	}
+}
+/*
+ * Tracepoint helpers.
+ */
+/*
+ * IRQ wakeup handling function.
+ */
+static void sw_handle_irq_wakeup_i(struct sw_collector_data *node, int irq)
+{
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	// char *dst_vals = (char *)(unsigned long)msg->p_payload;
+	char *dst_vals = msg->p_payload;
+
+	// msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+	msg->cpuidx = cpu;
+
+	/*
+	 * IRQ handling ==> only return the irq number
+	 */
+	*((int *)dst_vals) = irq;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+};
+/*
+ * TIMER wakeup handling function.
+ */
+static void sw_handle_timer_wakeup_i(struct sw_collector_data *node, pid_t pid,
+			      pid_t tid)
+{
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	// char *dst_vals = (char *)(unsigned long)msg->p_payload;
+	char *dst_vals = msg->p_payload;
+
+	// msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+	msg->cpuidx = cpu;
+
+	/*
+	 * TIMER handling ==> only return the pid, tid
+	 */
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+	pw_pr_debug("HANDLED timer expire for %d, %d\n", pid, tid);
+};
+/*
+ * Helper function for {hr}timer expires. Required for overhead tracking.
+ */
+void sw_handle_timer_wakeup_helper_i(struct sw_collector_data *curr,
+				     struct sw_trace_notifier_data *node,
+				     pid_t tid)
+{
+	pid_t pid = -1;
+
+	if (tid == 0) {
+		pid = 0;
+	} else {
+		struct task_struct *task =
+			pid_task(find_pid_ns(tid, &init_pid_ns), PIDTYPE_PID);
+		if (likely(task)) {
+			pid = task->tgid;
+		}
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		sw_handle_timer_wakeup_i(curr, pid, tid);
+	}
+};
+/*
+ * SCHED wakeup handling function.
+ */
+void sw_handle_sched_wakeup_i(struct sw_collector_data *node, int source_cpu,
+			      int target_cpu)
+{
+	int cpu = source_cpu;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	// char *dst_vals = (char *)(unsigned long)msg->p_payload;
+	char *dst_vals = msg->p_payload;
+
+	// msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+	msg->cpuidx = source_cpu;
+
+	/*
+	 * sched handling ==> only return the source, target CPUs
+	 */
+	*((int *)dst_vals) = source_cpu;
+	dst_vals += sizeof(source_cpu);
+	*((int *)dst_vals) = target_cpu;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_NONE)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+};
+/*
+ * APIC timer wakeup
+ */
+void sw_handle_apic_timer_wakeup_i(struct sw_collector_data *node)
+{
+	/*
+	 * Send an empty message back to Ring-3
+	 */
+	int cpu = RAW_CPU();
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	// char *dst_vals = (char *)(unsigned long)msg->p_payload;
+
+	// msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+	msg->cpuidx = cpu;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+	pw_pr_debug("HANDLED APIC timer wakeup for cpu = %d\n", cpu);
+};
+/*
+ * Helper function for workqueue executions. Required for overhead tracking.
+ */
+void sw_handle_workqueue_wakeup_helper_i(int cpu,
+					 struct sw_collector_data *node)
+{
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+
+	// msg->tsc = sw_timestamp(); // msg TSC assigned when msg is written to buffer
+	msg->cpuidx = cpu;
+
+	/*
+	 * Workqueue wakeup ==> empty message.
+	 */
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_error("WARNING: could NOT produce message!\n");
+	}
+};
+/*
+ * Helper function for sched_switch. Required for overhead tracking.
+ */
+void sw_handle_sched_switch_helper_i(void)
+{
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_SWITCH);
+		pw_pr_debug("SCHED SWITCH NODE = %p\n", node);
+	}
+	if (!node) {
+		return;
+	}
+	preempt_disable();
+	{
+		struct sw_collector_data *curr;
+
+		list_for_each_entry(curr, &node->list, list) {
+			unsigned long curr_jiff = jiffies,
+				      prev_jiff = curr->last_update_jiffies;
+			unsigned long delta_msecs =
+				jiffies_to_msecs(curr_jiff) -
+				jiffies_to_msecs(prev_jiff);
+			struct cpumask *mask = &curr->cpumask;
+			u16 timeout = curr->info->sampling_interval_msec;
+
+			if (!timeout) {
+				timeout = sw_min_polling_interval_msecs;
+			}
+			/* Has there been enough time since the last
+			   collection point? */
+			if (delta_msecs < timeout) {
+				continue;
+			}
+			/* Update timestamp and handle message */
+			if (cpumask_test_cpu(
+				    RAW_CPU(),
+				    mask) /* This msg must be handled on
+					     the current CPU */
+			    ||
+			    cpumask_empty(
+				    mask) /* This msg may be handled by
+					     any CPU */) {
+				if (!CAS64(&curr->last_update_jiffies,
+					   prev_jiff, curr_jiff)) {
+					/*
+					 * CAS failure should only be possible
+					 * for messages that can be handled
+					 * on any CPU, in which case it
+					 * indicates a different CPU already
+					 * handled this message.
+					 */
+					continue;
+				}
+				sw_handle_per_cpu_msg_no_sched(curr);
+			}
+		}
+	}
+	preempt_enable();
+};
+
+/*
+ * Probe functions.
+ */
+/*
+ * 1. TPS
+ */
+/*
+ * Check IPI wakeups within the cpu_idle tracepoint.
+ */
+void sw_tps_apic_i(int cpu)
+{
+	static struct sw_trace_notifier_data *apic_timer_node;
+
+	if (unlikely(apic_timer_node == NULL)) {
+		apic_timer_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IPI);
+		pw_pr_debug("apic NODE = %p\n", apic_timer_node);
+	}
+	if (apic_timer_node) {
+		bool local_apic_timer_fired = false;
+		u64 curr_num_local_apic = sw_my_local_arch_irq_stats_cpu_i();
+		u64 *old_num_local_apic =
+			&__get_cpu_var(sw_num_local_apic_timer_inters);
+
+		if (*old_num_local_apic &&
+		    (*old_num_local_apic != curr_num_local_apic)) {
+			local_apic_timer_fired = true;
+		}
+		*old_num_local_apic = curr_num_local_apic;
+
+		if (local_apic_timer_fired &&
+		    SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+			struct sw_collector_data *curr = NULL;
+			list_for_each_entry(curr, &apic_timer_node->list,
+					     list) {
+				sw_handle_apic_timer_wakeup_i(curr);
+			}
+		}
+	}
+};
+/*
+ * Perform any user-defined tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_tps_i(int cpu)
+{
+	static struct sw_trace_notifier_data *tps_node;
+
+	if (unlikely(tps_node == NULL)) {
+		tps_node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_IDLE);
+		pw_pr_debug("TPS NODE = %p\n", tps_node);
+	}
+	sw_handle_trace_notifier_i(tps_node);
+};
+/*
+ * Perform any wakeup-related tasks within the
+ * cpu_idle tracepoint.
+ */
+void sw_tps_wakeup_i(int cpu)
+{
+	/*
+	 * For now, assume we will always have to
+	 * do some wakeup book keeping. Later, we'll
+	 * need to detect if the user requested wakeups.
+	 */
+	sw_wakeup_event_flag = false;
+	RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+};
+void sw_tps_i(void)
+{
+	/*
+	 * Update: FIRST handle IPI wakeups
+	 * THEN handle TPS
+	 */
+	int cpu = RAW_CPU();
+
+	sw_tps_apic_i(cpu);
+	sw_tps_tps_i(cpu);
+	sw_tps_wakeup_i(cpu);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_TPS_PARAMS)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 38)
+	if (state == PWR_EVENT_EXIT) {
+		return;
+	}
+#endif
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tps_i);
+};
+
+/*
+ * 2. TPF
+ */
+/*
+ * Helper function for overhead measurements.
+ */
+void sw_tpf_i(int cpu, struct sw_trace_notifier_data *node)
+{
+	sw_handle_trace_notifier_on_cpu_i((int)cpu, node);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_TPF_PARAMS)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+	int cpu = RAW_CPU();
+#endif // version < 2.6.38
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_CPU_FREQUENCY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+
+/*
+ * 3. IRQ handler entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_IRQ_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_IRQ_HANDLER_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_irq_wakeup_i, curr, irq);
+	}
+};
+/*
+ * 4. TIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_TIMER_ARGS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	struct sw_collector_data *curr = NULL;
+	pid_t tid = GET_TIMER_THREAD_ID(t);
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_TIMER_EXPIRE_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+		return;
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
+				 tid);
+};
+/*
+ * 5. HRTIMER expire
+ */
+DEFINE_PROBE_FUNCTION(PROBE_HRTIMER_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+	pid_t tid = GET_TIMER_THREAD_ID(hrt);
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(
+			SW_TRACE_ID_HRTIMER_EXPIRE_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+		return;
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_timer_wakeup_helper_i, curr, node,
+				 tid);
+};
+/*
+ * 6. SCHED wakeup
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_WAKEUP_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+	int target_cpu = task_cpu(task), source_cpu = RAW_CPU();
+	/*
+	 * "Self-sched" samples are "don't care".
+	 */
+	if (target_cpu == source_cpu) {
+		return;
+	}
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_WAKEUP);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	/*
+	 * Unlike other wakeup sources, we check the per-cpu flag
+	 * of the TARGET cpu to decide if we should produce a sample.
+	 */
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(target_cpu)) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		// sw_handle_sched_wakeup_i(curr, source_cpu, target_cpu);
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_wakeup_i, curr,
+					 source_cpu, target_cpu);
+	}
+};
+/*
+ * 8. PROCESS fork
+ */
+/*
+ * Helper for PROCESS fork, PROCESS exit
+ */
+void sw_process_fork_exit_helper_i(struct sw_collector_data *node,
+				   struct task_struct *task, bool is_fork)
+{
+	int cpu = RAW_CPU();
+	pid_t pid = task->tgid, tid = task->pid;
+	const char *name = task->comm;
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+
+	msg->cpuidx = cpu;
+
+	/*
+	 * Fork/Exit ==> return pid, tid
+	 * Fork ==> also return name
+	 */
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+	dst_vals += sizeof(tid);
+	if (is_fork) {
+		memcpy(dst_vals, name, SW_MAX_PROC_NAME_SIZE);
+	}
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+	pw_pr_debug(
+		"HANDLED process %s event for task: pid = %d, tid = %d, name = %s\n",
+		is_fork ? "FORK" : "EXIT", pid, tid, name);
+};
+
+DEFINE_PROBE_FUNCTION(PROBE_PROCESS_FORK_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_FORK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
+					 child, true /* true ==> fork */);
+	}
+};
+/*
+ * 9. PROCESS exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_PROCESS_EXIT_PARAMS)
+{
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_SCHED_PROCESS_EXIT);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	if (!node) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_process_fork_exit_helper_i, curr,
+					 task, false /* false ==> exit */);
+	}
+};
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+/*
+ * 10. THERMAL_APIC entry
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_ENTRY_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_ENTRY);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+/*
+ * 10. THERMAL_APIC exit
+ */
+DEFINE_PROBE_FUNCTION(PROBE_THERMAL_APIC_EXIT_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_THERMAL_APIC_EXIT);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, (int)cpu, node);
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+
+#if IS_ENABLED(CONFIG_ANDROID)
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+/*
+ * Helper function to produce wake lock/unlock messages.
+ */
+void sw_produce_wakelock_msg_i(int cpu, struct sw_collector_data *node,
+			       const char *name, int type, u64 timeout, int pid,
+			       int tid, const char *proc_name)
+{
+	sw_driver_msg_t *msg = GET_MSG_SLOT_FOR_CPU(node->msg, cpu,
+						    node->per_msg_payload_size);
+	char *dst_vals = msg->p_payload;
+
+	msg->cpuidx = cpu;
+
+	/*
+	 * Protocol:
+	 * wakelock_timeout, wakelock_type, wakelock_name,
+	 * proc_pid, proc_tid, proc_name
+	 */
+	*((u64 *)dst_vals) = timeout;
+	dst_vals += sizeof(timeout);
+	*((int *)dst_vals) = type;
+	dst_vals += sizeof(type);
+	strncpy(dst_vals, name, SW_MAX_KERNEL_WAKELOCK_NAME_SIZE);
+	dst_vals += SW_MAX_KERNEL_WAKELOCK_NAME_SIZE;
+
+	*((int *)dst_vals) = pid;
+	dst_vals += sizeof(pid);
+	*((int *)dst_vals) = tid;
+	dst_vals += sizeof(tid);
+	strncpy(dst_vals, proc_name, SW_MAX_PROC_NAME_SIZE);
+	dst_vals += SW_MAX_PROC_NAME_SIZE;
+
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_warn("WARNING: could NOT produce message!\n");
+	}
+};
+/*
+ * Helper function to handle wake lock/unlock callbacks.
+ */
+void sw_handle_wakelock_i(int cpu, struct sw_trace_notifier_data *node,
+			  const char *name, int type, u64 timeout)
+{
+	int pid = PID(), tid = TID();
+	const char *proc_name = NAME();
+	struct sw_collector_data *curr = NULL;
+
+	if (!node) {
+		return;
+	}
+
+	list_for_each_entry(curr, &node->list, list) {
+		sw_produce_wakelock_msg_i(cpu, curr, name, type, timeout, pid,
+					  tid, proc_name);
+	}
+};
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_LOCK_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	enum sw_kernel_wakelock_type type = SW_WAKE_LOCK;
+	u64 timeout = 0;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+	const char *name = lock->name;
+#endif
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_LOCK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+	/*
+	 * Was this wakelock acquired with a timeout i.e.
+	 * is this an auto expire wakelock?
+	 */
+	if (lock->flags & (1U << 10)) {
+		type = SW_WAKE_LOCK_TIMEOUT;
+		timeout = jiffies_to_msecs(lock->expires - jiffies);
+	}
+#endif //LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
+				 (int)type, timeout);
+};
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WAKE_UNLOCK_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	enum sw_kernel_wakelock_type type = SW_WAKE_UNLOCK;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+	const char *name = lock->name;
+#endif
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(SW_TRACE_ID_WAKE_UNLOCK);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_wakelock_i, cpu, node, name,
+				 (int)type, 0 /*timeout*/);
+};
+#endif // CONFIG_ANDROID
+
+/*
+ * 12. WORKQUEUE
+ */
+DEFINE_PROBE_FUNCTION(PROBE_WORKQUEUE_PARAMS)
+{
+	int cpu = RAW_CPU();
+	static struct sw_trace_notifier_data *node;
+	struct sw_collector_data *curr = NULL;
+
+	if (unlikely(node == NULL)) {
+		node = GET_COLLECTOR_TRACE_NODE(
+			SW_TRACE_ID_WORKQUEUE_EXECUTE_START);
+		pw_pr_debug("NODE = %p\n", node);
+	}
+
+	if (!node || !SHOULD_PRODUCE_WAKEUP_SAMPLE(cpu)) {
+		return;
+	}
+	list_for_each_entry(curr, &node->list, list) {
+		DO_PER_CPU_OVERHEAD_FUNC(sw_handle_workqueue_wakeup_helper_i,
+					 cpu, curr);
+	}
+};
+
+/*
+ * 13. SCHED switch
+ */
+DEFINE_PROBE_FUNCTION(PROBE_SCHED_SWITCH_PARAMS)
+{
+	DO_PER_CPU_OVERHEAD_FUNC(sw_handle_sched_switch_helper_i);
+};
+
+/*
+ * 1. SUSPEND notifier
+ */
+static void sw_send_pm_notification_i(int value)
+{
+	struct sw_driver_msg *msg = NULL;
+	size_t buffer_len = sizeof(*msg) + sizeof(value);
+	char *buffer = vmalloc(buffer_len);
+
+	if (!buffer) {
+		pw_pr_error(
+			"couldn't allocate memory when sending suspend notification!\n");
+		return;
+	}
+	msg = (struct sw_driver_msg *)buffer;
+	msg->tsc = sw_timestamp();
+	msg->cpuidx = RAW_CPU();
+	msg->plugin_id = 0; // "0" indicates a system message
+	msg->metric_id = 1; // "1" indicates a suspend/resume message (TODO)
+	msg->msg_id =
+		0; /* don't care; TODO: use the 'msg_id' to encode the 'value'? */
+	msg->payload_len = sizeof(value);
+	msg->p_payload = buffer + sizeof(*msg);
+	*((int *)msg->p_payload) = value;
+	if (sw_produce_generic_msg(msg, SW_WAKEUP_ACTION_DIRECT)) {
+		pw_pr_error("couldn't produce generic message!\n");
+	}
+	vfree(buffer);
+}
+
+static u64 sw_pm_enter_tsc;
+static bool sw_is_reset_i(void)
+{
+	/*
+	 * TODO: rely on checking the IA32_FIXED_CTR2 instead?
+	 */
+	u64 curr_tsc = sw_tscval();
+	bool is_reset = sw_pm_enter_tsc > curr_tsc;
+
+	pw_pr_force("DEBUG: curr tsc = %llu, prev tsc = %llu, is reset = %s\n",
+		    curr_tsc, sw_pm_enter_tsc, is_reset ? "true" : "false");
+
+	return is_reset;
+}
+static void sw_probe_pm_helper_i(int id, int both_id, bool is_enter,
+				 enum sw_pm_action action, enum sw_pm_mode mode)
+{
+	struct sw_trace_notifier_data *node = GET_COLLECTOR_NOTIFIER_NODE(id);
+	struct sw_trace_notifier_data *both_node =
+		GET_COLLECTOR_NOTIFIER_NODE(both_id);
+	struct sw_trace_notifier_data *reset_node =
+		GET_COLLECTOR_NOTIFIER_NODE(SW_NOTIFIER_ID_COUNTER_RESET);
+	if (is_enter) {
+		/*
+		 * Entering HIBERNATION/SUSPEND
+		 */
+		sw_pm_enter_tsc = sw_tscval();
+	} else {
+		/*
+		 * Exitting HIBERNATION/SUSPEND
+		 */
+		if (sw_is_reset_i() && reset_node) {
+			sw_handle_reset_messages_i(reset_node);
+		}
+	}
+	if (node) {
+		sw_handle_trace_notifier_i(node);
+	}
+	if (both_node) {
+		sw_handle_trace_notifier_i(both_node);
+	}
+	/* Send the suspend-resume notification */
+	sw_send_pm_notification_i(SW_PM_VALUE(mode, action));
+}
+
+static bool sw_is_suspend_via_firmware(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4, 4, 0)
+	/* 'pm_suspend_via_firmware' only available in kernel >= 4.4 */
+	return pm_suspend_via_firmware();
+#endif
+	return true;
+}
+
+static int sw_probe_pm_notifier_i(struct notifier_block *block, unsigned long state,
+			   void *dummy)
+{
+	static const struct {
+		enum sw_pm_action action;
+		int node_id;
+		int both_id;
+		bool is_enter;
+	} pm_data[PM_POST_RESTORE] = {
+		[PM_HIBERNATION_PREPARE] = { SW_PM_ACTION_HIBERNATE_ENTER,
+					     SW_NOTIFIER_ID_HIBERNATE_ENTER,
+					     SW_NOTIFIER_ID_HIBERNATE, true },
+		[PM_POST_HIBERNATION] = { SW_PM_ACTION_HIBERNATE_EXIT,
+					  SW_NOTIFIER_ID_HIBERNATE_EXIT,
+					  SW_NOTIFIER_ID_HIBERNATE, false },
+		[PM_SUSPEND_PREPARE] = { SW_PM_ACTION_SUSPEND_ENTER,
+					 SW_NOTIFIER_ID_SUSPEND_ENTER,
+					 SW_NOTIFIER_ID_SUSPEND, true },
+		[PM_POST_SUSPEND] = { SW_PM_ACTION_SUSPEND_EXIT,
+				      SW_NOTIFIER_ID_SUSPEND_EXIT,
+				      SW_NOTIFIER_ID_SUSPEND, false },
+	};
+	enum sw_pm_action action = pm_data[state].action;
+	enum sw_pm_mode mode = sw_is_suspend_via_firmware() ?
+				       SW_PM_MODE_FIRMWARE :
+				       SW_PM_MODE_NONE;
+	if (action != SW_PM_ACTION_NONE) {
+		int node_id = pm_data[state].node_id,
+		    both_id = pm_data[state].both_id;
+		bool is_enter = pm_data[state].is_enter;
+
+		sw_probe_pm_helper_i(node_id, both_id, is_enter, action, mode);
+	} else {
+		/* Not supported */
+		pw_pr_error(
+			"ERROR: unknown state %lu passed to SWA pm notifier!\n",
+			state);
+	}
+	return NOTIFY_DONE;
+}
+
+static void sw_store_topology_change_i(enum cpu_action type, int cpu, int core_id,
+				int pkg_id)
+{
+	struct sw_topology_node *node = sw_kmalloc(sizeof(*node), GFP_ATOMIC);
+
+	if (!node) {
+		pw_pr_error(
+			"couldn't allocate a node for topology change tracking!\n");
+		return;
+	}
+	node->change.timestamp = sw_timestamp();
+	node->change.type = type;
+	node->change.cpu = cpu;
+	node->change.core = core_id;
+	node->change.pkg = pkg_id;
+
+	SW_LIST_ADD(&sw_topology_list, node, list);
+	++sw_num_topology_entries;
+}
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+int sw_probe_hotplug_notifier_i(struct notifier_block *block,
+				unsigned long action, void *pcpu)
+{
+	unsigned int cpu = (unsigned long)pcpu;
+	unsigned int pkg_id = topology_physical_package_id(cpu);
+	unsigned int core_id = topology_core_id(cpu);
+
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_UP_PREPARE_FROZEN:
+		/* CPU is coming online -- store top change */
+		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE_PREPARE, cpu,
+					   core_id, pkg_id);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) preparing to come online at tsc = %llu! Current cpu = %d\n",
+			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_ONLINE:
+	case CPU_ONLINE_FROZEN:
+		/* CPU is online -- first store top change
+		   then take BEGIN snapshot */
+		sw_store_topology_change_i(SW_CPU_ACTION_ONLINE, cpu, core_id,
+					   pkg_id);
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d (phys = %d, core = %d) online at tsc = %llu! Current cpu = %d\n",
+			cpu, pkg_id, core_id, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_DOWN_PREPARE:
+	case CPU_DOWN_PREPARE_FROZEN:
+		/* CPU is going offline -- take END snapshot */
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d preparing to go offline at tsc = %llu! Current cpu = %d\n",
+			cpu, sw_timestamp(), RAW_CPU());
+		break;
+	case CPU_DEAD:
+	case CPU_DEAD_FROZEN:
+		/* CPU is offline -- store top change */
+		sw_store_topology_change_i(SW_CPU_ACTION_OFFLINE, cpu, core_id,
+					   pkg_id);
+		pw_pr_debug(
+			"DEBUG: SoC Watch has cpu %d offlined at tsc = %llu! Current cpu = %d\n",
+			cpu, sw_timestamp(), RAW_CPU());
+		break;
+	default:
+		break;
+	}
+	return NOTIFY_OK;
+};
+#else
+static void sw_probe_cpuhp_helper_i(unsigned int cpu, enum cpu_action action)
+{
+	unsigned int pkg_id = topology_physical_package_id(cpu);
+	unsigned int core_id = topology_core_id(cpu);
+
+	switch (action) {
+	case SW_CPU_ACTION_ONLINE_PREPARE:
+		/* CPU is coming online -- store top change */
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		break;
+	case SW_CPU_ACTION_ONLINE:
+		/* CPU is online -- first store top change
+		   then take BEGIN snapshot */
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_BEGIN, cpu);
+		break;
+	case SW_CPU_ACTION_OFFLINE:
+		/* CPU is preparing to go offline -- take
+		   END snapshot then store top change */
+		sw_process_snapshot_on_cpu(SW_WHEN_TYPE_END, cpu);
+		sw_store_topology_change_i(action, cpu, core_id, pkg_id);
+		break;
+	default:
+		break;
+	}
+}
+static int sw_probe_cpu_offline_i(unsigned int cpu)
+{
+	printk(KERN_INFO "DEBUG: offline notification for cpu %u at %llu\n",
+	       cpu, sw_tscval());
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_OFFLINE);
+	return 0;
+}
+static int sw_probe_cpu_online_i(unsigned int cpu)
+{
+	printk(KERN_INFO "DEBUG: online notification for cpu %u at %llu\n", cpu,
+	       sw_tscval());
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE_PREPARE);
+	sw_probe_cpuhp_helper_i(cpu, SW_CPU_ACTION_ONLINE);
+	return 0;
+}
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+
+/*
+ * 2. CPUFREQ notifier
+ */
+static int sw_probe_cpufreq_notifier_i(struct notifier_block *block,
+				unsigned long state, void *data)
+{
+	struct cpufreq_freqs *freqs = data;
+	static struct sw_trace_notifier_data *node;
+	int cpu = freqs->cpu;
+
+	if (state == CPUFREQ_PRECHANGE) {
+		pw_pr_debug(
+			"CPU %d reports a CPUFREQ_PRECHANGE for target CPU %d at TSC = %llu\n",
+			RAW_CPU(), cpu, sw_timestamp());
+		if (unlikely(node == NULL)) {
+			node = GET_COLLECTOR_NOTIFIER_NODE(
+				SW_NOTIFIER_ID_CPUFREQ);
+			pw_pr_debug("NODE = %p\n", node);
+		}
+		/* Force an atomic context by disabling preemption */
+		get_cpu();
+		DO_PER_CPU_OVERHEAD_FUNC(sw_tpf_i, cpu, node);
+		put_cpu();
+	}
+	return NOTIFY_DONE;
+}
+/*
+ * 1. TPS.
+ */
+int sw_register_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_start,
+					sw_probe_power_start_i);
+#else // kernel version >= 2.6.38
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_cpu_idle_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_start,
+					  sw_probe_power_start_i);
+#else // kernel version >= 2.6.38
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_idle, sw_probe_cpu_idle_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+	return PW_SUCCESS;
+};
+/*
+ * 2. TPF
+ */
+int sw_register_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
+					sw_probe_power_frequency_i);
+#else // kernel version >= 2.6.38
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
+					sw_probe_cpu_frequency_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_cpu_frequency_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 38)
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, power_frequency,
+					  sw_probe_power_frequency_i);
+#else // kernel version >= 2.6.38
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, cpu_frequency,
+					  sw_probe_cpu_frequency_i);
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,38)
+	return PW_SUCCESS;
+};
+/*
+ * 3. IRQ handler entry
+ */
+int sw_register_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
+					sw_probe_irq_handler_entry_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_irq_handler_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, irq_handler_entry,
+					  sw_probe_irq_handler_entry_i);
+	return PW_SUCCESS;
+};
+/*
+ * 4. TIMER expire.
+ */
+int sw_register_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
+					sw_probe_timer_expire_entry_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_timer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, timer_expire_entry,
+					  sw_probe_timer_expire_entry_i);
+	return PW_SUCCESS;
+};
+/*
+ * 5. HRTIMER expire.
+ */
+int sw_register_trace_hrtimer_expire_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
+					sw_probe_hrtimer_expire_entry_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_hrtimer_expire_entry_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, hrtimer_expire_entry,
+					  sw_probe_hrtimer_expire_entry_i);
+	return PW_SUCCESS;
+};
+/*
+ * 6. SCHED wakeup
+ */
+int sw_register_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
+					sw_probe_sched_wakeup_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_wakeup_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_wakeup,
+					  sw_probe_sched_wakeup_i);
+	return PW_SUCCESS;
+};
+/*
+ * 8. PROCESS fork
+ */
+int sw_register_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
+					sw_probe_sched_process_fork_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_process_fork_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_fork,
+					  sw_probe_sched_process_fork_i);
+	return PW_SUCCESS;
+};
+/*
+ * 9. PROCESS exit
+ */
+int sw_register_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
+					sw_probe_sched_process_exit_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_process_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_process_exit,
+					  sw_probe_sched_process_exit_i);
+	return PW_SUCCESS;
+};
+/*
+ * 10. THERMAL_APIC entry
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 14, 0)
+int sw_register_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
+					sw_probe_thermal_apic_entry_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_thermal_apic_entry_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_entry,
+					  sw_probe_thermal_apic_entry_i);
+	return PW_SUCCESS;
+};
+/*
+ * 10. THERMAL_APIC exit
+ */
+int sw_register_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
+					sw_probe_thermal_apic_exit_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_thermal_apic_exit_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, thermal_apic_exit,
+					  sw_probe_thermal_apic_exit_i);
+	return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,14,0)
+/*
+ * 11. WAKE lock / WAKEUP source activate.
+ */
+#if IS_ENABLED(CONFIG_ANDROID)
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+int sw_register_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_lock, sw_probe_wake_lock_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_wake_lock_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_lock,
+					  sw_probe_wake_lock_i);
+	return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+int sw_register_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
+					sw_probe_wakeup_source_activate_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_wakeup_source_activate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_activate,
+					  sw_probe_wakeup_source_activate_i);
+	return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+/*
+ * 11. WAKE unlock / WAKEUP source deactivate.
+ */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(3, 4, 0)
+int sw_register_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
+					sw_probe_wake_unlock_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_wake_unlock_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wake_unlock,
+					  sw_probe_wake_unlock_i);
+	return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(3,4,0)
+int sw_register_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
+					sw_probe_wakeup_source_deactivate_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_wakeup_source_deactivate_i(
+	struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, wakeup_source_deactivate,
+					  sw_probe_wakeup_source_deactivate_i);
+	return PW_SUCCESS;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(3,4,0)
+#endif // CONFIG_ANDROID
+/*
+ * 12. WORKQUEUE execution.
+ */
+int sw_register_trace_workqueue_execution_i(struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 35)
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
+					sw_probe_workqueue_execution_i);
+#else
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
+					sw_probe_workqueue_execute_start_i);
+#endif
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_workqueue_execution_i(
+	struct sw_trace_notifier_data *node)
+{
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 35)
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execution,
+					  sw_probe_workqueue_execution_i);
+#else
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, workqueue_execute_start,
+					  sw_probe_workqueue_execute_start_i);
+#endif
+	return PW_SUCCESS;
+};
+/*
+ * 13. SCHED switch
+ */
+int sw_register_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+	/*
+	 * Set polling tick time, in jiffies.
+	 * Used by the context switch tracepoint to decide
+	 * if enough time has elapsed since the last
+	 * collection point to read resources again.
+	 */
+	{
+		int cpu = 0;
+		for_each_present_cpu(cpu) {
+			*(&per_cpu(sw_pcpu_polling_jiff, cpu)) = jiffies;
+		}
+	}
+	DO_REGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
+					sw_probe_sched_switch_i);
+	return PW_SUCCESS;
+};
+int sw_unregister_trace_sched_switch_i(struct sw_trace_notifier_data *node)
+{
+	DO_UNREGISTER_SW_TRACEPOINT_PROBE(node, sched_switch,
+					  sw_probe_sched_switch_i);
+	return PW_SUCCESS;
+};
+/*
+ * Notifier register/unregister functions.
+ */
+/*
+ * 1. SUSPEND notifier.
+ */
+static struct notifier_block sw_pm_notifier = {
+	.notifier_call = &sw_probe_pm_notifier_i,
+};
+int sw_register_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+	register_pm_notifier(&sw_pm_notifier);
+	return PW_SUCCESS;
+};
+int sw_unregister_pm_notifier_i(struct sw_trace_notifier_data *node)
+{
+	unregister_pm_notifier(&sw_pm_notifier);
+	return PW_SUCCESS;
+};
+/*
+ * 2. CPUFREQ notifier.
+ */
+static struct notifier_block sw_cpufreq_notifier = {
+	.notifier_call = &sw_probe_cpufreq_notifier_i,
+};
+int sw_register_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+	cpufreq_register_notifier(&sw_cpufreq_notifier,
+				  CPUFREQ_TRANSITION_NOTIFIER);
+	return PW_SUCCESS;
+};
+int sw_unregister_cpufreq_notifier_i(struct sw_trace_notifier_data *node)
+{
+	cpufreq_unregister_notifier(&sw_cpufreq_notifier,
+				    CPUFREQ_TRANSITION_NOTIFIER);
+	return PW_SUCCESS;
+};
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+/*
+ * 3. CPU hot plug notifier.
+ */
+struct notifier_block sw_cpu_hotplug_notifier = {
+	.notifier_call = &sw_probe_hotplug_notifier_i,
+};
+
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	register_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+	return PW_SUCCESS;
+};
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	unregister_hotcpu_notifier(&sw_cpu_hotplug_notifier);
+	return PW_SUCCESS;
+};
+#else // LINUX_VERSION_CODE >= KERNEL_VERSION(4,10,0)
+static int sw_cpuhp_state = -1;
+int sw_register_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	sw_cpuhp_state = cpuhp_setup_state_nocalls(CPUHP_AP_ONLINE_DYN,
+						   "socwatch:online",
+						   &sw_probe_cpu_online_i,
+						   &sw_probe_cpu_offline_i);
+	if (sw_cpuhp_state < 0) {
+		pw_pr_error("couldn't register socwatch hotplug callbacks!\n");
+		return -EIO;
+	}
+	return 0;
+};
+int sw_unregister_hotcpu_notifier_i(struct sw_trace_notifier_data *node)
+{
+	if (sw_cpuhp_state >= 0) {
+		cpuhp_remove_state_nocalls((enum cpuhp_state)sw_cpuhp_state);
+	}
+	return 0;
+};
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(4,10,0)
+
+/*
+ * Tracepoint extraction routines.
+ * Required for newer kernels (>=3.15)
+ */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0)
+static void sw_extract_tracepoint_callback(struct tracepoint *tp, void *priv)
+{
+	struct sw_trace_notifier_data *node = NULL;
+	int i = 0;
+	int *numStructsFound = (int *)priv;
+
+	if (*numStructsFound == NUM_VALID_TRACEPOINTS) {
+		/*
+		 * We've found all the tracepoints we need.
+		 */
+		return;
+	}
+	if (tp) {
+		FOR_EACH_TRACEPOINT_NODE(i, node)
+		{
+			if (node->tp == NULL && node->name) {
+				const char *name =
+					sw_get_trace_notifier_kernel_name(node);
+				if (name && !strcmp(tp->name, name)) {
+					node->tp = tp;
+					++*numStructsFound;
+					pw_pr_debug("OK, found TP %s\n",
+						    tp->name);
+				}
+			}
+		}
+	}
+};
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+
+/*
+ * Retrieve the list of tracepoint structs to use when registering and unregistering
+ * tracepoint handlers.
+ */
+int sw_extract_trace_notifier_providers(void)
+{
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0) &&                          \
+	defined(CONFIG_TRACEPOINTS)
+	int numCallbacks = 0;
+
+	for_each_kernel_tracepoint(&sw_extract_tracepoint_callback,
+				   &numCallbacks);
+	/*
+	 * Did we get the complete list?
+	 */
+	if (numCallbacks != NUM_VALID_TRACEPOINTS) {
+		printk(KERN_WARNING
+		       "WARNING: Could NOT find tracepoint structs for some tracepoints!\n");
+	}
+#endif // LINUX_VERSION_CODE >= KERNEL_VERSION(3,15,0)
+	return PW_SUCCESS;
+};
+
+void sw_reset_trace_notifier_providers(void)
+{
+	/*
+	 * Reset the wakeup flag. Not strictly required if we aren't probing
+	 * any of the wakeup tracepoints.
+	 */
+	{
+		int cpu = 0;
+
+		for_each_online_cpu(cpu) {
+			RESET_VALID_WAKEUP_EVENT_COUNTER(cpu);
+		}
+	}
+	/*
+	 * Reset the wakeup event flag. Not strictly required if we
+	 * aren't probing any of the wakeup tracepoints. Will be reset
+	 * in the power_start tracepoint if user requested a c-state
+	 * collection.
+	 */
+	sw_wakeup_event_flag = true;
+};
+
+void sw_print_trace_notifier_provider_overheads(void)
+{
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tps_i, "TPS");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_tpf_i, "TPF");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_irq_wakeup_i, "IRQ");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_timer_wakeup_helper_i,
+					 "TIMER_EXPIRE");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_wakeup_i,
+					 "SCHED WAKEUP");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_process_fork_exit_helper_i,
+					 "PROCESS FORK/EXIT");
+#if IS_ENABLED(CONFIG_ANDROID)
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_wakelock_i,
+					 "WAKE LOCK/UNLOCK");
+#endif // CONFIG_ANDROID
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_workqueue_wakeup_helper_i,
+					 "WORKQUEUE");
+	PRINT_CUMULATIVE_OVERHEAD_PARAMS(sw_handle_sched_switch_helper_i,
+					 "SCHED SWITCH");
+};
+/*
+ * Add all trace/notifier providers.
+ */
+int sw_add_trace_notifier_providers(void)
+{
+	struct sw_trace_notifier_data *node = NULL;
+	int i = 0;
+
+	FOR_EACH_TRACEPOINT_NODE(i, node)
+	{
+		if (sw_register_trace_notify_provider(node)) {
+			pw_pr_error("ERROR: couldn't add a trace provider!\n");
+			return -EIO;
+		}
+	}
+	FOR_EACH_NOTIFIER_NODE(i, node)
+	{
+		if (sw_register_trace_notify_provider(node)) {
+			pw_pr_error(
+				"ERROR: couldn't add a notifier provider!\n");
+			return -EIO;
+		}
+	}
+	/*
+	 * Add the cpu hot plug notifier.
+	 */
+	{
+		if (sw_register_trace_notify_provider(
+			    &s_hotplug_notifier_data)) {
+			pw_pr_error(
+				"ERROR: couldn't add cpu notifier provider!\n");
+			return -EIO;
+		}
+	}
+	return PW_SUCCESS;
+}
+/*
+ * Remove previously added providers.
+ */
+void sw_remove_trace_notifier_providers(void)
+{ /* NOP */
+}
diff --git a/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c b/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
new file mode 100644
index 000000000000..bc335ce9a65e
--- /dev/null
+++ b/drivers/platform/x86/socwatch/sw_tracepoint_handlers.c
@@ -0,0 +1,399 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#include "sw_structs.h"
+#include "sw_kernel_defines.h"
+#include "sw_types.h"
+#include "sw_tracepoint_handlers.h"
+#include "sw_trace_notifier_provider.h"
+#include "sw_mem.h"
+
+/* -------------------------------------------------
+ * Data structures and variable definitions.
+ * -------------------------------------------------
+ */
+struct sw_trace_list_node {
+	struct sw_trace_notifier_data *data;
+	int id;
+
+	SW_LIST_ENTRY(list, sw_trace_list_node);
+};
+static SW_DEFINE_LIST_HEAD(s_trace_list, sw_trace_list_node) =
+	SW_LIST_HEAD_INITIALIZER(s_trace_list);
+static SW_DEFINE_LIST_HEAD(s_notifier_list, sw_trace_list_node) =
+	SW_LIST_HEAD_INITIALIZER(s_notifier_list);
+static int s_trace_idx = -1, s_notifier_idx = -1;
+
+SW_DEFINE_LIST_HEAD(sw_topology_list, sw_topology_node) =
+	SW_LIST_HEAD_INITIALIZER(sw_topology_list);
+size_t sw_num_topology_entries = 0;
+
+/* -------------------------------------------------
+ * Function definitions.
+ * -------------------------------------------------
+ */
+int sw_extract_tracepoints(void)
+{
+	return sw_extract_trace_notifier_providers();
+}
+
+void sw_reset_trace_notifier_lists(void)
+{
+	sw_reset_trace_notifier_providers();
+}
+
+void sw_print_trace_notifier_overheads(void)
+{
+	sw_print_trace_notifier_provider_overheads();
+}
+
+static int sw_for_each_node_i(void *list_head,
+			      int (*func)(struct sw_trace_notifier_data *node,
+					  void *priv),
+			      void *priv, bool return_on_error) {
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
+	int retval = PW_SUCCESS;
+	struct sw_trace_list_node *lnode = NULL;
+
+	SW_LIST_FOR_EACH_ENTRY(lnode, head, list)
+	{
+		if ((*func)(lnode->data, priv)) {
+			retval = -EIO;
+			if (return_on_error) {
+				break;
+			}
+		}
+	}
+	return retval;
+}
+
+int sw_for_each_tracepoint_node(int (*func)(struct sw_trace_notifier_data *node,
+					    void *priv),
+				void *priv, bool return_on_error) {
+	if (func) {
+		return sw_for_each_node_i(&s_trace_list, func, priv,
+					  return_on_error);
+	}
+	return PW_SUCCESS;
+}
+
+int sw_for_each_notifier_node(int (*func)(struct sw_trace_notifier_data *node,
+					  void *priv),
+			      void *priv, bool return_on_error) {
+	if (func) {
+		return sw_for_each_node_i(&s_notifier_list, func, priv,
+					  return_on_error);
+	}
+	return PW_SUCCESS;
+}
+
+/*
+ * Retrieve the ID for the corresponding tracepoint/notifier.
+ */
+int sw_get_trace_notifier_id(struct sw_trace_notifier_data *tnode)
+{
+	struct sw_trace_list_node *lnode = NULL;
+
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = (void *)&s_trace_list;
+	if (!tnode) {
+		pw_pr_error(
+			"ERROR: cannot get ID for NULL trace/notifier data!\n");
+		return -EIO;
+	}
+	if (!(tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
+	      tnode->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+		pw_pr_error(
+			"ERROR: cannot get ID for invalid trace/notifier data!\n");
+		return -EIO;
+	}
+	if (!tnode->name || !tnode->name->abstract_name) {
+		pw_pr_error(
+			"ERROR: cannot get ID for trace/notifier data without valid name!\n");
+		return -EIO;
+	}
+#ifdef LINUX_VERSION_CODE
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(3, 15, 0) &&                          \
+	defined(CONFIG_TRACEPOINTS)
+	if (tnode->type == SW_TRACE_COLLECTOR_TRACEPOINT &&
+	    tnode->name->kernel_name && !tnode->tp) {
+		/* No tracepoint structure found so no ID possible */
+		return -EIO;
+	}
+#endif
+#endif
+	if (tnode->type == SW_TRACE_COLLECTOR_NOTIFIER) {
+		head = (void *)&s_notifier_list;
+	}
+	SW_LIST_FOR_EACH_ENTRY(lnode, head, list)
+	{
+		struct sw_trace_notifier_data *data = lnode->data;
+
+		if (!strcmp(data->name->abstract_name,
+			    tnode->name->abstract_name)) {
+			return lnode->id;
+		}
+	}
+	return -1;
+}
+/*
+ * Retrieve the "kernel" name for this tracepoint/notifier.
+ */
+const char *
+sw_get_trace_notifier_kernel_name(struct sw_trace_notifier_data *node)
+{
+	return node->name->kernel_name;
+};
+/*
+ * Retrieve the "abstract" name for this tracepoint/notifier.
+ */
+const char *
+sw_get_trace_notifier_abstract_name(struct sw_trace_notifier_data *node)
+{
+	return node->name->abstract_name;
+};
+
+/*
+ * Add a single TRACE/NOTIFY provider.
+ */
+int sw_register_trace_notify_provider(struct sw_trace_notifier_data *data)
+{
+	struct sw_trace_list_node *lnode = NULL;
+
+	if (!data) {
+		pw_pr_error(
+			"ERROR: cannot add NULL trace/notifier provider!\n");
+		return -EIO;
+	}
+	if (!(data->type == SW_TRACE_COLLECTOR_TRACEPOINT ||
+	      data->type == SW_TRACE_COLLECTOR_NOTIFIER)) {
+		pw_pr_error("ERROR: cannot add invalid trace/notifier data!\n");
+		return -EIO;
+	}
+	/*
+	 * Kernel name is allowed to be NULL, but abstract name MUST be present!
+	 */
+	if (!data->name || !data->name->abstract_name) {
+		pw_pr_error(
+			"ERROR: cannot add trace/notifier provider without an abstract name!\n");
+		pw_pr_error("ERROR: data->name = %p\n", data->name);
+		return -EIO;
+	}
+	lnode = sw_kmalloc(sizeof(*lnode), GFP_KERNEL);
+	if (!lnode) {
+		pw_pr_error(
+			"ERROR: couldn't allocate a list node when adding a trace/notifier provider!\n");
+		return -ENOMEM;
+	}
+	lnode->data = data;
+	SW_LIST_ENTRY_INIT(lnode, list);
+	if (data->type == SW_TRACE_COLLECTOR_TRACEPOINT) {
+		lnode->id = ++s_trace_idx;
+		SW_LIST_ADD(&s_trace_list, lnode, list);
+	} else {
+		lnode->id = ++s_notifier_idx;
+		SW_LIST_ADD(&s_notifier_list, lnode, list);
+	}
+	return PW_SUCCESS;
+}
+/*
+ * Add all TRACE/NOTIFY providers.
+ */
+int sw_add_trace_notify(void)
+{
+	return sw_add_trace_notifier_providers();
+}
+
+static void sw_free_trace_notifier_list_i(void *list_head)
+{
+	SW_LIST_HEAD_VAR(sw_trace_list_node) * head = list_head;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_trace_list_node *lnode =
+			SW_LIST_GET_HEAD_ENTRY(head, sw_trace_list_node, list);
+		SW_LIST_UNLINK(lnode, list);
+		sw_kfree(lnode);
+	}
+}
+/*
+ * Remove TRACE/NOTIFY providers.
+ */
+void sw_remove_trace_notify(void)
+{
+	/*
+	 * Free all nodes.
+	 */
+	sw_free_trace_notifier_list_i(&s_trace_list);
+	sw_free_trace_notifier_list_i(&s_notifier_list);
+	/*
+	 * Call our providers to deallocate resources.
+	 */
+	sw_remove_trace_notifier_providers();
+	/*
+	 * Clear out the topology list
+	 */
+	sw_clear_topology_list();
+}
+
+#define REG_FLAG (void *)1
+#define UNREG_FLAG (void *)2
+static int sw_reg_unreg_node_i(struct sw_trace_notifier_data *node,
+			       void *is_reg)
+{
+	if (is_reg == REG_FLAG) {
+		/*
+		 * Do we have anything to collect?
+		 * Update: or were we asked to always register?
+		 */
+		if (SW_LIST_EMPTY(&node->list) && !node->always_register) {
+			return PW_SUCCESS;
+		}
+		/*
+		 * Sanity: ensure we have a register AND an
+		 * unregister function before proceeding!
+		 */
+		if (node->probe_register == NULL ||
+		    node->probe_unregister == NULL) {
+			pw_pr_debug(
+				"WARNING: invalid trace/notifier register/unregister function for %s\n",
+				sw_get_trace_notifier_kernel_name(node));
+			/*
+			 * Don't flag this as an error --
+			 * some socwatch trace providers don't have a
+			 * register/unregister function
+			 */
+			return PW_SUCCESS;
+		}
+		if ((*node->probe_register)(node)) {
+			return -EIO;
+		}
+		node->was_registered = true;
+		return PW_SUCCESS;
+	} else if (is_reg == UNREG_FLAG) {
+		if (node->was_registered) {
+			/*
+			 * No need to check for validity of probe
+			 * unregister function -- 'sw_register_notifiers_i()'
+			 * would already have done so!
+			 */
+			WARN_ON((*node->probe_unregister)(node));
+			node->was_registered = false;
+			pw_pr_debug("OK, unregistered trace/notifier for %s\n",
+				    sw_get_trace_notifier_kernel_name(node));
+		}
+		return PW_SUCCESS;
+	}
+	pw_pr_error("ERROR: invalid reg/unreg flag value 0x%lx\n",
+		    (unsigned long)is_reg);
+	return -EIO;
+}
+/*
+ * Register all required tracepoints and notifiers.
+ */
+int sw_register_trace_notifiers(void)
+{
+	/*
+	 * First, the tracepoints.
+	 */
+	if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i, REG_FLAG,
+					true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	/*
+	 * And then the notifiers.
+	 */
+	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, REG_FLAG,
+				      true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	return PW_SUCCESS;
+};
+/*
+ * Unregister all previously registered tracepoints and notifiers.
+ */
+int sw_unregister_trace_notifiers(void)
+{
+	/*
+	 * First, the notifiers.
+	 */
+	if (sw_for_each_notifier_node(&sw_reg_unreg_node_i, UNREG_FLAG,
+				      true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	/*
+	 * And then the tracepoints.
+	 */
+	if (sw_for_each_tracepoint_node(&sw_reg_unreg_node_i, UNREG_FLAG,
+					true /* return on error */)) {
+		pw_pr_error("ERROR registering some tracepoints\n");
+		return -EIO;
+	}
+	return PW_SUCCESS;
+};
+
+void sw_clear_topology_list(void)
+{
+	SW_LIST_HEAD_VAR(sw_topology_node) * head = &sw_topology_list;
+	while (!SW_LIST_EMPTY(head)) {
+		struct sw_topology_node *lnode =
+			SW_LIST_GET_HEAD_ENTRY(head, sw_topology_node, list);
+		pw_pr_debug("Clearing topology node for cpu %d\n",
+			    lnode->change.cpu);
+		SW_LIST_UNLINK(lnode, list);
+		sw_kfree(lnode);
+	}
+	sw_num_topology_entries = 0;
+}
diff --git a/drivers/platform/x86/socwatchhv/Kconfig b/drivers/platform/x86/socwatchhv/Kconfig
new file mode 100644
index 000000000000..3226632de1fc
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/Kconfig
@@ -0,0 +1,6 @@
+menuconfig INTEL_SOCWATCH_HV
+	depends on X86 && ACRN_VHM && ACRN_SHARED_BUFFER
+	tristate "SocWatch Hypervisor Driver Support"
+	default m
+	help
+	  Say Y here to enable SocWatch hypervisor driver
diff --git a/drivers/platform/x86/socwatchhv/Makefile b/drivers/platform/x86/socwatchhv/Makefile
new file mode 100644
index 000000000000..bd4b58a61f06
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/Makefile
@@ -0,0 +1,20 @@
+#
+# Makefile for the socwatch hv driver.
+#
+
+DRIVER_BASE=socwatchhv
+DRIVER_MAJOR=2
+DRIVER_MINOR=0
+# basic name of driver
+DRIVER_NAME=${DRIVER_BASE}${DRIVER_MAJOR}_${DRIVER_MINOR}
+
+HYPERVISOR=2 # ACRN
+
+ccflags-y +=	-Idrivers/ \
+		-Idrivers/platform/x86/socwatchhv/inc/ \
+		-DHYPERVISOR=$(HYPERVISOR)
+
+obj-$(CONFIG_INTEL_SOCWATCH_HV)    += $(DRIVER_NAME).o
+
+$(DRIVER_NAME)-objs :=	swhv_driver.o \
+			swhv_acrn.o
diff --git a/drivers/platform/x86/socwatchhv/control.c b/drivers/platform/x86/socwatchhv/control.c
new file mode 100644
index 000000000000..4d1c384b1fe8
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/control.c
@@ -0,0 +1,141 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#include <linux/version.h>
+
+#include "control.h"
+#include <linux/sched.h>
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 27)
+#define SMP_CALL_FUNCTION(func, ctx, retry, wait)                              \
+	smp_call_function((func), (ctx), (wait))
+#define SMP_CALL_FUNCTION_SINGLE(cpuid, func, ctx, retry, wait)                \
+	smp_call_function_single((cpuid), (func), (ctx), (wait))
+#define ON_EACH_CPU(func, ctx, retry, wait) on_each_cpu((func), (ctx), (wait))
+#else
+#define SMP_CALL_FUNCTION(func, ctx, retry, wait)                              \
+	smp_call_function((func), (ctx), (retry), (wait))
+#define SMP_CALL_FUNCTION_SINGLE(cpuid, func, ctx, retry, wait)                \
+	smp_call_function_single((cpuid), (func), (ctx), (retry), (wait))
+#define ON_EACH_CPU(func, ctx, retry, wait)                                    \
+	on_each_cpu((func), (ctx), (retry), (wait))
+#endif
+
+extern int num_CPUs;
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn       VOID CONTROL_Invoke_Cpu (func, ctx, arg)
+ *
+ * @brief    Set up a DPC call and insert it into the queue
+ *
+ * @param    IN cpu_idx  - the core id to dispatch this function to
+ *           IN func     - function to be invoked by the specified core(s)
+ *           IN ctx      - pointer to the parameter block for each function
+ *                         invocation
+ *
+ * @return   None
+ *
+ * <I>Special Notes:</I>
+ *
+ */
+extern void CONTROL_Invoke_Cpu(int cpu_idx, void (*func)(pvoid), pvoid ctx)
+{
+	SMP_CALL_FUNCTION_SINGLE(cpu_idx, func, ctx, 0, 1);
+
+	return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_Service(func, ctx, blocking, exclude)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function invocation
+ * @param    blocking - Wait for invoked function to complete
+ * @param    exclude  - exclude the current core from executing the code
+ *
+ * @returns  None
+ *
+ * @brief    Service routine to handle all kinds of parallel invoke on all CPU calls
+ *
+ * <I>Special Notes:</I>
+ *           Invoke the function provided in parallel in either a blocking or
+ *           non-blocking mode.  The current core may be excluded if desired.
+ *           NOTE - Do not call this function directly from source code.
+ *           Use the aliases CONTROL_Invoke_Parallel(), CONTROL_Invoke_Parallel_NB(),
+ *           or CONTROL_Invoke_Parallel_XS().
+ *
+ */
+extern void CONTROL_Invoke_Parallel_Service(void (*func)(pvoid), pvoid ctx,
+					    int blocking, int exclude)
+{
+	if (num_CPUs == 1) {
+		if (!exclude) {
+			func(ctx);
+		}
+		return;
+	}
+	if (!exclude) {
+		ON_EACH_CPU(func, ctx, 0, blocking);
+		return;
+	}
+
+	preempt_disable();
+	SMP_CALL_FUNCTION(func, ctx, 0, blocking);
+	preempt_enable();
+
+	return;
+}
diff --git a/drivers/platform/x86/socwatchhv/inc/asm_helper.h b/drivers/platform/x86/socwatchhv/inc/asm_helper.h
new file mode 100644
index 000000000000..d09a3bbd19cb
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/asm_helper.h
@@ -0,0 +1,158 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _ASM_HELPER_H_
+#define _ASM_HELPER_H_
+
+#include <linux/version.h>
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 1, 0)
+
+#include <asm/dwarf2.h>
+#include <asm/calling.h>
+
+#else
+
+#ifdef CONFIG_AS_CFI
+
+#define CFI_STARTPROC (.cfi_startproc)
+#define CFI_ENDPROC (.cfi_endproc)
+#define CFI_ADJUST_CFA_OFFSET (.cfi_adjust_cfa_offset)
+#define CFI_REL_OFFSET (.cfi_rel_offset)
+#define CFI_RESTORE (.cfi_restore)
+
+#else
+
+.macro cfi_ignore a = 0, b = 0, c = 0, d = 0.endm
+
+#define CFI_STARTPROC cfi_ignore
+#define CFI_ENDPROC cfi_ignore
+#define CFI_ADJUST_CFA_OFFSET cfi_ignore
+#define CFI_REL_OFFSET cfi_ignore
+#define CFI_RESTORE cfi_ignore
+#endif
+
+#ifdef CONFIG_X86_64
+.macro SAVE_C_REGS_HELPER
+	offset = 0 rax = 1 rcx = 1 r8910 = 1 r11 = 1.if \r11 movq % r11,
+	6 * 8 +\offset(% rsp) CFI_REL_OFFSET r11, \offset.endif.if \r8910 movq
+							  % r10,
+	7 * 8 +\offset(% rsp) CFI_REL_OFFSET r10, \offset movq % r9,
+	8 * 8 +\offset(% rsp) CFI_REL_OFFSET r9, \offset movq % r8,
+	9 * 8 +\offset(% rsp) CFI_REL_OFFSET r8, \offset.endif.if \rax movq
+							 % rax,
+	10 * 8 +\offset(% rsp) CFI_REL_OFFSET rax, \offset.endif.if \rcx movq
+							   % rcx,
+	11 * 8 +\offset(% rsp) CFI_REL_OFFSET rcx, \offset.endif movq % rdx,
+	12 * 8 +\offset(% rsp) CFI_REL_OFFSET rdx, \offset movq % rsi,
+	13 * 8 +\offset(% rsp) CFI_REL_OFFSET rsi, \offset movq % rdi,
+	14 * 8 +\offset(% rsp) CFI_REL_OFFSET rdi, \offset.endm.macro
+						   SAVE_C_REGS offset =
+							   0 SAVE_C_REGS_HELPER \offset
+	,
+	1, 1, 1, 1.endm.macro SAVE_EXTRA_REGS offset = 0 movq % r15,
+	0 * 8 +\offset(% rsp) CFI_REL_OFFSET r15, \offset movq % r14,
+	1 * 8 +\offset(% rsp) CFI_REL_OFFSET r14, \offset movq % r13,
+	2 * 8 +\offset(% rsp) CFI_REL_OFFSET r13, \offset movq % r12,
+	3 * 8 +\offset(% rsp) CFI_REL_OFFSET r12, \offset movq % rbp,
+	4 * 8 +\offset(% rsp) CFI_REL_OFFSET rbp, \offset movq % rbx,
+	5 * 8 +\offset(% rsp) CFI_REL_OFFSET rbx, \offset.endm
+
+							  .macro
+						  RESTORE_EXTRA_REGS offset =
+							  0 movq 0 * 8 +\offset(
+								  % rsp),
+	% r15 CFI_RESTORE r15 movq 1 * 8 +\offset(% rsp),
+	% r14 CFI_RESTORE r14 movq 2 * 8 +\offset(% rsp),
+	% r13 CFI_RESTORE r13 movq 3 * 8 +\offset(% rsp),
+	% r12 CFI_RESTORE r12 movq 4 * 8 +\offset(% rsp),
+	% rbp CFI_RESTORE rbp movq 5 * 8 +\offset(% rsp),
+	% rbx CFI_RESTORE rbx.endm.macro RESTORE_C_REGS_HELPER rstor_rax = 1,
+	rstor_rcx = 1, rstor_r11 = 1,
+	rstor_r8910 = 1, rstor_rdx = 1.if \rstor_r11 movq 6 * 8(% rsp),
+	% r11 CFI_RESTORE r11.endif.if \rstor_r8910 movq 7 * 8(% rsp),
+	% r10 CFI_RESTORE r10 movq 8 * 8(% rsp),
+	% r9 CFI_RESTORE r9 movq 9 * 8(% rsp),
+	% r8 CFI_RESTORE r8.endif.if \rstor_rax movq 10 * 8(% rsp),
+	% rax CFI_RESTORE rax.endif.if \rstor_rcx movq 11 * 8(% rsp),
+	% rcx CFI_RESTORE rcx.endif.if \rstor_rdx movq 12 * 8(% rsp),
+	% rdx CFI_RESTORE rdx.endif movq 13 * 8(% rsp),
+	% rsi CFI_RESTORE rsi movq 14 * 8(% rsp),
+	% rdi CFI_RESTORE rdi.endm.macro RESTORE_C_REGS RESTORE_C_REGS_HELPER 1,
+	1, 1, 1,
+	1.endm
+
+		.macro ALLOC_PT_GPREGS_ON_STACK addskip = 0 subq $15 *
+								  8 +\addskip,
+	% rsp CFI_ADJUST_CFA_OFFSET
+			15 * 8 +\addskip.endm
+
+					.macro REMOVE_PT_GPREGS_FROM_STACK
+				addskip = 0 addq $15 * 8 +\addskip,
+	% rsp CFI_ADJUST_CFA_OFFSET -
+		(15 * 8 +\addskip)
+			.endm
+
+			.macro SAVE_ALL ALLOC_PT_GPREGS_ON_STACK SAVE_C_REGS
+		SAVE_EXTRA_REGS
+			.endm
+
+			.macro RESTORE_ALL RESTORE_EXTRA_REGS RESTORE_C_REGS
+		REMOVE_PT_GPREGS_FROM_STACK.endm
+#endif //CONFIG_X86_64
+#endif
+
+#endif
diff --git a/drivers/platform/x86/socwatchhv/inc/control.h b/drivers/platform/x86/socwatchhv/inc/control.h
new file mode 100644
index 000000000000..7403150dd679
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/control.h
@@ -0,0 +1,194 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _CONTROL_H_
+#define _CONTROL_H_
+
+#include <linux/smp.h>
+#include <linux/timer.h>
+#include <asm/io.h>
+#include <asm/atomic.h>
+
+#include "swhv_driver.h"
+/****************************************************************************
+ **  Handy Short cuts
+ ***************************************************************************/
+
+typedef void *pvoid;
+#define TRUE 1
+#define FALSE 0
+/*
+ *  These routines have macros defined in asm/system.h
+ */
+#define SYS_Local_Irq_Enable() local_irq_enable()
+#define SYS_Local_Irq_Disable() local_irq_disable()
+#define SYS_Local_Irq_Save(flags) local_irq_save(flags)
+#define SYS_Local_Irq_Restore(flags) local_irq_restore(flags)
+
+/*
+ * CONTROL_THIS_CPU()
+ *     Parameters
+ *         None
+ *     Returns
+ *         CPU number of the processor being executed on
+ *
+ */
+#define CONTROL_THIS_CPU() smp_processor_id()
+
+/****************************************************************************
+ **  Interface definitions
+ ***************************************************************************/
+
+/*
+ *  Execution Control Functions
+ */
+
+extern void CONTROL_Invoke_Cpu(s32 cpuid, void (*func)(pvoid), pvoid ctx);
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_Service(func, ctx, blocking, exclude)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function
+ *                      invocation
+ * @param    blocking - Wait for invoked function to complete
+ * @param    exclude  - exclude the current core from executing the code
+ *
+ * @returns  none
+ *
+ * @brief    Service routine to handle all kinds of parallel invoke on
+ *           all CPU calls
+ *
+ * <I>Special Notes:</I>
+ *         Invoke the function provided in parallel in either a
+ *         blocking/non-blocking mode.
+ *         The current core may be excluded if desired.
+ *         NOTE - Do not call this function directly from source code.
+ *         Use the aliases
+ *         CONTROL_Invoke_Parallel(), CONTROL_Invoke_Parallel_NB(),
+ *         CONTROL_Invoke_Parallel_XS().
+ *
+ */
+extern void CONTROL_Invoke_Parallel_Service(void (*func)(pvoid), pvoid ctx,
+					    s32 blocking, s32 exclude);
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel(func, ctx)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function
+ *                      invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel. Wait for all the
+ *           functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel, including the CPU
+ *        that the control is being invoked on
+ *
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel(a, b)                                          \
+	CONTROL_Invoke_Parallel_Service((a), (b), TRUE, FALSE)
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_NB(func, ctx)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function
+ *                      invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel. DO NOT Wait for all
+ *           the functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel, including the CPU
+ *        that the control is being invoked on
+ *
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel_NB(a, b)                                       \
+	CONTROL_Invoke_Parallel_Service((a), (b), FALSE, FALSE)
+
+/*
+ * @fn VOID CONTROL_Invoke_Parallel_XS(func, ctx)
+ *
+ * @param    func     - function to be invoked by each core in the system
+ * @param    ctx      - pointer to the parameter block for each function
+ *                      invocation
+ *
+ * @returns  none
+ *
+ * @brief    Invoke the named function in parallel. Wait for all
+ *           the functions to complete.
+ *
+ * <I>Special Notes:</I>
+ *        Invoke the function named in parallel, excluding the CPU
+ *        that the control is being invoked on
+ *
+ *        Macro built on the service routine
+ *
+ */
+#define CONTROL_Invoke_Parallel_XS(a, b)                                       \
+	CONTROL_Invoke_Parallel_Service((a), (b), TRUE, TRUE)
+
+#endif
diff --git a/drivers/platform/x86/socwatchhv/inc/pw_types.h b/drivers/platform/x86/socwatchhv/inc/pw_types.h
new file mode 100644
index 000000000000..b8a3ac855e53
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/pw_types.h
@@ -0,0 +1,132 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_TYPES_H_
+#define _PW_TYPES_H_
+
+#if defined(__linux__) || defined(__QNX__)
+
+#ifndef __KERNEL__
+/*
+ * Called from Ring-3.
+ */
+#include <stdint.h> // Grab 'uint64_t' etc.
+#include <unistd.h> // Grab 'pid_t'
+/*
+ * UNSIGNED types...
+ */
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+ * SIGNED types...
+ */
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#endif // __KERNEL__
+
+#elif defined(_WIN32)
+/*
+ * UNSIGNED types...
+ */
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long long u64;
+/*
+ * SIGNED types...
+ */
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long long s64;
+typedef s32 pid_t;
+typedef s32 ssize_t;
+
+#endif // _WIN32
+
+/* ************************************
+ * Common to both operating systems.
+ * ************************************
+ */
+/*
+ * UNSIGNED types...
+ */
+typedef u8 pw_u8_t;
+typedef u16 pw_u16_t;
+typedef u32 pw_u32_t;
+typedef u64 pw_u64_t;
+
+/*
+ * SIGNED types...
+ */
+typedef s8 pw_s8_t;
+typedef s16 pw_s16_t;
+typedef s32 pw_s32_t;
+typedef s64 pw_s64_t;
+
+typedef pid_t pw_pid_t;
+
+typedef void *pvoid;
+
+#define TRUE 1
+#define FALSE 0
+
+#endif // _PW_TYPES_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/pw_version.h b/drivers/platform/x86/socwatchhv/inc/pw_version.h
new file mode 100644
index 000000000000..8e1cf1cc4d62
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/pw_version.h
@@ -0,0 +1,67 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_VERSION_H_
+#define _PW_VERSION_H_ 1
+
+/*
+ * SOCWatch driver version
+ */
+#define SWHVDRV_VERSION_MAJOR 2
+#define SWHVDRV_VERSION_MINOR 0
+#define SWHVDRV_VERSION_OTHER 0
+
+#endif // _PW_VERSION_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_defines.h b/drivers/platform/x86/socwatchhv/inc/sw_defines.h
new file mode 100644
index 000000000000..9c8995805464
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_defines.h
@@ -0,0 +1,156 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_DEFINES_H_
+#define _PW_DEFINES_H_ 1
+
+#include "sw_version.h"
+
+/* ***************************************************
+ * Common to kernel and userspace.
+ * ***************************************************
+ */
+#define PW_SUCCESS 0
+#define PW_ERROR 1
+#define PW_SUCCESS_NO_COLLECT 2
+
+/*
+ * Helper macro to convert 'u64' to 'unsigned long long' to avoid gcc warnings.
+ */
+#define TO_ULL(x) (unsigned long long)(x)
+/*
+* Convert an arg to 'long long'
+*/
+#define TO_LL(x) (long long)(x)
+/*
+ * Convert an arg to 'unsigned long'
+ */
+#define TO_UL(x) (unsigned long)(x)
+/*
+ * Helper macro for string representation of a boolean value.
+ */
+#define GET_BOOL_STRING(b) ((b) ? "TRUE" : "FALSE")
+
+/*
+ * Circularly increment 'i' MODULO 'l'.
+ * ONLY WORKS IF 'l' is (power of 2 - 1) ie.
+ * l == (2 ^ x) - 1
+ */
+#define CIRCULAR_INC(index, mask) (((index) + 1) & (mask))
+#define CIRCULAR_ADD(index, val, mask) (((index) + (val)) & (mask))
+/*
+ * Circularly decrement 'i'.
+ */
+#define CIRCULAR_DEC(i, m)                                                     \
+	({                                                                     \
+		int __tmp1 = (i);                                              \
+		if (--__tmp1 < 0)                                              \
+			__tmp1 = (m);                                          \
+		__tmp1;                                                        \
+	})
+/*
+ * Retrieve size of an array.
+ */
+#define SW_ARRAY_SIZE(array) (sizeof(array) / sizeof((array)[0]))
+/*
+ * Should the driver count number of dropped samples?
+ */
+#define DO_COUNT_DROPPED_SAMPLES 1
+/*
+ * Extract F/W major, minor versions.
+ * Assumes version numbers are 8b unsigned ints.
+ */
+#define SW_GET_SCU_FW_VERSION_MAJOR(ver) (((ver) >> 8) & 0xff)
+#define SW_GET_SCU_FW_VERSION_MINOR(ver) ((ver)&0xff)
+/*
+ * Max size of process name retrieved from kernel.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+
+/*
+ * Number of SOCPERF counters.
+ * Needed by both Ring-0 and Ring-3
+ */
+#define SW_NUM_SOCPERF_COUNTERS 9
+
+/*
+ * Max size of process name retrieved from kernel space.
+ */
+#define SW_MAX_PROC_NAME_SIZE 16
+/*
+ * Max size of kernel wakelock name.
+ */
+#define SW_MAX_KERNEL_WAKELOCK_NAME_SIZE 100
+
+/* Data value read when a telemetry data read fails. */
+#define SW_TELEM_READ_FAIL_VALUE 0xF00DF00DF00DF00D
+
+#ifdef SWW_MERGE
+typedef enum {
+	SW_STOP_EVENT = 0,
+	SW_CS_EXIT_EVENT,
+	SW_COUNTER_RESET_EVENT,
+	SW_COUNTER_HOTKEY_EVENT,
+	SW_MAX_COLLECTION_EVENT
+} collector_stop_event_t;
+#endif // SWW_MERGE
+
+#define MAX_UNSIGNED_16_BIT_VALUE 0xFFFF
+#define MAX_UNSIGNED_24_BIT_VALUE 0xFFFFFF
+#define MAX_UNSIGNED_32_BIT_VALUE 0xFFFFFFFF
+#define MAX_UNSIGNED_64_BIT_VALUE 0xFFFFFFFFFFFFFFFF
+
+#endif // _PW_DEFINES_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_ioctl.h b/drivers/platform/x86/socwatchhv/inc/sw_ioctl.h
new file mode 100644
index 000000000000..baf93058c5c5
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_ioctl.h
@@ -0,0 +1,303 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_IOCTL_H__
+#define __SW_IOCTL_H__ 1
+
+#if defined(__linux__) || defined(__QNX__)
+#if __KERNEL__
+#include <linux/ioctl.h>
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#include <asm/compat.h>
+#include <linux/compat.h>
+#endif // COMPAT && x64
+#else // !__KERNEL__
+#include <sys/ioctl.h>
+#endif // __KERNEL__
+#endif // __linux__
+/*
+ * Ensure we pull in definition of 'DO_COUNT_DROPPED_SAMPLES'!
+ */
+#include "sw_defines.h"
+
+#ifdef ONECORE
+#ifndef __KERNEL__
+#include <winioctl.h>
+#endif //__KERNEL__
+#endif // ONECORE
+
+/*
+ * The APWR-specific IOCTL magic
+ * number -- used to ensure IOCTLs
+ * are delivered to the correct
+ * driver.
+ */
+// #define APWR_IOCTL_MAGIC_NUM 0xdead
+#define APWR_IOCTL_MAGIC_NUM 100
+
+/*
+ * The name of the device file
+ */
+// #define DEVICE_FILE_NAME "/dev/pw_driver_char_dev"
+#define PW_DEVICE_FILE_NAME "/dev/apwr_driver_char_dev"
+#define PW_DEVICE_NAME "apwr_driver_char_dev"
+
+enum sw_ioctl_cmd {
+	sw_ioctl_cmd_none = 0,
+	sw_ioctl_cmd_config,
+	sw_ioctl_cmd_cmd,
+	sw_ioctl_cmd_poll,
+	sw_ioctl_cmd_immediate_io,
+	sw_ioctl_cmd_scu_version,
+	sw_ioctl_cmd_read_immediate,
+	sw_ioctl_cmd_driver_version,
+	sw_ioctl_cmd_avail_trace,
+	sw_ioctl_cmd_avail_notify,
+	sw_ioctl_cmd_avail_collect,
+	sw_ioctl_cmd_topology_changes,
+};
+/*
+ * The actual IOCTL commands.
+ *
+ * From the kernel documentation:
+ * "_IOR" ==> Read IOCTL
+ * "_IOW" ==> Write IOCTL
+ * "_IOWR" ==> Read/Write IOCTL
+ *
+ * Where "Read" and "Write" are from the user's perspective
+ * (similar to the file "read" and "write" calls).
+ */
+#ifdef SWW_MERGE // Windows
+//
+// Device type           -- in the "User Defined" range."
+//
+#define POWER_I_CONF_TYPE 40000
+
+// List assigned tracepoint id
+#define CSIR_TRACEPOINT_ID_MASK 1
+#define DEVICE_STATE_TRACEPOINT_ID_MASK 2
+#define CSIR_SEPARATE_TRACEPOINT_ID_MASK 3
+#define RESET_TRACEPOINT_ID_MASK 4
+#define DISPLAY_ON_TRACEPOINT_ID_MASK 5
+
+#ifdef SWW_MERGE
+//
+// TELEM BAR CONFIG
+//
+#define MAX_TELEM_BAR_CFG 3
+#define TELEM_MCHBAR_CFG 0
+#define TELEM_IPC1BAR_CFG 1
+#define TELEM_SSRAMBAR_CFG 2
+#endif
+
+//
+// The IOCTL function codes from 0x800 to 0xFFF are for customer use.
+//
+#define PW_IOCTL_CONFIG                                                        \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x900, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_START_COLLECTION                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x901, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_STOP_COLLECTION                                               \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x902, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+// TODO: pause, resume, cancel not supported yet
+#define PW_IOCTL_PAUSE_COLLECTION                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x903, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_RESUME_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x904, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_CANCEL_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x905, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+#define PW_IOCTL_GET_PROCESSOR_GROUP_TOPOLOGY                                  \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x906, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_TOPOLOGY                                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x907, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x908, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x909, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DRV_CLEANUP                                                   \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90A, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_COLLECTION_EVENT                                          \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90B, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_TRY_STOP_EVENT                                                \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90C, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_PCH_ACTIVE_INTERVAL                                       \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90D, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_TELEM_BAR                                                 \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90E, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_METADATA                                                      \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x90F, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_SET_GBE_INTERVAL                                              \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x910, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_ENABLE_COLLECTION                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x911, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DISABLE_COLLECTION                                            \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x912, METHOD_BUFFERED, FILE_ANY_ACCESS)
+#define PW_IOCTL_DRIVER_BUILD_DATE                                             \
+	CTL_CODE(POWER_I_CONF_TYPE, 0x913, METHOD_BUFFERED, FILE_ANY_ACCESS)
+
+#elif !defined(__APPLE__)
+#define PW_IOCTL_CONFIG                                                        \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config,                        \
+	     struct sw_driver_ioctl_arg *)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD                                                           \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                          \
+	      struct sw_driver_ioctl_arg *)
+#else
+#define PW_IOCTL_CMD                                                           \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                           \
+	     struct sw_driver_ioctl_arg *)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io,                 \
+	      struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_SCU_FW_VERSION                                            \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version,                   \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_READ_IMMEDIATE                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate,               \
+	      struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_DRIVER_VERSION                                            \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version,                \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS                                     \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace,                   \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS                                       \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify,                  \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect,                 \
+	     struct sw_driver_ioctl_arg *)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes,              \
+	     struct sw_driver_ioctl_arg *)
+#else // __APPLE__
+#define PW_IOCTL_CONFIG                                                        \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config,                        \
+	     struct sw_driver_ioctl_arg)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD                                                           \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd,                          \
+	      struct sw_driver_ioctl_arg)
+#else
+#define PW_IOCTL_CMD                                                           \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, struct sw_driver_ioctl_arg)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO                                                  \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io,                 \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_SCU_FW_VERSION                                            \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version,                  \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_READ_IMMEDIATE                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate,               \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_DRIVER_VERSION                                            \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version,               \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS                                     \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace,                  \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS                                       \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify,                 \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS                                      \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect,                \
+	      struct sw_driver_ioctl_arg)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES                                          \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes,             \
+	      struct sw_driver_ioctl_arg)
+#endif // __APPLE__
+
+/*
+ * 32b-compatible version of the above
+ * IOCTL numbers. Required ONLY for
+ * 32b compatibility on 64b systems,
+ * and ONLY by the driver.
+ */
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#define PW_IOCTL_CONFIG32                                                      \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_config, compat_uptr_t)
+#if DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_CMD32                                                         \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#else
+#define PW_IOCTL_CMD32                                                         \
+	_IOW(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_cmd, compat_uptr_t)
+#endif // DO_COUNT_DROPPED_SAMPLES
+#define PW_IOCTL_POLL32 _IO(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_poll)
+#define PW_IOCTL_IMMEDIATE_IO32                                                \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_immediate_io, compat_uptr_t)
+#define PW_IOCTL_GET_SCU_FW_VERSION32                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_scu_version, compat_uptr_t)
+#define PW_IOCTL_READ_IMMEDIATE32                                              \
+	_IOWR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_read_immediate, compat_uptr_t)
+#define PW_IOCTL_GET_DRIVER_VERSION32                                          \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_driver_version, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_TRACEPOINTS32                                   \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_trace, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_NOTIFIERS32                                     \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_notify, compat_uptr_t)
+#define PW_IOCTL_GET_AVAILABLE_COLLECTORS32                                    \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_avail_collect, compat_uptr_t)
+#define PW_IOCTL_GET_TOPOLOGY_CHANGES32                                        \
+	_IOR(APWR_IOCTL_MAGIC_NUM, sw_ioctl_cmd_topology_changes, compat_uptr_t)
+#endif // defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#endif // __SW_IOCTL_H__
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_kernel_defines.h b/drivers/platform/x86/socwatchhv/inc/sw_kernel_defines.h
new file mode 100644
index 000000000000..23e939a732c7
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_kernel_defines.h
@@ -0,0 +1,164 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef _SW_KERNEL_DEFINES_H_
+#define _SW_KERNEL_DEFINES_H_ 1
+
+#include "sw_defines.h"
+
+#if defined(__APPLE__)
+#define likely(x) (x)
+#define unlikely(x) (x)
+#endif // __APPLE__
+
+#if !defined(__APPLE__)
+#define CPU() (raw_smp_processor_id())
+#define RAW_CPU() (raw_smp_processor_id())
+#else
+#define CPU() (cpu_number())
+#define RAW_CPU() (cpu_number())
+#endif // __APPLE__
+
+#define TID() (current->pid)
+#define PID() (current->tgid)
+#define NAME() (current->comm)
+#define PKG(c) (cpu_data(c).phys_proc_id)
+#define IT_REAL_INCR() (current->signal->it_real_incr.tv64)
+
+#define ATOMIC_CAS(ptr, old_val, new_val)                                      \
+	(cmpxchg((ptr), (old_val), (new_val)) == (old_val))
+
+/*
+ * Should we measure overheads?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_OVERHEAD_MEASUREMENTS 0
+/*
+ * Should we track memory usage?
+ * '1' ==> YES
+ * '0' ==> NO
+ */
+#define DO_TRACK_MEMORY_USAGE 0
+/*
+ * Are we compiling with driver profiling support
+ * turned ON? If YES then force 'DO_OVERHEAD_MEASUREMENTS'
+ * and 'DO_TRACK_MEMORY_USAGE' to be TRUE.
+ */
+#if IS_ENABLED(CONFIG_SOCWATCH_DRIVER_PROFILING)
+#if !DO_OVERHEAD_MEASUREMENTS
+#undef DO_OVERHEAD_MEASUREMENTS
+#define DO_OVERHEAD_MEASUREMENTS 1
+#endif // DO_OVERHEAD_MEASUREMENTS
+#if !DO_TRACK_MEMORY_USAGE
+#undef DO_TRACK_MEMORY_USAGE
+#define DO_TRACK_MEMORY_USAGE 1
+#endif // DO_TRACK_MEMORY_USAGE
+#endif // CONFIG_SOCWATCH_DRIVER_PROFILING
+/*
+ * Should we allow debug output.
+ * Set to: "1" ==> 'OUTPUT' is enabled.
+ *         "0" ==> 'OUTPUT' is disabled.
+ */
+#define DO_DEBUG_OUTPUT 0
+/*
+ * Control whether to output driver ERROR messages.
+ * These are independent of the 'OUTPUT' macro
+ * (which controls debug messages).
+ * Set to '1' ==> Print driver error messages (to '/var/log/messages')
+ *        '0' ==> Do NOT print driver error messages
+ */
+#define DO_PRINT_DRIVER_ERROR_MESSAGES 1
+/*
+ * Macros to control output printing.
+ */
+#if !defined(__APPLE__)
+#if DO_DEBUG_OUTPUT
+#define pw_pr_debug(...) printk(KERN_INFO __VA_ARGS__)
+#define pw_pr_warn(...) printk(KERN_WARNING __VA_ARGS__)
+#else
+#define pw_pr_debug(...)
+#define pw_pr_warn(...)
+#endif
+#define pw_pr_force(...) printk(KERN_INFO __VA_ARGS__)
+#else
+#if DO_DEBUG_OUTPUT
+#define pw_pr_debug(...) IOLog(__VA_ARGS__)
+#define pw_pr_warn(...) IOLog(__VA_ARGS__)
+#else
+#define pw_pr_debug(...)
+#define pw_pr_warn(...)
+#endif
+#define pw_pr_force(...) IOLog(__VA_ARGS__)
+#endif // __APPLE__
+
+/*
+ * Macro for driver error messages.
+ */
+#if !defined(__APPLE__)
+#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+#define pw_pr_error(...) printk(KERN_ERR __VA_ARGS__)
+#else
+#define pw_pr_error(...)
+#endif
+#else
+#if (DO_PRINT_DRIVER_ERROR_MESSAGES || DO_DEBUG_OUTPUT)
+#define pw_pr_error(...) IOLog(__VA_ARGS__)
+#else
+#define pw_pr_error(...)
+#endif
+#endif // __APPLE__
+
+#endif // _SW_KERNEL_DEFINES_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_structs.h b/drivers/platform/x86/socwatchhv/inc/sw_structs.h
new file mode 100644
index 000000000000..94e58b5244f4
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_structs.h
@@ -0,0 +1,501 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SW_STRUCTS_H__
+#define __SW_STRUCTS_H__ 1
+
+#include "sw_types.h"
+
+/*
+ * An enumeration of MSR types.
+ * Required if we want to differentiate
+ * between different types of MSRs.
+ */
+enum sw_msr_type {
+	SW_MSR_TYPE_THREAD,
+	SW_MSR_TYPE_CORE,
+	SW_MSR_TYPE_MODULE,
+	SW_MSR_TYPE_PACKAGE,
+	SW_MSR_TYPE_SOC,
+	SW_MSR_TYPE_MAX,
+};
+
+/*
+ * Convenience for a 'string' data type.
+ * Not strictly required.
+ */
+#pragma pack(push, 1)
+typedef struct sw_string_type {
+	pw_u16_t len;
+	char data[1];
+} sw_string_type_t;
+#pragma pack(pop)
+#define SW_STRING_TYPE_HEADER_SIZE()                                           \
+	(sizeof(struct sw_string_type) - sizeof(char[1]))
+
+#pragma pack(push, 1)
+struct sw_key_value_payload {
+	pw_u16_t m_numKeyValuePairs;
+	char data[1];
+};
+#pragma pack(pop)
+#define SW_KEY_VALUE_PAYLOAD_HEADER_SIZE()                                     \
+	(sizeof(struct sw_key_value_payload) - sizeof(char[1]))
+
+typedef enum sw_kernel_wakelock_type {
+	SW_WAKE_LOCK = 0, // A kernel wakelock was acquired
+	SW_WAKE_UNLOCK = 1, // A kernel wakelock was released
+	SW_WAKE_LOCK_TIMEOUT =
+		2, // A kernel wakelock was acquired with a timeout
+	SW_WAKE_LOCK_INITIAL = 3, // A kernel wakelock was acquired before the
+	//   collection started
+	SW_WAKE_UNLOCK_ALL = 4, // All previously held kernel wakelocks were
+	//   released -- used in ACPI S3 notifications
+} sw_kernel_wakelock_type_t;
+
+typedef enum sw_when_type {
+	SW_WHEN_TYPE_BEGIN = 0, /* Start snapshot */
+	SW_WHEN_TYPE_POLL,
+	SW_WHEN_TYPE_NOTIFIER,
+	SW_WHEN_TYPE_TRACEPOINT,
+	SW_WHEN_TYPE_END, /* Stop snapshot */
+	SW_WHEN_TYPE_NONE
+} sw_when_type_t;
+
+/**
+ * trigger_bits is defined to use type pw_u8_t that makes only up
+ * to 8 types possible
+ */
+#define SW_TRIGGER_BEGIN_MASK() (1U << SW_WHEN_TYPE_BEGIN)
+#define SW_TRIGGER_END_MASK() (1U << SW_WHEN_TYPE_END)
+#define SW_TRIGGER_POLL_MASK() (1U << SW_WHEN_TYPE_POLL)
+#define SW_TRIGGER_TRACEPOINT_MASK() (1U << SW_WHEN_TYPE_TRACEPOINT)
+#define SW_TRIGGER_NOTIFIER_MASK() (1U << SW_WHEN_TYPE_NOTIFIER)
+#define SW_GET_TRIGGER_MASK_VALUE(m) (1U << (m))
+#define SW_TRIGGER_MASK_ALL() (0xFF)
+
+enum sw_io_cmd { SW_IO_CMD_READ = 0, SW_IO_CMD_WRITE, SW_IO_CMD_MAX };
+
+#pragma pack(push, 1)
+struct sw_driver_msr_io_descriptor {
+	pw_u64_t address;
+	enum sw_msr_type type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_ipc_mmio_io_descriptor {
+	union {
+#ifdef SWW_MERGE
+#pragma warning(push)
+#pragma warning(                                                               \
+	disable : 4201) // disable C4201: nonstandard extension used: nameless struct/union
+#endif
+		struct {
+			pw_u16_t command;
+			pw_u16_t sub_command;
+		};
+#ifdef SWW_MERGE
+#pragma warning(pop) // enable C4201
+#endif
+		union {
+			pw_u32_t ipc_command; // (sub_command << 12) | (command)
+			pw_u8_t is_gbe; // Used only for GBE MMIO
+		};
+	};
+	// TODO: add a section for 'ctrl_address' and 'ctrl_remapped_address'
+	union {
+		pw_u64_t data_address; // Will be "io_remapped"
+		pw_u64_t data_remapped_address;
+	};
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pci_io_descriptor {
+	pw_u32_t bus;
+	pw_u32_t device;
+	pw_u32_t function;
+#ifdef __QNX__
+	union {
+		pw_u32_t offset;
+		pw_u32_t index;
+	};
+#else /* __QNX__ */
+	pw_u32_t offset;
+#endif /* __QNX__ */
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_configdb_io_descriptor {
+	// pw_u32_t port;
+	// pw_u32_t offset;
+	pw_u32_t address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_trace_args_io_descriptor {
+	pw_u8_t num_args; // Number of valid entries in the 'args' array, below; 1 <= num_args <= 7
+	pw_u8_t args[7]; // Max of 7 args can be recorded
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+/**
+ * struct - sw_driver_telem_io_descriptor - Telemetry Metric descriptor
+ *
+ * @id:    (Client & Driver) Telemetry ID of the counter to read.
+ * @idx:   (Driver only) index into telem array to read, or the row
+ *            of the telem_indirect table to lookup the telem array index.
+ * @unit:  Unit from which to collect:  0 = PMC, 1 = PUNIT
+ *              Values come from the telemetry_unit enum.
+ * @scale_op:  When there are multiple instances of a telem value (e.g.
+ *              module C-states) the operation to use when scaling the CPU ID
+ *              and adding it to the telemetry data ID.
+ * @scale_val: Amount to scale an ID (when scaling one.)
+ *
+ * Like all hardware mechanism descriptors, the client uses this to pass
+ * metric hardware properties (unit and ID) to the driver.  The driver
+ * uses it to program the telemetry unit.
+ *
+ * Users can specify that IDs should be scaled based on the CPU id, using
+ * the equation: ID = ID_value + (cpuid <scaling_op> <scaling_val>)
+ * where <scaling_op> is one of +, *, /, or %, and scaling_val is an integer
+ * value.  This gives you:
+ *            Operation             scale_op     scale_val
+ *       Single instance of an ID       *            0
+ *       Sequentially increasing
+ *          CPU-specific values         *            1
+ *       Per module cpu-specific
+ *          values (2 cores/module)     /            2
+ *       Round Robin assignment         %         cpu_count
+ *
+ * Note that scaling_value of 0 implies that no scaling should be
+ * applied.  While (*, 1) is equivalent to (+, 0), the scaling value of 0
+ * is reserved/defined to mean "no scaling", and is disallowed.
+ *
+ * If you're really tight on space, you could always fold unit and
+ * scale_op into a single byte without a lot of pain or even effort.
+ */
+struct sw_driver_telem_io_descriptor {
+	union {
+		pw_u16_t id;
+		pw_u8_t idx;
+	};
+	pw_u8_t unit;
+	pw_u8_t scale_op;
+	pw_u16_t scale_val;
+};
+#pragma pack(pop)
+enum telemetry_unit { TELEM_PUNIT = 0, TELEM_PMC, TELEM_UNIT_NONE };
+#define TELEM_MAX_ID 0xFFFF /* Maximum value of a Telemtry event ID. */
+#define TELEM_MAX_SCALE 0xFFFF /* Maximum ID scaling value. */
+#define TELEM_OP_ADD '+' /* Addition operator */
+#define TELEM_OP_MULT '*' /* Multiplication operator */
+#define TELEM_OP_DIV '/' /* Division operator */
+#define TELEM_OP_MOD '%' /* Modulus operator */
+#define TELEM_OP_NONE 'X' /* No operator--Not a scaled ID */
+
+#pragma pack(push, 1)
+struct sw_driver_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t interface_address;
+		pw_u64_t interface_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t data_address;
+		pw_u64_t data_remapped_address;
+	};
+	pw_u64_t command;
+	pw_u64_t command_mask;
+	pw_u16_t run_busy_bit;
+	pw_u16_t is_msr_type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct sw_driver_pch_mailbox_io_descriptor {
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mtpmc_address;
+		pw_u64_t mtpmc_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t msg_full_sts_address;
+		pw_u64_t msg_full_sts_remapped_address;
+	};
+	union {
+		/*
+		 * Will be "io_remapped"
+		 */
+		pw_u64_t mfpmc_address;
+		pw_u64_t mfpmc_remapped_address;
+	};
+	pw_u32_t data_address;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_io_descriptor {
+	pw_u16_t collection_type;
+	// TODO: specify READ/WRITE
+	pw_s16_t collection_command; // One of 'enum sw_io_cmd'
+	pw_u16_t counter_size_in_bytes; // The number of bytes to READ or WRITE
+	union {
+		struct sw_driver_msr_io_descriptor msr_descriptor;
+		struct sw_driver_ipc_mmio_io_descriptor ipc_descriptor;
+		struct sw_driver_ipc_mmio_io_descriptor mmio_descriptor;
+		struct sw_driver_pci_io_descriptor pci_descriptor;
+		struct sw_driver_configdb_io_descriptor configdb_descriptor;
+		struct sw_driver_trace_args_io_descriptor trace_args_descriptor;
+		struct sw_driver_telem_io_descriptor telem_descriptor;
+		struct sw_driver_pch_mailbox_io_descriptor
+			pch_mailbox_descriptor;
+		struct sw_driver_mailbox_io_descriptor mailbox_descriptor;
+	};
+	pw_u64_t write_value; // The value to WRITE
+} sw_driver_io_descriptor_t;
+#pragma pack(pop)
+
+/**
+ * sw_driver_interface_info is used to map data collected by kernel-level
+ * collectors to metrics.  The client passes one of these structs to the
+ * driver for each metric the driver should collect.  The driver tags the
+ * collected data (messages) using info from this struct. When processing
+ * data from the driver, the client uses its copy of this data to
+ * identify the plugin, metric, and message IDs of each message.
+ */
+#pragma pack(push, 1)
+struct sw_driver_interface_info {
+	pw_u64_t tracepoint_id_mask;
+	pw_u64_t notifier_id_mask;
+	pw_s16_t cpu_mask; // On which CPU(s) should the driver read the data?
+		// Currently:  -2 ==> read on ALL CPUs,
+		//             -1 ==> read on ANY CPU,
+		//           >= 0 ==> the specific CPU to read on
+	pw_s16_t plugin_id; // Metric Plugin SID
+	pw_s16_t metric_id; // Domain-specific ID assigned by each Metric Plugin
+	pw_s16_t msg_id; // Msg ID retrieved from the SoC Watch config file
+	pw_u16_t num_io_descriptors; // Number of descriptors in the array, below.
+	pw_u8_t trigger_bits; // Mask of 'when bits' to fire this collector.
+	pw_u16_t sampling_interval_msec; // Sampling interval, in msecs
+	pw_u8_t descriptors[1]; // Array of sw_driver_io_descriptor structs.
+};
+#pragma pack(pop)
+
+#define SW_DRIVER_INTERFACE_INFO_HEADER_SIZE()                                 \
+	(sizeof(struct sw_driver_interface_info) - sizeof(pw_u8_t[1]))
+
+#pragma pack(push, 1)
+struct sw_driver_interface_msg {
+	pw_u16_t num_infos; // Number of 'sw_driver_interface_info' structs contained within the 'infos' variable, below
+	pw_u16_t min_polling_interval_msecs; // Min time to wait before polling; used exclusively
+		// with the low overhead, context-switch based
+		// polling mode
+	// pw_u16_t infos_size_bytes; // Size of data inlined within the 'infos' variable, below
+	pw_u8_t infos[1];
+};
+#pragma pack(pop)
+#define SW_DRIVER_INTERFACE_MSG_HEADER_SIZE()                                  \
+	(sizeof(struct sw_driver_interface_msg) - sizeof(pw_u8_t[1]))
+
+typedef enum sw_name_id_type {
+	SW_NAME_TYPE_TRACEPOINT,
+	SW_NAME_TYPE_NOTIFIER,
+	SW_NAME_TYPE_COLLECTOR,
+	SW_NAME_TYPE_MAX,
+} sw_name_id_type_t;
+
+#pragma pack(push, 1)
+struct sw_name_id_pair {
+	pw_u16_t id;
+	pw_u16_t type; // One of 'sw_name_id_type'
+	struct sw_string_type name;
+};
+#pragma pack(pop)
+#define SW_NAME_ID_HEADER_SIZE()                                               \
+	(sizeof(struct sw_name_id_pair) - sizeof(struct sw_string_type))
+
+#pragma pack(push, 1)
+struct sw_name_info_msg {
+	pw_u16_t num_name_id_pairs;
+	pw_u16_t payload_len;
+	pw_u8_t pairs[1];
+};
+#pragma pack(pop)
+
+/**
+ * This is the basic data structure for passing data collected by the
+ * kernel-level collectors up to the client.  In addition to the data
+ * (payload), it contains the minimum metadata required for the client
+ * to identify the source of that data.
+ */
+#pragma pack(push, 1)
+typedef struct sw_driver_msg {
+	pw_u64_t tsc;
+	pw_u16_t cpuidx;
+	pw_u8_t plugin_id; // Cannot have more than 256 plugins
+	pw_u8_t metric_id; // Each plugin cannot handle more than 256 metrics
+	pw_u8_t msg_id; // Each metric cannot have more than 256 components
+	pw_u16_t payload_len;
+	// pw_u64_t p_payload;  // Ptr to payload
+	union {
+		pw_u64_t __dummy; // Ensure size of struct is consistent on x86, x64
+		char *p_payload; // Ptr to payload (collected data values).
+	};
+} sw_driver_msg_t;
+#pragma pack(pop)
+#define SW_DRIVER_MSG_HEADER_SIZE()                                            \
+	(sizeof(struct sw_driver_msg) - sizeof(pw_u64_t))
+
+typedef enum sw_driver_collection_cmd {
+	SW_DRIVER_START_COLLECTION = 1,
+	SW_DRIVER_STOP_COLLECTION = 2,
+	SW_DRIVER_PAUSE_COLLECTION = 3,
+	SW_DRIVER_RESUME_COLLECTION = 4,
+	SW_DRIVER_CANCEL_COLLECTION = 5,
+} sw_driver_collection_cmd_t;
+
+#pragma pack(push, 1)
+struct sw_driver_version_info {
+	pw_u16_t major;
+	pw_u16_t minor;
+	pw_u16_t other;
+};
+#pragma pack(pop)
+
+enum cpu_action {
+	SW_CPU_ACTION_NONE,
+	SW_CPU_ACTION_OFFLINE,
+	SW_CPU_ACTION_ONLINE_PREPARE,
+	SW_CPU_ACTION_ONLINE,
+	SW_CPU_ACTION_MAX,
+};
+#pragma pack(push, 1)
+struct sw_driver_topology_change {
+	pw_u64_t timestamp; // timestamp
+	enum cpu_action type; // One of 'enum cpu_action'
+	pw_u16_t cpu; // logical cpu
+	pw_u16_t core; // core id
+	pw_u16_t pkg; // pkg/physical id
+};
+struct sw_driver_topology_msg {
+	pw_u16_t num_entries;
+	pw_u8_t topology_entries[1];
+};
+#pragma pack(pop)
+
+/**
+ * An enumeration of possible pm states that
+ * SoC Watch is interested in
+ */
+enum sw_pm_action {
+	SW_PM_ACTION_NONE,
+	SW_PM_ACTION_SUSPEND_ENTER,
+	SW_PM_ACTION_SUSPEND_EXIT,
+	SW_PM_ACTION_HIBERNATE_ENTER,
+	SW_PM_ACTION_HIBERNATE_EXIT,
+	SW_PM_ACTION_MAX,
+};
+
+/**
+ * An enumeration of possible actions that trigger
+ * the power notifier
+ */
+enum sw_pm_mode {
+	SW_PM_MODE_FIRMWARE,
+	SW_PM_MODE_NONE,
+};
+
+#define SW_PM_VALUE(mode, action) ((mode) << 16 | (action))
+
+/*
+ * Wrapper for ioctl arguments.
+ * EVERY ioctl MUST use this struct!
+ */
+#pragma pack(push, 1)
+struct sw_driver_ioctl_arg {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	// pw_u64_t p_in_arg; // Pointer to input arg
+	// pw_u64_t p_out_arg; // Pointer to output arg
+	char *in_arg;
+	char *out_arg;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct sw_driver_msg_interval {
+	pw_u8_t plugin_id; // Cannot have more than 256 plugins
+	pw_u8_t metric_id; // Each plugin cannot handle more than 256 metrics
+	pw_u8_t msg_id; // Each metric cannot have more than 256 components
+	pw_u16_t interval; // collection interval
+} sw_driver_msg_interval_t;
+#pragma pack(pop)
+
+#endif // __SW_STRUCTS_H__
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_types.h b/drivers/platform/x86/socwatchhv/inc/sw_types.h
new file mode 100644
index 000000000000..914ce9806965
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_types.h
@@ -0,0 +1,152 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _PW_TYPES_H_
+#define _PW_TYPES_H_
+
+#if defined(__linux__) || defined(__APPLE__) || defined(__QNX__)
+
+#ifndef __KERNEL__
+/*
+ * Called from Ring-3.
+ */
+#include <stdint.h> // Grab 'uint64_t' etc.
+#include <unistd.h> // Grab 'pid_t'
+/*
+ * UNSIGNED types...
+ */
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+ * SIGNED types...
+ */
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+
+#else // __KERNEL__
+#if !defined(__APPLE__)
+#include <linux/types.h>
+#else // __APPLE__
+#include <sys/types.h>
+#include <stdint.h> // Grab 'uint64_t' etc.
+
+typedef uint8_t u8;
+typedef uint16_t u16;
+typedef uint32_t u32;
+typedef uint64_t u64;
+/*
+* SIGNED types...
+*/
+typedef int8_t s8;
+typedef int16_t s16;
+typedef int32_t s32;
+typedef int64_t s64;
+#endif // __APPLE__
+#endif // __KERNEL__
+
+#elif defined(_WIN32)
+typedef __int32 int32_t;
+typedef unsigned __int32 uint32_t;
+typedef __int64 int64_t;
+typedef unsigned __int64 uint64_t;
+
+/*
+ * UNSIGNED types...
+ */
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef unsigned long long u64;
+
+/*
+ * SIGNED types...
+ */
+typedef signed char s8;
+typedef signed short s16;
+typedef signed int s32;
+typedef signed long long s64;
+typedef s32 pid_t;
+typedef s32 ssize_t;
+
+#endif // _WIN32
+
+/* ************************************
+ * Common to both operating systems.
+ * ************************************
+ */
+/*
+ * UNSIGNED types...
+ */
+typedef u8 pw_u8_t;
+typedef u16 pw_u16_t;
+typedef u32 pw_u32_t;
+typedef u64 pw_u64_t;
+
+/*
+ * SIGNED types...
+ */
+typedef s8 pw_s8_t;
+typedef s16 pw_s16_t;
+typedef s32 pw_s32_t;
+typedef s64 pw_s64_t;
+
+typedef pid_t pw_pid_t;
+
+#endif // _PW_TYPES_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/sw_version.h b/drivers/platform/x86/socwatchhv/inc/sw_version.h
new file mode 100644
index 000000000000..5797edffa64d
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/sw_version.h
@@ -0,0 +1,74 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef __SW_VERSION_H__
+#define __SW_VERSION_H__ 1
+
+/*
+ * SOCWatch driver version
+ */
+#define SW_DRIVER_VERSION_MAJOR 2
+#define SW_DRIVER_VERSION_MINOR 6
+#define SW_DRIVER_VERSION_OTHER 2
+
+/*
+ * Every SOC Watch userspace component shares the same version number.
+ */
+#define SOCWATCH_VERSION_MAJOR 2
+#define SOCWATCH_VERSION_MINOR 8
+#define SOCWATCH_VERSION_OTHER 0
+
+#endif // __SW_VERSION_H__
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_acrn.h b/drivers/platform/x86/socwatchhv/inc/swhv_acrn.h
new file mode 100644
index 000000000000..06a9e090932b
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_acrn.h
@@ -0,0 +1,117 @@
+#ifndef _SWHV_ACRN_H_
+#define _SWHV_ACRN_H_ 1
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <asm/io.h>
+#include <linux/version.h> // LINUX_VERSION_CODE
+#include <linux/list.h> // for struct list_head
+
+#include "swhv_defines.h"
+#include "pw_version.h"
+
+#define SW_DEFINE_LIST_HEAD(name, dummy) struct list_head name
+#define SW_DECLARE_LIST_HEAD(name, dummy) extern struct list_head name
+#define SW_LIST_ENTRY(name, dummy) struct list_head name
+#define SW_LIST_HEAD_VAR(dummy) struct list_head
+#define SW_LIST_HEAD_INIT(head) INIT_LIST_HEAD(head)
+#define SW_LIST_ENTRY_INIT(node, field) INIT_LIST_HEAD(&node->field)
+#define SW_LIST_ADD(head, node, field) list_add_tail(&node->field, head)
+#define SW_LIST_GET_HEAD_ENTRY(head, type, field)                              \
+	list_first_entry(head, struct type, field)
+#define SW_LIST_UNLINK(node, field) list_del(&node->field)
+#define SW_LIST_FOR_EACH_ENTRY(node, head, field)                              \
+	list_for_each_entry(node, head, field)
+#define SW_LIST_EMPTY(head) list_empty(head)
+#define SW_LIST_HEAD_INITIALIZER(head) LIST_HEAD_INIT(head)
+
+int device_open_i(struct inode *inode, struct file *file);
+
+ssize_t device_read_i(struct file *file, /* see include/linux/fs.h   */
+		      char __user *buffer, /* buffer to be filled with data */
+		      size_t length, /* length of the buffer */
+		      loff_t *offset);
+
+long swhv_configure(struct swhv_driver_interface_msg __user *remote_msg,
+		    int local_len);
+long swhv_start(void);
+long swhv_stop(void);
+long swhv_get_cpu_count(u32 __user *remote_args);
+long swhv_get_clock(u32 __user *remote_in_args, u64 __user *remote_args);
+long swhv_get_topology(u64 __user *remote_args);
+long swhv_get_hypervisor_type(u32 __user *remote_args);
+int swhv_load_driver_i(void);
+void swhv_unload_driver_i(void);
+void cleanup_error_i(void);
+long swhv_msr_read(u32 __user *remote_in_args, u64 __user *remote_args);
+long swhv_collection_poll(void);
+
+enum MSR_CMD_TYPE {
+	MSR_OP_NONE = 0,
+	MSR_OP_READ,
+	MSR_OP_WRITE,
+	MSR_OP_READ_CLEAR
+};
+
+enum MSR_CMD_STATUS { MSR_OP_READY = 0, MSR_OP_REQUESTED, MSR_OP_HANDLED };
+
+struct profiling_msr_op {
+	/* value to write or location to write into */
+	uint64_t value;
+	/* MSR address to read/write; last entry will have value of -1 */
+	uint32_t msr_id;
+	/* parameter; usage depends on operation */
+	uint16_t param;
+	uint8_t msr_op_type;
+	uint8_t reg_type;
+};
+
+#define MAX_MSR_LIST_NUM 15
+struct profiling_msr_ops_list {
+	int32_t collector_id;
+	uint32_t num_entries;
+	int32_t msr_op_state; // enum value from 'MSR_CMD_STATUS'
+	struct profiling_msr_op entries[MAX_MSR_LIST_NUM];
+};
+
+#define COLLECTOR_SOCWATCH 1
+
+struct profiling_control {
+	int32_t collector_id;
+	int32_t reserved;
+	uint64_t switches;
+};
+
+/**
+ * struct - sw_collector_data
+ * Information about the collector to be invoked at collection time.
+ *
+ * The collector_lists array holds linked lists of collectors to
+ * be exercised at specific points in time during the collection
+ * (e.g. begin, poll, end, etc.).  At a trigger time, the driver walks
+ * that time's list of nodes, and exercises the collectors on that list.
+ *
+ * @list:                   List/link implementation
+ * @cpumask:                Collect if cpu matches mask
+ * @info:                   Ptr to metric info
+ * @ops:                    Ptr to collector's operations
+ * @last_update_jiffies:    Indicates when this node was last exercised.
+ * @per_msg_payload_size:   Data size
+ * @msg:                    Ptr to collected data
+ */
+typedef struct swhv_acrn_msr_collector_data {
+	SW_LIST_ENTRY(list, swhv_acrn_msr_collector_data);
+	pw_s16_t cpu_mask;
+	pw_s16_t sample_id;
+	struct profiling_msr_ops_list *msr_ops_list;
+	size_t per_msg_payload_size;
+} swhv_acrn_msr_collector_data_t;
+#endif // _SWHV_ACRN_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_acrn_sbuf.h b/drivers/platform/x86/socwatchhv/inc/swhv_acrn_sbuf.h
new file mode 100644
index 000000000000..c5a08d1025ae
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_acrn_sbuf.h
@@ -0,0 +1,186 @@
+#ifndef _SWHV_ACRN_SBUF_H_
+#define _SWHV_ACRN_SBUF_H_ 1
+
+#include <acrn/sbuf.h>
+
+/*
+ * Checks if the passed sbuf is empty.
+ */
+static inline bool sbuf_is_empty(struct shared_buf *sbuf)
+{
+	return (sbuf->head == sbuf->tail);
+}
+
+static inline uint32_t sbuf_next_ptr(uint32_t pos, uint32_t span,
+				     uint32_t scope)
+{
+	pos += span;
+	pos = (pos >= scope) ? (pos - scope) : pos;
+	return pos;
+}
+
+/*
+ * This function returns the available free space in the
+ * passed sbuf.
+ */
+inline uint32_t sbuf_available_space(struct shared_buf *sbuf)
+{
+	uint32_t remaining_space;
+	/*
+	 * if tail isn't wrapped around
+	 *      subtract difference of tail and head from size
+	 * otherwise
+	 *      difference between head and tail
+	 */
+	if (sbuf->tail >= sbuf->head)
+		remaining_space = sbuf->size - (sbuf->tail - sbuf->head);
+	else
+		remaining_space = sbuf->head - sbuf->tail;
+
+	return remaining_space;
+}
+
+/*
+ * This function retrieves the requested 'size' amount of data from
+ * the passed buffer.
+ * This is a much more efficient implementation than the default
+ * 'sbuf_get()' which retrieves one 'element' size at a time.
+ */
+int sbuf_get_variable(struct shared_buf *sbuf, void **data, uint32_t size)
+{
+	/*
+	 * 1. Check if buffer isn't empty and non-zero 'size'
+	 * 2. check if enough ('size' bytes) data to be read is present.
+	 * 3. Continue if buffer has enough data
+	 * 4. Copy data from buffer
+	 *      4a. copy data in 2 parts if there is a wrap-around
+	 *      4b. Otherwise do a simple copy
+	 */
+	const void *from;
+	uint32_t current_data_size, offset = 0, next_head;
+
+	if ((sbuf == NULL) || (*data == NULL))
+		return -EINVAL;
+
+	if (sbuf_is_empty(sbuf) || (size == 0)) {
+		/* no data available */
+		return 0;
+	}
+
+	current_data_size = sbuf->size - sbuf_available_space(sbuf);
+
+	/*
+	 * TODO If requested data size is greater than current buffer size,
+	 * consider at least copying the current buffer size.
+	 */
+	if (size > current_data_size) {
+		pw_pr_warn(
+			"Requested data size is greater than the current buffer size!");
+		/* not enough data to be read */
+		return 0;
+	}
+
+	next_head = sbuf_next_ptr(sbuf->head, size, sbuf->size);
+
+	from = (void *)sbuf + SBUF_HEAD_SIZE + sbuf->head;
+
+	if (next_head < sbuf->head) { // wrap-around
+		/* copy first part */
+		offset = sbuf->size - sbuf->head;
+		memcpy(*data, from, offset);
+
+		from = (void *)sbuf + SBUF_HEAD_SIZE;
+	}
+	memcpy((void *)*data + offset, from, size - offset);
+
+	sbuf->head = next_head;
+
+	return size;
+}
+
+/*
+ * This API can be used to retrieve complete samples at a time from the
+ * sbuf. It internally uses the sbuf_get() which retrieves 1 'element'
+ * at a time and is probably not very efficient for reading large amount
+ * of data.
+ * Note: Not used currently.
+ */
+int sbuf_get_wrapper(struct shared_buf *sbuf, uint8_t **data)
+{
+	uint8_t *sample;
+	uint8_t sample_offset;
+	acrn_msg_header *header;
+	uint32_t payload_size, sample_size, _size;
+
+	/*
+	 * Assumption: A partial variable sample will not be written
+	 *             to the buffer.
+	 * do while buf isn't empty
+	 *  Read header from the buffer
+	 *      write to data
+	 *      get size of payload
+	 *      check if the size of 'data' is enough for the
+	 *      variable sample to be read to
+	 *  Read the payload
+	 *      Keep reading ele_size chunks till available and write to data
+	 *      if the last chunk is less than ele_size, do a partial copy to
+	 *      data
+	 *
+	 *
+	 */
+	if ((sbuf == NULL) || (data == NULL))
+		return -EINVAL;
+
+	if (sbuf_is_empty(sbuf)) {
+		/* no data available */
+		return 0;
+	}
+
+	sample_offset = 0;
+
+	header = vmalloc(sizeof(ACRN_MSG_HEADER_SIZE));
+	memset(header, 0, sizeof(ACRN_MSG_HEADER_SIZE));
+	//read header
+	sbuf_get(sbuf, (uint8_t *)header);
+
+	payload_size = header->payload_size;
+
+	sample_size = ACRN_MSG_HEADER_SIZE + header->payload_size;
+
+	sample = vmalloc(sample_size);
+
+	//copy header
+	memcpy((void *)sample, (void *)header, ACRN_MSG_HEADER_SIZE);
+
+	sample_offset += ACRN_MSG_HEADER_SIZE;
+
+	_size = payload_size;
+	while (_size) {
+		if (_size >= sbuf->ele_size) {
+			sbuf_get(sbuf, (uint8_t *)(sample + sample_offset));
+			sample_offset += sbuf->ele_size;
+			_size -= sbuf->ele_size;
+		} else {
+			pw_pr_error(
+				"error: payload has to be multiple of 32\n");
+			return 0;
+			/*
+			 * This code can be enabled when support for variable
+			 * sized samples needs to be added.
+			 */
+#if 0
+			chunk = malloc(sbuf->ele_size);
+			sbuf_get(sbuf, chunk);
+			memcpys((void *)(sample + sample_offset), _size, chunk);
+			_size -= _size;
+			free(chunk);
+#endif
+		}
+	}
+
+	*data = sample;
+
+	vfree(header);
+	return sample_size;
+}
+#endif // _SWHV_ACRN_SBUF_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_defines.h b/drivers/platform/x86/socwatchhv/inc/swhv_defines.h
new file mode 100644
index 000000000000..65239d566ae1
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_defines.h
@@ -0,0 +1,111 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _SWHV_DEFINES_H_
+#define _SWHV_DEFINES_H_
+
+/* ***************************************************
+ * Common to kernel and userspace.
+ * ***************************************************
+ */
+#define PW_SUCCESS 0
+#define PW_ERROR 1
+#define PW_SUCCESS_NO_COLLECT 2
+
+//
+// Start off with none of the OS'es are defined
+//
+#undef SWDRV_OS_LINUX
+#undef SWDRV_OS_ANDROID
+#undef SWDRV_OS_UNIX
+
+//
+// Make sure none of the architectures is defined here
+//
+#undef SWDRV_IA32
+#undef SWDRV_EM64T
+
+//
+// Make sure one (and only one) of the OS'es gets defined here
+//
+// Unfortunately entirex defines _WIN32 so we need to check for linux
+// first.  The definition of these flags is one and only one
+// _OS_xxx is allowed to be defined.
+//
+#if defined(__ANDROID__)
+#define SWDRV_OS_ANDROID
+#define SWDRV_OS_UNIX
+#elif defined(__linux__)
+#define SWDRV_OS_LINUX
+#define SWDRV_OS_UNIX
+#else
+#error "Compiling for an unknown OS"
+#endif
+
+//
+// Make sure one (and only one) architecture is defined here
+// as well as one (and only one) pointer__ size
+//
+#if defined(_M_IX86) || defined(__i386__)
+#define SWDRV_IA32
+#elif defined(_M_AMD64) || defined(__x86_64__)
+#define SWDRV_EM64T
+#else
+#error "Unknown architecture for compilation"
+#endif
+
+#endif // _SWHV_DEFINES_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_driver.h b/drivers/platform/x86/socwatchhv/inc/swhv_driver.h
new file mode 100644
index 000000000000..8ad0d672f095
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_driver.h
@@ -0,0 +1,109 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _SWHV_DRIVER_H_
+#define _SWHV_DRIVER_H_ 1
+
+#include <linux/version.h> // LINUX_VERSION_CODE
+#include <linux/vmalloc.h> // vmalloc
+#include "swhv_defines.h"
+#include "sw_kernel_defines.h"
+#include "pw_version.h"
+
+#define MAX_CORE_COUNT 8
+
+#define MOBILEVISOR 1
+#define ACRN 2
+
+// define this flag to have IDT entry programmed for SoCWatch IRQ handler
+#define SOCWATCH_IDT_IRQ 1
+
+extern void SYS_Perfvec_Handler(void);
+extern short SYS_Get_cs(void);
+
+#if defined(SWDRV_IA32) && (SOCWATCH_IDT_IRQ)
+extern void *SYS_Get_IDT_Base_HWR(void); /// IDT base from hardware IDTR
+
+#define SYS_Get_IDT_Base SYS_Get_IDT_Base_HWR
+#endif // defined(SWDRV_IA32) && (SOCWATCH_IDT_IRQ)
+
+#if defined(SWDRV_EM64T) && (SOCWATCH_IDT_IRQ)
+extern void SYS_Get_IDT_Base(void **);
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 25)
+typedef struct gate_struct gate_struct_t;
+#else
+typedef struct gate_struct64 gate_struct_t;
+#endif // LINUX_VERSION_CODE < KERNEL_VERSION(2,6,25)
+#endif // defined(SWDRV_EM64T) && (SOCWATCH_IDT_IRQ)
+
+// miscellaneous defines
+#define CPU() (raw_smp_processor_id())
+#define GET_BOOL_STRING(b) ((b) ? "TRUE" : "FALSE")
+
+#define _STRINGIFY(x) #x
+#define STRINGIFY(x) _STRINGIFY(x)
+#define _STRINGIFY_W(x) (L#x)
+#define STRINGIFY_W(x) _STRINGIFY_W(x)
+
+/*
+ * 64bit Compare-and-swap.
+ */
+#define CAS64(p, o, n) (cmpxchg64((p), (o), (n)) == (o))
+
+typedef struct PWCollector_msg PWCollector_msg_t;
+
+#endif // _SWHV_DRIVER_H_
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_ioctl.h b/drivers/platform/x86/socwatchhv/inc/swhv_ioctl.h
new file mode 100644
index 000000000000..690bbcd5ccba
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_ioctl.h
@@ -0,0 +1,164 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#ifndef __SWHV_IOCTL_H__
+#define __SWHV_IOCTL_H__
+
+#include "pw_types.h"
+
+#if defined(__linux__) || defined(__QNX__)
+#if __KERNEL__
+#include <linux/ioctl.h>
+#if defined(HAVE_COMPAT_IOCTL) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+#endif // COMPAT && x64
+#else // !__KERNEL__
+#include <sys/ioctl.h>
+#endif // __KERNEL__
+#endif // __linux__
+/*
+ * Path to the Hypervisor driver device file.
+ */
+#define SWHV_DEVICE_NAME "swhypervdrv"
+#define SWHV_DEVICE_PATH "/dev/" SWHV_DEVICE_NAME
+
+/*
+ * The SoFIA-specific IOCTL magic
+ * number -- used to ensure IOCTLs
+ * are delivered to the correct
+ * driver.
+ */
+#define SP_IOC_MAGIC 99
+/*
+ * CONSTANTS that define the various operations.
+ * TODO: convert to enum?
+ */
+#define SWHVDRV_OPERATION_CONFIGURE 1 /* configure a collection */
+#define SWHVDRV_OPERATION_CMD 2 /* control a collection */
+#define SWHVDRV_OPERATION_VERSION 3 /* retrieve driver version info */
+#define SWHVDRV_OPERATION_CLOCK 4 /* retrieve STM clock */
+#define SWHVDRV_OPERATION_TOPOLOGY 5 /* retrieve CPU topology */
+#define SWHVDRV_OPERATION_CPUCOUNT 6 /* retrieve CPU count */
+#define SWHVDRV_OPERATION_HYPERVISOR_TYPE 7 /* retrieve hypervisor type */
+#define SWHVDRV_OPERATION_MSR_READ 8 /* retrieve MSR value */
+#define SWHVDRV_OPERATION_POLL 9 /* Polling tick */
+
+enum swhv_ioctl_cmd {
+	swhv_ioctl_cmd_none = 0,
+	swhv_ioctl_cmd_config,
+	swhv_ioctl_cmd_cmd,
+	swhv_ioctl_cmd_version,
+	swhv_ioctl_cmd_clock,
+	swhv_ioctl_cmd_topology,
+	swhv_ioctl_cmd_cpucount,
+	swhv_ioctl_cmd_hypervisor_type,
+};
+/*
+ * The actual IOCTL commands.
+ *
+ * From the kernel documentation:
+ * "_IOR" ==> Read IOCTL
+ * "_IOW" ==> Write IOCTL
+ * "_IOWR" ==> Read/Write IOCTL
+ *
+ * Where "Read" and "Write" are from the user's perspective
+ * (similar to the file "read" and "write" calls).
+ */
+#define SWHVDRV_IOCTL_CONFIGURE                                                \
+	_IOW(SP_IOC_MAGIC, SWHVDRV_OPERATION_CONFIGURE,                        \
+	     struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_CMD                                                      \
+	_IOW(SP_IOC_MAGIC, SWHVDRV_OPERATION_CMD, struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_VERSION                                                  \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_VERSION, struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_CLOCK                                                    \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_CLOCK, struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_TOPOLOGY                                                 \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_TOPOLOGY, struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_CPUCOUNT                                                 \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_CPUCOUNT, struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_HYPERVISOR_TYPE                                          \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_HYPERVISOR_TYPE,                  \
+	     struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_MSR_READ                                                 \
+	_IOWR(SP_IOC_MAGIC, SWHVDRV_OPERATION_MSR_READ,                        \
+	      struct spdrv_ioctl_arg *)
+#define SWHVDRV_IOCTL_POLL                                                     \
+	_IO(SP_IOC_MAGIC, SWHVDRV_OPERATION_POLL, struct spdrv_ioctl_arg *)
+
+#if defined(HAVE_COMPAT_IOCTL) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+
+#define SWHVDRV_IOCTL_CONFIGURE32                                              \
+	_IOW(SP_IOC_MAGIC, SWHVDRV_OPERATION_CONFIGURE, compat_uptr_t)
+#define SWHVDRV_IOCTL_CMD32                                                    \
+	_IOW(SP_IOC_MAGIC, SWHVDRV_OPERATION_CMD, compat_uptr_t)
+#define SWHVDRV_IOCTL_VERSION32                                                \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_VERSION, compat_uptr_t)
+#define SWHVDRV_IOCTL_CLOCK32                                                  \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_CLOCK, compat_uptr_t)
+#define SWHVDRV_IOCTL_TOPOLOGY32                                               \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_TOPOLOGY, compat_uptr_t)
+#define SWHVDRV_IOCTL_CPUCOUNT32                                               \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_CPUCOUNT, compat_uptr_t)
+#define SWHVDRV_IOCTL_HYPERVISOR_TYPE32                                        \
+	_IOR(SP_IOC_MAGIC, SWHVDRV_OPERATION_HYPERVISOR_TYPE, compat_uptr_t)
+#define SWHVDRV_IOCTL_MSR_READ32                                               \
+	_IOWR(SP_IOC_MAGIC, SWHVDRV_OPERATION_MSR_READ, compat_uptr_t)
+#define SWHVDRV_IOCTL_POLL32                                                   \
+	_IO(SP_IOC_MAGIC, SWHVDRV_OPERATION_POLL, compat_uptr_t)
+#endif // COMPAT && x64
+
+#endif // __SWHV_IOCTL_H__
diff --git a/drivers/platform/x86/socwatchhv/inc/swhv_structs.h b/drivers/platform/x86/socwatchhv/inc/swhv_structs.h
new file mode 100644
index 000000000000..67bac8e36ad4
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/inc/swhv_structs.h
@@ -0,0 +1,234 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+
+#ifndef _SWHV_STRUCTS_H_
+#define _SWHV_STRUCTS_H_ 1
+
+#include "sw_structs.h"
+
+enum swhv_hypervisor_type {
+	swhv_hypervisor_none = 0,
+	swhv_hypervisor_mobilevisor,
+	swhv_hypervisor_acrn,
+};
+
+/*
+ * Structure to return version information.
+ */
+#pragma pack(push)
+#pragma pack(1)
+struct sp_driver_version_info {
+	pw_s32_t major;
+	pw_s32_t minor;
+	pw_s32_t other;
+};
+
+struct spdrv_ioctl_arg {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	char *in_arg;
+	char *out_arg;
+};
+#pragma pack(pop)
+
+/*
+ * Various commands to control a collection.
+ */
+enum swhvdrv_cmd {
+	SWHVDRV_CMD_START,
+	SWHVDRV_CMD_STOP,
+	/* others here when appropriate */
+	SVHVDRV_CMD_MAX
+};
+
+enum swhv_collector_type {
+	SWHV_COLLECTOR_TYPE_NONE,
+	SWHV_COLLECTOR_TYPE_SWITCH,
+	SWHV_COLLECTOR_TYPE_MSR,
+};
+
+enum swhv_io_cmd { SWHV_IO_CMD_READ = 0, SWHV_IO_CMD_WRITE, SWHV_IO_CMD_MAX };
+
+#pragma pack(push, 1)
+struct swhv_driver_msr_io_descriptor {
+	pw_u64_t address;
+	enum sw_msr_type type;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct swhv_driver_switch_io_descriptor {
+	pw_u32_t switch_bitmask;
+};
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+typedef struct swhv_driver_io_descriptor {
+	pw_u16_t collection_type; // One of 'enum swhv_collector_type'
+	pw_s16_t collection_command; // One of 'enum swhv_io_cmd'
+	pw_u16_t counter_size_in_bytes; // The number of bytes to READ or WRITE
+	union {
+		struct swhv_driver_msr_io_descriptor msr_descriptor;
+		struct swhv_driver_switch_io_descriptor switch_descriptor;
+	};
+	pw_u64_t write_value; // The value to WRITE
+} swhv_driver_io_descriptor_t;
+#pragma pack(pop)
+
+#pragma pack(push, 1)
+struct swhv_driver_interface_info {
+	pw_s16_t cpu_mask; // On which CPU(s) should the driver read the data?
+		// Currently:  -2 ==> read on ALL CPUs,
+		//             -1 ==> read on ANY CPU,
+		//           >= 0 ==> the specific CPU to read on
+	pw_s16_t sample_id; // Sample ID, used to map it back to Metric Plugin, Metric and Msg ID combo
+	pw_u16_t num_io_descriptors; // Number of descriptors in the array, below.
+	pw_u8_t descriptors[1]; // Array of swhv_driver_io_descriptor structs.
+};
+#pragma pack(pop)
+#define SWHV_DRIVER_INTERFACE_INFO_HEADER_SIZE()                               \
+	(sizeof(struct swhv_driver_interface_info) - sizeof(pw_u8_t[1]))
+
+#pragma pack(push, 1)
+struct swhv_driver_interface_msg {
+	pw_u16_t num_infos; // Number of 'swhv_driver_interface_info' structs contained within the 'infos' variable, below
+	// pw_u16_t infos_size_bytes; // Size of data inlined within the 'infos' variable, below
+	pw_u8_t infos[1];
+};
+#pragma pack(pop)
+#define SWHV_DRIVER_INTERFACE_MSG_HEADER_SIZE()                                \
+	(sizeof(struct swhv_driver_interface_msg) - sizeof(pw_u8_t[1]))
+
+/*
+ * ACRN specific structs, copied from the ACRN profiling service
+ * DO NOT modify these below stucts
+ */
+#define SBUF_HEAD_SIZE 64 /* bytes */
+
+typedef enum PROFILING_SOCWATCH_FEATURE {
+	SOCWATCH_COMMAND = 0,
+	SOCWATCH_VM_SWITCH_TRACING,
+	MAX_SOCWATCH_FEATURE_ID,
+} profiling_socwatch_feature;
+
+typedef enum PROFILING_SOCWATCH_FEATURE acrn_type;
+
+/*
+ * current default ACRN header
+ */
+struct data_header {
+	uint32_t collector_id;
+	uint16_t cpu_id;
+	uint16_t data_type;
+	uint64_t tsc;
+	uint64_t payload_size;
+	uint64_t reserved;
+} __attribute__((aligned(32)));
+#define ACRN_MSG_HEADER_SIZE (sizeof(struct data_header))
+
+struct vm_switch_trace {
+	int32_t os_id;
+	uint64_t vmenter_tsc;
+	uint64_t vmexit_tsc;
+	uint64_t vmexit_reason;
+} __attribute__((aligned(32)));
+#define VM_SWITCH_TRACE_SIZE (sizeof(struct vm_switch_trace))
+
+#define MAX_NR_VCPUS 8
+#define MAX_NR_VMS 6
+
+struct profiling_vcpu_pcpu_map {
+	int32_t vcpu_id;
+	int32_t pcpu_id;
+	int32_t apic_id;
+} __attribute__((aligned(8)));
+
+struct profiling_vm_info {
+	int32_t vm_id;
+	unsigned char guid[16];
+	char vm_name[16];
+	int32_t num_vcpus;
+	struct profiling_vcpu_pcpu_map cpu_map[MAX_NR_VCPUS];
+} __attribute__((aligned(8)));
+
+struct profiling_vm_info_list {
+	int32_t num_vms;
+	struct profiling_vm_info vm_list[MAX_NR_VMS];
+} __attribute__((aligned(8)));
+
+/*
+ * End of ACRN specific structs, copied from the ACRN profiling service
+ */
+typedef struct data_header acrn_msg_header;
+typedef struct vm_switch_trace vmswitch_trace_t;
+
+/*
+ * ACRN specific constants shared between the driver and user-mode
+ */
+// Per CPU buffer size
+#define ACRN_BUF_SIZE ((4 * 1024 * 1024) - SBUF_HEAD_SIZE /* 64 bytes */)
+// Size of buffer at which data should be transferred to user-mode
+#define ACRN_BUF_TRANSFER_SIZE (ACRN_BUF_SIZE / 2)
+/*
+ * The ACRN 'sbuf' buffers consist of fixed size elements.
+ * This is how they are intended to be used, though SoCWatch only uses it to
+ * allocate the correct buffer size.
+ */
+#define ACRN_BUF_ELEMENT_SIZE 32 /* byte */
+#define ACRN_BUF_ELEMENT_NUM (ACRN_BUF_SIZE / ACRN_BUF_ELEMENT_SIZE)
+#define ACRN_BUF_FILLED_SIZE(sbuf) (sbuf->size - sbuf_available_space(sbuf))
+
+#endif // _SWHV_STRUCTS_H_
diff --git a/drivers/platform/x86/socwatchhv/swhv_acrn.c b/drivers/platform/x86/socwatchhv/swhv_acrn.c
new file mode 100644
index 000000000000..926ff09819a7
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/swhv_acrn.c
@@ -0,0 +1,747 @@
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/version.h>
+#include <asm/io.h>
+#include <linux/uaccess.h>
+
+#include <asm/hypervisor.h>
+#include <linux/vhm/acrn_hv_defs.h>
+#include <linux/vhm/vhm_hypercall.h>
+
+#include "swhv_defines.h"
+#include "swhv_driver.h"
+#include "swhv_ioctl.h"
+#include "swhv_structs.h"
+#include "control.h"
+#include "swhv_acrn.h"
+#include "swhv_acrn_sbuf.h"
+
+/* *******************************************
+ * Compile-time constants
+ * *******************************************
+ */
+#define foreach_cpu(cpu, cpu_num) for ((cpu) = 0; (cpu) < (cpu_num); (cpu)++)
+
+/* actual physical cpu number, initialized by module init */
+static int pcpu_num;
+bool flush_mode;
+
+wait_queue_head_t read_queue;
+
+//TODO is this needed?
+//module_param(nr_cpus, int, S_IRUSR | S_IWUSR);
+
+static struct shared_buf **sbuf_per_cpu;
+
+static pw_u64_t global_collection_switch;
+static SW_DEFINE_LIST_HEAD(swhv_msr_collector, swhv_acrn_msr_collector_data);
+
+/* used by the MSR read IOCTL */
+struct profiling_msr_ops_list *msr_read_ops_list;
+
+bool buffer_not_ready(int *cpu);
+
+struct swhv_acrn_msr_collector_data *swhv_alloc_msr_collector_node(void)
+{
+	struct swhv_acrn_msr_collector_data *node =
+		(struct swhv_acrn_msr_collector_data *)kmalloc(
+			sizeof(struct swhv_acrn_msr_collector_data),
+			GFP_KERNEL);
+	if (node) {
+		node->per_msg_payload_size = 0x0;
+		node->sample_id = 0x0;
+		node->msr_ops_list = kmalloc(
+			pcpu_num * sizeof(struct profiling_msr_ops_list),
+			GFP_KERNEL);
+		memset(node->msr_ops_list, 0,
+		       pcpu_num * sizeof(struct profiling_msr_ops_list));
+		SW_LIST_ENTRY_INIT(node, list);
+	}
+	return node;
+}
+struct swhv_acrn_msr_collector_data *
+swhv_add_driver_msr_info(void *list_head,
+			 const struct swhv_driver_interface_info *info)
+{
+	int cpu;
+
+	SW_LIST_HEAD_VAR(swhv_acrn_msr_collector_data) * head = list_head;
+
+	struct swhv_acrn_msr_collector_data *node =
+		swhv_alloc_msr_collector_node();
+	if (!node) {
+		pw_pr_error("ERROR allocating MSR collector node!\n");
+		return NULL;
+	}
+
+	node->sample_id = info->sample_id;
+	node->cpu_mask = info->cpu_mask;
+	foreach_cpu(cpu, pcpu_num)
+	{
+		node->msr_ops_list[cpu].collector_id = COLLECTOR_SOCWATCH;
+		node->msr_ops_list[cpu].msr_op_state = MSR_OP_REQUESTED;
+	}
+
+	SW_LIST_ADD(head, node, list);
+	return node;
+}
+
+int swhv_add_driver_msr_io_desc(struct swhv_acrn_msr_collector_data *node,
+				struct swhv_driver_io_descriptor *info)
+{
+	int idx, cpu;
+	pw_u16_t num_entries;
+	struct profiling_msr_op *msr_op = NULL;
+
+	// Confirm this is an MSR IO descriptor
+	if (info->collection_type != SWHV_COLLECTOR_TYPE_MSR) {
+		pw_pr_error(
+			"ERROR trying to configure MSR collector with other data!\n");
+		return -EINVAL;
+	}
+
+	foreach_cpu(cpu, pcpu_num)
+	{
+		num_entries = node->msr_ops_list[cpu].num_entries;
+		if (num_entries >= MAX_MSR_LIST_NUM) {
+			pw_pr_error(
+				"ERROR trying to add too many MSRs to collect!\n");
+			return -PW_ERROR;
+		}
+
+		idx = num_entries;
+
+		msr_op = &(node->msr_ops_list[cpu].entries[idx]);
+
+		msr_op->msr_id = info->msr_descriptor.address;
+		if (info->collection_command == SWHV_IO_CMD_READ) {
+			msr_op->msr_op_type = MSR_OP_READ;
+		} else if (info->collection_command == SWHV_IO_CMD_WRITE) {
+			msr_op->msr_op_type = MSR_OP_WRITE;
+		}
+
+		/*
+		 * Use the param field to set sample id.
+		 * This'll be used in the hypervisor to
+		 * set the id in the samples
+		 */
+		msr_op->param = (uint16_t)node->sample_id;
+
+		num_entries++;
+
+		if (num_entries < MAX_MSR_LIST_NUM) {
+			node->msr_ops_list[cpu].entries[num_entries].msr_id =
+				-1;
+		}
+		node->msr_ops_list[cpu].num_entries = num_entries;
+	}
+	return PW_SUCCESS;
+}
+
+int swhv_init_per_cpu_buffers(void)
+{
+	int i, ret, cpu;
+
+	sbuf_per_cpu = vmalloc(pcpu_num * sizeof(struct shared_buf *));
+
+	foreach_cpu(cpu, pcpu_num)
+	{
+		/* allocate shared_buf */
+		sbuf_per_cpu[cpu] = sbuf_allocate(ACRN_BUF_ELEMENT_NUM,
+						  ACRN_BUF_ELEMENT_SIZE);
+		if (!sbuf_per_cpu[cpu]) {
+			pw_pr_error("Failed  to allocate buffer for cpu %d\n",
+				    cpu);
+			ret = -ENOMEM;
+			goto out_free;
+		}
+	}
+
+	//TODO understand the use of this API
+	foreach_cpu(cpu, pcpu_num)
+	{
+		ret = sbuf_share_setup(cpu, ACRN_SOCWATCH, sbuf_per_cpu[cpu]);
+		if (ret < 0) {
+			pw_pr_error("Failed to setup buffer for cpu %d\n", cpu);
+			goto out_sbuf;
+		}
+	}
+
+	return PW_SUCCESS;
+out_sbuf:
+	for (i = --cpu; i >= 0; i--) {
+		sbuf_share_setup(i, ACRN_SOCWATCH, NULL);
+	}
+	cpu = pcpu_num;
+
+out_free:
+	for (i = --cpu; i >= 0; i--) {
+		sbuf_free(sbuf_per_cpu[i]);
+	}
+
+	vfree(sbuf_per_cpu);
+	return ret;
+}
+
+void swhv_destroy_per_cpu_buffers(void)
+{
+	int cpu;
+
+	pw_pr_debug("%s, pcpu_num: %d\n", __func__, pcpu_num);
+
+	foreach_cpu(cpu, pcpu_num)
+	{
+		//TODO anything else to de-register?
+		/* deregister devices */
+
+		/* set sbuf pointer to NULL in HV */
+		sbuf_share_setup(cpu, ACRN_SOCWATCH, NULL);
+
+		/* free sbuf, sbuf_per_cpu[cpu] should be set NULL */
+		sbuf_free(sbuf_per_cpu[cpu]);
+	}
+	vfree(sbuf_per_cpu);
+}
+
+void swhv_free_msr_collector_node(struct swhv_acrn_msr_collector_data *node)
+{
+	if (!node) {
+		return;
+	}
+
+	kfree(node->msr_ops_list);
+	kfree(node);
+	return;
+}
+
+void swhv_init_msr_collector_list(void)
+{
+	void *list_head = &swhv_msr_collector;
+
+	SW_LIST_HEAD_VAR(swhv_acrn_msr_collector_data) * head = list_head;
+	SW_LIST_HEAD_INIT(head);
+}
+
+void swhv_destroy_msr_collector_list(void)
+{
+	void *list_head = &swhv_msr_collector;
+
+	SW_LIST_HEAD_VAR(swhv_acrn_msr_collector_data) * head = list_head;
+	while (!SW_LIST_EMPTY(head)) {
+		struct swhv_acrn_msr_collector_data *curr =
+			SW_LIST_GET_HEAD_ENTRY(
+				head, swhv_acrn_msr_collector_data, list);
+		SW_LIST_UNLINK(curr, list);
+		swhv_free_msr_collector_node(curr);
+	}
+}
+
+void swhv_handle_hypervisor_collector(uint32_t control_cmd)
+{
+	struct profiling_control *acrn_profiling_control;
+
+	acrn_profiling_control =
+		kmalloc(sizeof(struct profiling_control), GFP_KERNEL);
+	memset(acrn_profiling_control, 0, sizeof(struct profiling_control));
+
+	acrn_profiling_control->collector_id = COLLECTOR_SOCWATCH;
+
+	if (control_cmd == 1) { // start collection + send switch bitmask
+		pw_pr_debug("STARTING ACRN PROFILING SERVICE\n");
+		global_collection_switch |=
+			control_cmd;	// first bit controls start/stop
+					// of collection
+	} else if (control_cmd == 0) { // stop collection + reset switch bitmask
+		pw_pr_debug("STOPPING ACRN PROFILING SERVICE\n");
+		global_collection_switch = control_cmd;
+	}
+	acrn_profiling_control->switches = global_collection_switch;
+
+	// send collection command + switch bitmask
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_SET_CONTROL_SWITCH,
+			virt_to_phys(acrn_profiling_control));
+	kfree(acrn_profiling_control);
+}
+
+int swhv_handle_msr_collector_list(void)
+{
+	void *list_head = &swhv_msr_collector;
+
+	SW_LIST_HEAD_VAR(swhv_acrn_msr_collector_data) * head = list_head;
+	int retVal = PW_SUCCESS;
+	int dummy_cpu = 0;
+	struct swhv_acrn_msr_collector_data *curr = NULL;
+
+	if (SW_LIST_EMPTY(&swhv_msr_collector)) {
+		pw_pr_debug("DEBUG: EMPTY MSR COLLECTOR LIST\n");
+		return retVal;
+	}
+
+	if (!head) {
+		return -PW_ERROR;
+	}
+	SW_LIST_FOR_EACH_ENTRY(curr, head, list)
+	{
+		pw_pr_debug("HANDLING MSR NODE\n");
+
+		//hypervisor call to do immediate MSR read
+		acrn_hypercall2(HC_PROFILING_OPS, PROFILING_MSR_OPS,
+				virt_to_phys(curr->msr_ops_list));
+	}
+	if (buffer_not_ready(&dummy_cpu) == false) {
+		/*
+		 * force the device_read function to check if any buffers are
+		 * filled with data above 'ACRN_BUF_TRANSFER_SIZE' size and
+		 * if yes, copy to userspace
+		 */
+		wake_up_interruptible(&read_queue);
+	}
+	return retVal;
+}
+
+long swhv_configure(struct swhv_driver_interface_msg __user *remote_msg,
+		    int local_len)
+{
+	struct swhv_driver_interface_info *local_info = NULL;
+	struct swhv_driver_io_descriptor *local_io_desc = NULL;
+	struct swhv_driver_interface_msg *local_msg = vmalloc(local_len);
+	pw_u16_t num_infos = 0, num_io_desc = 0;
+	pw_u32_t local_config_bitmap = 0;
+	int done = 0;
+	bool driver_info_added = false;
+
+	char *__data = (char *)local_msg->infos;
+	size_t dst_idx = 0, desc_idx = 0;
+	struct swhv_acrn_msr_collector_data *msr_collector_node = NULL;
+
+	if (!local_msg) {
+		pw_pr_error("ERROR allocating space for local message!\n");
+		return -EFAULT;
+	}
+	if (copy_from_user(local_msg, remote_msg, local_len)) {
+		pw_pr_error("ERROR copying message from user space!\n");
+		vfree(local_msg);
+		return -EFAULT;
+	}
+
+	flush_mode = false;
+
+	pw_pr_debug("local_len: %d\n", local_len);
+	/*
+	 * We aren't allowed to config the driver multiple times between
+	 * collections. Clear out any previous config values.
+	 */
+	swhv_destroy_msr_collector_list();
+
+	// clear the collection bitmask
+	global_collection_switch = 0;
+
+	num_infos = local_msg->num_infos;
+	pw_pr_debug("LOCAL NUM INFOS = %u\n", num_infos);
+	for (; num_infos > 0 && !done; --num_infos) {
+		local_info =
+			(struct swhv_driver_interface_info *)&__data[dst_idx];
+		desc_idx = dst_idx + SWHV_DRIVER_INTERFACE_INFO_HEADER_SIZE();
+		dst_idx += (SWHV_DRIVER_INTERFACE_INFO_HEADER_SIZE() +
+			    local_info->num_io_descriptors *
+				    sizeof(struct swhv_driver_io_descriptor));
+		pw_pr_debug("# msrs = %u\n",
+			    (unsigned int)local_info->num_io_descriptors);
+
+		num_io_desc = local_info->num_io_descriptors;
+		pw_pr_debug("LOCAL NUM IO DESC = %u\n", num_io_desc);
+
+		driver_info_added = false;
+		for (; num_io_desc > 0; --num_io_desc) {
+			local_io_desc = (struct swhv_driver_io_descriptor
+						 *)&__data[desc_idx];
+			desc_idx += sizeof(struct swhv_driver_io_descriptor);
+			if (local_io_desc->collection_type ==
+			    SWHV_COLLECTOR_TYPE_MSR) {
+				if (!driver_info_added) {
+					msr_collector_node =
+						swhv_add_driver_msr_info(
+							&swhv_msr_collector,
+							local_info);
+					if (msr_collector_node == NULL) {
+						return -PW_ERROR;
+					}
+					driver_info_added = true;
+				}
+
+				pw_pr_debug(
+					"MSR - addr: 0x%llx, type: %u, read/write: %u\n",
+					local_io_desc->msr_descriptor.address,
+					local_io_desc->msr_descriptor.type,
+					local_io_desc->collection_command);
+				swhv_add_driver_msr_io_desc(msr_collector_node,
+							    local_io_desc);
+			} else if (local_io_desc->collection_type ==
+				   SWHV_COLLECTOR_TYPE_SWITCH) {
+				local_config_bitmap =
+					local_io_desc->switch_descriptor
+						.switch_bitmask;
+				pw_pr_debug("local bitmask = %u\n",
+					    local_config_bitmap);
+
+				global_collection_switch = local_config_bitmap;
+
+				// only one set of collection switches are
+				// expected, we are done configuring
+				done = 1;
+				break;
+			} else {
+				pw_pr_error(
+					"WARNING: unknown collector configuration requested, collector id: %u!\n",
+					local_io_desc->collection_type);
+			}
+		}
+		driver_info_added = false;
+	}
+	vfree(local_msg);
+	return PW_SUCCESS;
+}
+
+long swhv_stop(void)
+{
+	uint32_t control = 0; // stop collection command
+
+	pw_pr_debug("socwatch: stop called\n");
+
+	//If MSR ops are present, perform them to get begin snapshot data.
+	swhv_handle_msr_collector_list();
+
+	// stop collection + reset switch bitmask
+	swhv_handle_hypervisor_collector(control);
+
+	// flush partially filled hypervisor buffers
+	flush_mode = true;
+
+	// force the device_read function to check if any
+	// buffers are partially filled with data
+	wake_up_interruptible(&read_queue);
+
+	/*
+	 * Clear out the MSR collector list.
+	 */
+	swhv_destroy_msr_collector_list();
+
+	return PW_SUCCESS;
+}
+
+long swhv_start(void)
+{
+	uint32_t control = 1; // start collection command
+#if 0
+	struct profiling_vm_info_list *vm_info_list = NULL;
+	int i;
+#endif
+	pw_pr_debug("socwatch: start called\n");
+
+	flush_mode = false;
+
+	// start collection + send switch bitmask
+	swhv_handle_hypervisor_collector(control);
+
+	//If MSR ops are present, perform them to get begin snapshot data.
+	swhv_handle_msr_collector_list();
+
+#if 0
+	// Expand this eventually to retrieve VM-realted info from the hypervisor
+	// Leaving it here for now.
+	vm_info_list = kmalloc(sizeof(struct profiling_vm_info_list),
+			       GFP_KERNEL);
+	memset(vm_info_list, 0, sizeof(struct profiling_vm_info_list));
+
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_GET_VMINFO,
+			virt_to_phys(vm_info_list));
+
+	pw_pr_debug("Number of VMs: %d\n", vm_info_list->num_vms);
+	for (i = 0; i < vm_info_list->num_vms; ++i) {
+		pw_pr_debug("VM id: %d\n", vm_info_list->vm_list[i].vm_id_num);
+		pw_pr_debug("VM name: %s\n", vm_info_list->vm_list[i].vm_name);
+	}
+#endif
+	return PW_SUCCESS;
+}
+
+long swhv_get_cpu_count(u32 __user *remote_args)
+{
+	uint32_t num_CPUs = pcpu_num;
+
+	return copy_to_user(remote_args, &num_CPUs, sizeof(num_CPUs));
+};
+
+int device_open_i(struct inode *inode, struct file *file)
+{
+	pw_pr_debug("socwatch: device_open_i() called\n");
+	return PW_SUCCESS;
+}
+
+long swhv_get_clock(u32 __user *remote_in_args, u64 __user *remote_args)
+{
+	return -1;
+}
+
+long swhv_get_topology(u64 __user *remote_args)
+{
+	return -1;
+}
+
+long swhv_get_hypervisor_type(u32 __user *remote_args)
+{
+	uint32_t hypervisor_type = swhv_hypervisor_acrn;
+
+	return copy_to_user(remote_args, &hypervisor_type,
+			    sizeof(hypervisor_type));
+}
+
+long swhv_msr_read(u32 __user *remote_in_args, u64 __user *remote_args)
+{
+	int cpu;
+	uint64_t msr_addr = 0, value;
+	int ret = PW_SUCCESS;
+
+	if (get_user(msr_addr, remote_in_args)) {
+		pw_pr_error(
+			"ERROR: couldn't copy remote args for read MSR IOCTL!\n");
+		return -1;
+	}
+
+	if (!msr_read_ops_list) {
+		msr_read_ops_list = kmalloc(
+			pcpu_num * sizeof(struct profiling_msr_ops_list),
+			GFP_KERNEL);
+		if (!msr_read_ops_list) {
+			pw_pr_error(
+				"couldn't allocate memory for doing an MSR read!\n");
+			return -1;
+		}
+		memset(msr_read_ops_list, 0,
+		       pcpu_num * sizeof(struct profiling_msr_ops_list));
+	}
+
+	/*
+	 * The hypercall is set in such a way that the MSR read will occur on
+	 * all CPUs and as a result we have to set up structures for each CPU.
+	 */
+	foreach_cpu(cpu, pcpu_num)
+	{
+		msr_read_ops_list[cpu].collector_id = COLLECTOR_SOCWATCH;
+		msr_read_ops_list[cpu].msr_op_state = MSR_OP_REQUESTED;
+		msr_read_ops_list[cpu].num_entries = 1;
+		msr_read_ops_list[cpu].entries[0].msr_id = msr_addr;
+		msr_read_ops_list[cpu].entries[0].msr_op_type = MSR_OP_READ;
+		msr_read_ops_list[cpu].entries[1].msr_id =
+			-1; // the next entry is expected to be set to -1
+		msr_read_ops_list[cpu].entries[1].param =
+			0; // set to 0 to not generate sample in hypervisor
+	}
+
+	//hypervisor call to do immediate MSR read
+	acrn_hypercall2(HC_PROFILING_OPS, PROFILING_MSR_OPS,
+			virt_to_phys(msr_read_ops_list));
+
+	// copy value to remote args, pick from any CPU
+	value = msr_read_ops_list[0].entries[0].value;
+
+	if (copy_to_user(remote_args, &value, sizeof(value))) {
+		pw_pr_error("ERROR: unable to copy MSR value to userspace!\n");
+		ret = -PW_ERROR;
+	}
+
+	return ret;
+}
+
+long swhv_collection_poll(void)
+{
+	int ret = PW_SUCCESS;
+	/*
+	 * Handle 'POLL' timer expirations.
+	 */
+	if (SW_LIST_EMPTY(&swhv_msr_collector)) {
+		pw_pr_debug("DEBUG: EMPTY MSR COLLECTOR POLL LIST\n");
+	}
+
+	if (swhv_handle_msr_collector_list()) {
+		pw_pr_error("ERROR: unable to copy MSR value to userspace!\n");
+		ret = -PW_ERROR;
+	}
+	return ret;
+}
+
+ssize_t swhv_transfer_data(void *user_buffer, struct shared_buf *sbuf_to_copy,
+			   size_t bytes_to_read)
+{
+	unsigned long bytes_not_copied;
+	ssize_t bytes_read;
+	ssize_t ret = 0;
+	void *data_read = NULL;
+
+	if (bytes_to_read == 0) {
+		pw_pr_debug(
+			"%s - 0 bytes requested to transfer! Returning...\n",
+			__func__);
+
+		return bytes_to_read;
+	}
+
+	data_read = vmalloc(bytes_to_read);
+	if (!data_read) {
+		pw_pr_error(
+			"couldn't allocate memory when trying to transfer data to userspace!\n");
+		return 0;
+	}
+
+	pw_pr_debug("%s - bytes to transfer %zu\n", __func__, bytes_to_read);
+
+	if (sbuf_to_copy) {
+		bytes_read = sbuf_get_variable(sbuf_to_copy, &data_read,
+					       bytes_to_read);
+
+		if (bytes_read != bytes_to_read) {
+			pw_pr_warn("%s - bytes read (%zu bytes) are not equal to expected bytes (%zu bytes) to be read!", __func__, bytes_read, bytes_to_read);
+		}
+
+		if (bytes_read < 0) {
+			pw_pr_error("Error reading this buffer\n");
+			ret = -PW_ERROR;
+			goto ret_free;
+		}
+		if (bytes_read) {
+			// copy data to device file
+			if (bytes_read > bytes_to_read) {
+				pw_pr_error("user buffer is too small\n");
+				ret = -PW_ERROR;
+				goto ret_free;
+			}
+
+			bytes_not_copied = copy_to_user(user_buffer, data_read,
+							bytes_read);
+			//TODO check if this is meaningful enough to have
+			//*offset += bytes_read - bytes_not_copied;
+
+			if (bytes_not_copied) {
+				pw_pr_error(
+					"transferring data to user mode failed, bytes %ld\n",
+					bytes_not_copied);
+				// copy_to_user returns an unsigned
+				ret = -EIO;
+				goto ret_free;
+			}
+			ret = bytes_read;
+			goto ret_free;
+		} else {
+			pw_pr_debug(
+				"Buffer empty! nothing more to read from this buffer\n");
+		}
+	}
+
+ret_free:
+	vfree(data_read);
+	return ret;
+}
+
+bool buffer_not_ready(int *cpu)
+{
+	// cycle through and confirm buffers on all CPUs
+	// are less than ACRN_BUF_TRANSFER_SIZE
+	// as well as flush mode has not been requested
+	int i = 0;
+	bool not_enough_data = true;
+
+	pw_pr_debug(
+		"checking if a buffer is ready to be copied to the device file\n");
+	/*
+	 * It's possible that the buffer from cpu0 may always have
+	 * data to transfer and can potentially prevent buffers from
+	 * other cpus from ever being serviced.
+	 * TODO Consider adding an optimization to check for last cpu read.
+	 */
+	for (i = 0; i < pcpu_num; ++i) {
+		if (ACRN_BUF_FILLED_SIZE(sbuf_per_cpu[i]) >=
+		    ACRN_BUF_TRANSFER_SIZE ||
+		    (flush_mode && ACRN_BUF_FILLED_SIZE(sbuf_per_cpu[i]))) {
+			not_enough_data = false;
+			*cpu = i;
+			pw_pr_debug(
+				"buffer ready (flush_mode=%d) on cpu %d, waking up read queue\n",
+				flush_mode, *cpu);
+			break;
+		}
+	}
+	return not_enough_data && !flush_mode;
+}
+
+ssize_t device_read_i(struct file *file, char __user *user_buffer,
+		      size_t length, loff_t *offset)
+{
+	ssize_t bytes_read = 0;
+	int cpu = 0;
+
+	pw_pr_debug("%s - usermode attempting to read device file\n", __func__);
+
+	if (wait_event_interruptible(read_queue, !buffer_not_ready(&cpu))) {
+		pw_pr_error("%s - wait_event_interruptible failed\n", __func__);
+		return -ERESTARTSYS;
+	}
+	pw_pr_debug("%s - wait_event cleared\n", __func__);
+
+	if (flush_mode) {
+		pw_pr_debug("flush mode on, ready to flush a buffer\n");
+	}
+	length = ACRN_BUF_FILLED_SIZE(sbuf_per_cpu[cpu]);
+	pw_pr_debug("on cpu %d, buffer size is %zu bytes\n", cpu, length);
+
+	bytes_read = swhv_transfer_data(user_buffer, sbuf_per_cpu[cpu], length);
+
+	return bytes_read;
+}
+
+void cleanup_error_i(void)
+{
+	// NOP for acrn
+}
+
+int swhv_load_driver_i(void)
+{
+	int ret = PW_SUCCESS;
+
+	if (x86_hyper_type != X86_HYPER_ACRN) {
+		pw_pr_error("Non-ACRN hypervisor not supported!\n");
+		return -EINVAL;
+	}
+
+	/* TODO: we could get the cpu count by querying the hypervisor later */
+	pcpu_num = num_present_cpus();
+	pw_pr_debug("%s, pcpu_num: %d\n", __func__, pcpu_num);
+
+	ret = swhv_init_per_cpu_buffers();
+	if (ret < 0) {
+		return ret;
+	}
+
+	// initialize a work queue to be used for signalling when
+	// data is ready to copy to usermode
+	init_waitqueue_head(&read_queue);
+
+	swhv_init_msr_collector_list();
+
+	return ret;
+}
+
+void swhv_unload_driver_i(void)
+{
+	swhv_destroy_per_cpu_buffers();
+
+	/* used by the MSR read IOCTL */
+	kfree(msr_read_ops_list);
+}
diff --git a/drivers/platform/x86/socwatchhv/swhv_driver.c b/drivers/platform/x86/socwatchhv/swhv_driver.c
new file mode 100644
index 000000000000..369d8a69158f
--- /dev/null
+++ b/drivers/platform/x86/socwatchhv/swhv_driver.c
@@ -0,0 +1,375 @@
+/*
+
+  This file is provided under a dual BSD/GPLv2 license.  When using or
+  redistributing this file, you may do so under either license.
+
+  GPL LICENSE SUMMARY
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  This program is free software; you can redistribute it and/or modify
+  it under the terms of version 2 of the GNU General Public License as
+  published by the Free Software Foundation.
+
+  This program is distributed in the hope that it will be useful, but
+  WITHOUT ANY WARRANTY; without even the implied warranty of
+  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+  General Public License for more details.
+
+  Contact Information:
+  SoC Watch Developer Team <socwatchdevelopers@intel.com>
+  Intel Corporation,
+  1300 S Mopac Expwy,
+  Austin, TX 78746
+
+  BSD LICENSE
+
+  Copyright(c) 2014 - 2018 Intel Corporation.
+
+  Redistribution and use in source and binary forms, with or without
+  modification, are permitted provided that the following conditions
+  are met:
+
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in
+      the documentation and/or other materials provided with the
+      distribution.
+    * Neither the name of Intel Corporation nor the names of its
+      contributors may be used to endorse or promote products derived
+      from this software without specific prior written permission.
+
+  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+  OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+  SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+  LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+  DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+  THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+  (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+*/
+#define MOD_AUTHOR "SoCWatch Team"
+#define MOD_DESC "SoCWatch kernel module to communicate with hypervisors"
+
+#include "swhv_defines.h"
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/timer.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/interrupt.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/version.h>
+#include <linux/io.h>
+#include <linux/uaccess.h>
+
+#include "swhv_driver.h"
+#include "swhv_ioctl.h"
+#include "swhv_structs.h"
+#if HYPERVISOR == MOBILEVISOR
+#include "swhv_mobilevisor.h"
+#include "swhv_mobilevisor_buffer.h"
+#elif HYPERVISOR == ACRN
+#include "swhv_acrn.h"
+#endif
+
+/* *******************************************
+ * Compile-time constants
+ * *******************************************
+ */
+/* *******************************************
+ * Local data structures.
+ * *******************************************
+ */
+#if defined(HAVE_COMPAT_IOCTL) && defined(CONFIG_X86_64)
+#include <linux/compat.h>
+/*
+ * Helper struct used to translate IOCTLs
+ * from 32b user programs in 64b kernels.
+ */
+struct spdrv_ioctl_arg32 {
+	pw_s32_t in_len;
+	pw_s32_t out_len;
+	compat_caddr_t in_arg;
+	compat_caddr_t out_arg;
+};
+#endif // COMPAT && x64
+
+static int sp_dev_major_num = -1;
+static dev_t sp_dev;
+static struct cdev *sp_cdev;
+static struct class *sp_class;
+
+/* *******************************************
+ * Variables.
+ * *******************************************
+ */
+
+/* Per-CPU variable containing the currently running vcpu. */
+//static DEFINE_PER_CPU(int, curr_vcpu) = 0;
+
+/* *******************************************
+ * Function definitions.
+ * *******************************************
+ */
+
+static long swhv_handle_cmd(u32 __user *remote_cmd)
+{
+	u32 local_cmd = 0;
+	long status = 0;
+
+	if (get_user(local_cmd, remote_cmd)) {
+		pw_pr_error("ERROR: couldn't copy in remote command!\n");
+		return -1;
+	}
+	switch (local_cmd) {
+	case SWHVDRV_CMD_START:
+		pw_pr_debug("RECEIVED CMD START!\n");
+		status = swhv_start();
+		break;
+	case SWHVDRV_CMD_STOP:
+		pw_pr_debug("RECEIVED CMD STOP!\n");
+		status = swhv_stop();
+		break;
+	default:
+		pw_pr_error(
+			"ERROR: invalid command %d passed to the SoFIA driver!\n",
+			local_cmd);
+		status = -1;
+		break;
+	}
+	return status;
+};
+
+long swhv_get_version(u64 __user *remote_args)
+{
+	u64 local_version = (u64)SWHVDRV_VERSION_MAJOR << 32 |
+			    (u64)SWHVDRV_VERSION_MINOR << 16 |
+			    (u64)SWHVDRV_VERSION_OTHER;
+
+	return put_user(local_version, remote_args);
+};
+
+#if defined(CONFIG_COMPAT) && defined(CONFIG_X86_64)
+#define MATCH_IOCTL(num, pred) ((num) == (pred) || (num) == (pred##32))
+#else
+#define MATCH_IOCTL(num, pred) ((num) == (pred))
+#endif
+
+static long handle_ioctl(unsigned int ioctl_num,
+			 struct spdrv_ioctl_arg __user *remote_args)
+{
+	long status = 0;
+	struct spdrv_ioctl_arg local_args;
+
+	int local_in_len, local_out_len;
+
+	if (copy_from_user(&local_args, remote_args, sizeof(local_args))) {
+		pw_pr_error("ERROR: couldn't copy in remote args!\n");
+		return -1;
+	}
+	pw_pr_debug("Invoking IOCTL!\n");
+
+	local_in_len = local_args.in_len;
+	local_out_len = local_args.out_len;
+
+	switch (ioctl_num) {
+	case SWHVDRV_OPERATION_CMD:
+		status = swhv_handle_cmd((u32 __user *)local_args.in_arg);
+		break;
+
+	case SWHVDRV_OPERATION_CONFIGURE:
+		pw_pr_debug("Trying to configure driver!\n");
+		status = swhv_configure(
+			(struct swhv_driver_interface_msg __user *)
+				local_args.in_arg,
+			local_in_len);
+		break;
+
+	case SWHVDRV_OPERATION_VERSION:
+		pw_pr_debug("Trying to get driver version!\n");
+		status = swhv_get_version((u64 __user *)local_args.out_arg);
+		break;
+
+	case SWHVDRV_OPERATION_CLOCK:
+		pw_pr_debug("Trying to get hypervisor type!\n");
+		status = swhv_get_clock((u32 __user *)local_args.in_arg,
+					(u64 __user *)local_args.out_arg);
+		break;
+
+	case SWHVDRV_OPERATION_TOPOLOGY:
+		pw_pr_debug("Trying to get CPU topology!\n");
+		status = swhv_get_topology((u64 __user *)local_args.out_arg);
+		break;
+
+	case SWHVDRV_OPERATION_CPUCOUNT:
+		pw_pr_debug("Trying to get CPU count!\n");
+		status = swhv_get_cpu_count((u32 __user *)local_args.out_arg);
+		break;
+
+	case SWHVDRV_OPERATION_HYPERVISOR_TYPE:
+		pw_pr_debug("Trying to get hypervisor type!\n");
+		status = swhv_get_hypervisor_type(
+			(u32 __user *)local_args.out_arg);
+		break;
+
+	case SWHVDRV_OPERATION_MSR_READ:
+		pw_pr_debug("Trying to do MSR read!\n");
+		status = swhv_msr_read((u32 __user *)local_args.in_arg,
+				       (u64 __user *)local_args.out_arg);
+		break;
+	case SWHVDRV_OPERATION_POLL:
+		pw_pr_debug("Polling tick!\n");
+		status = swhv_collection_poll();
+		break;
+	}
+	return status;
+}
+
+static long device_unlocked_ioctl(struct file *filep, unsigned int ioctl_num,
+				  unsigned long ioctl_param)
+{
+	return handle_ioctl(_IOC_NR(ioctl_num),
+			    (struct spdrv_ioctl_arg __user *)ioctl_param);
+};
+
+#if defined(HAVE_COMPAT_IOCTL) && defined(CONFIG_X86_64)
+static long device_compat_ioctl(struct file *file, unsigned int ioctl_num,
+				unsigned long ioctl_param)
+{
+	struct spdrv_ioctl_arg32 __user *remote_args32 =
+		compat_ptr(ioctl_param);
+	struct spdrv_ioctl_arg __user *remote_args =
+		compat_alloc_user_space(sizeof(*remote_args));
+	int tmp;
+	u32 data;
+
+	if (!remote_args) {
+		return -1;
+	}
+	if (get_user(tmp, &remote_args32->in_len) ||
+	    put_user(tmp, &remote_args->in_len)) {
+		return -1;
+	}
+	if (get_user(tmp, &remote_args32->out_len) ||
+	    put_user(tmp, &remote_args->out_len)) {
+		return -1;
+	}
+	if (get_user(data, &remote_args32->in_arg) ||
+	    put_user(compat_ptr(data), &remote_args->in_arg)) {
+		return -1;
+	}
+	if (get_user(data, &remote_args32->out_arg) ||
+	    put_user(compat_ptr(data), &remote_args->out_arg)) {
+		return -1;
+	}
+	return handle_ioctl(_IOC_NR(ioctl_num), remote_args);
+};
+#endif // COMPAT && x64
+
+static int device_open(struct inode *inode, struct file *file)
+{
+	return device_open_i(inode, file);
+}
+
+static ssize_t
+device_read(struct file *file, /* see include/linux/fs.h */
+	    char __user *buffer, /* buffer to be filled with data */
+	    size_t length, /* length of the buffer */
+	    loff_t *offset)
+{
+	return device_read_i(file, buffer, length, offset);
+}
+
+static struct file_operations s_fops = {
+	.open = &device_open,
+	.read = &device_read,
+	.unlocked_ioctl = &device_unlocked_ioctl,
+#if defined(HAVE_COMPAT_IOCTL) && defined(CONFIG_X86_64)
+	.compat_ioctl = &device_compat_ioctl,
+#endif // COMPAT && x64
+};
+
+static void cleanup_error(void)
+{
+	unregister_chrdev(sp_dev_major_num, SWHV_DEVICE_NAME);
+	device_destroy(sp_class, sp_dev);
+	class_destroy(sp_class);
+	unregister_chrdev_region(sp_dev, 1);
+	cdev_del(sp_cdev);
+}
+
+int __init swhv_load_driver(void)
+{
+	int error;
+	struct device *dev;
+
+	// create the char device "sp"
+	alloc_chrdev_region(&sp_dev, 0, 1, SWHV_DEVICE_NAME);
+	sp_dev_major_num = MAJOR(sp_dev);
+	sp_class = class_create(THIS_MODULE, SWHV_DEVICE_NAME);
+	if (IS_ERR(sp_class)) {
+		error = PTR_ERR(sp_class);
+		pw_pr_error("Error registering sp class\n");
+		goto cleanup_return_error;
+	}
+
+	dev = device_create(sp_class, NULL, sp_dev, NULL, SWHV_DEVICE_NAME);
+	if (dev == NULL) {
+		error = PTR_ERR(dev);
+		pw_pr_error("Error during call to device_create\n");
+		goto cleanup_return_error;
+	}
+
+	sp_cdev = cdev_alloc();
+	if (sp_cdev == NULL) {
+		error = -ENOMEM;
+		pw_pr_error("Error allocating character device\n");
+		goto cleanup_return_error;
+	}
+	sp_cdev->owner = THIS_MODULE;
+	sp_cdev->ops = &s_fops;
+	if (cdev_add(sp_cdev, sp_dev, 1) < 0) {
+		error = -1;
+		pw_pr_error("Error registering device driver\n");
+		goto cleanup_return_error;
+	}
+
+	error = swhv_load_driver_i();
+	if (error < 0) {
+		pw_pr_error("Error initializing device driver\n");
+		goto cleanup_return_error;
+	}
+
+	return 0;
+
+cleanup_return_error:
+	cleanup_error_i();
+
+	// release char device
+	cleanup_error();
+	return error;
+}
+
+static void __exit swhv_unload_driver(void)
+{
+	swhv_unload_driver_i();
+
+	// release char device
+	cleanup_error();
+}
+
+module_init(swhv_load_driver);
+module_exit(swhv_unload_driver);
+
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_AUTHOR(MOD_AUTHOR);
+MODULE_DESCRIPTION(MOD_DESC);
-- 
2.22.0.545.g9c9b961d7e

