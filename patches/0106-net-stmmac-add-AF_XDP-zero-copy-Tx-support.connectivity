From a75ffb7df821d2e58ceda2b2ca29b900d3542615 Mon Sep 17 00:00:00 2001
From: Ong Boon Leong <boon.leong.ong@intel.com>
Date: Fri, 4 Oct 2019 21:51:06 +0800
Subject: [PATCH 106/108] net: stmmac: add AF_XDP zero-copy Tx support

Signed-off-by: Ong Boon Leong <boon.leong.ong@intel.com>
Signed-off-by: Voon Weifeng <weifeng.voon@intel.com>
---
 drivers/net/ethernet/stmicro/stmmac/stmmac.h  |   6 +
 .../ethernet/stmicro/stmmac/stmmac_ethtool.c  |   8 +
 .../net/ethernet/stmicro/stmmac/stmmac_main.c |  63 ++++-
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.c  | 248 ++++++++++++++++++
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.h  |   5 +
 5 files changed, 323 insertions(+), 7 deletions(-)

diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac.h b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
index aae4806e510d..52f0852b3e86 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
@@ -67,6 +67,7 @@ struct stmmac_tx_queue {
 	u32 mss;
 	struct xdp_umem *xsk_umem;
 	struct zero_copy_allocator zca; /* ZC allocator */
+	spinlock_t xdp_xmit_lock;
 };
 
 struct stmmac_rx_buffer {
@@ -344,6 +345,7 @@ int stmmac_resume_main(struct stmmac_priv *priv, struct net_device *ndev);
 #define STMMAC_XDP_REDIR	BIT(2)
 
 #define STMMAC_RX_BUFFER_WRITE	32	/* Must be power of 2 */
+#define STMMAC_TX_BUFFER_BUDGET	32
 
 #define STMMAC_RX_DMA_ATTR \
 	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
@@ -384,6 +386,10 @@ static inline struct stmmac_tx_queue *get_tx_queue(struct stmmac_priv *priv,
 	((((x)->cur_rx > (x)->dirty_rx) ? 0 : priv->dma_rx_size) + \
 	(x)->cur_rx - (x)->dirty_rx - 1)
 
+#define STMMAC_TX_DESC_TO_CLEAN(x)	\
+	(((x)->dirty_tx <= (x)->cur_tx) ? (x)->cur_tx - (x)->dirty_tx : \
+	priv->dma_tx_size - (x)->dirty_tx + (x)->cur_tx)
+
 int stmmac_xmit_xdp_tx_queue(struct xdp_buff *xdp,
 			     struct stmmac_tx_queue *xdp_q);
 void stmmac_xdp_queue_update_tail(struct stmmac_tx_queue *xdp_q);
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_ethtool.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_ethtool.c
index 3c933de62852..e733b7910ed8 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_ethtool.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_ethtool.c
@@ -19,6 +19,7 @@
 #include "stmmac.h"
 #include "dwmac_dma.h"
 #include "dwxgmac2.h"
+#include "stmmac_xsk.h"
 
 #define REG_SPACE_SIZE	0x1060
 #define MAC100_ETHTOOL_NAME	"st_mac100"
@@ -495,6 +496,13 @@ static int stmmac_set_ringparam(struct net_device *netdev,
 	    !is_power_of_2(ring->tx_pending))
 		return -EINVAL;
 
+	/* If there is a AF_XDP UMEM attached to any of Rx queues,
+	 * disallow changing the number of descriptors -- regardless
+	 * if the netdev is running or not.
+	 */
+	if (stmmac_xsk_any_rx_ring_enabled(netdev))
+		return -EBUSY;
+
 	return stmmac_reinit_ringparam(netdev, ring->rx_pending,
 				       ring->tx_pending);
 }
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index 5e13df35f6d3..6be40e494923 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -1329,6 +1329,25 @@ static struct xdp_umem *stmmac_xsk_rx_umem(struct stmmac_priv *priv, u32 queue)
 	return xdp_get_umem_from_qid(priv->dev, queue);
 }
 
+/**
+ * stmmac_xsk_tx_umem - Retrieve the AF_XDP ZC if XDP and ZC is enabled
+ * @priv: private structure
+ * @queue: TX or TX XDP queue index
+ * Returns the UMEM or NULL.
+ **/
+static struct xdp_umem *stmmac_xsk_tx_umem(struct stmmac_priv *priv, u32 queue)
+{
+	bool xdp_on = stmmac_enabled_xdp(priv);
+
+	if (queue_is_xdp(priv, queue))
+		queue -= priv->plat->num_queue_pairs;
+
+	if (!xdp_on || !test_bit(queue, &priv->af_xdp_zc_qps))
+		return NULL;
+
+	return xdp_get_umem_from_qid(priv->dev, queue);
+}
+
 bool stmmac_alloc_rx_buffers(struct stmmac_rx_queue *rx_q, u32 count)
 {
 	struct stmmac_priv *priv = rx_q->priv_data;
@@ -1487,6 +1506,11 @@ static void init_dma_tx_desc_ring(struct stmmac_priv *priv, u32 queue)
 		  "(%s) dma_tx_phy=0x%08x\n", __func__,
 		  (u32)tx_q->dma_tx_phy);
 
+	if (queue_is_xdp(priv, queue)) {
+		spin_lock_init(&tx_q->xdp_xmit_lock);
+		tx_q->xsk_umem = stmmac_xsk_tx_umem(priv, queue);
+	}
+
 	/* Setup the chained descriptor addresses */
 	if (priv->mode == STMMAC_CHAIN_MODE) {
 		if (priv->extend_desc)
@@ -1584,10 +1608,15 @@ static int init_dma_desc_rings(struct net_device *dev, gfp_t flags)
  */
 static void dma_free_rx_skbufs(struct stmmac_priv *priv, u32 queue)
 {
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[queue];
 	int i;
 
-	for (i = 0; i < priv->dma_rx_size; i++)
-		stmmac_free_rx_buffer(priv, queue, i);
+	if (rx_q->xsk_umem) {
+		stmmac_xsk_clean_rx_queue(rx_q);
+	} else {
+		for (i = 0; i < priv->dma_rx_size; i++)
+			stmmac_free_rx_buffer(priv, queue, i);
+	}
 }
 
 /**
@@ -1597,10 +1626,15 @@ static void dma_free_rx_skbufs(struct stmmac_priv *priv, u32 queue)
  */
 static void dma_free_tx_skbufs(struct stmmac_priv *priv, u32 queue)
 {
+	struct stmmac_tx_queue *tx_q = get_tx_queue(priv, queue);
 	int i;
 
-	for (i = 0; i < priv->dma_tx_size; i++)
-		stmmac_free_tx_buffer(priv, queue, i);
+	if (queue_is_xdp(priv, queue) && tx_q->xsk_umem) {
+		stmmac_xsk_clean_tx_queue(tx_q);
+	} else {
+		for (i = 0; i < priv->dma_tx_size; i++)
+			stmmac_free_tx_buffer(priv, queue, i);
+	}
 }
 
 static void free_dma_rx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
@@ -1674,6 +1708,7 @@ static void free_dma_tx_desc_resources_q(struct stmmac_priv *priv, u32 queue)
 	kfree(tx_q->tx_skbuff_dma);
 	kfree(tx_q->tx_skbuff);
 	kfree(tx_q->xdpf);
+	tx_q->xsk_umem = NULL;
 }
 
 /**
@@ -4334,6 +4369,7 @@ void stmmac_xdp_queue_update_tail(struct stmmac_tx_queue *xdp_q)
 }
 
 /**
+
  * stmmac_finalize_xdp_rx - Bump XDP Tx tail
  * @rx_q: Rx queue
  * @xdp_res: Result of the receive batch
@@ -4700,15 +4736,18 @@ static int stmmac_napi_poll_tx(struct napi_struct *napi, int budget)
 
 	priv->xstats.napi_poll++;
 
-	work_done = stmmac_tx_clean(priv, priv->dma_tx_size, chan);
+	tx_q = get_tx_queue(priv, chan);
+
+	work_done = tx_q->xsk_umem ?
+		    stmmac_xdp_tx_clean(priv, budget, chan) :
+		    stmmac_tx_clean(priv, priv->dma_tx_size, chan);
+
 	work_done = min(work_done, budget);
 
 	if (work_done < budget)
 		napi_complete_done(napi, work_done);
 
 	/* Force transmission restart */
-	tx_q = get_tx_queue(priv, chan);
-
 	if (tx_q->cur_tx != tx_q->dirty_tx) {
 		stmmac_enable_dma_transmission(priv, priv->ioaddr);
 		stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr,
@@ -5539,6 +5578,15 @@ static int stmmac_xdp_setup(struct stmmac_priv *priv,
 	if (old_prog)
 		bpf_prog_put(old_prog);
 
+	/* Kick start the NAPI context if there is an AF_XDP socket open
+	 * on that queue id. This so that receiving will start.
+	 */
+	if (need_reset && prog)
+		for (i = 0; i < priv->plat->num_queue_pairs; i++)
+			if (priv->xdp_queue[i].xsk_umem)
+				(void)stmmac_xsk_wakeup(priv->dev, i,
+							XDP_WAKEUP_TX);
+
 	return 0;
 }
 
@@ -5849,6 +5897,7 @@ static const struct net_device_ops stmmac_netdev_ops = {
 	.ndo_vlan_rx_kill_vid = stmmac_vlan_rx_kill_vid,
 	.ndo_bpf = stmmac_xdp,
 	.ndo_xdp_xmit = stmmac_xdp_xmit,
+	.ndo_xsk_wakeup = stmmac_xsk_wakeup,
 };
 
 static void stmmac_reset_subtask(struct stmmac_priv *priv)
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
index 3969fd40a094..af96417db99b 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
@@ -6,6 +6,7 @@
 #include <net/xdp.h>
 
 #include "stmmac.h"
+#include "stmmac_xsk.h"
 
 /**
  * stmmac_xsk_umem_dma_map - DMA maps all UMEM memory for the netdev
@@ -110,6 +111,11 @@ static int stmmac_xsk_umem_enable(struct stmmac_priv *priv,
 		err = stmmac_queue_pair_enable(priv, qid);
 		if (err)
 			return err;
+
+		/* Kick start the NAPI context so that receiving will start */
+		err = stmmac_xsk_wakeup(priv->dev, qid, XDP_WAKEUP_RX);
+		if (err)
+			return err;
 	}
 
 	return 0;
@@ -677,3 +683,245 @@ int stmmac_rx_zc(struct stmmac_priv *priv, int budget, u32 queue)
 
 	return failure ? budget : (int)total_rx_packets;
 }
+
+/**
+ * stmmac_xmit_zc - Performs zero-copy TX AF_XDP
+ * @xdp_q: XDP Tx queue
+ * @budget: NAPI budget
+ *
+ * Returns true if the work is finished.
+ **/
+static bool stmmac_xmit_zc(struct stmmac_tx_queue *xdp_q, unsigned int budget)
+{
+	struct stmmac_priv *priv = xdp_q->priv_data;
+	struct dma_desc *tx_desc = NULL;
+	bool work_done = true;
+	struct xdp_desc desc;
+	dma_addr_t dma;
+	int entry = xdp_q->cur_tx;
+	int first_entry = xdp_q->cur_tx;
+
+	while (budget-- > 0) {
+		if (!unlikely(STMMAC_TX_DESC_UNUSED(xdp_q))) {
+			work_done = false;
+			break;
+		}
+
+		if (!xsk_umem_consume_tx(xdp_q->xsk_umem, &desc))
+			break;
+
+		dma = xdp_umem_get_dma(xdp_q->xsk_umem, desc.addr);
+
+		dma_sync_single_for_device(priv->device, dma, desc.len,
+					   DMA_BIDIRECTIONAL);
+
+		if (likely(priv->extend_desc))
+			tx_desc = (struct dma_desc *)(xdp_q->dma_etx + entry);
+		else if (priv->enhanced_tx_desc)
+			tx_desc = &xdp_q->dma_enhtx[entry].basic;
+		else
+			tx_desc = xdp_q->dma_tx + entry;
+
+		xdp_q->tx_skbuff_dma[entry].buf = dma;
+		xdp_q->tx_skbuff_dma[entry].len = desc.len;
+		xdp_q->tx_skbuff_dma[entry].map_as_page = false;
+		xdp_q->tx_skbuff_dma[entry].last_segment = 1;
+		xdp_q->tx_skbuff_dma[entry].is_jumbo = 0;
+
+		stmmac_set_desc_addr(priv, tx_desc, dma);
+
+		stmmac_prepare_tx_desc(priv, tx_desc, /* Tx descriptor */
+				       1, /* is first descriptor */
+				       desc.len,
+				       1, /* checksum offload enabled */
+				       priv->mode,
+				       1, /* Tx OWN bit */
+				       1, /* is last segment */
+				       desc.len); /* Total packet length */
+
+		wmb();
+
+		entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
+		xdp_q->cur_tx = entry;
+	}
+
+	if (first_entry != entry) {
+		stmmac_xdp_queue_update_tail(xdp_q);
+		xsk_umem_consume_tx_done(xdp_q->xsk_umem);
+	}
+
+	return !!budget && work_done;
+}
+
+/**
+ * stmac_clean_xdp_tx_buffer - Frees and unmaps an XDP Tx entry
+ * @priv: driver private structure
+ * @queue: TX XDP queue
+ * @entry: entry to be cleared
+ **/
+static void stmac_clean_xdp_tx_buffer(struct stmmac_priv *priv, u32 queue,
+				      u32 entry)
+{
+	struct stmmac_tx_queue *xdp_q = get_tx_queue(priv, queue);
+
+	xdp_return_frame(xdp_q->xdpf[entry]);
+	dma_unmap_single(priv->device,
+			 xdp_q->tx_skbuff_dma[entry].buf,
+			 xdp_q->tx_skbuff_dma[entry].len,
+			 DMA_TO_DEVICE);
+	xdp_q->tx_skbuff_dma[entry].len = 0;
+	xdp_q->tx_skbuff_dma[entry].buf = 0;
+}
+
+/**
+ * stmmac_xdp_tx_clean - Completes AF_XDP entries, and cleans XDP entries
+ * @tx_q: XDP Tx queue
+ * @tx_bi: Tx buffer info to clean
+ **/
+int stmmac_xdp_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
+{
+	struct stmmac_tx_queue *xdp_q = get_tx_queue(priv, queue);
+	u32 i, frames_ready, xsk_frames = 0, completed_frames = 0;
+	struct xdp_umem *umem = xdp_q->xsk_umem;
+	u32 entry, total_bytes = 0;
+
+	frames_ready = STMMAC_TX_DESC_TO_CLEAN(xdp_q);
+
+	if (frames_ready == 0)
+		goto out_xmit;
+	else if (frames_ready > budget)
+		completed_frames = budget;
+	else
+		completed_frames = frames_ready;
+
+	entry = xdp_q->dirty_tx;
+
+	for (i = 0; i < completed_frames; i++) {
+		if (xdp_q->xdpf[entry])
+			stmac_clean_xdp_tx_buffer(priv, queue, entry);
+		else
+			xsk_frames++;
+
+		xdp_q->xdpf[entry] =  NULL;
+		total_bytes += xdp_q->tx_skbuff_dma[entry].len;
+
+		entry = STMMAC_GET_ENTRY(entry, priv->dma_tx_size);
+	}
+
+	if (entry != xdp_q->dirty_tx)
+		xdp_q->dirty_tx = entry;
+
+	if (xsk_frames)
+		xsk_umem_complete_tx(umem, xsk_frames);
+
+	priv->dev->stats.tx_bytes += total_bytes;
+	priv->dev->stats.tx_packets += completed_frames;
+
+out_xmit:
+	if (spin_trylock(&xdp_q->xdp_xmit_lock)) {
+		stmmac_xmit_zc(xdp_q, budget);
+		spin_unlock(&xdp_q->xdp_xmit_lock);
+	}
+
+	return completed_frames;
+}
+
+/**
+ * stmmac_xsk_wakeup - Implements the ndo_xsk_wakeup
+ * @dev: the netdevice
+ * @queue_id: queue id to wake up
+ *
+ * Returns <0 for errors, 0 otherwise.
+ **/
+int stmmac_xsk_wakeup(struct net_device *dev, u32 queue, u32 flags)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	u16 qp_num = priv->plat->num_queue_pairs;
+	struct stmmac_tx_queue *xdp_q;
+	struct stmmac_channel *ch;
+
+	xdp_q = &priv->xdp_queue[queue];
+	ch = &priv->channel[queue + qp_num];
+
+	if (test_bit(STMMAC_DOWN, &priv->state))
+		return -ENETDOWN;
+
+	if (!stmmac_enabled_xdp(priv))
+		return -ENXIO;
+
+	if (queue >= priv->plat->num_queue_pairs)
+		return -ENXIO;
+
+	if (!xdp_q->xsk_umem)
+		return -ENXIO;
+
+	spin_lock(&xdp_q->xdp_xmit_lock);
+	stmmac_xmit_zc(xdp_q, priv->dma_tx_size);
+	spin_unlock(&xdp_q->xdp_xmit_lock);
+
+	/* The idea here is that if NAPI is running, mark a miss, so
+	 * it will run again. Since we do not have interrupt here,
+	 * we directly call the stmmac_xmit_zc() instead
+	 */
+	if (!napi_if_scheduled_mark_missed(&ch->tx_napi)) {
+		if (likely(napi_schedule_prep(&ch->tx_napi)))
+			__napi_schedule(&ch->tx_napi);
+	}
+
+	return 0;
+}
+
+void stmmac_xsk_clean_rx_queue(struct stmmac_rx_queue *rx_q)
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	u16 i;
+
+	for (i = 0; i < priv->dma_rx_size; i++) {
+		struct stmmac_rx_buffer *buf = &rx_q->buf_pool[i];
+
+		if (!buf->umem_addr)
+			continue;
+
+		xsk_umem_fq_reuse(rx_q->xsk_umem, buf->umem_handle);
+		buf->umem_addr = NULL;
+	}
+}
+
+void stmmac_xsk_clean_tx_queue(struct stmmac_tx_queue *tx_q)
+{
+	u16 ntc = tx_q->dirty_tx, ntu = tx_q->cur_tx;
+	struct stmmac_priv *priv = tx_q->priv_data;
+	struct xdp_umem *umem = tx_q->xsk_umem;
+	u32 queue = tx_q->queue_index;
+	u32 xsk_frames = 0;
+
+	while (ntc != ntu) {
+		if (tx_q->xdpf[ntc])
+			stmac_clean_xdp_tx_buffer(priv, queue, ntc);
+		else
+			xsk_frames++;
+
+		ntc = STMMAC_GET_ENTRY(ntc, priv->dma_tx_size);
+	}
+
+	if (xsk_frames)
+		xsk_umem_complete_tx(umem, xsk_frames);
+}
+
+/**
+ * stmmac_xsk_any_rx_ring_enabled - Checks if Rx rings have AF_XDP UMEM attached
+ *
+ * Returns true if any of the Rx rings has an AF_XDP UMEM attached
+ **/
+bool stmmac_xsk_any_rx_ring_enabled(struct net_device *dev)
+{
+	struct stmmac_priv *priv = netdev_priv(dev);
+	int i;
+
+	for (i = 0; i < priv->plat->num_queue_pairs; i++) {
+		if (xdp_get_umem_from_qid(dev, i))
+			return true;
+	}
+
+	return false;
+}
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
index fbf1e70c7a5a..c9a38fe577d1 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
@@ -12,5 +12,10 @@ int stmmac_xsk_umem_setup(struct stmmac_priv *priv, struct xdp_umem *umem,
 void stmmac_zca_free(struct zero_copy_allocator *alloc, unsigned long handle);
 bool stmmac_alloc_rx_buffers_zc(struct stmmac_rx_queue *rx_q, u16 count);
 int stmmac_rx_zc(struct stmmac_priv *priv, int limit, u32 queue);
+int stmmac_xdp_tx_clean(struct stmmac_priv *priv, int budget, u32 queue);
+int stmmac_xsk_wakeup(struct net_device *dev, u32 queue, u32 flags);
+void stmmac_xsk_clean_rx_queue(struct stmmac_rx_queue *rx_q);
+void stmmac_xsk_clean_tx_queue(struct stmmac_tx_queue *tx_q);
+bool stmmac_xsk_any_rx_ring_enabled(struct net_device *dev);
 
 #endif /* _STMMAC_XSK_H_ */
-- 
2.17.1

