From 1ad2c82bcdcc6d5b22511fa7add01506dbb1ffbd Mon Sep 17 00:00:00 2001
From: "Wong, Vincent Por Yin" <vincent.por.yin.wong@intel.com>
Date: Fri, 9 Aug 2019 22:03:29 +0800
Subject: [PATCH 100/104] net: stmmac: introduce AF_XDP ZC Receive support

- support for XDP_REDIRECT, XDP_DROP, XDP_PASS, XDP_ABORTED
- also introduces umem support

Signed-off-by: Wong, Vincent Por Yin <vincent.por.yin.wong@intel.com>
---
 drivers/net/ethernet/stmicro/stmmac/Makefile  |   2 +-
 drivers/net/ethernet/stmicro/stmmac/stmmac.h  |  13 +
 .../net/ethernet/stmicro/stmmac/stmmac_main.c |  52 +-
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.c  | 649 ++++++++++++++++++
 .../net/ethernet/stmicro/stmmac/stmmac_xsk.h  |  16 +
 5 files changed, 725 insertions(+), 7 deletions(-)
 create mode 100644 drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c

diff --git a/drivers/net/ethernet/stmicro/stmmac/Makefile b/drivers/net/ethernet/stmicro/stmmac/Makefile
index cd663017ea20..d34b4fd48b9b 100644
--- a/drivers/net/ethernet/stmicro/stmmac/Makefile
+++ b/drivers/net/ethernet/stmicro/stmmac/Makefile
@@ -6,7 +6,7 @@ stmmac-objs:= stmmac_main.o stmmac_ethtool.o stmmac_mdio.o ring_mode.o	\
 	      mmc_core.o stmmac_hwtstamp.o stmmac_ptp.o dwmac4_descs.o	\
 	      dwmac4_dma.o dwmac4_lib.o dwmac4_core.o dwmac5.o hwif.o \
 	      stmmac_tc.o dwxgmac2_core.o dwxgmac2_dma.o dwxgmac2_descs.o \
-	      intel_serdes.o stmmac_tsn.o dwmac5_tsn.o $(stmmac-y)
+	      intel_serdes.o stmmac_tsn.o dwmac5_tsn.o stmmac_xsk.o $(stmmac-y)
 
 stmmac-$(CONFIG_STMMAC_SELFTESTS) += stmmac_selftests.o
 
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac.h b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
index c92662ed57b1..1bb4c4351e61 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac.h
@@ -67,9 +67,17 @@ struct stmmac_tx_queue {
 	u32 mss;
 };
 
+/* How many Rx Buffers do we bundle into one write to the hardware ? */
+#define STMMAC_RX_BUFFER_WRITE	16	/* Must be power of 2 */
+#define STMMAC_RX_DMA_ATTR \
+	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
+
 struct stmmac_rx_buffer {
 	struct page *page;
 	dma_addr_t dma_addr;
+	/* For XSK UMEM */
+	void *addr;
+	u64 handle;
 };
 
 struct stmmac_rx_queue {
@@ -88,6 +96,10 @@ struct stmmac_rx_queue {
 	struct bpf_prog *xdp_prog;
 	struct xdp_rxq_info xdp_rxq;
 	u16 next_to_alloc;
+	/* For XSK UMEM */
+	struct xdp_umem *xsk_umem;
+	struct zero_copy_allocator zca; /* ZC allocator func() anchor */
+	u16 xsk_buf_len;
 };
 
 struct stmmac_channel {
@@ -252,6 +264,7 @@ struct stmmac_priv {
 
 	/* XDP */
 	struct bpf_prog *xdp_prog;
+	unsigned long *has_xdp_zc_umem; //per queue bitmap flag
 };
 
 enum stmmac_state {
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
index d580e6d07fb1..9991e5ef8b59 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_main.c
@@ -1788,13 +1788,24 @@ static int alloc_dma_tx_desc_resources(struct stmmac_priv *priv)
  */
 static int alloc_dma_desc_resources(struct stmmac_priv *priv)
 {
+	u32 rx_count = priv->plat->rx_queues_to_use;
+	u32 tx_count = priv->plat->tx_queues_to_use;
+	u32 max_count;
+
 	/* RX Allocation */
 	int ret = alloc_dma_rx_desc_resources(priv);
-
 	if (ret)
 		return ret;
 
 	ret = alloc_dma_tx_desc_resources(priv);
+	if (ret)
+		return ret;
+
+	max_count = (rx_count > tx_count ? rx_count : tx_count);
+
+	priv->has_xdp_zc_umem = bitmap_zalloc(max_count, GFP_KERNEL);
+	if (!priv->has_xdp_zc_umem)
+		bitmap_free(priv->has_xdp_zc_umem);
 
 	return ret;
 }
@@ -1886,6 +1897,10 @@ static void free_dma_desc_resources(struct stmmac_priv *priv)
 
 	/* Release the DMA TX socket buffers */
 	free_dma_tx_desc_resources(priv);
+
+	/* Release the AF_XDP ZC flag */
+	if (priv->has_xdp_zc_umem)
+		bitmap_free(priv->has_xdp_zc_umem);
 }
 
 /**
@@ -2050,9 +2065,15 @@ static void stmmac_free_rx_queue(struct stmmac_priv *priv, u16 qid)
 {
 	struct stmmac_rx_queue *rx_q = &priv->rx_queue[qid];
 
+	if (rx_q->xsk_umem) {
+		stmmac_xsk_free_rx_ring(rx_q);
+		goto out;
+	}
+
 	/* Free all the Rx ring sk_buffs */
 	dma_free_rx_skbufs(priv, qid);
 
+out:
 	rx_q->cur_rx = 0;
 	rx_q->dirty_rx = 0;
 	rx_q->next_to_alloc = 0;
@@ -2081,9 +2102,19 @@ static void stmmac_configure_rx_queue(struct stmmac_priv *priv, u16 qid)
 	struct stmmac_rx_queue *rx_q = &priv->rx_queue[qid];
 
 	xdp_rxq_info_unreg_mem_model(&rx_q->xdp_rxq);
-	WARN_ON(xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
-					   MEM_TYPE_PAGE_SHARED,
-					   NULL));
+	rx_q->xsk_umem = stmmac_xsk_umem(priv, qid);
+	if (rx_q->xsk_umem) {
+		rx_q->zca.free = stmmac_zca_free;
+		WARN_ON(xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
+						   MEM_TYPE_ZERO_COPY,
+						   &rx_q->zca));
+		rx_q->xsk_buf_len = rx_q->xsk_umem->chunk_size_nohr -
+							XDP_PACKET_HEADROOM;
+	} else {
+		WARN_ON(xdp_rxq_info_reg_mem_model(&rx_q->xdp_rxq,
+						   MEM_TYPE_PAGE_SHARED,
+						   NULL));
+	}
 
 	/* Init Rx DMA - DMA_CHAN_RX_BASE_ADDR */
 	stmmac_init_rx_chan(priv, priv->ioaddr, priv->plat->dma_cfg,
@@ -2094,7 +2125,10 @@ static void stmmac_configure_rx_queue(struct stmmac_priv *priv, u16 qid)
 			       qid);
 
 	/* Populate Rx Buffers to Rx DMA Ring */
-	stmmac_alloc_rx_buffers(rx_q, priv->dma_rx_size);
+	if (rx_q->xsk_umem)
+		stmmac_alloc_rx_buffers_slow_zc(rx_q, priv->dma_rx_size);
+	else
+		stmmac_alloc_rx_buffers(rx_q, priv->dma_rx_size);
 }
 
 /**
@@ -2244,6 +2278,8 @@ static int stmmac_tx_clean(struct stmmac_priv *priv, int budget, u32 queue)
 		else
 			p = tx_q->dma_tx + entry;
 
+		prefetch(p);
+
 		status = stmmac_tx_status(priv, &priv->dev->stats,
 				&priv->xstats, p, priv->ioaddr);
 		/* Check if the descriptor is owned by the DMA */
@@ -4483,12 +4519,16 @@ static int stmmac_napi_poll_rx(struct napi_struct *napi, int budget)
 	struct stmmac_channel *ch =
 		container_of(napi, struct stmmac_channel, rx_napi);
 	struct stmmac_priv *priv = ch->priv_data;
+	struct stmmac_rx_queue *rx_q;
 	u32 chan = ch->index;
 	int work_done;
 
 	priv->xstats.napi_poll++;
 
-	work_done = stmmac_rx(priv, budget, chan);
+	rx_q = &priv->rx_queue[chan];
+	work_done = rx_q->xsk_umem ? stmmac_rx_zc(priv, budget, chan) :
+				     stmmac_rx(priv, budget, chan);
+
 	if (work_done < budget && napi_complete_done(napi, work_done))
 		stmmac_enable_dma_irq(priv, priv->ioaddr, chan);
 	return work_done;
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
new file mode 100644
index 000000000000..856b5e99d7a8
--- /dev/null
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.c
@@ -0,0 +1,649 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2019 Intel Corporation. */
+
+#include <linux/bpf_trace.h>
+#include <net/xdp_sock.h>
+#include <net/xdp.h>
+
+#include "stmmac.h"
+#include "stmmac_xsk.h"
+
+struct xdp_umem *stmmac_xsk_umem(struct stmmac_priv *priv, u16 qid)
+{
+	bool xdp_on = stmmac_enabled_xdp(priv);
+
+	if (!xdp_on)
+		return NULL;
+
+	if (!queue_is_xdp(qid))
+		return NULL;
+
+	if (!test_bit(qid, priv->has_xdp_zc_umem))
+		return NULL;
+
+	return xdp_get_umem_from_qid(priv->dev, qid);
+}
+
+static int stmmac_xsk_umem_dma_map(struct stmmac_priv *priv,
+				   struct xdp_umem *umem)
+{
+	struct device *dev = priv->device;
+	unsigned int i, j;
+	dma_addr_t dma;
+
+	for (i = 0; i < umem->npgs; i++) {
+		dma = dma_map_page_attrs(dev, umem->pgs[i], 0, PAGE_SIZE,
+					 DMA_BIDIRECTIONAL,
+					 STMMAC_RX_DMA_ATTR);
+		if (dma_mapping_error(dev, dma))
+			goto out_unmap;
+
+		umem->pages[i].dma = dma;
+	}
+
+	return 0;
+
+out_unmap:
+	for (j = 0; j < i; j++) {
+		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
+				     DMA_BIDIRECTIONAL, STMMAC_RX_DMA_ATTR);
+		umem->pages[i].dma = 0;
+	}
+
+	return -EPERM;
+}
+
+static void stmmac_xsk_umem_dma_unmap(struct stmmac_priv *priv,
+				      struct xdp_umem *umem)
+{
+	struct device *dev = priv->device;
+	unsigned int i;
+
+	for (i = 0; i < umem->npgs; i++) {
+		dma_unmap_page_attrs(dev, umem->pages[i].dma, PAGE_SIZE,
+				     DMA_BIDIRECTIONAL, STMMAC_RX_DMA_ATTR);
+
+		umem->pages[i].dma = 0;
+	}
+}
+
+static int stmmac_xsk_umem_enable(struct stmmac_priv *priv,
+				  struct xdp_umem *umem,
+				  u16 qid)
+{
+	struct xdp_umem_fq_reuse *reuseq;
+	bool if_running;
+	int err;
+
+	if (qid >= priv->plat->rx_queues_to_use ||
+	    qid >= priv->plat->tx_queues_to_use)
+		return -EINVAL;
+
+	reuseq = xsk_reuseq_prepare(priv->dma_rx_size);
+	if (!reuseq)
+		return -ENOMEM;
+
+	xsk_reuseq_free(xsk_reuseq_swap(umem, reuseq));
+
+	err = stmmac_xsk_umem_dma_map(priv, umem);
+	if (err)
+		return err;
+
+	set_bit(qid, priv->has_xdp_zc_umem);
+
+	if_running = netif_running(priv->dev) && READ_ONCE(priv->xdp_prog);
+
+	if (if_running) {
+		stmmac_txrx_ring_disable(priv, qid);
+		stmmac_txrx_ring_enable(priv, qid);
+		/* Kick start the NAPI context so that receiving will start */
+		err = stmmac_xsk_async_xmit(priv->dev, qid);
+		if (err)
+			return err;
+	}
+
+	return 0;
+}
+
+static int stmmac_xsk_umem_disable(struct stmmac_priv *priv, u16 qid)
+{
+	struct xdp_umem *umem;
+	bool if_running;
+
+	umem = xdp_get_umem_from_qid(priv->dev, qid);
+	if (!umem)
+		return -EINVAL;
+
+	if_running = netif_running(priv->dev) && READ_ONCE(priv->xdp_prog);
+
+	if (if_running)
+		stmmac_txrx_ring_disable(priv, qid);
+
+	clear_bit(qid, priv->has_xdp_zc_umem);
+
+	/* UMEM is shared for both Tx & Rx, we unmap once */
+	stmmac_xsk_umem_dma_unmap(priv, umem);
+
+	if (if_running)
+		stmmac_txrx_ring_enable(priv, qid);
+
+	return 0;
+}
+
+int stmmac_xsk_umem_setup(struct stmmac_priv *priv, struct xdp_umem *umem,
+			  u16 qid)
+{
+	return umem ? stmmac_xsk_umem_enable(priv, umem, qid) :
+		      stmmac_xsk_umem_disable(priv, qid);
+}
+
+int stmmac_run_xdp_zc(struct stmmac_priv *priv, struct stmmac_rx_queue *rx_q,
+		      struct xdp_buff *xdp)
+{
+	int err, result = STMMAC_XDP_PASS;
+	struct bpf_prog *xdp_prog;
+	struct xdp_frame *xdpf;
+	u32 act;
+
+	rcu_read_lock();
+	xdp_prog = READ_ONCE(rx_q->xdp_prog);
+	act = bpf_prog_run_xdp(xdp_prog, xdp);
+	xdp->handle += xdp->data - xdp->data_hard_start;
+	switch (act) {
+	case XDP_PASS:
+		break;
+	case XDP_REDIRECT:
+		err = xdp_do_redirect(priv->dev, xdp, xdp_prog);
+		result = (err >= 0) ? STMMAC_XDP_REDIRECT : STMMAC_XDP_DROP;
+		break;
+	default:
+		bpf_warn_invalid_xdp_action(act);
+		/* Fallthrough */
+	case XDP_ABORTED:
+		trace_xdp_exception(priv->dev, xdp_prog, act);
+		/* Fallthrough -- handle aborts by dropping packet */
+	case XDP_DROP:
+		result = STMMAC_XDP_DROP;
+		break;
+	}
+	rcu_read_unlock();
+	return result;
+}
+
+struct stmmac_rx_buffer *stmmac_get_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+						 unsigned int size)
+{
+	struct stmmac_rx_buffer *buf;
+
+	buf = &rx_q->buf_pool[rx_q->cur_rx];
+
+	/* We are reusing so sync this buffer for CPU use */
+	dma_sync_single_range_for_cpu(rx_q->priv_data->device,
+				      buf->dma_addr, 0,
+				      size,
+				      DMA_BIDIRECTIONAL);
+
+	return buf;
+}
+
+void stmmac_reuse_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+			       struct stmmac_rx_buffer *obuf)
+{
+	unsigned long mask = (unsigned long)rx_q->xsk_umem->chunk_mask;
+	u64 hr = rx_q->xsk_umem->headroom + XDP_PACKET_HEADROOM;
+	u16 nta = rx_q->next_to_alloc;
+	struct stmmac_rx_buffer *nbuf;
+
+	nbuf = &rx_q->buf_pool[nta];
+	/* Update, and store next to alloc */
+	nta++;
+	rx_q->next_to_alloc = (nta < rx_q->priv_data->dma_rx_size) ? nta : 0;
+
+	/* Transfer page from old buffer to new buffer */
+	nbuf->dma_addr = obuf->dma_addr & mask;
+	nbuf->dma_addr += hr;
+
+	nbuf->addr = (void *)((unsigned long)obuf->addr & mask);
+	nbuf->addr += hr;
+
+	nbuf->handle = obuf->handle & mask;
+	nbuf->handle += rx_q->xsk_umem->headroom;
+
+	obuf->addr = NULL;
+}
+
+void stmmac_zca_free(struct zero_copy_allocator *alloc, unsigned long handle)
+{
+	struct stmmac_rx_buffer *buf;
+	struct stmmac_rx_queue *rx_q;
+	struct stmmac_priv *priv;
+	unsigned int nta;
+	u64 hr, mask;
+
+	rx_q = container_of(alloc, struct stmmac_rx_queue, zca);
+	priv = rx_q->priv_data;
+
+	hr = rx_q->xsk_umem->headroom + XDP_PACKET_HEADROOM;
+	mask = rx_q->xsk_umem->chunk_mask;
+
+	nta = rx_q->next_to_alloc;
+	buf = &rx_q->buf_pool[nta];
+	nta++;
+	rx_q->next_to_alloc = (nta < priv->dma_rx_size) ? nta : 0;
+
+	handle &= mask;
+
+	buf->dma_addr = xdp_umem_get_dma(rx_q->xsk_umem, handle);
+	buf->dma_addr += hr;
+
+	buf->addr = xdp_umem_get_data(rx_q->xsk_umem, handle);
+	buf->addr += hr;
+
+	buf->handle = (u64)handle + rx_q->xsk_umem->headroom;
+}
+
+static bool stmmac_alloc_buffer_fast_zc(struct stmmac_rx_queue *rx_q,
+					struct stmmac_rx_buffer *buf)
+{
+	struct xdp_umem *umem = rx_q->xsk_umem;
+	void *addr = buf->addr;
+	u64 handle, hr;
+
+	if (addr)
+		return true;
+
+	if (!xsk_umem_peek_addr(umem, &handle))
+		return false;
+
+	/* TODO: Add XDP statistics for alloc failures */
+
+	hr = umem->headroom + XDP_PACKET_HEADROOM;
+
+	buf->dma_addr = xdp_umem_get_dma(umem, handle);
+	buf->dma_addr += hr;
+
+	buf->addr = xdp_umem_get_data(umem, handle);
+	buf->addr += hr;
+
+	buf->handle = handle + umem->headroom;
+
+	xsk_umem_discard_addr(umem);
+	return true;
+}
+
+static bool stmmac_alloc_buffer_slow_zc(struct stmmac_rx_queue *rx_q,
+					struct stmmac_rx_buffer *buf)
+{
+	struct xdp_umem *umem = rx_q->xsk_umem;
+	u64 handle, hr;
+
+	if (!xsk_umem_peek_addr_rq(umem, &handle))
+		return false;
+
+	/* TODO: Add XDP statistics for alloc failures */
+
+	handle &= rx_q->xsk_umem->chunk_mask;
+
+	hr = umem->headroom + XDP_PACKET_HEADROOM;
+
+	buf->dma_addr = xdp_umem_get_dma(umem, handle);
+	buf->dma_addr += hr;
+
+	buf->addr = xdp_umem_get_data(umem, handle);
+	buf->addr += hr;
+
+	buf->handle = handle + umem->headroom;
+
+	xsk_umem_discard_addr_rq(umem);
+	return true;
+}
+
+static __always_inline bool
+__stmmac_alloc_rx_buffers_zc(struct stmmac_rx_queue *rx_q, u16 cleaned_count,
+			     bool alloc(struct stmmac_rx_queue *rx_q,
+					struct stmmac_rx_buffer *buf))
+{
+	bool ok = true;
+	u16 i = rx_q->dirty_rx;
+	u16 entry = rx_q->dirty_rx;
+	struct stmmac_priv *priv = rx_q->priv_data;
+
+	/* Nothing to do */
+	if (!cleaned_count)
+		return true;
+
+	i -= rx_q->priv_data->dma_rx_size;
+
+	do {
+		struct dma_desc *p;
+		struct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];
+
+		if (!alloc(rx_q, buf)) {
+			ok = false;
+			break;
+		}
+
+		if (priv->extend_desc)
+			p = (struct dma_desc *)(rx_q->dma_erx + entry);
+		else
+			p = rx_q->dma_rx + entry;
+
+		/* Sync the buffer for use by the device */
+		dma_sync_single_range_for_device(priv->device,
+						 buf->dma_addr,
+						 0, //buf->page_offset,
+						 rx_q->xsk_buf_len,
+						 DMA_BIDIRECTIONAL);
+
+		/* Refresh the desc even if buffer_addrs didn't change
+		 * because each write-back erases this info.
+		 */
+		stmmac_set_desc_addr(priv, p, buf->dma_addr);
+		stmmac_refill_desc3(priv, rx_q, p);
+
+		dma_wmb();
+		stmmac_set_rx_owner(priv, p, priv->use_riwt);
+		dma_wmb();
+
+		i++;
+		entry = STMMAC_GET_ENTRY(entry,  priv->dma_rx_size);
+		if (unlikely(!i))
+			i -= rx_q->priv_data->dma_rx_size;
+
+		cleaned_count--;
+	} while (cleaned_count);
+
+	i += rx_q->priv_data->dma_rx_size;
+
+	if (rx_q->dirty_rx != i) {
+		/* Update next to alloc since we have filled the ring */
+		rx_q->next_to_alloc = i;
+		rx_q->dirty_rx = i;
+
+		/* Init Rx DMA - DMA_CHAN_RX_END_ADDR */
+		rx_q->rx_tail_addr = rx_q->dma_rx_phy +
+				     (priv->dma_rx_size *
+				      sizeof(struct dma_desc));
+		stmmac_set_rx_tail_ptr(priv, priv->ioaddr,
+				       rx_q->rx_tail_addr, rx_q->queue_index);
+	}
+
+	return ok;
+}
+
+void stmmac_alloc_rx_buffers_slow_zc(struct stmmac_rx_queue *rx_q,
+				     u16 cleaned_count)
+{
+	__stmmac_alloc_rx_buffers_zc(rx_q, cleaned_count,
+				     stmmac_alloc_buffer_slow_zc);
+}
+
+bool stmmac_alloc_rx_buffers_fast_zc(struct stmmac_rx_queue *rx_q,
+				     u16 cleaned_count)
+{
+	return __stmmac_alloc_rx_buffers_zc(rx_q, cleaned_count,
+					    stmmac_alloc_buffer_fast_zc);
+}
+
+static struct sk_buff *stmmac_construct_skb_zc(struct stmmac_rx_queue *rx_q,
+					       struct stmmac_rx_buffer *buf,
+					       struct xdp_buff *xdp)
+{
+	struct sk_buff *skb;
+	unsigned int metasize = xdp->data - xdp->data_meta;
+	unsigned int datasize = xdp->data_end - xdp->data;
+	struct stmmac_priv *priv = rx_q->priv_data;
+	u32 qid = rx_q->queue_index;
+	struct stmmac_channel *ch = &priv->channel[qid];
+
+	/* allocate a skb to store the frags */
+	skb = __napi_alloc_skb(&ch->rx_napi,
+			       xdp->data_end - xdp->data_hard_start,
+			       GFP_ATOMIC | __GFP_NOWARN);
+	if (unlikely(!skb))
+		return NULL;
+
+	skb_reserve(skb, xdp->data - xdp->data_hard_start);
+	memcpy(__skb_put(skb, datasize), xdp->data, datasize);
+	if (metasize)
+		skb_metadata_set(skb, metasize);
+
+	stmmac_reuse_rx_buffer_zc(rx_q, buf);
+	return skb;
+}
+
+/* RX Q next_to_clean increment, prefetch and rollover function */
+static struct dma_desc *stmmac_inc_ntc(struct stmmac_rx_queue *rx_q)
+{
+	struct dma_desc *np;
+	struct stmmac_priv *priv = rx_q->priv_data;
+	u32 ntc = rx_q->cur_rx + 1;
+
+	ntc = (ntc < priv->dma_rx_size) ? ntc : 0;
+	rx_q->cur_rx = ntc;
+
+	if (priv->extend_desc)
+		np = (struct dma_desc *)(rx_q->dma_erx + ntc);
+	else
+		np = rx_q->dma_rx + ntc;
+
+	prefetch(np);
+
+	return np;
+}
+
+int stmmac_rx_zc(struct stmmac_priv *priv, const int budget, u32 qid)
+{
+	unsigned int total_rx_bytes = 0, total_rx_packets = 0;
+	struct stmmac_rx_queue *rx_q = &priv->rx_queue[qid];
+	struct stmmac_channel *ch = &priv->channel[qid];
+	unsigned int xdp_res, xdp_xmit = 0;
+	int coe = priv->hw->rx_csum;
+	bool failure = false;
+	struct sk_buff *skb;
+	struct xdp_buff xdp;
+	unsigned int entry;
+	u16 cleaned_count;
+	int count = 0;
+
+	xdp.rxq = &rx_q->xdp_rxq;
+	cleaned_count = stmmac_rx_dirty(rx_q);
+	entry = rx_q->cur_rx - 1;	/* Offset the while loop's entry++ */
+
+	while (likely(total_rx_packets < budget) && count < priv->dma_rx_size) {
+		struct dma_desc *rx_desc, *rx_desc_n;
+		struct stmmac_rx_buffer *buf;
+		int size, status;
+
+		entry++;
+
+		if (cleaned_count >= STMMAC_RX_BUFFER_WRITE) {
+			failure = failure ||
+				  !stmmac_alloc_rx_buffers_fast_zc(rx_q,
+								 cleaned_count);
+			cleaned_count = 0;
+		}
+
+		if (priv->extend_desc)
+			rx_desc = (struct dma_desc *)(rx_q->dma_erx + entry);
+		else
+			rx_desc = rx_q->dma_rx + entry;
+
+		/* Read the status of the incoming frame */
+		status = stmmac_rx_status(priv, &priv->dev->stats,
+					  &priv->xstats, rx_desc);
+
+		/* check if managed by the DMA otherwise go ahead */
+		if (unlikely(status & dma_own))
+			break;
+
+		if (priv->extend_desc)
+			stmmac_rx_extended_status(priv, &priv->dev->stats,
+						  &priv->xstats,
+						  rx_q->dma_erx + entry);
+
+		count++;
+
+		size = stmmac_get_rx_frame_len(priv, rx_desc, coe);
+
+		/* This memory barrier is needed to keep us from reading
+		 * any other fields out of the rx_desc until we know the
+		 * descriptor has been written back.
+		 */
+		dma_rmb();
+
+		buf = stmmac_get_rx_buffer_zc(rx_q, size);
+
+		/* Check if valid dma addr */
+		if (!unlikely(buf->dma_addr)) {
+			stmmac_reuse_rx_buffer_zc(rx_q, buf);
+			cleaned_count++;
+			continue;
+		}
+
+		if (unlikely(size <= 0)) {
+			stmmac_reuse_rx_buffer_zc(rx_q, buf);
+			cleaned_count++;
+			stmmac_inc_ntc(rx_q);
+			continue;
+		}
+
+		xdp.data = buf->addr;
+		xdp.data_meta = xdp.data;
+		xdp.data_hard_start = xdp.data - XDP_PACKET_HEADROOM;
+		xdp.data_end = xdp.data + size;
+		xdp.handle = buf->handle;
+
+		xdp_res = stmmac_run_xdp_zc(priv, rx_q, &xdp);
+
+		if (xdp_res) {
+			if (xdp_res & (STMMAC_XDP_TX | STMMAC_XDP_REDIRECT)) {
+				xdp_xmit |= xdp_res;
+				buf->addr = NULL;
+			} else {
+				stmmac_reuse_rx_buffer_zc(rx_q, buf);
+			}
+			total_rx_packets++;
+			total_rx_bytes += size;
+
+			cleaned_count++;
+			stmmac_inc_ntc(rx_q);
+
+			continue; /* packets processed, go to the next one */
+		}
+
+		/* XDP_PASS path */
+		skb = stmmac_construct_skb_zc(rx_q, buf, &xdp);
+		if (!skb)
+			break;
+
+		/* TODO: Add XDP statistics */
+
+		cleaned_count++;
+		rx_desc_n = stmmac_inc_ntc(rx_q);
+
+		if (eth_skb_pad(skb))
+			continue;
+
+		total_rx_bytes += skb->len;
+		total_rx_packets++;
+
+		if (netif_msg_pktdata(priv)) {
+			netdev_dbg(priv->dev, "frame received (%dbytes)",
+				   size);
+			print_pkt(skb->data, size);
+		}
+
+		stmmac_get_rx_hwtstamp(priv, rx_desc, rx_desc_n, skb);
+
+		if (-EINVAL == stmmac_rx_hw_vlan(priv, priv->dev,
+						 priv->hw, rx_desc, skb))
+			stmmac_rx_vlan(priv->dev, skb);
+
+		skb->protocol = eth_type_trans(skb, priv->dev);
+
+		if (unlikely(!coe))
+			skb_checksum_none_assert(skb);
+		else
+			skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		napi_gro_receive(&ch->rx_napi, skb);
+	}
+
+	if (xdp_xmit & STMMAC_XDP_REDIRECT)
+		xdp_do_flush_map();
+
+	if (xdp_xmit & STMMAC_XDP_TX) {
+		struct stmmac_tx_queue *tx_q = &priv->tx_queue[qid];
+
+		stmmac_enable_dma_transmission(priv, priv->ioaddr);
+
+		/* Force memory writes to complete before letting h/w
+		 * know there are new descriptors to fetch.
+		 */
+		wmb();
+
+		if (priv->extend_desc)
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_extended_desc));
+		else if (priv->enhanced_tx_desc)
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_enhanced_tx_desc));
+		else
+			tx_q->tx_tail_addr = tx_q->dma_tx_phy + (tx_q->cur_tx *
+					sizeof(struct dma_desc));
+		stmmac_set_tx_tail_ptr(priv, priv->ioaddr, tx_q->tx_tail_addr,
+				       qid);
+	}
+
+	priv->xstats.rx_pkt_n += total_rx_packets;
+
+	switch (qid) {
+	case 0x0:
+		priv->xstats.q0_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x1:
+		priv->xstats.q1_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x2:
+		priv->xstats.q2_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x3:
+		priv->xstats.q3_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x4:
+		priv->xstats.q4_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x5:
+		priv->xstats.q5_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x6:
+		priv->xstats.q6_rx_pkt_n += total_rx_packets;
+		break;
+	case 0x7:
+		priv->xstats.q7_rx_pkt_n += total_rx_packets;
+		break;
+	default:
+		break;
+	}
+
+	return failure ? budget : (int)total_rx_packets;
+}
+
+void stmmac_xsk_free_rx_ring(struct stmmac_rx_queue *rx_q)
+{
+	struct stmmac_priv *priv = rx_q->priv_data;
+	unsigned int entry = rx_q->cur_rx;
+
+	struct stmmac_rx_buffer *buf = &rx_q->buf_pool[entry];
+
+	while (entry != rx_q->next_to_alloc) {
+		xsk_umem_fq_reuse(rx_q->xsk_umem, buf->handle);
+		entry++;
+		buf++;
+		if (entry == priv->dma_rx_size) {
+			entry = 0;
+			buf = rx_q->buf_pool;
+		}
+	}
+}
diff --git a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
index 776a7eee27c0..e6287b6e0109 100644
--- a/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
+++ b/drivers/net/ethernet/stmicro/stmmac/stmmac_xsk.h
@@ -20,4 +20,20 @@ static inline bool stmmac_enabled_xdp(struct stmmac_priv *priv)
 
 int stmmac_xdp_xmit_queue(struct stmmac_priv *priv, u32 queue,
 			  struct xdp_frame *xdpf);
+struct xdp_umem *stmmac_xsk_umem(struct stmmac_priv *priv, u16 qid);
+int stmmac_xsk_umem_setup(struct stmmac_priv *priv, struct xdp_umem *umem,
+			  u16 qid);
+void stmmac_zca_free(struct zero_copy_allocator *alloc, unsigned long handle);
+int stmmac_run_xdp_zc(struct stmmac_priv *priv, struct stmmac_rx_queue *rx_q,
+		      struct xdp_buff *xdp);
+void stmmac_alloc_rx_buffers_slow_zc(struct stmmac_rx_queue *rx_q,
+				     u16 cleaned_count);
+bool stmmac_alloc_rx_buffers_fast_zc(struct stmmac_rx_queue *rx_q,
+				     u16 cleaned_count);
+struct stmmac_rx_buffer *stmmac_get_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+						 unsigned int size);
+void stmmac_reuse_rx_buffer_zc(struct stmmac_rx_queue *rx_q,
+			       struct stmmac_rx_buffer *obi);
+int stmmac_rx_zc(struct stmmac_priv *priv, const int budget, u32 qid);
+void stmmac_xsk_free_rx_ring(struct stmmac_rx_queue *rx_q);
 #endif /* __STMMAC_XSK_H__ */
-- 
2.17.1

